{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# On-policy Entropy Regularized RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import mujoco_py\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from apmd_on.apmd import PMD\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EvaluateCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0, eval_env=None, model=None):\n",
    "        super(EvaluateCallback, self).__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.eval_env = eval_env\n",
    "        self.model = model\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        self.iter = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=5, deterministic=False)\n",
    "        print(f\"Iter {self.iter:d} mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        self.means.append(mean_reward)\n",
    "        self.stds.append(std_reward)\n",
    "        self.iter += 1\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Learning rate schedule \n",
    "from typing import Callable\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "def plot_costs(rewards, names, smoothing_window=10, n=3, fig_name=\"acrobot.png\", stds=None):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    colors = ['tomato', 'royalblue', 'mediumpurple']\n",
    "    for i in range(n):\n",
    "        extend = np.concatenate([np.ones(smoothing_window)*rewards[i][0], rewards[i]])\n",
    "        rewards_smoothed = pd.Series(extend).rolling(smoothing_window, min_periods=smoothing_window).mean().to_numpy()\n",
    "        rewards_smoothed = rewards_smoothed[smoothing_window-1:]\n",
    "        rewards_smoothed = rewards_smoothed[:5000]\n",
    "        x = np.linspace(1, 5000, num=5000)\n",
    "        if stds is None:\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3)\n",
    "        else:\n",
    "            lower = rewards_smoothed - stds[i][ :5000]\n",
    "            upper = rewards_smoothed + stds[i][ :5000]\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3, c=colors[i])\n",
    "            plt.fill_between(x, y1=lower, y2=upper, interpolate=True, c=colors[i], alpha=0.5)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episodic Costs\")\n",
    "    plt.title(fig_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "    # plt.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=1000, episode_reward=-1.66 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.66    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1.30 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-0.96 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.964   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.42    |\n",
      "|    critic_loss     | 3.54     |\n",
      "|    ent_coef        | 0.978    |\n",
      "|    ent_coef_loss   | -0.225   |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 10       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1.24 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.24    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.78 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.781   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.83    |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.93     |\n",
      "|    ent_coef_loss   | -0.73    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 20       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-1.03 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-2.71 +/- 0.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.71    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.48    |\n",
      "|    critic_loss     | 0.644    |\n",
      "|    ent_coef        | 0.885    |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 30       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-2.88 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.88    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -281     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-0.78 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.776   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.26    |\n",
      "|    critic_loss     | 0.446    |\n",
      "|    ent_coef        | 0.841    |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 40       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-0.66 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.658   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-8.23 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.23    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.35    |\n",
      "|    critic_loss     | 0.312    |\n",
      "|    ent_coef        | 0.8      |\n",
      "|    ent_coef_loss   | -2.25    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 50       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-7.15 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.15    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -299     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-6.16 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -6.16    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.88    |\n",
      "|    critic_loss     | 0.37     |\n",
      "|    ent_coef        | 0.761    |\n",
      "|    ent_coef_loss   | -2.72    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 60       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-6.92 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -6.92    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-7.68 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.68    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.75    |\n",
      "|    critic_loss     | 0.313    |\n",
      "|    ent_coef        | 0.724    |\n",
      "|    ent_coef_loss   | -3.23    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 70       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-8.05 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.05    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -301     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-10.80 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.63    |\n",
      "|    critic_loss     | 0.283    |\n",
      "|    ent_coef        | 0.689    |\n",
      "|    ent_coef_loss   | -3.73    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 80       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-10.65 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-9.89 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.89    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.39    |\n",
      "|    critic_loss     | 0.32     |\n",
      "|    ent_coef        | 0.656    |\n",
      "|    ent_coef_loss   | -4.21    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 90       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-9.63 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.63    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -291     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-12.02 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.3      |\n",
      "|    ent_coef        | 0.624    |\n",
      "|    ent_coef_loss   | -4.72    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 100      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-11.56 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-11.35 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.594    |\n",
      "|    ent_coef_loss   | -5.22    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 110      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-11.19 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -283     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 700      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-16.44 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.02    |\n",
      "|    critic_loss     | 0.247    |\n",
      "|    ent_coef        | 0.565    |\n",
      "|    ent_coef_loss   | -5.71    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 120      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-16.70 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-21.73 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.85    |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.538    |\n",
      "|    ent_coef_loss   | -6.19    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 130      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-21.55 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -276     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 702      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-23.66 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.81    |\n",
      "|    critic_loss     | 0.257    |\n",
      "|    ent_coef        | 0.512    |\n",
      "|    ent_coef_loss   | -6.66    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 140      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-22.70 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-24.41 +/- 0.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 0.194    |\n",
      "|    ent_coef        | 0.487    |\n",
      "|    ent_coef_loss   | -7.16    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 150      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-23.64 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -262     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-22.18 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.43    |\n",
      "|    critic_loss     | 0.173    |\n",
      "|    ent_coef        | 0.463    |\n",
      "|    ent_coef_loss   | -7.64    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 160      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-21.94 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-30.10 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -30.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.267    |\n",
      "|    ent_coef        | 0.441    |\n",
      "|    ent_coef_loss   | -8.13    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 170      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-29.93 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -29.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -262     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-26.01 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2     |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.42     |\n",
      "|    ent_coef_loss   | -8.6     |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 180      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-26.22 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-40.25 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.928   |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.4      |\n",
      "|    ent_coef_loss   | -9.03    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 190      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-39.56 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -39.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -258     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-32.52 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.959   |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.381    |\n",
      "|    ent_coef_loss   | -9.49    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 200      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-32.90 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-34.86 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -34.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-22.19 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.855   |\n",
      "|    critic_loss     | 0.215    |\n",
      "|    ent_coef        | 0.362    |\n",
      "|    ent_coef_loss   | -9.89    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 210      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -257     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-22.18 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-49.62 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.67    |\n",
      "|    critic_loss     | 0.209    |\n",
      "|    ent_coef        | 0.345    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 220      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-50.24 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-56.22 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.692   |\n",
      "|    critic_loss     | 0.235    |\n",
      "|    ent_coef        | 0.329    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 230      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -249     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-56.68 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-51.47 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -51.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.311   |\n",
      "|    critic_loss     | 0.162    |\n",
      "|    ent_coef        | 0.313    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 240      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-52.19 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-55.87 +/- 31.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -55.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.179   |\n",
      "|    critic_loss     | 0.249    |\n",
      "|    ent_coef        | 0.298    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 250      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -250     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 703      |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-56.60 +/- 30.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-51.14 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -51.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.457   |\n",
      "|    critic_loss     | 0.212    |\n",
      "|    ent_coef        | 0.284    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 260      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-52.54 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-53.65 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.19    |\n",
      "|    critic_loss     | 0.229    |\n",
      "|    ent_coef        | 0.271    |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 270      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -245     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-53.64 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-80.59 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.144   |\n",
      "|    critic_loss     | 0.189    |\n",
      "|    ent_coef        | 0.258    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 280      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-81.61 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-80.81 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0511   |\n",
      "|    critic_loss     | 0.165    |\n",
      "|    ent_coef        | 0.246    |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 290      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-82.09 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -82.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-58.82 +/- 36.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -58.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0355   |\n",
      "|    critic_loss     | 0.154    |\n",
      "|    ent_coef        | 0.235    |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-41.62 +/- 44.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-61.77 +/- 16.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -61.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.00949  |\n",
      "|    critic_loss     | 0.219    |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 310      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -235     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 703      |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-70.89 +/- 1.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-52.36 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.282    |\n",
      "|    critic_loss     | 0.179    |\n",
      "|    ent_coef        | 0.213    |\n",
      "|    ent_coef_loss   | -14.5    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 320      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-52.56 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-69.10 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.314    |\n",
      "|    critic_loss     | 0.146    |\n",
      "|    ent_coef        | 0.203    |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 330      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 700      |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-69.53 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-24.43 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.441    |\n",
      "|    critic_loss     | 0.177    |\n",
      "|    ent_coef        | 0.194    |\n",
      "|    ent_coef_loss   | -15.4    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 340      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-24.55 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-36.74 +/- 33.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.419    |\n",
      "|    critic_loss     | 0.218    |\n",
      "|    ent_coef        | 0.185    |\n",
      "|    ent_coef_loss   | -15.6    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 350      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 699      |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-36.74 +/- 32.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-47.01 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -47      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.571    |\n",
      "|    critic_loss     | 0.166    |\n",
      "|    ent_coef        | 0.176    |\n",
      "|    ent_coef_loss   | -16      |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 360      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-48.25 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-56.18 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.59     |\n",
      "|    critic_loss     | 0.167    |\n",
      "|    ent_coef        | 0.168    |\n",
      "|    ent_coef_loss   | -16.7    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 370      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 76000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-56.97 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -57      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-26.17 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.593    |\n",
      "|    critic_loss     | 0.24     |\n",
      "|    ent_coef        | 0.161    |\n",
      "|    ent_coef_loss   | -16.8    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 380      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-27.45 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -27.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-90.15 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.512    |\n",
      "|    critic_loss     | 0.259    |\n",
      "|    ent_coef        | 0.153    |\n",
      "|    ent_coef_loss   | -17.5    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 390      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -218     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 699      |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-90.84 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-63.03 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.758    |\n",
      "|    critic_loss     | 0.257    |\n",
      "|    ent_coef        | 0.146    |\n",
      "|    ent_coef_loss   | -17.4    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 400      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-62.85 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -62.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-72.40 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -72.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.881    |\n",
      "|    critic_loss     | 0.22     |\n",
      "|    ent_coef        | 0.14     |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 410      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 700      |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-72.54 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -72.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-72.79 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -72.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-22.11 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.857    |\n",
      "|    critic_loss     | 0.3      |\n",
      "|    ent_coef        | 0.133    |\n",
      "|    ent_coef_loss   | -18.1    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 420      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-24.34 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -213     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 702      |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-69.62 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.731    |\n",
      "|    critic_loss     | 0.223    |\n",
      "|    ent_coef        | 0.127    |\n",
      "|    ent_coef_loss   | -18.1    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 430      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-68.50 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -68.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-78.30 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -78.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.825    |\n",
      "|    critic_loss     | 0.254    |\n",
      "|    ent_coef        | 0.122    |\n",
      "|    ent_coef_loss   | -20      |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 440      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-79.30 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -79.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 703      |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-49.60 +/- 39.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1        |\n",
      "|    critic_loss     | 0.203    |\n",
      "|    ent_coef        | 0.116    |\n",
      "|    ent_coef_loss   | -19.6    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 450      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-81.43 +/- 32.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-45.82 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.12     |\n",
      "|    critic_loss     | 0.146    |\n",
      "|    ent_coef        | 0.111    |\n",
      "|    ent_coef_loss   | -19.6    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 460      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-46.87 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -46.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 703      |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-41.53 +/- 7.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.04     |\n",
      "|    critic_loss     | 0.198    |\n",
      "|    ent_coef        | 0.106    |\n",
      "|    ent_coef_loss   | -21      |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 470      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-45.69 +/- 8.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-57.89 +/- 19.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -57.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.13     |\n",
      "|    critic_loss     | 0.144    |\n",
      "|    ent_coef        | 0.101    |\n",
      "|    ent_coef_loss   | -19.5    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 480      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-50.21 +/- 19.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -205     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 141      |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-86.44 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.16     |\n",
      "|    critic_loss     | 0.116    |\n",
      "|    ent_coef        | 0.0966   |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 490      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-76.05 +/- 25.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -76.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-77.33 +/- 6.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.16     |\n",
      "|    critic_loss     | 0.176    |\n",
      "|    ent_coef        | 0.0922   |\n",
      "|    ent_coef_loss   | -21.1    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 500      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-77.87 +/- 6.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -202     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-54.21 +/- 2.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -54.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.28     |\n",
      "|    critic_loss     | 0.165    |\n",
      "|    ent_coef        | 0.0881   |\n",
      "|    ent_coef_loss   | -21.9    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 510      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-54.90 +/- 2.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -54.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-203.79 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -204     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.04     |\n",
      "|    critic_loss     | 0.252    |\n",
      "|    ent_coef        | 0.0841   |\n",
      "|    ent_coef_loss   | -22.1    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 520      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-205.71 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -195     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 153      |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-100.07 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.44     |\n",
      "|    critic_loss     | 0.192    |\n",
      "|    ent_coef        | 0.0804   |\n",
      "|    ent_coef_loss   | -21.2    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 530      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-99.72 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-104.07 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 111000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.59     |\n",
      "|    critic_loss     | 0.157    |\n",
      "|    ent_coef        | 0.0769   |\n",
      "|    ent_coef_loss   | -21.6    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 540      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-105.49 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-103.33 +/- 1.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.33     |\n",
      "|    critic_loss     | 0.193    |\n",
      "|    ent_coef        | 0.0736   |\n",
      "|    ent_coef_loss   | -23.5    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 550      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-104.15 +/- 2.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-101.37 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.09     |\n",
      "|    critic_loss     | 0.163    |\n",
      "|    ent_coef        | 0.0703   |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 560      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-99.87 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -178     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 706      |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-115.06 +/- 24.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.12     |\n",
      "|    critic_loss     | 0.296    |\n",
      "|    ent_coef        | 0.0672   |\n",
      "|    ent_coef_loss   | -24.6    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 570      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-118.18 +/- 23.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-84.52 +/- 36.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 119000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.63     |\n",
      "|    critic_loss     | 0.143    |\n",
      "|    ent_coef        | 0.0641   |\n",
      "|    ent_coef_loss   | -22.7    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 580      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-71.22 +/- 9.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -71.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -173     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 707      |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-70.51 +/- 2.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.32     |\n",
      "|    critic_loss     | 0.152    |\n",
      "|    ent_coef        | 0.0613   |\n",
      "|    ent_coef_loss   | -24.6    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 590      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-92.57 +/- 47.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=367.10 +/- 96.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 367      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 123000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.12     |\n",
      "|    critic_loss     | 0.184    |\n",
      "|    ent_coef        | 0.0586   |\n",
      "|    ent_coef_loss   | -24.8    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 600      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=124000, episode_reward=419.37 +/- 17.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 419      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -164     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 701      |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-265.65 +/- 11.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -266     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 125000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.603    |\n",
      "|    critic_loss     | 0.273    |\n",
      "|    ent_coef        | 0.0561   |\n",
      "|    ent_coef_loss   | -22      |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 610      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-267.75 +/- 11.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -268     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-52.40 +/- 2.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 127000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.43     |\n",
      "|    critic_loss     | 0.207    |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 620      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-50.48 +/- 4.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -158     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-49.23 +/- 3.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-98.45 +/- 2.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.17     |\n",
      "|    critic_loss     | 0.307    |\n",
      "|    ent_coef        | 0.0515   |\n",
      "|    ent_coef_loss   | -26.9    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 630      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-99.25 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-128.63 +/- 4.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.1      |\n",
      "|    critic_loss     | 0.312    |\n",
      "|    ent_coef        | 0.0493   |\n",
      "|    ent_coef_loss   | -23.2    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 640      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-131.83 +/- 4.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=49.48 +/- 8.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 49.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.728    |\n",
      "|    critic_loss     | 0.32     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | -21.1    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 650      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=47.70 +/- 10.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 47.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-3.88 +/- 207.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.88    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.734    |\n",
      "|    critic_loss     | 0.445    |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | -22.4    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 660      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 136000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-80.14 +/- 182.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=11.46 +/- 190.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.891    |\n",
      "|    critic_loss     | 0.481    |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -20.6    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 670      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-33.19 +/- 144.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -33.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-95.68 +/- 15.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.987    |\n",
      "|    critic_loss     | 0.262    |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -22.8    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 680      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-98.12 +/- 13.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=60.84 +/- 152.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 60.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.03     |\n",
      "|    critic_loss     | 0.364    |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -21.1    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 690      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-49.47 +/- 18.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-224.35 +/- 36.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.22     |\n",
      "|    critic_loss     | 0.999    |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | -21.6    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 700      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -135     |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-216.65 +/- 30.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -217     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-69.62 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.27     |\n",
      "|    critic_loss     | 0.459    |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | -19.2    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 710      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-69.06 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-63.21 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 148000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.55     |\n",
      "|    critic_loss     | 0.27     |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | -24.2    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 720      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -135     |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 148000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-63.92 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-132.86 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.52     |\n",
      "|    critic_loss     | 0.367    |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -26      |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 730      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-133.00 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-357.71 +/- 5.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -358     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 152000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.78     |\n",
      "|    critic_loss     | 0.392    |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -26.1    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 740      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -133     |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 699      |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 152000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-352.54 +/- 5.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-40.45 +/- 2.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 154000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.429    |\n",
      "|    critic_loss     | 0.943    |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 750      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-40.59 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-190.32 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 156000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.82     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 760      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -138     |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 156000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-189.50 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -189     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-145.93 +/- 24.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 158000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.31     |\n",
      "|    critic_loss     | 0.558    |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | -25      |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 770      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=-147.26 +/- 26.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-33.52 +/- 4.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -33.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.12     |\n",
      "|    critic_loss     | 0.645    |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -27.3    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 780      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -140     |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 229      |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=-31.97 +/- 3.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-308.14 +/- 3.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 162000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.79     |\n",
      "|    critic_loss     | 0.376    |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -20.2    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 790      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-304.29 +/- 3.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -304     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-169.02 +/- 4.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -169     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 164000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 0.296    |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | -19.7    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 800      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -142     |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 164000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-204.75 +/- 41.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -205     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-762.57 +/- 40.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -763     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 166000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.98     |\n",
      "|    critic_loss     | 0.78     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 810      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-780.82 +/- 41.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -781     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=370.00 +/- 3.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 370      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 168000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.97     |\n",
      "|    critic_loss     | 0.665    |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | -18.6    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 820      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -142     |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 241      |\n",
      "|    total_timesteps | 168000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=361.29 +/- 9.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 361      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-147.15 +/- 158.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.65     |\n",
      "|    critic_loss     | 0.537    |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -21.9    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 830      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-258.39 +/- 116.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-135.25 +/- 167.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -145     |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=-297.93 +/- 92.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -298     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 0.434    |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -29.5    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 840      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-373.06 +/- 81.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-48.07 +/- 2.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.738    |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -7.07    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 850      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-45.03 +/- 2.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 253      |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-143.31 +/- 2.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.85     |\n",
      "|    critic_loss     | 0.406    |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 860      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-142.46 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-52.86 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 179000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 0.257    |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -29      |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 870      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-53.35 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -146     |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-560.24 +/- 2.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9      |\n",
      "|    critic_loss     | 0.298    |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -18.9    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 880      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-559.85 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-106.65 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.3      |\n",
      "|    critic_loss     | 0.358    |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | -5.31    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 890      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-107.60 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -155     |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-96.19 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -96.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 185000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.82     |\n",
      "|    critic_loss     | 0.389    |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 900      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-98.03 +/- 2.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-190.71 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.58     |\n",
      "|    critic_loss     | 0.33     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 910      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-189.83 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -160     |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-743.00 +/- 19.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -743     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 189000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.04     |\n",
      "|    critic_loss     | 0.515    |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | -21      |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 920      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-727.04 +/- 13.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -727     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-116.25 +/- 16.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.84     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -17.8    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 930      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-104.59 +/- 17.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -169     |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=-150.66 +/- 49.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 193000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.67     |\n",
      "|    critic_loss     | 0.511    |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | -27.6    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 940      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-183.43 +/- 18.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-139.04 +/- 20.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0726   |\n",
      "|    critic_loss     | 0.904    |\n",
      "|    ent_coef        | 0.0183   |\n",
      "|    ent_coef_loss   | -23.2    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 950      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-157.26 +/- 33.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 283      |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-283.06 +/- 51.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -283     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 197000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.47     |\n",
      "|    critic_loss     | 0.3      |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | -8.83    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 960      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-273.07 +/- 35.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -273     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-84.24 +/- 325.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 199000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.09     |\n",
      "|    critic_loss     | 0.219    |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | -14.9    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 970      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=223.15 +/- 358.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 223      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -183     |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 289      |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=-246.86 +/- 16.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -247     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 201000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.15     |\n",
      "|    critic_loss     | 0.392    |\n",
      "|    ent_coef        | 0.0169   |\n",
      "|    ent_coef_loss   | -18.2    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 980      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=-241.16 +/- 11.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=-253.70 +/- 16.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -254     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 203000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.25     |\n",
      "|    critic_loss     | 0.188    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 990      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-252.79 +/- 9.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -253     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -189     |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 295      |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-379.86 +/- 16.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -380     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 205000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.25     |\n",
      "|    critic_loss     | 0.107    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 1000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=-335.36 +/- 96.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=596.63 +/- 115.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 597      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 207000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.71     |\n",
      "|    critic_loss     | 0.0742   |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1010     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=208000, episode_reward=160.71 +/- 339.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 161      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=387.56 +/- 51.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 388      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 209000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.59     |\n",
      "|    critic_loss     | 0.336    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=403.98 +/- 48.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 404      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=-718.34 +/- 5.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -718     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 211000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.74     |\n",
      "|    critic_loss     | 0.482    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-719.37 +/- 9.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -719     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -196     |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 307      |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=-277.91 +/- 14.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -278     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 213000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.81     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 43.4     |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=-272.37 +/- 15.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -272     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-270.11 +/- 9.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -270     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 215000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-381.00 +/- 11.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.34     |\n",
      "|    critic_loss     | 0.449    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -20      |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 313      |\n",
      "|    total_timesteps | 216000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=-382.90 +/- 10.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 217000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=-250.60 +/- 7.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -251     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.05     |\n",
      "|    critic_loss     | 0.247    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -19.6    |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=-240.57 +/- 3.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 219000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-248.43 +/- 4.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.42     |\n",
      "|    critic_loss     | 0.171    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -24.8    |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -219     |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 318      |\n",
      "|    total_timesteps | 220000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=-248.45 +/- 10.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 221000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-289.11 +/- 6.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -289     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 222000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.49     |\n",
      "|    critic_loss     | 0.172    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -25      |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=-275.91 +/- 17.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -276     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 223000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-278.85 +/- 28.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -279     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.3      |\n",
      "|    critic_loss     | 0.597    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -19      |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1090     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -232     |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 324      |\n",
      "|    total_timesteps | 224000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-159.73 +/- 203.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 225000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=-144.36 +/- 106.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 226000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.05     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -16.9    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=6.27 +/- 150.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 6.27     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 227000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-42.79 +/- 2.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 228000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.35     |\n",
      "|    critic_loss     | 0.361    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | -26.2    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1110     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 329      |\n",
      "|    total_timesteps | 228000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=-40.71 +/- 3.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 229000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-576.66 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -577     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.44     |\n",
      "|    critic_loss     | 0.586    |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=-576.27 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -576     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 231000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-234.28 +/- 2.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 232000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.51     |\n",
      "|    critic_loss     | 0.0944   |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -9.15    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1130     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -253     |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 335      |\n",
      "|    total_timesteps | 232000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=-233.16 +/- 2.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 233000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-405.67 +/- 4.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -406     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 234000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.08     |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-409.80 +/- 11.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -410     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-288.89 +/- 3.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -289     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 236000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.09     |\n",
      "|    critic_loss     | 0.0867   |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | -20.3    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1150     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -268     |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 340      |\n",
      "|    total_timesteps | 236000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=-286.98 +/- 2.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -287     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=-224.77 +/- 33.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 238000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.91     |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | -26.6    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=-250.76 +/- 2.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -251     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-211.43 +/- 22.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.76     |\n",
      "|    critic_loss     | 0.297    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -27.8    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1170     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -276     |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 346      |\n",
      "|    total_timesteps | 240000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=-228.67 +/- 4.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=-241.45 +/- 24.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 242000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.58     |\n",
      "|    critic_loss     | 0.154    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -28.7    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=-251.10 +/- 26.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -251     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-259.95 +/- 48.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -260     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 244000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.04     |\n",
      "|    critic_loss     | 0.377    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -24.4    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1190     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -280     |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 244000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-279.71 +/- 8.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -280     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-341.56 +/- 2.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -342     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 246000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.09     |\n",
      "|    critic_loss     | 0.291    |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=-350.09 +/- 8.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-377.46 +/- 76.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 248000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.24     |\n",
      "|    critic_loss     | 0.181    |\n",
      "|    ent_coef        | 0.00982  |\n",
      "|    ent_coef_loss   | -8.02    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1210     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -292     |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 357      |\n",
      "|    total_timesteps | 248000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=-366.05 +/- 75.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -366     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-525.47 +/- 3.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -525     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.41     |\n",
      "|    critic_loss     | 0.246    |\n",
      "|    ent_coef        | 0.00963  |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=-520.57 +/- 5.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -521     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=6.95 +/- 5.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 6.95     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 252000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.54     |\n",
      "|    critic_loss     | 0.124    |\n",
      "|    ent_coef        | 0.00946  |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1230     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -302     |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 364      |\n",
      "|    total_timesteps | 252000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=7.58 +/- 8.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 7.58     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=-314.27 +/- 9.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -314     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 254000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.56     |\n",
      "|    critic_loss     | 0.187    |\n",
      "|    ent_coef        | 0.00923  |\n",
      "|    ent_coef_loss   | -27.9    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-318.52 +/- 13.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -319     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-306.96 +/- 2.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -307     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -301     |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 370      |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=-243.51 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 257000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.12     |\n",
      "|    critic_loss     | 0.0556   |\n",
      "|    ent_coef        | 0.00896  |\n",
      "|    ent_coef_loss   | -7.08    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-242.44 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 258000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=-201.70 +/- 2.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -202     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 259000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.79     |\n",
      "|    critic_loss     | 0.071    |\n",
      "|    ent_coef        | 0.00878  |\n",
      "|    ent_coef_loss   | -24      |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-200.10 +/- 3.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -305     |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 377      |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=-460.34 +/- 4.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -460     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.09     |\n",
      "|    critic_loss     | 0.0571   |\n",
      "|    ent_coef        | 0.00854  |\n",
      "|    ent_coef_loss   | -23.8    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=-461.26 +/- 8.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -461     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 262000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=-178.67 +/- 3.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -179     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 263000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.3      |\n",
      "|    critic_loss     | 0.0557   |\n",
      "|    ent_coef        | 0.0083   |\n",
      "|    ent_coef_loss   | -16.1    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-176.37 +/- 2.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -176     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -314     |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 383      |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-238.06 +/- 10.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 265000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.95     |\n",
      "|    critic_loss     | 0.0501   |\n",
      "|    ent_coef        | 0.00807  |\n",
      "|    ent_coef_loss   | -22.9    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=-229.27 +/- 7.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 266000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=-191.10 +/- 11.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 267000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.14     |\n",
      "|    critic_loss     | 0.0441   |\n",
      "|    ent_coef        | 0.00784  |\n",
      "|    ent_coef_loss   | -23.4    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-183.38 +/- 2.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -320     |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 388      |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=-239.52 +/- 9.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -240     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 269000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.86     |\n",
      "|    critic_loss     | 0.0431   |\n",
      "|    ent_coef        | 0.0076   |\n",
      "|    ent_coef_loss   | -23.2    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-249.17 +/- 17.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -249     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=-84.70 +/- 106.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 271000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.89     |\n",
      "|    critic_loss     | 0.0363   |\n",
      "|    ent_coef        | 0.00736  |\n",
      "|    ent_coef_loss   | -26.5    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-62.13 +/- 91.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -62.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -320     |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 394      |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=39.38 +/- 78.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.48     |\n",
      "|    critic_loss     | 0.0665   |\n",
      "|    ent_coef        | 0.00712  |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=65.58 +/- 38.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 65.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 274000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-120.20 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 275000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.88     |\n",
      "|    critic_loss     | 0.135    |\n",
      "|    ent_coef        | 0.00691  |\n",
      "|    ent_coef_loss   | -7.5     |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-34.86 +/- 114.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -34.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -314     |\n",
      "| time/              |          |\n",
      "|    episodes        | 276      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 400      |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=52.75 +/- 102.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 52.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 277000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.26     |\n",
      "|    critic_loss     | 0.167    |\n",
      "|    ent_coef        | 0.00677  |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=55.08 +/- 103.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 55.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=32.53 +/- 5.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 279000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 0.135    |\n",
      "|    ent_coef        | 0.00664  |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=34.73 +/- 6.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 34.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -305     |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 406      |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=47.84 +/- 7.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 47.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 281000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.4      |\n",
      "|    critic_loss     | 0.0679   |\n",
      "|    ent_coef        | 0.00651  |\n",
      "|    ent_coef_loss   | -14      |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=62.22 +/- 12.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 62.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=22.27 +/- 4.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 22.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 283000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.37     |\n",
      "|    critic_loss     | 0.0832   |\n",
      "|    ent_coef        | 0.00639  |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=35.19 +/- 11.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 35.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -290     |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-53.08 +/- 4.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 285000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.61     |\n",
      "|    critic_loss     | 0.0741   |\n",
      "|    ent_coef        | 0.00624  |\n",
      "|    ent_coef_loss   | -20.9    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=-48.16 +/- 7.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=-17.22 +/- 118.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -17.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 287000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.56     |\n",
      "|    critic_loss     | 0.0682   |\n",
      "|    ent_coef        | 0.00609  |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=73.53 +/- 33.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 73.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -280     |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 417      |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=-414.87 +/- 2.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -415     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 289000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.34     |\n",
      "|    critic_loss     | 0.0964   |\n",
      "|    ent_coef        | 0.00597  |\n",
      "|    ent_coef_loss   | -6.67    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-179.55 +/- 292.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -180     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=-346.86 +/- 13.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 291000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.73     |\n",
      "|    critic_loss     | 0.0901   |\n",
      "|    ent_coef        | 0.0059   |\n",
      "|    ent_coef_loss   | -6.79    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-352.75 +/- 3.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -270     |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 423      |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=-334.68 +/- 3.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 293000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.28     |\n",
      "|    critic_loss     | 0.0368   |\n",
      "|    ent_coef        | 0.00583  |\n",
      "|    ent_coef_loss   | -6.23    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=-329.75 +/- 3.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=171.48 +/- 23.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 171      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 295000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.08     |\n",
      "|    critic_loss     | 0.0377   |\n",
      "|    ent_coef        | 0.00576  |\n",
      "|    ent_coef_loss   | -22.1    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=175.09 +/- 19.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 175      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -264     |\n",
      "| time/              |          |\n",
      "|    episodes        | 296      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 428      |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=393.00 +/- 6.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 393      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 297000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.07     |\n",
      "|    critic_loss     | 0.117    |\n",
      "|    ent_coef        | 0.00564  |\n",
      "|    ent_coef_loss   | -3.66    |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=374.19 +/- 20.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=392.55 +/- 3.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 393      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 299000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=327.96 +/- 19.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 328      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.48     |\n",
      "|    critic_loss     | 0.126    |\n",
      "|    ent_coef        | 0.00558  |\n",
      "|    ent_coef_loss   | 0.44     |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1460     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 434      |\n",
      "|    total_timesteps | 300000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=301000, episode_reward=340.27 +/- 10.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 340      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 301000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=241.71 +/- 11.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 242      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 302000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.7      |\n",
      "|    critic_loss     | 0.104    |\n",
      "|    ent_coef        | 0.00555  |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=245.97 +/- 18.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 246      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 303000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=66.86 +/- 7.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 66.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.89     |\n",
      "|    critic_loss     | 0.0887   |\n",
      "|    ent_coef        | 0.0055   |\n",
      "|    ent_coef_loss   | -2.23    |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    episodes        | 304      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 440      |\n",
      "|    total_timesteps | 304000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=39.99 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 305000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=168.03 +/- 8.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 168      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.17     |\n",
      "|    critic_loss     | 0.0789   |\n",
      "|    ent_coef        | 0.00546  |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=307000, episode_reward=185.90 +/- 16.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 186      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 307000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=334.30 +/- 29.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 334      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 308000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.06     |\n",
      "|    critic_loss     | 0.116    |\n",
      "|    ent_coef        | 0.00539  |\n",
      "|    ent_coef_loss   | -4.21    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -204     |\n",
      "| time/              |          |\n",
      "|    episodes        | 308      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 445      |\n",
      "|    total_timesteps | 308000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=321.41 +/- 39.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 309000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=511.64 +/- 28.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 512      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.61     |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.00535  |\n",
      "|    ent_coef_loss   | 2.39     |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=311000, episode_reward=491.91 +/- 21.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 492      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 311000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=320.76 +/- 32.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 312000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.64     |\n",
      "|    critic_loss     | 0.204    |\n",
      "|    ent_coef        | 0.00532  |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1520     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -191     |\n",
      "| time/              |          |\n",
      "|    episodes        | 312      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 451      |\n",
      "|    total_timesteps | 312000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=313000, episode_reward=328.50 +/- 50.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 329      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 313000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=136.73 +/- 28.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 137      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 314000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.14     |\n",
      "|    critic_loss     | 0.187    |\n",
      "|    ent_coef        | 0.00523  |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=45.95 +/- 43.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 45.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 315000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=110.31 +/- 14.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 316000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.11     |\n",
      "|    critic_loss     | 0.237    |\n",
      "|    ent_coef        | 0.00514  |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1540     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -171     |\n",
      "| time/              |          |\n",
      "|    episodes        | 316      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 456      |\n",
      "|    total_timesteps | 316000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=317000, episode_reward=119.80 +/- 18.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 120      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 317000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=196.61 +/- 11.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 197      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 318000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.5      |\n",
      "|    critic_loss     | 0.162    |\n",
      "|    ent_coef        | 0.00505  |\n",
      "|    ent_coef_loss   | -17.3    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=319000, episode_reward=212.27 +/- 9.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 212      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 319000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=124.30 +/- 25.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 124      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.01     |\n",
      "|    critic_loss     | 0.168    |\n",
      "|    ent_coef        | 0.00494  |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1560     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 462      |\n",
      "|    total_timesteps | 320000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=321000, episode_reward=96.71 +/- 33.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=329.32 +/- 34.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 329      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 322000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.91     |\n",
      "|    critic_loss     | 0.274    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -6.19    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323000, episode_reward=331.80 +/- 23.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 332      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-61.13 +/- 47.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -61.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 324000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.55     |\n",
      "|    critic_loss     | 0.239    |\n",
      "|    ent_coef        | 0.00479  |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1580     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    episodes        | 324      |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 467      |\n",
      "|    total_timesteps | 324000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-9.05 +/- 29.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.05    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=124.75 +/- 53.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 125      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 326000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.46     |\n",
      "|    critic_loss     | 0.151    |\n",
      "|    ent_coef        | 0.00476  |\n",
      "|    ent_coef_loss   | -19.3    |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=68.43 +/- 44.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 68.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=272.99 +/- 16.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 273      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 328000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.22     |\n",
      "|    critic_loss     | 0.113    |\n",
      "|    ent_coef        | 0.00468  |\n",
      "|    ent_coef_loss   | -4.31    |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1600     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -119     |\n",
      "| time/              |          |\n",
      "|    episodes        | 328      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 473      |\n",
      "|    total_timesteps | 328000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=329000, episode_reward=286.81 +/- 28.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 287      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=250.63 +/- 21.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 251      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.54     |\n",
      "|    critic_loss     | 0.14     |\n",
      "|    ent_coef        | 0.00464  |\n",
      "|    ent_coef_loss   | 4.96     |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331000, episode_reward=236.36 +/- 26.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=308.98 +/- 28.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 332000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.87     |\n",
      "|    critic_loss     | 0.127    |\n",
      "|    ent_coef        | 0.00464  |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1620     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -94.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 332      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 480      |\n",
      "|    total_timesteps | 332000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=298.92 +/- 16.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 299      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=288.77 +/- 14.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 289      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 334000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.6      |\n",
      "|    critic_loss     | 0.145    |\n",
      "|    ent_coef        | 0.00466  |\n",
      "|    ent_coef_loss   | 10.8     |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=308.85 +/- 21.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=366.85 +/- 33.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 367      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 336000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.89     |\n",
      "|    critic_loss     | 0.121    |\n",
      "|    ent_coef        | 0.00469  |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1640     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -68.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 336      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 486      |\n",
      "|    total_timesteps | 336000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=337000, episode_reward=332.59 +/- 20.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 337000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=368.29 +/- 22.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 368      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 338000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.8      |\n",
      "|    critic_loss     | 0.124    |\n",
      "|    ent_coef        | 0.00467  |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339000, episode_reward=386.79 +/- 19.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 387      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=470.66 +/- 36.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 471      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.42     |\n",
      "|    critic_loss     | 0.183    |\n",
      "|    ent_coef        | 0.00467  |\n",
      "|    ent_coef_loss   | 4.73     |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1660     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -46.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 492      |\n",
      "|    total_timesteps | 340000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=341000, episode_reward=469.55 +/- 46.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=490.43 +/- 32.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 490      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=343000, episode_reward=370.61 +/- 31.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 371      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 343000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.17     |\n",
      "|    critic_loss     | 0.239    |\n",
      "|    ent_coef        | 0.00469  |\n",
      "|    ent_coef_loss   | 2.25     |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=389.07 +/- 20.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 389      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -22.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 344      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 497      |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=279.61 +/- 23.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 345000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.68     |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.00469  |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=308.83 +/- 7.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 346000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=347000, episode_reward=148.19 +/- 27.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 148      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.59     |\n",
      "|    critic_loss     | 0.184    |\n",
      "|    ent_coef        | 0.00465  |\n",
      "|    ent_coef_loss   | -0.0869  |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=155.16 +/- 58.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 155      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77     |\n",
      "| time/              |          |\n",
      "|    episodes        | 348      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 503      |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=349000, episode_reward=191.73 +/- 25.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 192      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 349000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.76     |\n",
      "|    critic_loss     | 0.389    |\n",
      "|    ent_coef        | 0.00465  |\n",
      "|    ent_coef_loss   | 19.4     |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=76.37 +/- 241.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 76.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=351000, episode_reward=221.69 +/- 254.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 222      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 351000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3        |\n",
      "|    critic_loss     | 0.231    |\n",
      "|    ent_coef        | 0.0047   |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=280.55 +/- 51.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 281      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 352      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 510      |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=353000, episode_reward=137.22 +/- 370.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 137      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 353000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.21     |\n",
      "|    critic_loss     | 0.283    |\n",
      "|    ent_coef        | 0.00467  |\n",
      "|    ent_coef_loss   | -8.08    |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=-313.16 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -313     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 354000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=64.90 +/- 278.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 64.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 355000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.05     |\n",
      "|    critic_loss     | 0.242    |\n",
      "|    ent_coef        | 0.00463  |\n",
      "|    ent_coef_loss   | 13       |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=261.37 +/- 43.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 261      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 356      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 515      |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=-143.23 +/- 80.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 357000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.23     |\n",
      "|    critic_loss     | 0.331    |\n",
      "|    ent_coef        | 0.00468  |\n",
      "|    ent_coef_loss   | 17.9     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=43.54 +/- 173.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 43.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 358000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=359000, episode_reward=-75.27 +/- 81.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -75.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.78     |\n",
      "|    critic_loss     | 0.192    |\n",
      "|    ent_coef        | 0.00477  |\n",
      "|    ent_coef_loss   | 12.3     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-183.89 +/- 181.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -184     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 521      |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=361000, episode_reward=282.45 +/- 31.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 282      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 361000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.7      |\n",
      "|    critic_loss     | 0.123    |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=50.75 +/- 244.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 50.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 362000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=26.71 +/- 219.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 26.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 363000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.42     |\n",
      "|    critic_loss     | 0.189    |\n",
      "|    ent_coef        | 0.00491  |\n",
      "|    ent_coef_loss   | 6.41     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=26.11 +/- 220.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 26.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 82.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 364      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 527      |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=350.75 +/- 19.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 351      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 365000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.44     |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.00495  |\n",
      "|    ent_coef_loss   | 1.92     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=379.09 +/- 2.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 379      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=367000, episode_reward=297.24 +/- 10.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 297      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 367000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.35     |\n",
      "|    critic_loss     | 0.152    |\n",
      "|    ent_coef        | 0.00498  |\n",
      "|    ent_coef_loss   | -0.779   |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=304.15 +/- 17.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 304      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    episodes        | 368      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 532      |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=21.85 +/- 393.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 21.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 369000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.25     |\n",
      "|    critic_loss     | 0.125    |\n",
      "|    ent_coef        | 0.00498  |\n",
      "|    ent_coef_loss   | -9.01    |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=31.66 +/- 400.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=371000, episode_reward=333.05 +/- 16.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 371000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.24     |\n",
      "|    critic_loss     | 0.23     |\n",
      "|    ent_coef        | 0.00494  |\n",
      "|    ent_coef_loss   | -2.47    |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=156.03 +/- 294.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 156      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 117      |\n",
      "| time/              |          |\n",
      "|    episodes        | 372      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 538      |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=373000, episode_reward=382.43 +/- 16.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 382      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 373000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.22     |\n",
      "|    critic_loss     | 0.148    |\n",
      "|    ent_coef        | 0.00494  |\n",
      "|    ent_coef_loss   | 12.3     |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=377.23 +/- 15.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=478.03 +/- 21.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 478      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 375000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.23     |\n",
      "|    critic_loss     | 0.174    |\n",
      "|    ent_coef        | 0.00496  |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=453.02 +/- 41.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 453      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 131      |\n",
      "| time/              |          |\n",
      "|    episodes        | 376      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 543      |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=377000, episode_reward=501.36 +/- 19.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 501      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 377000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9      |\n",
      "|    critic_loss     | 0.209    |\n",
      "|    ent_coef        | 0.00492  |\n",
      "|    ent_coef_loss   | -4.54    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=498.25 +/- 46.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 498      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=379000, episode_reward=618.09 +/- 25.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 618      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 379000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.93     |\n",
      "|    critic_loss     | 0.313    |\n",
      "|    ent_coef        | 0.0049   |\n",
      "|    ent_coef_loss   | 5.41     |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1850     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=380000, episode_reward=642.52 +/- 38.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 643      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 138      |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 549      |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=699.96 +/- 27.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 700      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 381000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.37     |\n",
      "|    critic_loss     | 0.242    |\n",
      "|    ent_coef        | 0.0049   |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1860     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=382000, episode_reward=705.81 +/- 26.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 706      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=383000, episode_reward=558.31 +/- 43.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 558      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 383000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.79     |\n",
      "|    critic_loss     | 0.198    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -5.24    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=569.30 +/- 33.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 569      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    episodes        | 384      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 556      |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=594.57 +/- 53.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 595      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 385000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=-60.69 +/- 335.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -60.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 386000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9      |\n",
      "|    critic_loss     | 0.266    |\n",
      "|    ent_coef        | 0.0048   |\n",
      "|    ent_coef_loss   | -7.05    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=119.21 +/- 429.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 119      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 387000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=103.67 +/- 433.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 388000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.92     |\n",
      "|    critic_loss     | 0.279    |\n",
      "|    ent_coef        | 0.00476  |\n",
      "|    ent_coef_loss   | 3.33     |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1890     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 178      |\n",
      "| time/              |          |\n",
      "|    episodes        | 388      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 562      |\n",
      "|    total_timesteps | 388000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=389000, episode_reward=282.41 +/- 437.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 282      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 389000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=399.25 +/- 44.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.72     |\n",
      "|    critic_loss     | 0.27     |\n",
      "|    ent_coef        | 0.00475  |\n",
      "|    ent_coef_loss   | -3.22    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391000, episode_reward=312.18 +/- 283.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 312      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 391000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=451.48 +/- 358.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 392000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 0.281    |\n",
      "|    ent_coef        | 0.00473  |\n",
      "|    ent_coef_loss   | -8.19    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 204      |\n",
      "| time/              |          |\n",
      "|    episodes        | 392      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 568      |\n",
      "|    total_timesteps | 392000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=607.31 +/- 61.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 607      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 393000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=478.08 +/- 44.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 478      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 394000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.83     |\n",
      "|    critic_loss     | 0.247    |\n",
      "|    ent_coef        | 0.00469  |\n",
      "|    ent_coef_loss   | 4.34     |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=307.97 +/- 291.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 308      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 395000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=452.66 +/- 355.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 453      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.72     |\n",
      "|    critic_loss     | 0.319    |\n",
      "|    ent_coef        | 0.00471  |\n",
      "|    ent_coef_loss   | 15.5     |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1930     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 229      |\n",
      "| time/              |          |\n",
      "|    episodes        | 396      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 573      |\n",
      "|    total_timesteps | 396000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397000, episode_reward=450.24 +/- 356.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 450      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 397000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=289.30 +/- 24.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 289      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 398000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 0.207    |\n",
      "|    ent_coef        | 0.00476  |\n",
      "|    ent_coef_loss   | -9.79    |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=93.65 +/- 278.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 93.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 399000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=617.67 +/- 447.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 618      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.73     |\n",
      "|    critic_loss     | 0.291    |\n",
      "|    ent_coef        | 0.00475  |\n",
      "|    ent_coef_loss   | -5.33    |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1950     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 235      |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 580      |\n",
      "|    total_timesteps | 400000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401000, episode_reward=398.69 +/- 550.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 401000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=254.63 +/- 450.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 255      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 402000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.22     |\n",
      "|    critic_loss     | 0.314    |\n",
      "|    ent_coef        | 0.00473  |\n",
      "|    ent_coef_loss   | 8.53     |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403000, episode_reward=632.32 +/- 13.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 632      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 403000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=596.89 +/- 24.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 597      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 404000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.83     |\n",
      "|    critic_loss     | 0.274    |\n",
      "|    ent_coef        | 0.00475  |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1970     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 254      |\n",
      "| time/              |          |\n",
      "|    episodes        | 404      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 586      |\n",
      "|    total_timesteps | 404000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=623.38 +/- 27.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 623      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=608.87 +/- 24.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 609      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 406000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.7      |\n",
      "|    critic_loss     | 0.282    |\n",
      "|    ent_coef        | 0.00478  |\n",
      "|    ent_coef_loss   | 11.8     |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 1980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=407000, episode_reward=596.49 +/- 35.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 596      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=585.60 +/- 40.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 586      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 408000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.21     |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.00483  |\n",
      "|    ent_coef_loss   | -0.16    |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 1990     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 269      |\n",
      "| time/              |          |\n",
      "|    episodes        | 408      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 592      |\n",
      "|    total_timesteps | 408000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409000, episode_reward=573.50 +/- 37.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 574      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=554.52 +/- 44.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 555      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.99     |\n",
      "|    critic_loss     | 0.204    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -3.23    |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=513.48 +/- 52.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 513      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=560.35 +/- 33.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 560      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 412000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.14     |\n",
      "|    critic_loss     | 0.318    |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | 7.63     |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 2010     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    episodes        | 412      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 597      |\n",
      "|    total_timesteps | 412000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=413000, episode_reward=593.61 +/- 17.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 594      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=650.04 +/- 20.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 650      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 414000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.04     |\n",
      "|    critic_loss     | 0.33     |\n",
      "|    ent_coef        | 0.0049   |\n",
      "|    ent_coef_loss   | 7.34     |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 2020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=692.44 +/- 21.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 692      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=119.35 +/- 480.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 119      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 416000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.86     |\n",
      "|    critic_loss     | 0.278    |\n",
      "|    ent_coef        | 0.00494  |\n",
      "|    ent_coef_loss   | -3.01    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2030     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 300      |\n",
      "| time/              |          |\n",
      "|    episodes        | 416      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 603      |\n",
      "|    total_timesteps | 416000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=-73.57 +/- 396.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -73.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=75.84 +/- 445.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 75.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 418000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.8      |\n",
      "|    critic_loss     | 0.336    |\n",
      "|    ent_coef        | 0.00495  |\n",
      "|    ent_coef_loss   | 0.556    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=419000, episode_reward=577.16 +/- 43.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 577      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=337.54 +/- 267.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 338      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.87     |\n",
      "|    critic_loss     | 0.293    |\n",
      "|    ent_coef        | 0.00495  |\n",
      "|    ent_coef_loss   | -8.46    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 319      |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 608      |\n",
      "|    total_timesteps | 420000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421000, episode_reward=193.42 +/- 315.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=392.80 +/- 325.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 393      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 422000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.59     |\n",
      "|    critic_loss     | 0.287    |\n",
      "|    ent_coef        | 0.0049   |\n",
      "|    ent_coef_loss   | -6.15    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=237.75 +/- 402.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 238      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=567.17 +/- 30.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 567      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 424000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 0.245    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -2.9     |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 329      |\n",
      "| time/              |          |\n",
      "|    episodes        | 424      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 614      |\n",
      "|    total_timesteps | 424000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=563.63 +/- 15.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 564      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=706.07 +/- 31.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 706      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 426000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.16     |\n",
      "|    critic_loss     | 0.308    |\n",
      "|    ent_coef        | 0.00482  |\n",
      "|    ent_coef_loss   | 6.21     |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2080     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=427000, episode_reward=726.01 +/- 29.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 726      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=428000, episode_reward=708.48 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 708      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 350      |\n",
      "| time/              |          |\n",
      "|    episodes        | 428      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 620      |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=809.97 +/- 20.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 810      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 429000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.43     |\n",
      "|    critic_loss     | 0.282    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 1.73     |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2090     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=430000, episode_reward=808.78 +/- 21.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 809      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=431000, episode_reward=655.87 +/- 23.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 656      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 431000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.42     |\n",
      "|    critic_loss     | 0.229    |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | -4.61    |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=638.17 +/- 36.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 638      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 366      |\n",
      "| time/              |          |\n",
      "|    episodes        | 432      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 627      |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=433000, episode_reward=559.21 +/- 19.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 559      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 0.282    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 0.203    |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=565.74 +/- 33.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 566      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 434000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=576.31 +/- 417.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 576      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 435000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.77     |\n",
      "|    critic_loss     | 0.325    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 3.84     |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=161.30 +/- 514.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 161      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 381      |\n",
      "| time/              |          |\n",
      "|    episodes        | 436      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 632      |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=437000, episode_reward=665.02 +/- 20.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 665      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 437000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.44     |\n",
      "|    critic_loss     | 0.284    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -4.22    |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=688.43 +/- 27.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 688      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 438000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=439000, episode_reward=-94.06 +/- 408.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -94.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 439000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.67     |\n",
      "|    critic_loss     | 0.323    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=133.53 +/- 502.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 134      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 397      |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 639      |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=441000, episode_reward=297.64 +/- 40.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 298      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 441000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.38     |\n",
      "|    critic_loss     | 0.303    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 6.63     |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=293.28 +/- 8.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 293      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 442000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443000, episode_reward=750.09 +/- 28.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 750      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 443000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.79     |\n",
      "|    critic_loss     | 0.282    |\n",
      "|    ent_coef        | 0.00487  |\n",
      "|    ent_coef_loss   | -6.22    |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=769.93 +/- 33.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 770      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 402      |\n",
      "| time/              |          |\n",
      "|    episodes        | 444      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 645      |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=505.31 +/- 39.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 505      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.66     |\n",
      "|    critic_loss     | 0.292    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -2.48    |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=460.51 +/- 38.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 461      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 446000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=634.25 +/- 17.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 634      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 447000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.41     |\n",
      "|    critic_loss     | 0.323    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 4.33     |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=641.01 +/- 22.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 641      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 416      |\n",
      "| time/              |          |\n",
      "|    episodes        | 448      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 651      |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=449000, episode_reward=-386.32 +/- 1.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -386     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 449000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.72     |\n",
      "|    critic_loss     | 0.272    |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | 9.04     |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-387.41 +/- 2.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -387     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=451000, episode_reward=481.53 +/- 27.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 482      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 451000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 0.295    |\n",
      "|    ent_coef        | 0.00492  |\n",
      "|    ent_coef_loss   | 11.8     |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=501.11 +/- 31.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 501      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 433      |\n",
      "| time/              |          |\n",
      "|    episodes        | 452      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 657      |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=453000, episode_reward=481.39 +/- 29.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 481      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 453000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.13     |\n",
      "|    critic_loss     | 0.253    |\n",
      "|    ent_coef        | 0.00501  |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=469.97 +/- 34.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=280.27 +/- 32.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 455000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.24     |\n",
      "|    critic_loss     | 0.262    |\n",
      "|    ent_coef        | 0.00503  |\n",
      "|    ent_coef_loss   | -1.76    |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=266.02 +/- 19.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 266      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 438      |\n",
      "| time/              |          |\n",
      "|    episodes        | 456      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 663      |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=457000, episode_reward=576.54 +/- 33.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 577      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 457000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.95     |\n",
      "|    critic_loss     | 0.249    |\n",
      "|    ent_coef        | 0.00503  |\n",
      "|    ent_coef_loss   | -4.99    |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=565.15 +/- 22.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 565      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=464.63 +/- 40.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 459000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.86     |\n",
      "|    critic_loss     | 0.292    |\n",
      "|    ent_coef        | 0.005    |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=442.35 +/- 35.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 442      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 456      |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 669      |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=461000, episode_reward=662.65 +/- 17.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 663      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 461000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 0.321    |\n",
      "|    ent_coef        | 0.00498  |\n",
      "|    ent_coef_loss   | 8.35     |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=662.51 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 663      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463000, episode_reward=569.60 +/- 20.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 570      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 463000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.84     |\n",
      "|    critic_loss     | 0.276    |\n",
      "|    ent_coef        | 0.00501  |\n",
      "|    ent_coef_loss   | -3.6     |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=561.11 +/- 41.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 561      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 464000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 472      |\n",
      "| time/              |          |\n",
      "|    episodes        | 464      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 675      |\n",
      "|    total_timesteps | 464000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-115.03 +/- 356.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 465000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9      |\n",
      "|    critic_loss     | 0.305    |\n",
      "|    ent_coef        | 0.00503  |\n",
      "|    ent_coef_loss   | 11.2     |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=-122.38 +/- 349.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 466000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=467000, episode_reward=-426.22 +/- 2.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -426     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 467000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.85     |\n",
      "|    critic_loss     | 0.321    |\n",
      "|    ent_coef        | 0.00509  |\n",
      "|    ent_coef_loss   | 12       |\n",
      "|    learning_rate   | 0.00453  |\n",
      "|    n_updates       | 2280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=-425.93 +/- 2.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -426     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 468      |\n",
      "| time/              |          |\n",
      "|    episodes        | 468      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 681      |\n",
      "|    total_timesteps | 468000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=469000, episode_reward=-89.64 +/- 384.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -89.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 469000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.26     |\n",
      "|    critic_loss     | 0.163    |\n",
      "|    ent_coef        | 0.00519  |\n",
      "|    ent_coef_loss   | 4.7      |\n",
      "|    learning_rate   | 0.00453  |\n",
      "|    n_updates       | 2290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-96.06 +/- 384.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -96.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471000, episode_reward=268.68 +/- 451.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 269      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 471000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-263.46 +/- 2.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -263     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 472000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.76     |\n",
      "|    critic_loss     | 0.339    |\n",
      "|    ent_coef        | 0.00527  |\n",
      "|    ent_coef_loss   | 11.5     |\n",
      "|    learning_rate   | 0.00453  |\n",
      "|    n_updates       | 2300     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 484      |\n",
      "| time/              |          |\n",
      "|    episodes        | 472      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 686      |\n",
      "|    total_timesteps | 472000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=473000, episode_reward=33.24 +/- 366.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 33.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 473000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=-297.00 +/- 1.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -297     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 474000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.32     |\n",
      "|    critic_loss     | 0.312    |\n",
      "|    ent_coef        | 0.00536  |\n",
      "|    ent_coef_loss   | 4.71     |\n",
      "|    learning_rate   | 0.00453  |\n",
      "|    n_updates       | 2310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=-298.30 +/- 1.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -298     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 475000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=-511.64 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -512     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 476000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.4      |\n",
      "|    critic_loss     | 0.419    |\n",
      "|    ent_coef        | 0.00544  |\n",
      "|    ent_coef_loss   | 7.75     |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2320     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 485      |\n",
      "| time/              |          |\n",
      "|    episodes        | 476      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 692      |\n",
      "|    total_timesteps | 476000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477000, episode_reward=-507.86 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -508     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 477000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=-264.34 +/- 365.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -264     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 478000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.09     |\n",
      "|    critic_loss     | 0.117    |\n",
      "|    ent_coef        | 0.00554  |\n",
      "|    ent_coef_loss   | 4.81     |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=479000, episode_reward=-274.82 +/- 341.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -275     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 479000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=37.63 +/- 435.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.21     |\n",
      "|    critic_loss     | 0.386    |\n",
      "|    ent_coef        | 0.00561  |\n",
      "|    ent_coef_loss   | 6.66     |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2340     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 473      |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 699      |\n",
      "|    total_timesteps | 480000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=481000, episode_reward=226.40 +/- 360.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 226      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 481000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=-198.84 +/- 504.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -199     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 482000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.62     |\n",
      "|    critic_loss     | 0.322    |\n",
      "|    ent_coef        | 0.00568  |\n",
      "|    ent_coef_loss   | 8.13     |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483000, episode_reward=-612.60 +/- 115.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -613     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 483000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=476.96 +/- 78.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 477      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 484000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.54     |\n",
      "|    critic_loss     | 0.342    |\n",
      "|    ent_coef        | 0.00577  |\n",
      "|    ent_coef_loss   | 11.3     |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2360     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 457      |\n",
      "| time/              |          |\n",
      "|    episodes        | 484      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 705      |\n",
      "|    total_timesteps | 484000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=500.09 +/- 44.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 485000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=443.63 +/- 40.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 444      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 486000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.21     |\n",
      "|    critic_loss     | 0.366    |\n",
      "|    ent_coef        | 0.00587  |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.00451  |\n",
      "|    n_updates       | 2370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487000, episode_reward=158.70 +/- 480.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 159      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 487000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=389.47 +/- 41.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 389      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 488000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.78     |\n",
      "|    critic_loss     | 0.442    |\n",
      "|    ent_coef        | 0.00594  |\n",
      "|    ent_coef_loss   | 9.98     |\n",
      "|    learning_rate   | 0.00451  |\n",
      "|    n_updates       | 2380     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 454      |\n",
      "| time/              |          |\n",
      "|    episodes        | 488      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 711      |\n",
      "|    total_timesteps | 488000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=489000, episode_reward=353.77 +/- 39.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 354      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 489000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=534.36 +/- 16.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 534      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.51     |\n",
      "|    critic_loss     | 0.387    |\n",
      "|    ent_coef        | 0.00603  |\n",
      "|    ent_coef_loss   | 2.26     |\n",
      "|    learning_rate   | 0.00451  |\n",
      "|    n_updates       | 2390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=491000, episode_reward=537.14 +/- 13.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 537      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=480.18 +/- 17.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 480      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 492000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.26     |\n",
      "|    critic_loss     | 0.425    |\n",
      "|    ent_coef        | 0.00608  |\n",
      "|    ent_coef_loss   | -2.74    |\n",
      "|    learning_rate   | 0.00451  |\n",
      "|    n_updates       | 2400     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 451      |\n",
      "| time/              |          |\n",
      "|    episodes        | 492      |\n",
      "|    fps             | 685      |\n",
      "|    time_elapsed    | 717      |\n",
      "|    total_timesteps | 492000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=493000, episode_reward=466.54 +/- 27.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 467      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=460.37 +/- 38.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 460      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 494000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.36     |\n",
      "|    critic_loss     | 0.429    |\n",
      "|    ent_coef        | 0.00609  |\n",
      "|    ent_coef_loss   | 5.46     |\n",
      "|    learning_rate   | 0.00451  |\n",
      "|    n_updates       | 2410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=475.56 +/- 35.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 476      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 495000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-3.73 +/- 305.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.73    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 496000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.15     |\n",
      "|    critic_loss     | 0.349    |\n",
      "|    ent_coef        | 0.00613  |\n",
      "|    ent_coef_loss   | 1.88     |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2420     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 449      |\n",
      "| time/              |          |\n",
      "|    episodes        | 496      |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 724      |\n",
      "|    total_timesteps | 496000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=497000, episode_reward=-371.22 +/- 3.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 497000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=-82.85 +/- 433.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -82.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 498000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.65     |\n",
      "|    critic_loss     | 0.304    |\n",
      "|    ent_coef        | 0.00616  |\n",
      "|    ent_coef_loss   | -3.57    |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=499000, episode_reward=-244.57 +/- 411.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=199.02 +/- 609.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 199      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.75     |\n",
      "|    critic_loss     | 0.241    |\n",
      "|    ent_coef        | 0.00615  |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2440     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 437      |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 683      |\n",
      "|    time_elapsed    | 732      |\n",
      "|    total_timesteps | 500000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=501000, episode_reward=-42.10 +/- 616.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=502000, episode_reward=170.51 +/- 544.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 171      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 502000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.28     |\n",
      "|    critic_loss     | 0.415    |\n",
      "|    ent_coef        | 0.00616  |\n",
      "|    ent_coef_loss   | 9.74     |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=503000, episode_reward=-47.71 +/- 548.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -47.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 503000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=600.87 +/- 479.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 601      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 504000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.62     |\n",
      "|    critic_loss     | 0.322    |\n",
      "|    ent_coef        | 0.00623  |\n",
      "|    ent_coef_loss   | -0.375   |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2460     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 428      |\n",
      "| time/              |          |\n",
      "|    episodes        | 504      |\n",
      "|    fps             | 681      |\n",
      "|    time_elapsed    | 739      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=596.87 +/- 477.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 597      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=506000, episode_reward=-19.24 +/- 428.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -19.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 506000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.54     |\n",
      "|    critic_loss     | 0.402    |\n",
      "|    ent_coef        | 0.00626  |\n",
      "|    ent_coef_loss   | 0.0174   |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507000, episode_reward=358.70 +/- 364.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 359      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 507000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=104.88 +/- 449.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 105      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 508000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.43     |\n",
      "|    critic_loss     | 0.506    |\n",
      "|    ent_coef        | 0.00628  |\n",
      "|    ent_coef_loss   | 7.08     |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 432      |\n",
      "| time/              |          |\n",
      "|    episodes        | 508      |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 746      |\n",
      "|    total_timesteps | 508000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=509000, episode_reward=310.61 +/- 378.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 311      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 509000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=220.44 +/- 510.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 220      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.12     |\n",
      "|    critic_loss     | 0.269    |\n",
      "|    ent_coef        | 0.00635  |\n",
      "|    ent_coef_loss   | 1.39     |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=511000, episode_reward=617.77 +/- 32.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 618      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=636.07 +/- 25.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 636      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 423      |\n",
      "| time/              |          |\n",
      "|    episodes        | 512      |\n",
      "|    fps             | 678      |\n",
      "|    time_elapsed    | 754      |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=513000, episode_reward=479.30 +/- 18.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 479      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 513000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.95     |\n",
      "|    critic_loss     | 0.446    |\n",
      "|    ent_coef        | 0.00638  |\n",
      "|    ent_coef_loss   | -3.38    |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514000, episode_reward=-70.02 +/- 441.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 514000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=837.03 +/- 85.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 837      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 515000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.67     |\n",
      "|    critic_loss     | 0.407    |\n",
      "|    ent_coef        | 0.00636  |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2510     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=516000, episode_reward=779.75 +/- 28.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 780      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 516000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 425      |\n",
      "| time/              |          |\n",
      "|    episodes        | 516      |\n",
      "|    fps             | 678      |\n",
      "|    time_elapsed    | 760      |\n",
      "|    total_timesteps | 516000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=517000, episode_reward=626.59 +/- 24.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 627      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 517000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.27     |\n",
      "|    critic_loss     | 0.408    |\n",
      "|    ent_coef        | 0.00635  |\n",
      "|    ent_coef_loss   | 0.601    |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518000, episode_reward=624.44 +/- 32.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 624      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 518000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=519000, episode_reward=398.64 +/- 42.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 519000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.78     |\n",
      "|    critic_loss     | 0.609    |\n",
      "|    ent_coef        | 0.00635  |\n",
      "|    ent_coef_loss   | 8.11     |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=432.08 +/- 32.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 432      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 421      |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 678      |\n",
      "|    time_elapsed    | 766      |\n",
      "|    total_timesteps | 520000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=521000, episode_reward=620.80 +/- 24.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 621      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 521000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.86     |\n",
      "|    critic_loss     | 0.431    |\n",
      "|    ent_coef        | 0.00642  |\n",
      "|    ent_coef_loss   | 4.31     |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=605.74 +/- 22.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 606      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 522000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=523000, episode_reward=500.36 +/- 64.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 523000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.23     |\n",
      "|    critic_loss     | 0.408    |\n",
      "|    ent_coef        | 0.00645  |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=518.96 +/- 39.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 519      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 524000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 424      |\n",
      "| time/              |          |\n",
      "|    episodes        | 524      |\n",
      "|    fps             | 679      |\n",
      "|    time_elapsed    | 771      |\n",
      "|    total_timesteps | 524000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=449.18 +/- 24.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 449      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 525000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.33     |\n",
      "|    critic_loss     | 0.408    |\n",
      "|    ent_coef        | 0.00632  |\n",
      "|    ent_coef_loss   | -1.18    |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=526000, episode_reward=461.39 +/- 27.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 461      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 526000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=527000, episode_reward=217.63 +/- 34.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 218      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 527000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.49     |\n",
      "|    critic_loss     | 0.443    |\n",
      "|    ent_coef        | 0.00627  |\n",
      "|    ent_coef_loss   | 13.4     |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=223.60 +/- 55.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 224      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 528000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 413      |\n",
      "| time/              |          |\n",
      "|    episodes        | 528      |\n",
      "|    fps             | 679      |\n",
      "|    time_elapsed    | 777      |\n",
      "|    total_timesteps | 528000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=529000, episode_reward=654.15 +/- 33.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 654      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 529000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.2      |\n",
      "|    critic_loss     | 0.39     |\n",
      "|    ent_coef        | 0.00637  |\n",
      "|    ent_coef_loss   | 4.99     |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=674.41 +/- 28.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 674      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=531000, episode_reward=782.81 +/- 35.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 783      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 531000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 0.463    |\n",
      "|    ent_coef        | 0.00645  |\n",
      "|    ent_coef_loss   | -2.7     |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=792.74 +/- 57.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 793      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 532000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 411      |\n",
      "| time/              |          |\n",
      "|    episodes        | 532      |\n",
      "|    fps             | 679      |\n",
      "|    time_elapsed    | 782      |\n",
      "|    total_timesteps | 532000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=533000, episode_reward=171.98 +/- 35.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 172      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 533000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.25     |\n",
      "|    critic_loss     | 0.438    |\n",
      "|    ent_coef        | 0.00645  |\n",
      "|    ent_coef_loss   | 0.73     |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=143.73 +/- 30.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 144      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=557.21 +/- 15.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 557      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 535000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.5      |\n",
      "|    critic_loss     | 0.486    |\n",
      "|    ent_coef        | 0.00648  |\n",
      "|    ent_coef_loss   | 11.8     |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=532.35 +/- 31.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 532      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 400      |\n",
      "| time/              |          |\n",
      "|    episodes        | 536      |\n",
      "|    fps             | 679      |\n",
      "|    time_elapsed    | 788      |\n",
      "|    total_timesteps | 536000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=537000, episode_reward=448.07 +/- 25.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 448      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 537000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.13     |\n",
      "|    critic_loss     | 0.372    |\n",
      "|    ent_coef        | 0.00659  |\n",
      "|    ent_coef_loss   | 3.9      |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=538000, episode_reward=408.76 +/- 37.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 409      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 538000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=539000, episode_reward=600.76 +/- 46.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 601      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 539000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.25     |\n",
      "|    critic_loss     | 0.406    |\n",
      "|    ent_coef        | 0.00668  |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=537.77 +/- 19.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 538      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 392      |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 794      |\n",
      "|    total_timesteps | 540000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=541000, episode_reward=661.80 +/- 92.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 662      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 541000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.12     |\n",
      "|    critic_loss     | 0.492    |\n",
      "|    ent_coef        | 0.00674  |\n",
      "|    ent_coef_loss   | 7.83     |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=542000, episode_reward=588.17 +/- 64.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 588      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=543000, episode_reward=676.02 +/- 45.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 676      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 543000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.43     |\n",
      "|    critic_loss     | 0.394    |\n",
      "|    ent_coef        | 0.00684  |\n",
      "|    ent_coef_loss   | 11.8     |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=685.72 +/- 28.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 686      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 544000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 399      |\n",
      "| time/              |          |\n",
      "|    episodes        | 544      |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 799      |\n",
      "|    total_timesteps | 544000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=810.18 +/- 40.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 810      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 545000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.59     |\n",
      "|    critic_loss     | 0.502    |\n",
      "|    ent_coef        | 0.007    |\n",
      "|    ent_coef_loss   | 3.98     |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=769.94 +/- 41.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 770      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 546000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=547000, episode_reward=338.57 +/- 350.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 339      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 547000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.38     |\n",
      "|    critic_loss     | 0.455    |\n",
      "|    ent_coef        | 0.00711  |\n",
      "|    ent_coef_loss   | 5.39     |\n",
      "|    learning_rate   | 0.00445  |\n",
      "|    n_updates       | 2670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=503.47 +/- 51.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 503      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 404      |\n",
      "| time/              |          |\n",
      "|    episodes        | 548      |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 805      |\n",
      "|    total_timesteps | 548000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=549000, episode_reward=452.37 +/- 26.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 452      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 549000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.99     |\n",
      "|    critic_loss     | 0.54     |\n",
      "|    ent_coef        | 0.00722  |\n",
      "|    ent_coef_loss   | 16.7     |\n",
      "|    learning_rate   | 0.00445  |\n",
      "|    n_updates       | 2680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=446.67 +/- 49.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 447      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=551000, episode_reward=719.89 +/- 45.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 720      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 551000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.72     |\n",
      "|    critic_loss     | 0.419    |\n",
      "|    ent_coef        | 0.00742  |\n",
      "|    ent_coef_loss   | -2.69    |\n",
      "|    learning_rate   | 0.00445  |\n",
      "|    n_updates       | 2690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=505.52 +/- 430.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 506      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 552000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 405      |\n",
      "| time/              |          |\n",
      "|    episodes        | 552      |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 810      |\n",
      "|    total_timesteps | 552000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=553000, episode_reward=546.94 +/- 19.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 547      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 553000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.54     |\n",
      "|    critic_loss     | 0.438    |\n",
      "|    ent_coef        | 0.0075   |\n",
      "|    ent_coef_loss   | 5.3      |\n",
      "|    learning_rate   | 0.00445  |\n",
      "|    n_updates       | 2700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=554000, episode_reward=560.50 +/- 27.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 560      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=605.66 +/- 18.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 606      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 555000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=730.43 +/- 46.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 730      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 556000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2        |\n",
      "|    critic_loss     | 0.471    |\n",
      "|    ent_coef        | 0.00759  |\n",
      "|    ent_coef_loss   | 9.85     |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2710     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 414      |\n",
      "| time/              |          |\n",
      "|    episodes        | 556      |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 816      |\n",
      "|    total_timesteps | 556000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=557000, episode_reward=703.49 +/- 48.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 703      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 557000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=657.69 +/- 39.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 658      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 558000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.41     |\n",
      "|    critic_loss     | 0.558    |\n",
      "|    ent_coef        | 0.00773  |\n",
      "|    ent_coef_loss   | 5.01     |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=559000, episode_reward=626.73 +/- 35.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 627      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 559000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=741.86 +/- 62.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 742      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.61     |\n",
      "|    critic_loss     | 0.476    |\n",
      "|    ent_coef        | 0.00787  |\n",
      "|    ent_coef_loss   | 8.99     |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2730     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 420      |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 681      |\n",
      "|    time_elapsed    | 822      |\n",
      "|    total_timesteps | 560000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=561000, episode_reward=742.99 +/- 42.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 743      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 561000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=562000, episode_reward=746.95 +/- 43.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 747      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 562000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.49     |\n",
      "|    critic_loss     | 0.446    |\n",
      "|    ent_coef        | 0.00802  |\n",
      "|    ent_coef_loss   | 4.49     |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=563000, episode_reward=712.82 +/- 37.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 713      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 563000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=590.55 +/- 44.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 591      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 564000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.89     |\n",
      "|    critic_loss     | 0.477    |\n",
      "|    ent_coef        | 0.00813  |\n",
      "|    ent_coef_loss   | 2.67     |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2750     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 417      |\n",
      "| time/              |          |\n",
      "|    episodes        | 564      |\n",
      "|    fps             | 681      |\n",
      "|    time_elapsed    | 827      |\n",
      "|    total_timesteps | 564000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=566.46 +/- 26.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 566      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 565000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=566000, episode_reward=631.52 +/- 25.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 632      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 566000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.8      |\n",
      "|    critic_loss     | 0.355    |\n",
      "|    ent_coef        | 0.00822  |\n",
      "|    ent_coef_loss   | 3.75     |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=567000, episode_reward=616.25 +/- 35.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 616      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 567000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=802.65 +/- 32.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 803      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 568000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.8      |\n",
      "|    critic_loss     | 0.461    |\n",
      "|    ent_coef        | 0.0083   |\n",
      "|    ent_coef_loss   | 1.88     |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2770     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 435      |\n",
      "| time/              |          |\n",
      "|    episodes        | 568      |\n",
      "|    fps             | 681      |\n",
      "|    time_elapsed    | 833      |\n",
      "|    total_timesteps | 568000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=569000, episode_reward=785.76 +/- 53.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 786      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 569000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=749.91 +/- 25.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 750      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.17     |\n",
      "|    critic_loss     | 0.503    |\n",
      "|    ent_coef        | 0.00836  |\n",
      "|    ent_coef_loss   | 6.83     |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=571000, episode_reward=729.56 +/- 26.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 730      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 571000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=777.00 +/- 30.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 777      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 572000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.21     |\n",
      "|    critic_loss     | 0.564    |\n",
      "|    ent_coef        | 0.00849  |\n",
      "|    ent_coef_loss   | 10.9     |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2790     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 445      |\n",
      "| time/              |          |\n",
      "|    episodes        | 572      |\n",
      "|    fps             | 681      |\n",
      "|    time_elapsed    | 839      |\n",
      "|    total_timesteps | 572000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573000, episode_reward=771.73 +/- 24.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 772      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 573000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=574000, episode_reward=875.81 +/- 26.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 876      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 574000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.49     |\n",
      "|    critic_loss     | 0.421    |\n",
      "|    ent_coef        | 0.00867  |\n",
      "|    ent_coef_loss   | 0.724    |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2800     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=575000, episode_reward=845.26 +/- 75.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 845      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 575000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=691.55 +/- 34.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 692      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 576000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.629    |\n",
      "|    critic_loss     | 0.524    |\n",
      "|    ent_coef        | 0.00879  |\n",
      "|    ent_coef_loss   | 11.6     |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2810     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 461      |\n",
      "| time/              |          |\n",
      "|    episodes        | 576      |\n",
      "|    fps             | 681      |\n",
      "|    time_elapsed    | 844      |\n",
      "|    total_timesteps | 576000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=577000, episode_reward=701.90 +/- 39.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 702      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 577000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=578000, episode_reward=813.42 +/- 39.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 813      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 578000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.67     |\n",
      "|    critic_loss     | 0.42     |\n",
      "|    ent_coef        | 0.00896  |\n",
      "|    ent_coef_loss   | 1.45     |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=579000, episode_reward=828.43 +/- 36.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 828      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=676.91 +/- 14.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 677      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.769    |\n",
      "|    critic_loss     | 0.431    |\n",
      "|    ent_coef        | 0.0091   |\n",
      "|    ent_coef_loss   | 7.15     |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2830     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 492      |\n",
      "| time/              |          |\n",
      "|    episodes        | 580      |\n",
      "|    fps             | 681      |\n",
      "|    time_elapsed    | 850      |\n",
      "|    total_timesteps | 580000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=581000, episode_reward=704.83 +/- 17.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 705      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 581000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=588.87 +/- 14.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 589      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 582000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.58     |\n",
      "|    critic_loss     | 0.496    |\n",
      "|    ent_coef        | 0.00923  |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=583000, episode_reward=601.99 +/- 21.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 602      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 583000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=650.62 +/- 58.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 651      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 584000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.85     |\n",
      "|    critic_loss     | 0.453    |\n",
      "|    ent_coef        | 0.00932  |\n",
      "|    ent_coef_loss   | 4.49     |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2850     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 516      |\n",
      "| time/              |          |\n",
      "|    episodes        | 584      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 856      |\n",
      "|    total_timesteps | 584000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=693.65 +/- 12.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 694      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=586000, episode_reward=743.24 +/- 78.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 743      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 586000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.58     |\n",
      "|    critic_loss     | 0.429    |\n",
      "|    ent_coef        | 0.00942  |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=587000, episode_reward=742.15 +/- 47.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 742      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 587000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=650.29 +/- 43.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 650      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 588000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.67     |\n",
      "|    critic_loss     | 0.495    |\n",
      "|    ent_coef        | 0.00948  |\n",
      "|    ent_coef_loss   | 5.58     |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2870     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 524      |\n",
      "| time/              |          |\n",
      "|    episodes        | 588      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 861      |\n",
      "|    total_timesteps | 588000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=589000, episode_reward=643.32 +/- 30.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 643      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 589000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=793.16 +/- 57.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 793      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.92     |\n",
      "|    critic_loss     | 0.464    |\n",
      "|    ent_coef        | 0.00959  |\n",
      "|    ent_coef_loss   | 2.19     |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=591000, episode_reward=778.70 +/- 37.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 779      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=831.04 +/- 20.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 831      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 592000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.11     |\n",
      "|    critic_loss     | 0.561    |\n",
      "|    ent_coef        | 0.00967  |\n",
      "|    ent_coef_loss   | 1.94     |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2890     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 534      |\n",
      "| time/              |          |\n",
      "|    episodes        | 592      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 867      |\n",
      "|    total_timesteps | 592000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=593000, episode_reward=818.18 +/- 29.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 818      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 593000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=762.69 +/- 38.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 763      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 594000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.07     |\n",
      "|    critic_loss     | 0.517    |\n",
      "|    ent_coef        | 0.00976  |\n",
      "|    ent_coef_loss   | 8.64     |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=762.59 +/- 68.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 763      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 595000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=767.42 +/- 21.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 767      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 596000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.08     |\n",
      "|    critic_loss     | 0.478    |\n",
      "|    ent_coef        | 0.00992  |\n",
      "|    ent_coef_loss   | 5.54     |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 547      |\n",
      "| time/              |          |\n",
      "|    episodes        | 596      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 873      |\n",
      "|    total_timesteps | 596000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=597000, episode_reward=787.12 +/- 44.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 787      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598000, episode_reward=752.05 +/- 33.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 752      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 598000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=599000, episode_reward=948.50 +/- 35.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 949      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 599000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.05     |\n",
      "|    critic_loss     | 0.456    |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | 1.64     |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2920     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=600000, episode_reward=921.82 +/- 39.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 922      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 575      |\n",
      "| time/              |          |\n",
      "|    episodes        | 600      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 879      |\n",
      "|    total_timesteps | 600000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=601000, episode_reward=988.17 +/- 23.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 988      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 601000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.832    |\n",
      "|    critic_loss     | 0.541    |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | 4.1      |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2930     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=602000, episode_reward=974.50 +/- 35.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 974      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 602000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=603000, episode_reward=716.03 +/- 32.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 716      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 603000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.249    |\n",
      "|    critic_loss     | 0.537    |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 3.97     |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=760.56 +/- 33.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 761      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 604000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 592      |\n",
      "| time/              |          |\n",
      "|    episodes        | 604      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 884      |\n",
      "|    total_timesteps | 604000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=731.77 +/- 44.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 732      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 605000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.45     |\n",
      "|    critic_loss     | 0.597    |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 0.815    |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=764.23 +/- 47.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 764      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 606000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=607000, episode_reward=995.84 +/- 89.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 996      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 607000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.81     |\n",
      "|    critic_loss     | 0.524    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -0.541   |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 2960     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=608000, episode_reward=989.57 +/- 30.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 990      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 608000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 600      |\n",
      "| time/              |          |\n",
      "|    episodes        | 608      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 890      |\n",
      "|    total_timesteps | 608000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=609000, episode_reward=798.44 +/- 29.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 798      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 609000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0181   |\n",
      "|    critic_loss     | 0.579    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 2.34     |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 2970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=787.36 +/- 29.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 787      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=611000, episode_reward=655.80 +/- 31.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 656      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 611000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.16     |\n",
      "|    critic_loss     | 0.538    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 3.67     |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 2980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=647.74 +/- 16.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 648      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 612000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 616      |\n",
      "| time/              |          |\n",
      "|    episodes        | 612      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 896      |\n",
      "|    total_timesteps | 612000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=613000, episode_reward=837.66 +/- 26.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 838      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 613000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.68     |\n",
      "|    critic_loss     | 0.577    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | 2.58     |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 2990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=614000, episode_reward=843.52 +/- 21.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 844      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 614000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-244.71 +/- 3.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 615000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.861    |\n",
      "|    critic_loss     | 0.62     |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 5.16     |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=234.46 +/- 592.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 234      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 616000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 623      |\n",
      "| time/              |          |\n",
      "|    episodes        | 616      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 902      |\n",
      "|    total_timesteps | 616000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=617000, episode_reward=712.16 +/- 32.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 712      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 617000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.15     |\n",
      "|    critic_loss     | 0.683    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | 3.11     |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=707.75 +/- 33.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 708      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 618000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=619000, episode_reward=570.81 +/- 36.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 571      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 619000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.78     |\n",
      "|    critic_loss     | 0.662    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | 5.71     |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=574.60 +/- 42.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 575      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 630      |\n",
      "| time/              |          |\n",
      "|    episodes        | 620      |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 907      |\n",
      "|    total_timesteps | 620000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=621000, episode_reward=930.77 +/- 9.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 931      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 621000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.29     |\n",
      "|    critic_loss     | 0.536    |\n",
      "|    ent_coef        | 0.0112   |\n",
      "|    ent_coef_loss   | 3.02     |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=622000, episode_reward=927.79 +/- 20.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 928      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=623000, episode_reward=981.45 +/- 31.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 981      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 623000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.518    |\n",
      "|    critic_loss     | 0.602    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=999.09 +/- 48.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 999      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 624000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 646      |\n",
      "| time/              |          |\n",
      "|    episodes        | 624      |\n",
      "|    fps             | 683      |\n",
      "|    time_elapsed    | 913      |\n",
      "|    total_timesteps | 624000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=811.45 +/- 25.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 811      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 625000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.265    |\n",
      "|    critic_loss     | 0.692    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | 10.6     |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626000, episode_reward=782.93 +/- 46.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 783      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 626000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=627000, episode_reward=838.31 +/- 17.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 838      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 627000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.01     |\n",
      "|    critic_loss     | 0.645    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=809.05 +/- 20.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 809      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 665      |\n",
      "| time/              |          |\n",
      "|    episodes        | 628      |\n",
      "|    fps             | 683      |\n",
      "|    time_elapsed    | 919      |\n",
      "|    total_timesteps | 628000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=629000, episode_reward=810.32 +/- 26.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 810      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 629000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.05     |\n",
      "|    critic_loss     | 0.642    |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | 4.19     |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=785.37 +/- 44.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 785      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=631000, episode_reward=878.15 +/- 36.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 878      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 631000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.904    |\n",
      "|    critic_loss     | 0.652    |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | 3.04     |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=860.11 +/- 34.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 860      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 632000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 673      |\n",
      "| time/              |          |\n",
      "|    episodes        | 632      |\n",
      "|    fps             | 683      |\n",
      "|    time_elapsed    | 924      |\n",
      "|    total_timesteps | 632000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=633000, episode_reward=775.31 +/- 53.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 775      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 633000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.952    |\n",
      "|    critic_loss     | 0.713    |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 0.419    |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=634000, episode_reward=768.30 +/- 25.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 768      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=772.91 +/- 61.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 773      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 635000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.2      |\n",
      "|    critic_loss     | 0.648    |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 2.48     |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=768.49 +/- 68.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 768      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 636000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 690      |\n",
      "| time/              |          |\n",
      "|    episodes        | 636      |\n",
      "|    fps             | 683      |\n",
      "|    time_elapsed    | 929      |\n",
      "|    total_timesteps | 636000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=637000, episode_reward=869.84 +/- 72.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 870      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 637000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.862    |\n",
      "|    critic_loss     | 0.67     |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 6.34     |\n",
      "|    learning_rate   | 0.00436  |\n",
      "|    n_updates       | 3110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638000, episode_reward=819.70 +/- 49.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 820      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 638000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=639000, episode_reward=765.16 +/- 39.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 765      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 639000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.809    |\n",
      "|    critic_loss     | 0.641    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.00436  |\n",
      "|    n_updates       | 3120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=781.13 +/- 33.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 781      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 701      |\n",
      "| time/              |          |\n",
      "|    episodes        | 640      |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 935      |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641000, episode_reward=787.73 +/- 51.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 788      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 641000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=925.42 +/- 92.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 925       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 642000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.27      |\n",
      "|    critic_loss     | 0.628     |\n",
      "|    ent_coef        | 0.0124    |\n",
      "|    ent_coef_loss   | -0.000708 |\n",
      "|    learning_rate   | 0.00436   |\n",
      "|    n_updates       | 3130      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=643000, episode_reward=869.62 +/- 36.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 870      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 643000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=928.89 +/- 44.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 929      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 644000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.07     |\n",
      "|    critic_loss     | 0.724    |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | 6.34     |\n",
      "|    learning_rate   | 0.00436  |\n",
      "|    n_updates       | 3140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 708      |\n",
      "| time/              |          |\n",
      "|    episodes        | 644      |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 940      |\n",
      "|    total_timesteps | 644000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=947.88 +/- 46.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 948      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 645000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=646000, episode_reward=921.79 +/- 36.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 922      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 646000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.518    |\n",
      "|    critic_loss     | 0.644    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 0.597    |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=647000, episode_reward=937.68 +/- 38.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 938      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 647000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=923.34 +/- 52.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 923      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 648000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.3      |\n",
      "|    critic_loss     | 0.661    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -0.337   |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3160     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 718      |\n",
      "| time/              |          |\n",
      "|    episodes        | 648      |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 946      |\n",
      "|    total_timesteps | 648000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649000, episode_reward=895.50 +/- 60.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 895      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 649000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-189.62 +/- 3.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.177    |\n",
      "|    critic_loss     | 0.67     |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | 2.06     |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=651000, episode_reward=58.62 +/- 498.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 58.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 651000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=-372.58 +/- 4.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 652000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.22     |\n",
      "|    critic_loss     | 0.663    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -2.89    |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3180     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 719      |\n",
      "| time/              |          |\n",
      "|    episodes        | 652      |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 952      |\n",
      "|    total_timesteps | 652000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=653000, episode_reward=-372.50 +/- 3.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 653000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=1093.24 +/- 41.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 654000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.8      |\n",
      "|    critic_loss     | 0.16     |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | -24.2    |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3190     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=655000, episode_reward=1110.68 +/- 33.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 655000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=656000, episode_reward=-1095.93 +/- 438.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 656000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.211   |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | 6.67     |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3200     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 716      |\n",
      "| time/              |          |\n",
      "|    episodes        | 656      |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 957      |\n",
      "|    total_timesteps | 656000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=657000, episode_reward=-864.62 +/- 377.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -865     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 657000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=658000, episode_reward=301.66 +/- 209.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 658000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.39     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 4.79     |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=659000, episode_reward=387.72 +/- 158.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 388      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 659000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=310.38 +/- 183.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 310      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.15     |\n",
      "|    critic_loss     | 0.931    |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | 0.347    |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3220     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 703      |\n",
      "| time/              |          |\n",
      "|    episodes        | 660      |\n",
      "|    fps             | 685      |\n",
      "|    time_elapsed    | 963      |\n",
      "|    total_timesteps | 660000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=661000, episode_reward=146.37 +/- 217.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 146      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 661000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=662000, episode_reward=-341.95 +/- 3.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -342     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 662000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5        |\n",
      "|    critic_loss     | 0.712    |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | -5.85    |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663000, episode_reward=-340.66 +/- 4.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -341     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 663000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=-192.57 +/- 2.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -193     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 664000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.73     |\n",
      "|    critic_loss     | 0.477    |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | -19.9    |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3240     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 682      |\n",
      "| time/              |          |\n",
      "|    episodes        | 664      |\n",
      "|    fps             | 685      |\n",
      "|    time_elapsed    | 969      |\n",
      "|    total_timesteps | 664000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=-192.06 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=666000, episode_reward=-421.37 +/- 40.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -421     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 666000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.67     |\n",
      "|    critic_loss     | 0.296    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -17.7    |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=667000, episode_reward=-443.04 +/- 2.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -443     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 667000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=-76.00 +/- 5.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -76      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 668000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.86     |\n",
      "|    critic_loss     | 0.938    |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | -7.22    |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3260     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 650      |\n",
      "| time/              |          |\n",
      "|    episodes        | 668      |\n",
      "|    fps             | 685      |\n",
      "|    time_elapsed    | 974      |\n",
      "|    total_timesteps | 668000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669000, episode_reward=-77.62 +/- 2.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 669000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=256.75 +/- 596.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 257      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.59     |\n",
      "|    critic_loss     | 0.498    |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | -1.31    |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=671000, episode_reward=-20.21 +/- 544.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=-38.36 +/- 459.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -38.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 672000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.16     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | -1.42    |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3280     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 616      |\n",
      "| time/              |          |\n",
      "|    episodes        | 672      |\n",
      "|    fps             | 685      |\n",
      "|    time_elapsed    | 980      |\n",
      "|    total_timesteps | 672000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=673000, episode_reward=474.09 +/- 409.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 474      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 673000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=674000, episode_reward=-267.40 +/- 1.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 674000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.6      |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | 3.36     |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-266.78 +/- 1.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 675000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=-500.51 +/- 1.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -501     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 676000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.29     |\n",
      "|    critic_loss     | 0.549    |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 27.4     |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3300     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 590      |\n",
      "| time/              |          |\n",
      "|    episodes        | 676      |\n",
      "|    fps             | 685      |\n",
      "|    time_elapsed    | 986      |\n",
      "|    total_timesteps | 676000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=677000, episode_reward=-499.65 +/- 1.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -500     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=-483.25 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -483     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 678000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.33     |\n",
      "|    critic_loss     | 0.122    |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | 7.66     |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=679000, episode_reward=-482.89 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -483     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 679000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-522.36 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -522     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 0.359    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | 12.6     |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3320     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 542      |\n",
      "| time/              |          |\n",
      "|    episodes        | 680      |\n",
      "|    fps             | 685      |\n",
      "|    time_elapsed    | 991      |\n",
      "|    total_timesteps | 680000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681000, episode_reward=-522.05 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -522     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 681000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=682000, episode_reward=-422.15 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -422     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 682000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.19     |\n",
      "|    critic_loss     | 0.211    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | 15.1     |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=683000, episode_reward=-420.64 +/- 1.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -421     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=-420.57 +/- 2.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -421     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 684000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 503      |\n",
      "| time/              |          |\n",
      "|    episodes        | 684      |\n",
      "|    fps             | 685      |\n",
      "|    time_elapsed    | 997      |\n",
      "|    total_timesteps | 684000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=-453.08 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -453     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 685000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.94     |\n",
      "|    critic_loss     | 0.223    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | 21.8     |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686000, episode_reward=-451.95 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -452     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 686000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=687000, episode_reward=-543.14 +/- 1.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -543     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 687000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.92     |\n",
      "|    critic_loss     | 0.0526   |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 11       |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=-542.15 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -542     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 459      |\n",
      "| time/              |          |\n",
      "|    episodes        | 688      |\n",
      "|    fps             | 685      |\n",
      "|    time_elapsed    | 1002     |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=689000, episode_reward=-484.07 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -484     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 689000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.56     |\n",
      "|    critic_loss     | 0.0637   |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 12       |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-483.20 +/- 1.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -483     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=691000, episode_reward=-438.45 +/- 1.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -438     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 691000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.35     |\n",
      "|    critic_loss     | 0.0655   |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 14.6     |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=-437.81 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -438     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 692000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 415      |\n",
      "| time/              |          |\n",
      "|    episodes        | 692      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 1008     |\n",
      "|    total_timesteps | 692000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=693000, episode_reward=-545.16 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -545     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 693000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.51     |\n",
      "|    critic_loss     | 0.0443   |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 9.57     |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=694000, episode_reward=-544.84 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -545     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 694000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=-509.29 +/- 1.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -509     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 695000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 0.0346   |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 4.32     |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=-506.90 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -507     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 696000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 365      |\n",
      "| time/              |          |\n",
      "|    episodes        | 696      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 1014     |\n",
      "|    total_timesteps | 696000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=697000, episode_reward=-468.27 +/- 1.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -468     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 697000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.61     |\n",
      "|    critic_loss     | 0.0388   |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | 5.06     |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=698000, episode_reward=-466.86 +/- 1.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -467     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 698000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699000, episode_reward=-467.44 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -467     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 699000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.54     |\n",
      "|    critic_loss     | 0.0316   |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | -1.39    |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-466.94 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -467     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 316      |\n",
      "| time/              |          |\n",
      "|    episodes        | 700      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 1019     |\n",
      "|    total_timesteps | 700000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=701000, episode_reward=-418.66 +/- 1.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -419     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 701000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.54     |\n",
      "|    critic_loss     | 0.0206   |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -0.964   |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=702000, episode_reward=-417.25 +/- 2.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -417     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 702000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=703000, episode_reward=-434.27 +/- 2.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -434     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 703000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.45     |\n",
      "|    critic_loss     | 0.0357   |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -3.28    |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=-432.33 +/- 2.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -432     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 704000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 268      |\n",
      "| time/              |          |\n",
      "|    episodes        | 704      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 1025     |\n",
      "|    total_timesteps | 704000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=-383.17 +/- 2.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 705000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.39     |\n",
      "|    critic_loss     | 0.029    |\n",
      "|    ent_coef        | 0.0158   |\n",
      "|    ent_coef_loss   | -6.13    |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=706000, episode_reward=-382.07 +/- 1.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 706000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=707000, episode_reward=-365.23 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -365     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 707000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.3      |\n",
      "|    critic_loss     | 0.0333   |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -5.98    |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=-364.34 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -364     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 222      |\n",
      "| time/              |          |\n",
      "|    episodes        | 708      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 1031     |\n",
      "|    total_timesteps | 708000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=709000, episode_reward=-347.80 +/- 1.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -348     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 709000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.17     |\n",
      "|    critic_loss     | 0.0377   |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -7.04    |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-346.81 +/- 1.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=711000, episode_reward=-353.10 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 711000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.1      |\n",
      "|    critic_loss     | 0.0369   |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -5.85    |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=-351.95 +/- 2.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -352     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 712000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 181      |\n",
      "| time/              |          |\n",
      "|    episodes        | 712      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 1036     |\n",
      "|    total_timesteps | 712000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=713000, episode_reward=-417.81 +/- 2.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -418     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 713000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.16     |\n",
      "|    critic_loss     | 0.0256   |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -2.21    |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=-417.15 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -417     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=-376.47 +/- 1.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 715000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.48     |\n",
      "|    critic_loss     | 0.0199   |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -3.06    |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=-375.58 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 716000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 136      |\n",
      "| time/              |          |\n",
      "|    episodes        | 716      |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 1042     |\n",
      "|    total_timesteps | 716000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=717000, episode_reward=-395.11 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -395     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 717000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.41     |\n",
      "|    critic_loss     | 0.0339   |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -1.93    |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=718000, episode_reward=-393.61 +/- 1.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -394     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 718000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=719000, episode_reward=-361.31 +/- 6.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -361     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 719000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.13     |\n",
      "|    critic_loss     | 0.0262   |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | -0.385   |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-350.05 +/- 7.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 96       |\n",
      "| time/              |          |\n",
      "|    episodes        | 720      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1047     |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=721000, episode_reward=-320.75 +/- 5.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -321     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 721000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.35     |\n",
      "|    critic_loss     | 0.0321   |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -2.26    |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=722000, episode_reward=-317.01 +/- 5.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -317     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 722000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=723000, episode_reward=-398.40 +/- 2.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -398     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 723000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.23     |\n",
      "|    critic_loss     | 0.0353   |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -2.3     |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=-397.11 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -397     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 724000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 48.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 724      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1053     |\n",
      "|    total_timesteps | 724000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=-359.77 +/- 1.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -360     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 725000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.46     |\n",
      "|    critic_loss     | 0.0338   |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -0.814   |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=-357.99 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -358     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727000, episode_reward=-358.06 +/- 2.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -358     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 727000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=-373.32 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 728000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.34     |\n",
      "|    critic_loss     | 0.028    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -2.2     |\n",
      "|    learning_rate   | 0.00427  |\n",
      "|    n_updates       | 3550     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.3      |\n",
      "| time/              |          |\n",
      "|    episodes        | 728      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1059     |\n",
      "|    total_timesteps | 728000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729000, episode_reward=-372.52 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 729000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-334.43 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -334     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 0.025    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -2.12    |\n",
      "|    learning_rate   | 0.00427  |\n",
      "|    n_updates       | 3560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=731000, episode_reward=-334.24 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -334     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 731000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=-343.67 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -344     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 732000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.26     |\n",
      "|    critic_loss     | 0.0295   |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -3.26    |\n",
      "|    learning_rate   | 0.00427  |\n",
      "|    n_updates       | 3570     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -42      |\n",
      "| time/              |          |\n",
      "|    episodes        | 732      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1064     |\n",
      "|    total_timesteps | 732000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=733000, episode_reward=-342.15 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -342     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 733000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=734000, episode_reward=-309.56 +/- 2.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -310     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 734000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.27     |\n",
      "|    critic_loss     | 0.0271   |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -3.14    |\n",
      "|    learning_rate   | 0.00427  |\n",
      "|    n_updates       | 3580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=-308.06 +/- 1.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 735000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=-329.34 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -329     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 736000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.28     |\n",
      "|    critic_loss     | 0.0362   |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -2.75    |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3590     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -85.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 736      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1070     |\n",
      "|    total_timesteps | 736000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=737000, episode_reward=-326.96 +/- 1.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -327     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 737000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=-312.99 +/- 1.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -313     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 738000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.2      |\n",
      "|    critic_loss     | 0.0317   |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | -0.896   |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=739000, episode_reward=-312.04 +/- 1.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -312     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 739000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-312.88 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -313     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.29     |\n",
      "|    critic_loss     | 0.0343   |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | -2.85    |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3610     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -128     |\n",
      "| time/              |          |\n",
      "|    episodes        | 740      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1076     |\n",
      "|    total_timesteps | 740000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741000, episode_reward=-312.52 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -313     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 741000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=742000, episode_reward=-305.16 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -305     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 742000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.32     |\n",
      "|    critic_loss     | 0.0286   |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=743000, episode_reward=-304.98 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -305     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 743000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=-310.70 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -311     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 744000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.26     |\n",
      "|    critic_loss     | 0.0286   |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | -2.22    |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3630     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -170     |\n",
      "| time/              |          |\n",
      "|    episodes        | 744      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1081     |\n",
      "|    total_timesteps | 744000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=-310.77 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -311     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 745000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=746000, episode_reward=-215.14 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 746000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.32     |\n",
      "|    critic_loss     | 0.0373   |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -3.35    |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=747000, episode_reward=-215.26 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 747000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=-214.38 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -214     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 748000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.28     |\n",
      "|    critic_loss     | 0.0365   |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | -2.68    |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3650     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -215     |\n",
      "| time/              |          |\n",
      "|    episodes        | 748      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1087     |\n",
      "|    total_timesteps | 748000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749000, episode_reward=-213.99 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -214     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 749000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-218.02 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.3      |\n",
      "|    critic_loss     | 0.0466   |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | -3.11    |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=751000, episode_reward=-217.48 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -217     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=-383.17 +/- 1.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 752000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.34     |\n",
      "|    critic_loss     | 0.0593   |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3670     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -245     |\n",
      "| time/              |          |\n",
      "|    episodes        | 752      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1092     |\n",
      "|    total_timesteps | 752000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753000, episode_reward=-381.89 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 753000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=754000, episode_reward=-402.53 +/- 1.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 754000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.72     |\n",
      "|    critic_loss     | 0.0511   |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | 4.61     |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=-402.13 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 755000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=-516.19 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -516     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 756000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 0.0357   |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 17.5     |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3690     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -281     |\n",
      "| time/              |          |\n",
      "|    episodes        | 756      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1098     |\n",
      "|    total_timesteps | 756000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=757000, episode_reward=-516.29 +/- 1.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -516     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=758000, episode_reward=-479.86 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -480     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 758000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.15     |\n",
      "|    critic_loss     | 0.016    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 15.5     |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=759000, episode_reward=-479.63 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -480     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 759000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-514.42 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -514     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.13     |\n",
      "|    critic_loss     | 0.0392   |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 14.4     |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3710     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -313     |\n",
      "| time/              |          |\n",
      "|    episodes        | 760      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1104     |\n",
      "|    total_timesteps | 760000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=761000, episode_reward=-513.60 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -514     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 761000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=-519.96 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -520     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 762000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.28     |\n",
      "|    critic_loss     | 0.022    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 14.3     |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=763000, episode_reward=-519.80 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -520     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=-513.38 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -513     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 764000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.29     |\n",
      "|    critic_loss     | 0.0219   |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 8.83     |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3730     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -330     |\n",
      "| time/              |          |\n",
      "|    episodes        | 764      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1109     |\n",
      "|    total_timesteps | 764000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=-511.69 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -512     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 765000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766000, episode_reward=-494.71 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -495     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 766000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.3      |\n",
      "|    critic_loss     | 0.0307   |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | 5.62     |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=767000, episode_reward=-494.13 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -494     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 767000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=-492.11 +/- 4.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -492     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -340     |\n",
      "| time/              |          |\n",
      "|    episodes        | 768      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1115     |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=769000, episode_reward=-407.53 +/- 1.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -408     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 769000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.24     |\n",
      "|    critic_loss     | 0.0384   |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | 1.61     |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-407.69 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -408     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=771000, episode_reward=-210.75 +/- 3.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 771000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.31     |\n",
      "|    critic_loss     | 0.336    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=-210.58 +/- 2.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 772000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -336     |\n",
      "| time/              |          |\n",
      "|    episodes        | 772      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1121     |\n",
      "|    total_timesteps | 772000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=773000, episode_reward=-263.76 +/- 4.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -264     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 773000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.06     |\n",
      "|    critic_loss     | 0.565    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -16      |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=-246.64 +/- 31.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -247     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 774000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=-290.60 +/- 118.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -291     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 775000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.53     |\n",
      "|    critic_loss     | 0.524    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -8.52    |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=-338.09 +/- 118.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -338     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 776000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -343     |\n",
      "| time/              |          |\n",
      "|    episodes        | 776      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1126     |\n",
      "|    total_timesteps | 776000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=777000, episode_reward=-372.84 +/- 5.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 777000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.74     |\n",
      "|    critic_loss     | 0.224    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -8.56    |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778000, episode_reward=-377.09 +/- 13.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 778000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=779000, episode_reward=-473.50 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -474     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 779000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.62     |\n",
      "|    critic_loss     | 0.309    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-470.64 +/- 2.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -471     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -344     |\n",
      "| time/              |          |\n",
      "|    episodes        | 780      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1132     |\n",
      "|    total_timesteps | 780000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=781000, episode_reward=-302.91 +/- 60.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -303     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 781000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.7      |\n",
      "|    critic_loss     | 0.203    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 6.76     |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782000, episode_reward=-361.25 +/- 57.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -361     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 782000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=783000, episode_reward=129.17 +/- 425.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 129      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 783000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.65     |\n",
      "|    critic_loss     | 0.273    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -4.94    |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=-214.78 +/- 351.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 784000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -346     |\n",
      "| time/              |          |\n",
      "|    episodes        | 784      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1137     |\n",
      "|    total_timesteps | 784000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=124.63 +/- 230.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 125      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 785000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.31     |\n",
      "|    critic_loss     | 0.237    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -5.8     |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=-115.37 +/- 274.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 786000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=787000, episode_reward=-69.31 +/- 508.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 787000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.75     |\n",
      "|    critic_loss     | 0.353    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -5.46    |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=-40.31 +/- 494.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 788000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -325     |\n",
      "| time/              |          |\n",
      "|    episodes        | 788      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1143     |\n",
      "|    total_timesteps | 788000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=789000, episode_reward=194.03 +/- 488.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 194      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 789000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.59     |\n",
      "|    critic_loss     | 0.487    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -3.85    |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-188.61 +/- 421.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -189     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=791000, episode_reward=652.25 +/- 38.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 652      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 791000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.48     |\n",
      "|    critic_loss     | 0.331    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -3.99    |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=462.69 +/- 363.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 463      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 792000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -310     |\n",
      "| time/              |          |\n",
      "|    episodes        | 792      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1148     |\n",
      "|    total_timesteps | 792000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=793000, episode_reward=313.76 +/- 334.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 314      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 793000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 0.415    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | -5.4     |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794000, episode_reward=299.21 +/- 328.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 299      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=668.20 +/- 297.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 668      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 795000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.72     |\n",
      "|    critic_loss     | 0.323    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | -4.03    |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=754.86 +/- 61.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 755      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 796000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -277     |\n",
      "| time/              |          |\n",
      "|    episodes        | 796      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1154     |\n",
      "|    total_timesteps | 796000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=797000, episode_reward=163.58 +/- 392.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 164      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 797000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.81     |\n",
      "|    critic_loss     | 0.298    |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -4.28    |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=302.23 +/- 310.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 798000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=799000, episode_reward=187.19 +/- 444.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 187      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 799000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.48     |\n",
      "|    critic_loss     | 0.541    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | 3.75     |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=185.72 +/- 447.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 186      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -247     |\n",
      "| time/              |          |\n",
      "|    episodes        | 800      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1160     |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=801000, episode_reward=80.18 +/- 329.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 80.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 801000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.85     |\n",
      "|    critic_loss     | 0.37     |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | 3.94     |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802000, episode_reward=79.97 +/- 323.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 80       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 802000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=803000, episode_reward=104.13 +/- 305.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 803000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.09     |\n",
      "|    critic_loss     | 0.308    |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | 2.6      |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=-5.65 +/- 320.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.65    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 804000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -233     |\n",
      "| time/              |          |\n",
      "|    episodes        | 804      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1165     |\n",
      "|    total_timesteps | 804000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=-296.60 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -297     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 805000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.52     |\n",
      "|    critic_loss     | 0.155    |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806000, episode_reward=-297.69 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -298     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=807000, episode_reward=95.82 +/- 469.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 807000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.73     |\n",
      "|    critic_loss     | 0.343    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | -5.38    |\n",
      "|    learning_rate   | 0.00419  |\n",
      "|    n_updates       | 3940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=81.71 +/- 453.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 81.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 808000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 808      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1171     |\n",
      "|    total_timesteps | 808000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=809000, episode_reward=519.65 +/- 403.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 520      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 809000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.37     |\n",
      "|    critic_loss     | 0.392    |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | 2.2      |\n",
      "|    learning_rate   | 0.00419  |\n",
      "|    n_updates       | 3950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=114.96 +/- 489.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 115      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=811000, episode_reward=320.92 +/- 496.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 811000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=585.16 +/- 18.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 585      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 812000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.87     |\n",
      "|    critic_loss     | 0.585    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | -3.67    |\n",
      "|    learning_rate   | 0.00419  |\n",
      "|    n_updates       | 3960     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    episodes        | 812      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1176     |\n",
      "|    total_timesteps | 812000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813000, episode_reward=597.68 +/- 44.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 598      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 813000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=814000, episode_reward=701.21 +/- 18.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 701      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 814000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.14     |\n",
      "|    critic_loss     | 0.54     |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 2.25     |\n",
      "|    learning_rate   | 0.00419  |\n",
      "|    n_updates       | 3970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=715.74 +/- 41.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 716      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 815000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=682.89 +/- 34.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 683      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 816000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.31     |\n",
      "|    critic_loss     | 0.432    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 0.0869   |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 3980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    episodes        | 816      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1182     |\n",
      "|    total_timesteps | 816000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=817000, episode_reward=689.06 +/- 34.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 689      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 817000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818000, episode_reward=600.71 +/- 17.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 601      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 818000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.54     |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -7.41    |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 3990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=819000, episode_reward=432.05 +/- 356.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 432      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 819000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=78.13 +/- 445.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 78.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.29     |\n",
      "|    critic_loss     | 0.421    |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | -0.462   |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -106     |\n",
      "| time/              |          |\n",
      "|    episodes        | 820      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1187     |\n",
      "|    total_timesteps | 820000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=821000, episode_reward=430.76 +/- 358.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 821000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=768.58 +/- 28.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 769      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 822000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.92     |\n",
      "|    critic_loss     | 0.439    |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 4010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=823000, episode_reward=780.16 +/- 17.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 780      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 823000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=-340.24 +/- 1.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 824000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.756    |\n",
      "|    critic_loss     | 0.457    |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 4.56     |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 4020     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -62.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 824      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1193     |\n",
      "|    total_timesteps | 824000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-117.11 +/- 449.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 825000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826000, episode_reward=829.73 +/- 29.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 830      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 826000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.31     |\n",
      "|    critic_loss     | 0.615    |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | 8.85     |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=827000, episode_reward=615.18 +/- 444.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 615      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 827000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=538.74 +/- 60.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 539      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 828000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.756    |\n",
      "|    critic_loss     | 0.455    |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | 2.59     |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4040     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -19.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 828      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1198     |\n",
      "|    total_timesteps | 828000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=829000, episode_reward=515.61 +/- 67.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 516      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 829000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=628.77 +/- 50.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 629      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.79     |\n",
      "|    critic_loss     | 0.457    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 1.79     |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=831000, episode_reward=632.23 +/- 14.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 632      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 831000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=865.79 +/- 39.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 866      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 832000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.79     |\n",
      "|    critic_loss     | 0.422    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -0.513   |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4060     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 15.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 832      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1205     |\n",
      "|    total_timesteps | 832000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=833000, episode_reward=873.65 +/- 31.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 874      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 833000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=851.30 +/- 38.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 851      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 834000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.04     |\n",
      "|    critic_loss     | 0.489    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=604.10 +/- 443.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 604      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 835000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=836000, episode_reward=713.32 +/- 21.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 713      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 836000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.25     |\n",
      "|    critic_loss     | 0.545    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | 3.79     |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4080     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 57.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 836      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1210     |\n",
      "|    total_timesteps | 836000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=837000, episode_reward=675.54 +/- 38.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 676      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838000, episode_reward=832.31 +/- 41.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 832      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 838000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.88     |\n",
      "|    critic_loss     | 0.468    |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | 3.17     |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=839000, episode_reward=831.35 +/- 41.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 831      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 839000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=791.62 +/- 528.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 792      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.27     |\n",
      "|    critic_loss     | 0.437    |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4100     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 98.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 840      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1216     |\n",
      "|    total_timesteps | 840000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=841000, episode_reward=775.58 +/- 520.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 776      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 841000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842000, episode_reward=-285.78 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -286     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 842000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0512  |\n",
      "|    critic_loss     | 0.548    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 5.16     |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=843000, episode_reward=81.04 +/- 451.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 81       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=1001.33 +/- 27.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 844000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.37     |\n",
      "|    critic_loss     | 0.624    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | 1.79     |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4120     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 140      |\n",
      "| time/              |          |\n",
      "|    episodes        | 844      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1222     |\n",
      "|    total_timesteps | 844000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=1014.07 +/- 27.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 845000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=-41.19 +/- 359.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 846000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.303    |\n",
      "|    critic_loss     | 0.599    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 0.333    |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=847000, episode_reward=139.98 +/- 474.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 140      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 847000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=597.12 +/- 39.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 597      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 848000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.68     |\n",
      "|    critic_loss     | 0.843    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 4.67     |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 181      |\n",
      "| time/              |          |\n",
      "|    episodes        | 848      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1227     |\n",
      "|    total_timesteps | 848000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849000, episode_reward=618.20 +/- 60.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 618      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-47.76 +/- 401.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -47.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.19     |\n",
      "|    critic_loss     | 0.576    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.951    |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=851000, episode_reward=334.34 +/- 477.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 334      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 851000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=155.65 +/- 479.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 156      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 852000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.54     |\n",
      "|    critic_loss     | 0.568    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -7.49    |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4160     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 217      |\n",
      "| time/              |          |\n",
      "|    episodes        | 852      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1233     |\n",
      "|    total_timesteps | 852000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=853000, episode_reward=-36.47 +/- 395.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 853000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854000, episode_reward=-17.90 +/- 430.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -17.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 854000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=185.16 +/- 563.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 185      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 855000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.82     |\n",
      "|    critic_loss     | 0.646    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -2.14    |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=155.77 +/- 525.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 156      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 856000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 262      |\n",
      "| time/              |          |\n",
      "|    episodes        | 856      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1239     |\n",
      "|    total_timesteps | 856000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=857000, episode_reward=865.80 +/- 34.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 866      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 857000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.58     |\n",
      "|    critic_loss     | 0.588    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 1.46     |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=869.69 +/- 29.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 870      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 858000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=859000, episode_reward=819.73 +/- 30.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 820      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 859000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.597    |\n",
      "|    critic_loss     | 0.583    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 2.19     |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=849.42 +/- 41.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 849      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 314      |\n",
      "| time/              |          |\n",
      "|    episodes        | 860      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1244     |\n",
      "|    total_timesteps | 860000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=861000, episode_reward=684.38 +/- 37.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 684      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 861000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.02     |\n",
      "|    critic_loss     | 0.52     |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 0.48     |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862000, episode_reward=718.46 +/- 39.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 718      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 862000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=863000, episode_reward=966.08 +/- 27.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 966      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 863000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.61     |\n",
      "|    critic_loss     | 0.579    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 3.01     |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=962.72 +/- 19.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 963      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 864000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 365      |\n",
      "| time/              |          |\n",
      "|    episodes        | 864      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1254     |\n",
      "|    total_timesteps | 864000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=998.95 +/- 32.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 999      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 865000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.246    |\n",
      "|    critic_loss     | 0.595    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -0.368   |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866000, episode_reward=1014.35 +/- 29.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 866000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=867000, episode_reward=928.90 +/- 34.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 929      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 867000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.113    |\n",
      "|    critic_loss     | 0.681    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=937.01 +/- 37.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 937      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 868000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 420      |\n",
      "| time/              |          |\n",
      "|    episodes        | 868      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1260     |\n",
      "|    total_timesteps | 868000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=869000, episode_reward=926.36 +/- 30.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 926      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 869000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.867    |\n",
      "|    critic_loss     | 0.595    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.838   |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=893.06 +/- 49.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 893      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=871000, episode_reward=926.04 +/- 18.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 926      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 871000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.526    |\n",
      "|    critic_loss     | 0.554    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.0228  |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=916.40 +/- 40.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 916      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 872000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 456      |\n",
      "| time/              |          |\n",
      "|    episodes        | 872      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1265     |\n",
      "|    total_timesteps | 872000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=873000, episode_reward=747.38 +/- 499.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 747      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 873000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.394    |\n",
      "|    critic_loss     | 0.514    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -1.83    |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874000, episode_reward=1013.21 +/- 51.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 874000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=990.93 +/- 16.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 991      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 875000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0874   |\n",
      "|    critic_loss     | 0.553    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -0.151   |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=751.04 +/- 494.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 751      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 876000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 499      |\n",
      "| time/              |          |\n",
      "|    episodes        | 876      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1271     |\n",
      "|    total_timesteps | 876000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=877000, episode_reward=948.68 +/- 30.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 949      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 877000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.276    |\n",
      "|    critic_loss     | 0.589    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.637    |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=878000, episode_reward=962.43 +/- 24.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 962      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 878000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=879000, episode_reward=904.66 +/- 22.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 905      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 879000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.456    |\n",
      "|    critic_loss     | 0.686    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.727    |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=899.53 +/- 24.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 900      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 554      |\n",
      "| time/              |          |\n",
      "|    episodes        | 880      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1276     |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=881000, episode_reward=941.66 +/- 10.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 942      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 881000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.26     |\n",
      "|    critic_loss     | 0.712    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.843    |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882000, episode_reward=947.17 +/- 54.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 947      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 882000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=883000, episode_reward=978.14 +/- 27.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 978      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 883000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.375    |\n",
      "|    critic_loss     | 0.609    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.061   |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=1012.09 +/- 27.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 884000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 608      |\n",
      "| time/              |          |\n",
      "|    episodes        | 884      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1282     |\n",
      "|    total_timesteps | 884000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=942.08 +/- 31.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 942      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 885000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.279    |\n",
      "|    critic_loss     | 0.593    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -3.67    |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=886000, episode_reward=971.79 +/- 31.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 972      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887000, episode_reward=985.60 +/- 17.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 986      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 887000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.777    |\n",
      "|    critic_loss     | 0.666    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -0.231   |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=1005.53 +/- 13.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 888000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 643      |\n",
      "| time/              |          |\n",
      "|    episodes        | 888      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1287     |\n",
      "|    total_timesteps | 888000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=889000, episode_reward=1106.93 +/- 26.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 889000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.165    |\n",
      "|    critic_loss     | 0.619    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.3     |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=1087.27 +/- 12.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=891000, episode_reward=993.13 +/- 40.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 993      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 891000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.183   |\n",
      "|    critic_loss     | 0.651    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 0.448    |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=980.87 +/- 17.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 981      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 684      |\n",
      "| time/              |          |\n",
      "|    episodes        | 892      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1293     |\n",
      "|    total_timesteps | 892000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=893000, episode_reward=1041.40 +/- 17.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 893000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.565    |\n",
      "|    critic_loss     | 0.698    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 1.49     |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=1015.19 +/- 25.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 894000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=1009.69 +/- 35.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 895000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.238    |\n",
      "|    critic_loss     | 0.717    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=974.86 +/- 28.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 975      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 709      |\n",
      "| time/              |          |\n",
      "|    episodes        | 896      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1298     |\n",
      "|    total_timesteps | 896000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=897000, episode_reward=1012.60 +/- 42.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 897000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=898000, episode_reward=1009.12 +/- 35.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 898000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.455    |\n",
      "|    critic_loss     | 0.742    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 0.779    |\n",
      "|    learning_rate   | 0.0041   |\n",
      "|    n_updates       | 4380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899000, episode_reward=1019.20 +/- 18.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 899000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-243.37 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.079    |\n",
      "|    critic_loss     | 0.665    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0041   |\n",
      "|    n_updates       | 4390     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 736      |\n",
      "| time/              |          |\n",
      "|    episodes        | 900      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1304     |\n",
      "|    total_timesteps | 900000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=901000, episode_reward=-243.41 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 901000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=902000, episode_reward=-232.68 +/- 2.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 902000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.196    |\n",
      "|    critic_loss     | 0.794    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -1.75    |\n",
      "|    learning_rate   | 0.0041   |\n",
      "|    n_updates       | 4400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903000, episode_reward=505.21 +/- 607.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 505      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 903000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=851.41 +/- 547.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 851      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 904000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.443    |\n",
      "|    critic_loss     | 0.704    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 2.33     |\n",
      "|    learning_rate   | 0.0041   |\n",
      "|    n_updates       | 4410     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 778      |\n",
      "| time/              |          |\n",
      "|    episodes        | 904      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1309     |\n",
      "|    total_timesteps | 904000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=905000, episode_reward=1097.34 +/- 25.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 905000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=1021.02 +/- 38.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 906000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.418   |\n",
      "|    critic_loss     | 0.745    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -1.38    |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907000, episode_reward=1046.58 +/- 57.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 907000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=908000, episode_reward=-330.54 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -331     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 908000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.151   |\n",
      "|    critic_loss     | 0.732    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 1.8      |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4430     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 810      |\n",
      "| time/              |          |\n",
      "|    episodes        | 908      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1319     |\n",
      "|    total_timesteps | 908000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=909000, episode_reward=-329.82 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 909000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=-161.52 +/- 1.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.29     |\n",
      "|    critic_loss     | 0.76     |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -4.78    |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=911000, episode_reward=266.31 +/- 525.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 266      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 911000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=981.38 +/- 31.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 981      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 912000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.771    |\n",
      "|    critic_loss     | 0.823    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | 0.846    |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4450     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 817      |\n",
      "| time/              |          |\n",
      "|    episodes        | 912      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 1325     |\n",
      "|    total_timesteps | 912000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=913000, episode_reward=973.31 +/- 37.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 973      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 913000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=914000, episode_reward=1028.36 +/- 54.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 914000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.551    |\n",
      "|    critic_loss     | 0.841    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | 2.4      |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=1048.76 +/- 39.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 915000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=916000, episode_reward=966.99 +/- 19.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 967      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 916000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.321    |\n",
      "|    critic_loss     | 0.827    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4470     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 828      |\n",
      "| time/              |          |\n",
      "|    episodes        | 916      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1331     |\n",
      "|    total_timesteps | 916000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=917000, episode_reward=1016.08 +/- 33.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 917000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=1112.67 +/- 35.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 918000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.11     |\n",
      "|    critic_loss     | 0.755    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 5.67     |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4480     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=919000, episode_reward=1118.54 +/- 34.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 919000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=920000, episode_reward=1122.47 +/- 25.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.818    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.69    |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4490     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 857      |\n",
      "| time/              |          |\n",
      "|    episodes        | 920      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1336     |\n",
      "|    total_timesteps | 920000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921000, episode_reward=1111.35 +/- 23.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 921000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=922000, episode_reward=898.19 +/- 34.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 898      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 922000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 0.688    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -2.03    |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=923000, episode_reward=911.34 +/- 22.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 911      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=1127.68 +/- 32.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 924000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.522    |\n",
      "|    critic_loss     | 0.86     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4510     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 867      |\n",
      "| time/              |          |\n",
      "|    episodes        | 924      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1342     |\n",
      "|    total_timesteps | 924000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=1112.32 +/- 25.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 925000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=926000, episode_reward=1265.99 +/- 26.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 926000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.83    |\n",
      "|    critic_loss     | 0.714    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | 1.87     |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4520     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=927000, episode_reward=1250.95 +/- 37.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 927000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=871.39 +/- 15.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 871      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 928000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.11    |\n",
      "|    critic_loss     | 0.903    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 3.22     |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4530     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 883      |\n",
      "| time/              |          |\n",
      "|    episodes        | 928      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1348     |\n",
      "|    total_timesteps | 928000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=929000, episode_reward=887.55 +/- 17.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 888      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=865.68 +/- 4.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 866      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.751    |\n",
      "|    critic_loss     | 0.896    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -0.413   |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=931000, episode_reward=833.14 +/- 35.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 833      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 931000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=1104.29 +/- 24.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 932000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.951    |\n",
      "|    critic_loss     | 0.812    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4550     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 894      |\n",
      "| time/              |          |\n",
      "|    episodes        | 932      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1353     |\n",
      "|    total_timesteps | 932000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933000, episode_reward=1143.89 +/- 38.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 933000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=934000, episode_reward=1001.66 +/- 22.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 934000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.241   |\n",
      "|    critic_loss     | 0.896    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -2.01    |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=1067.97 +/- 53.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=785.08 +/- 20.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 785      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 936000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.101   |\n",
      "|    critic_loss     | 0.895    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -0.0343  |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4570     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 904      |\n",
      "| time/              |          |\n",
      "|    episodes        | 936      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1359     |\n",
      "|    total_timesteps | 936000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=937000, episode_reward=795.82 +/- 36.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 796      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 937000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=938000, episode_reward=1019.14 +/- 37.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 938000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.965    |\n",
      "|    critic_loss     | 0.889    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -0.743   |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939000, episode_reward=1010.29 +/- 34.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 939000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=1008.46 +/- 33.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 910      |\n",
      "| time/              |          |\n",
      "|    episodes        | 940      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1364     |\n",
      "|    total_timesteps | 940000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=941000, episode_reward=1010.70 +/- 14.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 941000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic_loss     | 0.815    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=1035.07 +/- 36.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 942000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=943000, episode_reward=1143.38 +/- 24.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 943000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0628  |\n",
      "|    critic_loss     | 0.885    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -0.764   |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=1076.18 +/- 31.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 944000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 920      |\n",
      "| time/              |          |\n",
      "|    episodes        | 944      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 1370     |\n",
      "|    total_timesteps | 944000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=1064.47 +/- 17.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 945000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.105   |\n",
      "|    critic_loss     | 0.895    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.71     |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=946000, episode_reward=1079.61 +/- 71.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 946000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947000, episode_reward=1133.57 +/- 57.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 947000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0828  |\n",
      "|    critic_loss     | 0.823    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=1060.05 +/- 80.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 948000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 931      |\n",
      "| time/              |          |\n",
      "|    episodes        | 948      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1375     |\n",
      "|    total_timesteps | 948000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=949000, episode_reward=1122.55 +/- 29.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 949000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.49    |\n",
      "|    critic_loss     | 0.869    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.802    |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=1088.67 +/- 17.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=951000, episode_reward=1210.75 +/- 33.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 951000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.608   |\n",
      "|    critic_loss     | 0.814    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -1.85    |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=1224.83 +/- 28.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 952000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 948      |\n",
      "| time/              |          |\n",
      "|    episodes        | 952      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1381     |\n",
      "|    total_timesteps | 952000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=953000, episode_reward=1161.57 +/- 42.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 953000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.19    |\n",
      "|    critic_loss     | 0.851    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.598   |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954000, episode_reward=1200.18 +/- 53.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 954000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=1193.55 +/- 34.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 955000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.733   |\n",
      "|    critic_loss     | 0.93     |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -0.622   |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=1165.89 +/- 44.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 956000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 964      |\n",
      "| time/              |          |\n",
      "|    episodes        | 956      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1386     |\n",
      "|    total_timesteps | 956000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=957000, episode_reward=1071.41 +/- 26.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 957000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.947   |\n",
      "|    critic_loss     | 0.939    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.625    |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=958000, episode_reward=786.89 +/- 535.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 787      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 958000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=959000, episode_reward=1068.57 +/- 26.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 959000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.297   |\n",
      "|    critic_loss     | 0.842    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 1.63     |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=1106.40 +/- 24.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 972      |\n",
      "| time/              |          |\n",
      "|    episodes        | 960      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1392     |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=961000, episode_reward=1059.81 +/- 32.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 961000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.398   |\n",
      "|    critic_loss     | 0.82     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.481    |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962000, episode_reward=1057.86 +/- 28.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 962000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=963000, episode_reward=1018.01 +/- 33.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 963000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.521   |\n",
      "|    critic_loss     | 0.73     |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | 3.01     |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=961.03 +/- 37.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 961      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 964000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 981      |\n",
      "| time/              |          |\n",
      "|    episodes        | 964      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1397     |\n",
      "|    total_timesteps | 964000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=1049.62 +/- 40.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 965000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.102   |\n",
      "|    critic_loss     | 0.691    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -0.169   |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=1057.61 +/- 32.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967000, episode_reward=1208.16 +/- 66.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 967000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.293   |\n",
      "|    critic_loss     | 0.766    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -0.966   |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=1204.50 +/- 23.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 968000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 988      |\n",
      "| time/              |          |\n",
      "|    episodes        | 968      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 1402     |\n",
      "|    total_timesteps | 968000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=969000, episode_reward=1214.94 +/- 32.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 969000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.848    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 3.3      |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=1203.70 +/- 22.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=971000, episode_reward=1182.28 +/- 39.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 971000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.953   |\n",
      "|    critic_loss     | 0.877    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 1.85     |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=1193.44 +/- 23.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 997      |\n",
      "| time/              |          |\n",
      "|    episodes        | 972      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1408     |\n",
      "|    total_timesteps | 972000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=973000, episode_reward=1022.65 +/- 37.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 973000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.08    |\n",
      "|    critic_loss     | 0.748    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974000, episode_reward=1040.29 +/- 40.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 974000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=915.39 +/- 21.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 915      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 975000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0547   |\n",
      "|    critic_loss     | 0.842    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 2.65     |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=935.33 +/- 44.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 935      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 976000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 998      |\n",
      "| time/              |          |\n",
      "|    episodes        | 976      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1413     |\n",
      "|    total_timesteps | 976000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=977000, episode_reward=1193.37 +/- 28.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 977000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.737    |\n",
      "|    critic_loss     | 0.677    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.0699   |\n",
      "|    learning_rate   | 0.00402  |\n",
      "|    n_updates       | 4770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=1194.62 +/- 44.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=979000, episode_reward=1047.75 +/- 18.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 979000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.734   |\n",
      "|    critic_loss     | 0.721    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 1.91     |\n",
      "|    learning_rate   | 0.00402  |\n",
      "|    n_updates       | 4780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=1104.72 +/- 19.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 980      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1419     |\n",
      "|    total_timesteps | 980000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=981000, episode_reward=785.23 +/- 42.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 785      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 981000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.454   |\n",
      "|    critic_loss     | 0.859    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 2.68     |\n",
      "|    learning_rate   | 0.00402  |\n",
      "|    n_updates       | 4790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982000, episode_reward=769.35 +/- 35.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 769      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 982000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=983000, episode_reward=765.75 +/- 23.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 766      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 983000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=1003.38 +/- 43.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 984000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.892    |\n",
      "|    critic_loss     | 0.942    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 3.31     |\n",
      "|    learning_rate   | 0.00402  |\n",
      "|    n_updates       | 4800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 984      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1424     |\n",
      "|    total_timesteps | 984000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=1024.42 +/- 20.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 985000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=986000, episode_reward=1219.54 +/- 60.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 986000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0128  |\n",
      "|    critic_loss     | 0.8      |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 2.52     |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987000, episode_reward=1220.96 +/- 42.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 987000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=1113.57 +/- 34.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 988000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.908    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 0.973    |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4820     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 988      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1430     |\n",
      "|    total_timesteps | 988000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=989000, episode_reward=1108.87 +/- 48.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 989000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=1215.76 +/- 42.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.955   |\n",
      "|    critic_loss     | 0.752    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -2.62    |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991000, episode_reward=1222.28 +/- 51.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 991000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=1081.89 +/- 26.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 992000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.19    |\n",
      "|    critic_loss     | 0.872    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | -2.26    |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4840     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 992      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 1435     |\n",
      "|    total_timesteps | 992000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=993000, episode_reward=1097.55 +/- 45.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 993000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=994000, episode_reward=922.18 +/- 34.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 922      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 994000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.324   |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 1.52     |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=962.67 +/- 19.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 963      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 995000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=1057.81 +/- 24.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 996000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.326    |\n",
      "|    critic_loss     | 0.989    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4860     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 996      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 1441     |\n",
      "|    total_timesteps | 996000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=997000, episode_reward=1079.45 +/- 59.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 997000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=998000, episode_reward=1178.02 +/- 29.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 998000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.396   |\n",
      "|    critic_loss     | 0.824    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 1.51     |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999000, episode_reward=1182.10 +/- 37.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 999000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=898.14 +/- 27.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 898      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.934    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 3.47     |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4880     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1000     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 1446     |\n",
      "|    total_timesteps | 1000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1001000, episode_reward=912.34 +/- 41.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 912      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1001000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1002000, episode_reward=794.37 +/- 46.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 794      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1002000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.937    |\n",
      "|    critic_loss     | 0.916    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 0.15     |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1003000, episode_reward=800.39 +/- 21.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 800      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1003000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1004000, episode_reward=1181.81 +/- 42.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1004000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.38     |\n",
      "|    critic_loss     | 0.865    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4900     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1004     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 1452     |\n",
      "|    total_timesteps | 1004000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1005000, episode_reward=1202.27 +/- 33.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1005000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1006000, episode_reward=1189.93 +/- 56.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1006000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | 2.78     |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1007000, episode_reward=1192.95 +/- 24.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1007000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=1064.25 +/- 20.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1008000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.913   |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | 2.91     |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4920     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1008     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 1457     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1009000, episode_reward=1037.29 +/- 28.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1009000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=1103.98 +/- 16.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.172   |\n",
      "|    critic_loss     | 0.966    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 0.71     |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1011000, episode_reward=1085.12 +/- 31.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1011000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1012000, episode_reward=1093.43 +/- 29.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1012000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.525   |\n",
      "|    critic_loss     | 0.971    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4940     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1012     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 1463     |\n",
      "|    total_timesteps | 1012000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1013000, episode_reward=1039.14 +/- 31.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1013000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1014000, episode_reward=1162.47 +/- 46.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1014000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.823   |\n",
      "|    critic_loss     | 0.93     |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -1.38    |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1015000, episode_reward=1146.40 +/- 46.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1015000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1016000, episode_reward=1167.12 +/- 32.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1016000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.39    |\n",
      "|    critic_loss     | 0.94     |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -0.191   |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 4960     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1016     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 1468     |\n",
      "|    total_timesteps | 1016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1017000, episode_reward=1146.32 +/- 16.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1017000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1018000, episode_reward=1102.93 +/- 44.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1018000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.696   |\n",
      "|    critic_loss     | 0.927    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 1.87     |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 4970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1019000, episode_reward=1185.16 +/- 36.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1019000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=1050.20 +/- 20.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.09    |\n",
      "|    critic_loss     | 0.895    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | 0.589    |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 4980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1020     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 1474     |\n",
      "|    total_timesteps | 1020000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1021000, episode_reward=1083.04 +/- 31.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1021000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1022000, episode_reward=966.27 +/- 32.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 966      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1022000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.289   |\n",
      "|    critic_loss     | 0.879    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | -2.31    |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 4990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1023000, episode_reward=949.73 +/- 24.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 950      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1023000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1024000, episode_reward=950.79 +/- 52.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 951      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1024000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1024     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 1479     |\n",
      "|    total_timesteps | 1024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1025000, episode_reward=1083.50 +/- 62.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1025000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.08     |\n",
      "|    critic_loss     | 0.818    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -1.33    |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 5000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1026000, episode_reward=1113.57 +/- 41.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1026000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1027000, episode_reward=1253.19 +/- 26.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1027000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0833  |\n",
      "|    critic_loss     | 0.777    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -0.669   |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028000, episode_reward=1250.48 +/- 28.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1028000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1028     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 1485     |\n",
      "|    total_timesteps | 1028000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1029000, episode_reward=1203.69 +/- 31.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1029000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.5     |\n",
      "|    critic_loss     | 0.892    |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | -1.38    |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=1221.55 +/- 18.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1030000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1031000, episode_reward=1037.66 +/- 22.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1031000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.936    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 1.39     |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1032000, episode_reward=1021.69 +/- 34.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1032000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1032     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 1490     |\n",
      "|    total_timesteps | 1032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1033000, episode_reward=1035.97 +/- 18.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1033000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.39    |\n",
      "|    critic_loss     | 0.979    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -0.721   |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1034000, episode_reward=1046.85 +/- 37.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1034000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1035000, episode_reward=1181.13 +/- 47.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1035000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.1     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -0.711   |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1036000, episode_reward=1180.85 +/- 37.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1036000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1036     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 1495     |\n",
      "|    total_timesteps | 1036000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1037000, episode_reward=1101.08 +/- 50.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1037000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.965   |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -0.541   |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1038000, episode_reward=1129.12 +/- 20.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1038000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1039000, episode_reward=1133.62 +/- 25.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1039000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.05    |\n",
      "|    critic_loss     | 0.909    |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | 0.238    |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=1121.34 +/- 34.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1040000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1040     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 1501     |\n",
      "|    total_timesteps | 1040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1041000, episode_reward=1148.86 +/- 29.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1041000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.71    |\n",
      "|    critic_loss     | 0.89     |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1042000, episode_reward=1123.81 +/- 40.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1042000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1043000, episode_reward=1159.69 +/- 42.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1043000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.887   |\n",
      "|    critic_loss     | 0.791    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1044000, episode_reward=1170.31 +/- 31.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1044000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1044     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 1506     |\n",
      "|    total_timesteps | 1044000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1045000, episode_reward=1132.12 +/- 38.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1045000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.13    |\n",
      "|    critic_loss     | 0.845    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 0.0502   |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1046000, episode_reward=1116.39 +/- 43.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1046000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1047000, episode_reward=1100.50 +/- 30.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1047000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.917   |\n",
      "|    critic_loss     | 0.834    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 0.247    |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048000, episode_reward=1139.49 +/- 32.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1048000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1048     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 1512     |\n",
      "|    total_timesteps | 1048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1049000, episode_reward=1176.85 +/- 27.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1049000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.983   |\n",
      "|    critic_loss     | 0.916    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=1180.82 +/- 24.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1050000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1051000, episode_reward=1102.21 +/- 46.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1051000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.937    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -1.85    |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1052000, episode_reward=1081.20 +/- 36.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1052000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1052     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 1517     |\n",
      "|    total_timesteps | 1052000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1053000, episode_reward=1163.04 +/- 18.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1053000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.292   |\n",
      "|    critic_loss     | 0.923    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -0.914   |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1054000, episode_reward=1180.42 +/- 17.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1054000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1055000, episode_reward=1155.33 +/- 25.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1055000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.936   |\n",
      "|    critic_loss     | 0.906    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 0.691    |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1056000, episode_reward=1170.97 +/- 36.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1056000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1056     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 1523     |\n",
      "|    total_timesteps | 1056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1057000, episode_reward=1141.19 +/- 17.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1057000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.951   |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 2.14     |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1058000, episode_reward=1168.79 +/- 29.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1058000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1059000, episode_reward=1083.57 +/- 36.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1059000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | 2.17     |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=1076.61 +/- 29.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1060000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1060     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 1528     |\n",
      "|    total_timesteps | 1060000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1061000, episode_reward=1050.02 +/- 24.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1061000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.692   |\n",
      "|    critic_loss     | 0.923    |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | -0.108   |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1062000, episode_reward=1055.07 +/- 38.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1062000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1063000, episode_reward=1316.67 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1063000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.299   |\n",
      "|    critic_loss     | 0.934    |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | -2.72    |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5190     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1064000, episode_reward=1275.38 +/- 16.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1064000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1064     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 1534     |\n",
      "|    total_timesteps | 1064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1065000, episode_reward=1179.24 +/- 35.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1065000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.48    |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | 2.26     |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1066000, episode_reward=1187.16 +/- 42.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1066000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1067000, episode_reward=1202.96 +/- 18.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1067000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068000, episode_reward=1222.28 +/- 18.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1068000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | 3.59     |\n",
      "|    learning_rate   | 0.00393  |\n",
      "|    n_updates       | 5210     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1068     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 1539     |\n",
      "|    total_timesteps | 1068000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1069000, episode_reward=1219.44 +/- 35.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1069000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=1094.67 +/- 15.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.49    |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0169   |\n",
      "|    ent_coef_loss   | 0.136    |\n",
      "|    learning_rate   | 0.00393  |\n",
      "|    n_updates       | 5220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1071000, episode_reward=1049.02 +/- 33.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1071000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1072000, episode_reward=1128.69 +/- 46.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1072000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.527   |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | -0.944   |\n",
      "|    learning_rate   | 0.00393  |\n",
      "|    n_updates       | 5230     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1072     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 1545     |\n",
      "|    total_timesteps | 1072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1073000, episode_reward=1141.07 +/- 30.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1073000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1074000, episode_reward=1064.91 +/- 18.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1074000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.655   |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | 0.463    |\n",
      "|    learning_rate   | 0.00393  |\n",
      "|    n_updates       | 5240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1075000, episode_reward=1087.67 +/- 31.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1075000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1076000, episode_reward=1250.33 +/- 26.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1076000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.642   |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | 3.68     |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5250     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1076     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 1550     |\n",
      "|    total_timesteps | 1076000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1077000, episode_reward=1258.59 +/- 26.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1077000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1078000, episode_reward=1162.16 +/- 19.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1078000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.51    |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | 2.53     |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1079000, episode_reward=1155.54 +/- 15.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1079000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=1031.84 +/- 58.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0176   |\n",
      "|    ent_coef_loss   | 3.72     |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5270     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1080     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1555     |\n",
      "|    total_timesteps | 1080000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1081000, episode_reward=986.57 +/- 14.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 987      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1081000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1082000, episode_reward=1057.16 +/- 29.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1082000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0956  |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0179   |\n",
      "|    ent_coef_loss   | -0.0119  |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1083000, episode_reward=1067.74 +/- 23.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1083000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1084000, episode_reward=1337.27 +/- 2.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1084000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5290     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1084     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1561     |\n",
      "|    total_timesteps | 1084000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1085000, episode_reward=1333.24 +/- 22.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1085000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1086000, episode_reward=1167.67 +/- 35.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1086000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.66    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | -0.926   |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1087000, episode_reward=1213.12 +/- 20.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1087000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088000, episode_reward=1025.39 +/- 23.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1088000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.929    |\n",
      "|    ent_coef        | 0.0179   |\n",
      "|    ent_coef_loss   | -2.2     |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5310     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1088     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1566     |\n",
      "|    total_timesteps | 1088000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1089000, episode_reward=1023.24 +/- 16.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1089000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=1162.23 +/- 48.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.114   |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | 0.818    |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1091000, episode_reward=1143.27 +/- 43.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1091000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1092000, episode_reward=1142.19 +/- 30.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1092000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | 0.65     |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5330     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1092     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1572     |\n",
      "|    total_timesteps | 1092000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1093000, episode_reward=1150.16 +/- 20.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1093000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1094000, episode_reward=1283.38 +/- 18.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1094000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.12    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1095000, episode_reward=1298.80 +/- 18.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1095000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1096000, episode_reward=1279.75 +/- 31.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1096000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.08    |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | -1.18    |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5350     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1096     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1577     |\n",
      "|    total_timesteps | 1096000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1097000, episode_reward=1268.31 +/- 56.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1097000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1098000, episode_reward=1181.52 +/- 40.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1098000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.41    |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0175   |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1099000, episode_reward=1161.30 +/- 39.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1099000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=1065.13 +/- 24.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.67    |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | 1.99     |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5370     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1100     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1583     |\n",
      "|    total_timesteps | 1100000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1101000, episode_reward=1058.95 +/- 19.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1101000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1102000, episode_reward=1262.18 +/- 9.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1102000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.235   |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0176   |\n",
      "|    ent_coef_loss   | 1.94     |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1103000, episode_reward=1261.29 +/- 50.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1103000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1104000, episode_reward=1267.06 +/- 16.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1104000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.6     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | 0.648    |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5390     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1104     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1588     |\n",
      "|    total_timesteps | 1104000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1105000, episode_reward=1306.73 +/- 30.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1105000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1106000, episode_reward=1256.48 +/- 10.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1106000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.72    |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1107000, episode_reward=1257.60 +/- 34.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1107000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108000, episode_reward=1290.05 +/- 43.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1108000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.81    |\n",
      "|    critic_loss     | 0.961    |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | -0.377   |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5410     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1108     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1594     |\n",
      "|    total_timesteps | 1108000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1109000, episode_reward=1347.38 +/- 43.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1109000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1110000, episode_reward=1303.86 +/- 31.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1110000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1111000, episode_reward=1202.83 +/- 36.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1111000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.32    |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | 0.258    |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1112000, episode_reward=1197.78 +/- 15.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1112000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1112     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1599     |\n",
      "|    total_timesteps | 1112000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1113000, episode_reward=1180.23 +/- 29.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1113000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.21    |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1114000, episode_reward=1220.50 +/- 17.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1114000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1115000, episode_reward=1192.73 +/- 27.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1115000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0176   |\n",
      "|    ent_coef_loss   | 1.42     |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1116000, episode_reward=1176.56 +/- 43.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1116000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1116     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1605     |\n",
      "|    total_timesteps | 1116000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1117000, episode_reward=1219.42 +/- 12.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1117000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.42    |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | 4.3      |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1118000, episode_reward=1214.32 +/- 21.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1118000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1119000, episode_reward=1295.49 +/- 34.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1119000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.43    |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | 2.18     |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=1315.20 +/- 21.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1120000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1120     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1610     |\n",
      "|    total_timesteps | 1120000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1121000, episode_reward=1140.24 +/- 15.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1121000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.9     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0184   |\n",
      "|    ent_coef_loss   | 1.83     |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1122000, episode_reward=1141.97 +/- 23.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1122000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1123000, episode_reward=1297.58 +/- 26.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1123000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.15    |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1124000, episode_reward=1297.82 +/- 22.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1124000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1124     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1616     |\n",
      "|    total_timesteps | 1124000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1125000, episode_reward=1263.13 +/- 60.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1125000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.29    |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0188   |\n",
      "|    ent_coef_loss   | 0.00796  |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1126000, episode_reward=1247.02 +/- 41.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1126000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1127000, episode_reward=977.86 +/- 25.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 978      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1127000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.42    |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128000, episode_reward=1026.79 +/- 33.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1128000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1128     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1621     |\n",
      "|    total_timesteps | 1128000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1129000, episode_reward=1183.50 +/- 29.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1129000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0341   |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | -0.055   |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=1184.82 +/- 46.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1130000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1131000, episode_reward=1026.41 +/- 22.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1131000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -0.181   |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1132000, episode_reward=1021.80 +/- 32.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1132000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1132     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1627     |\n",
      "|    total_timesteps | 1132000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1133000, episode_reward=1291.52 +/- 22.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1133000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.224   |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -2.12    |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1134000, episode_reward=1305.90 +/- 31.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1134000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1135000, episode_reward=1278.05 +/- 27.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1135000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | 1.44     |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1136000, episode_reward=1265.24 +/- 40.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1136000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1136     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1632     |\n",
      "|    total_timesteps | 1136000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1137000, episode_reward=1266.77 +/- 29.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1137000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.67    |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | -0.706   |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1138000, episode_reward=1242.43 +/- 26.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1138000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1139000, episode_reward=1254.43 +/- 24.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1139000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.81    |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 2.49     |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=1242.75 +/- 52.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1140000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1140     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1637     |\n",
      "|    total_timesteps | 1140000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1141000, episode_reward=1248.44 +/- 20.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1141000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.52    |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | 0.0263   |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1142000, episode_reward=1263.14 +/- 36.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1142000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1143000, episode_reward=1185.10 +/- 33.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1143000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.77    |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | -1.4     |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1144000, episode_reward=1186.07 +/- 30.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1144000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1144     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1643     |\n",
      "|    total_timesteps | 1144000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1145000, episode_reward=1117.07 +/- 22.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1145000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.57    |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -0.678   |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1146000, episode_reward=1138.29 +/- 24.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1146000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1147000, episode_reward=1229.58 +/- 28.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1147000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.531   |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 0.134    |\n",
      "|    learning_rate   | 0.00385  |\n",
      "|    n_updates       | 5600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1148000, episode_reward=1212.55 +/- 31.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1148000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1148     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1648     |\n",
      "|    total_timesteps | 1148000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149000, episode_reward=1357.38 +/- 22.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1149000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.19    |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 3.15     |\n",
      "|    learning_rate   | 0.00385  |\n",
      "|    n_updates       | 5610     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1150000, episode_reward=1356.26 +/- 22.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1150000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1151000, episode_reward=1250.21 +/- 33.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1151000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.34    |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0194   |\n",
      "|    ent_coef_loss   | 1.42     |\n",
      "|    learning_rate   | 0.00385  |\n",
      "|    n_updates       | 5620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1152000, episode_reward=1259.54 +/- 30.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1152000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1152     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1654     |\n",
      "|    total_timesteps | 1152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1153000, episode_reward=1243.02 +/- 23.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1153000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1154000, episode_reward=1189.37 +/- 58.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1154000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.42    |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | 0.16     |\n",
      "|    learning_rate   | 0.00385  |\n",
      "|    n_updates       | 5630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1155000, episode_reward=1216.73 +/- 56.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1155000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1156000, episode_reward=1104.13 +/- 30.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1156000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5640     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1156     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1659     |\n",
      "|    total_timesteps | 1156000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1157000, episode_reward=1168.33 +/- 31.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1157000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1158000, episode_reward=1239.70 +/- 59.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1158000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.916   |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | -0.952   |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1159000, episode_reward=1238.21 +/- 45.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1159000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=1308.43 +/- 36.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | 1.35     |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5660     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1160     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1665     |\n",
      "|    total_timesteps | 1160000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1161000, episode_reward=1260.42 +/- 55.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1161000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1162000, episode_reward=1160.88 +/- 44.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1162000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 1.81     |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1163000, episode_reward=1162.85 +/- 34.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1163000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1164000, episode_reward=-219.29 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1164000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.346   |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5680     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1164     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1670     |\n",
      "|    total_timesteps | 1164000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1165000, episode_reward=-220.06 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -220     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1165000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1166000, episode_reward=1200.72 +/- 25.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1166000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.51     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -2.7     |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1167000, episode_reward=1192.17 +/- 18.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1167000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1168000, episode_reward=1154.95 +/- 22.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1168000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.84    |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | -0.3     |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5700     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1168     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1676     |\n",
      "|    total_timesteps | 1168000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1169000, episode_reward=1180.12 +/- 17.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1169000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=1291.63 +/- 43.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.943   |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 0.444    |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1171000, episode_reward=1308.03 +/- 25.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1171000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1172000, episode_reward=1306.12 +/- 45.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1172000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.71    |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5720     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1172     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1681     |\n",
      "|    total_timesteps | 1172000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1173000, episode_reward=1287.55 +/- 22.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1173000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1174000, episode_reward=-339.55 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1174000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.65    |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 0.0384   |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1175000, episode_reward=-340.71 +/- 2.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -341     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1175000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1176000, episode_reward=-384.45 +/- 2.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -384     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1176000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.3      |\n",
      "|    critic_loss     | 0.484    |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5740     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1176     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1687     |\n",
      "|    total_timesteps | 1176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1177000, episode_reward=-384.94 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -385     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1177000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1178000, episode_reward=1126.77 +/- 41.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1178000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.43     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -0.511   |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1179000, episode_reward=1134.72 +/- 29.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1179000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=1112.00 +/- 30.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.717   |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0188   |\n",
      "|    ent_coef_loss   | 2.19     |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5760     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1180     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1692     |\n",
      "|    total_timesteps | 1180000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1181000, episode_reward=1095.25 +/- 14.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1181000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1182000, episode_reward=1253.69 +/- 34.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1182000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.853    |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | -0.655   |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1183000, episode_reward=1231.50 +/- 42.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1183000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1184000, episode_reward=1202.42 +/- 44.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1184000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.12    |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | 2.1      |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5780     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1184     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1702     |\n",
      "|    total_timesteps | 1184000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1185000, episode_reward=1171.73 +/- 63.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1185000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1186000, episode_reward=-273.37 +/- 1.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -273     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1186000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.54    |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | 2.79     |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1187000, episode_reward=-274.09 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -274     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1187000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1188000, episode_reward=298.69 +/- 635.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 299      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1188000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.282    |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -0.878   |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1188     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1708     |\n",
      "|    total_timesteps | 1188000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189000, episode_reward=39.34 +/- 514.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 39.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1189000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=1115.60 +/- 44.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.647   |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -0.136   |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1191000, episode_reward=1083.44 +/- 58.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1191000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1192000, episode_reward=1339.39 +/- 65.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1192000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.442   |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | 1.93     |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5820     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1192     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1714     |\n",
      "|    total_timesteps | 1192000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1193000, episode_reward=1275.17 +/- 91.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1193000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1194000, episode_reward=1275.36 +/- 47.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1194000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.47    |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | 0.312    |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1195000, episode_reward=1295.85 +/- 24.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1195000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1196000, episode_reward=1329.05 +/- 40.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1196000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1196     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1719     |\n",
      "|    total_timesteps | 1196000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1197000, episode_reward=1251.49 +/- 93.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1197000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | 0.00686  |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1198000, episode_reward=1201.61 +/- 74.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1198000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1199000, episode_reward=1174.34 +/- 71.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1199000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.455   |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | -0.331   |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=1121.47 +/- 65.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1200     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1725     |\n",
      "|    total_timesteps | 1200000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1201000, episode_reward=-192.77 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -193     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1201000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.27    |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1202000, episode_reward=112.66 +/- 608.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 113      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1202000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1203000, episode_reward=1289.74 +/- 42.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1203000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.265   |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | -0.743   |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1204000, episode_reward=1221.23 +/- 36.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1204000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1204     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1730     |\n",
      "|    total_timesteps | 1204000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1205000, episode_reward=1165.09 +/- 32.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1205000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1206000, episode_reward=1251.45 +/- 26.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1206000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1207000, episode_reward=1382.90 +/- 11.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1207000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.5     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | 1.62     |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5890     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1208000, episode_reward=1423.51 +/- 16.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1208000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1208     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1736     |\n",
      "|    total_timesteps | 1208000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209000, episode_reward=1459.37 +/- 30.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1209000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.55    |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 0.93     |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5900     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1210000, episode_reward=1427.08 +/- 30.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1210000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1211000, episode_reward=1403.63 +/- 20.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1211000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.77    |\n",
      "|    critic_loss     | 1.59     |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | 3.88     |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1212000, episode_reward=1389.04 +/- 23.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1212000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1212     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1741     |\n",
      "|    total_timesteps | 1212000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1213000, episode_reward=1375.31 +/- 45.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1213000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.37    |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 2.38     |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1214000, episode_reward=1392.99 +/- 18.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1214000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1215000, episode_reward=1363.24 +/- 54.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1215000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.58    |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | 1.88     |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1216000, episode_reward=1362.35 +/- 30.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1216000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1216     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1747     |\n",
      "|    total_timesteps | 1216000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1217000, episode_reward=1341.14 +/- 13.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1217000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.62    |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | 3.93     |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1218000, episode_reward=1375.54 +/- 10.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1218000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1219000, episode_reward=1240.95 +/- 18.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1219000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.71    |\n",
      "|    critic_loss     | 1.59     |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 3.47     |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=1232.70 +/- 47.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1220000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1220     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1752     |\n",
      "|    total_timesteps | 1220000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1221000, episode_reward=1008.24 +/- 41.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1221000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.16    |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 2.97     |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1222000, episode_reward=1013.20 +/- 67.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1222000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1223000, episode_reward=1037.97 +/- 36.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1223000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.685    |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0234   |\n",
      "|    ent_coef_loss   | -0.136   |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1224000, episode_reward=1058.85 +/- 57.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1224000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1224     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1758     |\n",
      "|    total_timesteps | 1224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1225000, episode_reward=1151.49 +/- 72.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1225000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.458    |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0236   |\n",
      "|    ent_coef_loss   | -1.89    |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1226000, episode_reward=1148.42 +/- 33.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1226000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1227000, episode_reward=1243.19 +/- 25.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1227000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.401   |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -0.875   |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 5990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1228000, episode_reward=1189.23 +/- 42.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1228000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1228     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1763     |\n",
      "|    total_timesteps | 1228000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229000, episode_reward=1174.48 +/- 32.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1229000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.21    |\n",
      "|    critic_loss     | 1.6      |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -2.25    |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=1200.20 +/- 40.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1230000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1231000, episode_reward=1215.57 +/- 44.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1231000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.559   |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -0.65    |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 6010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1232000, episode_reward=1197.23 +/- 79.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1232000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1232     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1769     |\n",
      "|    total_timesteps | 1232000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1233000, episode_reward=1331.19 +/- 44.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1233000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.84    |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | 0.82     |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 6020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1234000, episode_reward=1343.01 +/- 5.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1234000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1235000, episode_reward=1293.34 +/- 24.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1235000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.32    |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 3.85     |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 6030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1236000, episode_reward=1288.77 +/- 48.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1236000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1236     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1774     |\n",
      "|    total_timesteps | 1236000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1237000, episode_reward=1266.06 +/- 57.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1237000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.68    |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | 2.4      |\n",
      "|    learning_rate   | 0.00376  |\n",
      "|    n_updates       | 6040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1238000, episode_reward=1277.97 +/- 29.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1238000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1239000, episode_reward=1267.85 +/- 49.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1239000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=1106.84 +/- 34.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | 1.63     |\n",
      "|    learning_rate   | 0.00376  |\n",
      "|    n_updates       | 6050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1240     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1780     |\n",
      "|    total_timesteps | 1240000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1241000, episode_reward=1153.56 +/- 79.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1241000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1242000, episode_reward=1262.79 +/- 22.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1242000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.479   |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | 0.622    |\n",
      "|    learning_rate   | 0.00376  |\n",
      "|    n_updates       | 6060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1243000, episode_reward=1242.38 +/- 43.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1243000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1244000, episode_reward=1323.41 +/- 20.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1244000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.61    |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | 1.78     |\n",
      "|    learning_rate   | 0.00376  |\n",
      "|    n_updates       | 6070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1244     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1785     |\n",
      "|    total_timesteps | 1244000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1245000, episode_reward=1300.84 +/- 22.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1245000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1246000, episode_reward=1322.85 +/- 36.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1246000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | 1        |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1247000, episode_reward=1352.07 +/- 32.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1247000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1248000, episode_reward=1384.69 +/- 22.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1248000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.8     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | 0.861    |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6090     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1248     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1791     |\n",
      "|    total_timesteps | 1248000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1249000, episode_reward=1376.53 +/- 18.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1249000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=1204.77 +/- 42.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.95    |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | 0.403    |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1251000, episode_reward=1218.39 +/- 16.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1251000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1252000, episode_reward=1246.04 +/- 26.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1252000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.795   |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 0.853    |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6110     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1252     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1796     |\n",
      "|    total_timesteps | 1252000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1253000, episode_reward=1253.44 +/- 44.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1253000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1254000, episode_reward=1241.22 +/- 20.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1254000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.76    |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | -0.0777  |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1255000, episode_reward=1221.45 +/- 64.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1255000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1256000, episode_reward=1146.74 +/- 51.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1256000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | -0.755   |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6130     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1256     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1802     |\n",
      "|    total_timesteps | 1256000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1257000, episode_reward=1168.67 +/- 41.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1257000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1258000, episode_reward=1300.50 +/- 22.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1258000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.597   |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | 0.963    |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1259000, episode_reward=1258.99 +/- 20.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1259000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=1328.88 +/- 27.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.92    |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | 0.105    |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6150     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1260     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1807     |\n",
      "|    total_timesteps | 1260000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1261000, episode_reward=1363.90 +/- 17.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1261000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1262000, episode_reward=1303.27 +/- 44.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1262000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2       |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | -0.783   |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1263000, episode_reward=1307.71 +/- 54.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1263000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1264000, episode_reward=1375.05 +/- 22.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1264000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.85    |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | -0.34    |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6170     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1264     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1813     |\n",
      "|    total_timesteps | 1264000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1265000, episode_reward=1382.02 +/- 33.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1265000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1266000, episode_reward=1336.23 +/- 42.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1266000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.02    |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | -2.17    |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1267000, episode_reward=1355.56 +/- 46.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1267000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1268000, episode_reward=1367.43 +/- 20.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1268000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.96    |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6190     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1268     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1818     |\n",
      "|    total_timesteps | 1268000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1269000, episode_reward=1402.06 +/- 39.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1269000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=1065.55 +/- 75.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.89    |\n",
      "|    critic_loss     | 1.95     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -0.603   |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1271000, episode_reward=1115.37 +/- 23.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1271000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1272000, episode_reward=1187.05 +/- 31.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1272000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.211    |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -0.824   |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6210     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1272     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1824     |\n",
      "|    total_timesteps | 1272000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1273000, episode_reward=1172.77 +/- 43.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1273000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1274000, episode_reward=1183.84 +/- 31.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1274000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.395   |\n",
      "|    critic_loss     | 1.59     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -0.0936  |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1275000, episode_reward=1122.07 +/- 59.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1275000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1276000, episode_reward=1376.73 +/- 12.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1276000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.652   |\n",
      "|    critic_loss     | 1.59     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6230     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1276     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1829     |\n",
      "|    total_timesteps | 1276000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1277000, episode_reward=1363.13 +/- 40.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1277000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1278000, episode_reward=1400.91 +/- 31.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1278000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | 1.42     |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1279000, episode_reward=1391.93 +/- 35.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1279000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=1411.80 +/- 14.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1280000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1280     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1834     |\n",
      "|    total_timesteps | 1280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1281000, episode_reward=1238.51 +/- 29.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1281000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.35    |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | 2.13     |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1282000, episode_reward=1236.15 +/- 32.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1282000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1283000, episode_reward=1287.23 +/- 31.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1283000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.828   |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | 2.34     |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1284000, episode_reward=1286.49 +/- 20.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1284000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1284     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1840     |\n",
      "|    total_timesteps | 1284000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1285000, episode_reward=1263.77 +/- 35.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1285000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.21    |\n",
      "|    critic_loss     | 1.86     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | 0.414    |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1286000, episode_reward=1284.78 +/- 35.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1286000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1287000, episode_reward=397.73 +/- 669.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 398      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1287000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.02    |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | 0.66     |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1288000, episode_reward=-145.52 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1288000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1288     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1845     |\n",
      "|    total_timesteps | 1288000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1289000, episode_reward=-171.40 +/- 1.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -171     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1289000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.5     |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | 0.373    |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=-171.95 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -172     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1290000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1291000, episode_reward=-217.06 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -217     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1291000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.91    |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1292000, episode_reward=-215.85 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -216     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1292000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1292     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1851     |\n",
      "|    total_timesteps | 1292000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1293000, episode_reward=1268.09 +/- 41.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1293000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.934    |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -4.91    |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1294000, episode_reward=1338.85 +/- 37.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1294000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1295000, episode_reward=1376.24 +/- 10.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1295000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 1.86     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | -0.944   |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1296000, episode_reward=1390.27 +/- 32.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1296000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1296     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1856     |\n",
      "|    total_timesteps | 1296000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1297000, episode_reward=1357.67 +/- 20.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1297000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.19    |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 0.394    |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1298000, episode_reward=1360.59 +/- 37.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1298000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1299000, episode_reward=1300.50 +/- 53.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1299000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.31    |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | 0.698    |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=1323.24 +/- 42.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1300     |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 1862     |\n",
      "|    total_timesteps | 1300000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1301000, episode_reward=1179.65 +/- 23.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1301000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.97    |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1302000, episode_reward=1188.00 +/- 36.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1302000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1303000, episode_reward=1109.14 +/- 6.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1303000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.512   |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | 2.13     |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1304000, episode_reward=1124.88 +/- 41.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1304000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1304     |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 1867     |\n",
      "|    total_timesteps | 1304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1305000, episode_reward=1218.79 +/- 58.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1305000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.594   |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | 0.218    |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1306000, episode_reward=1214.84 +/- 51.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1306000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1307000, episode_reward=1439.05 +/- 18.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1307000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.11    |\n",
      "|    critic_loss     | 2.02     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 0.794    |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1308000, episode_reward=1434.39 +/- 36.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1308000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1308     |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 1873     |\n",
      "|    total_timesteps | 1308000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1309000, episode_reward=1327.85 +/- 30.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1309000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.14    |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | 1.42     |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=1328.93 +/- 32.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1310000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1311000, episode_reward=1206.50 +/- 46.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1311000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.53    |\n",
      "|    critic_loss     | 2        |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | 1.92     |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1312000, episode_reward=1215.23 +/- 20.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1312000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1312     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1879     |\n",
      "|    total_timesteps | 1312000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1313000, episode_reward=1001.11 +/- 34.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1313000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 2.33     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 2.26     |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1314000, episode_reward=1033.07 +/- 17.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1314000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1315000, episode_reward=1145.99 +/- 22.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1315000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.348    |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | -0.307   |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1316000, episode_reward=1183.75 +/- 30.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1316000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1316     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1886     |\n",
      "|    total_timesteps | 1316000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1317000, episode_reward=1190.16 +/- 28.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1317000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.486   |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | -0.385   |\n",
      "|    learning_rate   | 0.00368  |\n",
      "|    n_updates       | 6430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1318000, episode_reward=1152.78 +/- 21.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1318000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1319000, episode_reward=1349.08 +/- 38.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1319000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.265   |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | 0.308    |\n",
      "|    learning_rate   | 0.00368  |\n",
      "|    n_updates       | 6440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=1408.11 +/- 30.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1320000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1320     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1893     |\n",
      "|    total_timesteps | 1320000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1321000, episode_reward=1196.12 +/- 20.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1321000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.12    |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 0.566    |\n",
      "|    learning_rate   | 0.00368  |\n",
      "|    n_updates       | 6450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1322000, episode_reward=1184.76 +/- 36.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1322000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1323000, episode_reward=1194.99 +/- 34.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1323000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1324000, episode_reward=1349.36 +/- 42.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1324000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.863   |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | 0.386    |\n",
      "|    learning_rate   | 0.00368  |\n",
      "|    n_updates       | 6460     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1324     |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 1899     |\n",
      "|    total_timesteps | 1324000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1325000, episode_reward=1353.41 +/- 31.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1325000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1326000, episode_reward=1334.19 +/- 23.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1326000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.31    |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | 0.257    |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1327000, episode_reward=1290.02 +/- 24.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1327000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1328000, episode_reward=1431.81 +/- 12.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1328000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 0.577    |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1328     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1905     |\n",
      "|    total_timesteps | 1328000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1329000, episode_reward=1386.08 +/- 27.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1329000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=-188.34 +/- 2.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -188     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1331000, episode_reward=-188.22 +/- 2.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -188     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1331000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1332000, episode_reward=-228.92 +/- 1.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1332000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.947   |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -2.25    |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1332     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1912     |\n",
      "|    total_timesteps | 1332000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1333000, episode_reward=-229.20 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1333000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1334000, episode_reward=1404.10 +/- 12.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1334000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.19     |\n",
      "|    critic_loss     | 0.869    |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1335000, episode_reward=1385.84 +/- 13.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1335000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1336000, episode_reward=1276.12 +/- 57.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1336000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.87    |\n",
      "|    critic_loss     | 2        |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | -0.243   |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6520     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1336     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1918     |\n",
      "|    total_timesteps | 1336000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1337000, episode_reward=1264.60 +/- 94.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1337000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1338000, episode_reward=1311.11 +/- 36.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1338000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1339000, episode_reward=1341.63 +/- 36.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1339000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=1263.68 +/- 47.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.58    |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | 0.462    |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6540     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1340     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1924     |\n",
      "|    total_timesteps | 1340000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1341000, episode_reward=1296.00 +/- 16.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1341000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1342000, episode_reward=1378.93 +/- 58.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1342000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 0.627    |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1343000, episode_reward=1370.08 +/- 63.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1343000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1344000, episode_reward=1244.54 +/- 44.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1344000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.72    |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6560     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1344     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1930     |\n",
      "|    total_timesteps | 1344000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1345000, episode_reward=1199.57 +/- 28.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1345000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1346000, episode_reward=1271.09 +/- 28.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1346000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.823   |\n",
      "|    critic_loss     | 2.01     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -0.278   |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1347000, episode_reward=1290.42 +/- 18.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1347000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1348000, episode_reward=1263.37 +/- 32.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1348000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.09    |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6580     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1348     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1936     |\n",
      "|    total_timesteps | 1348000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1349000, episode_reward=1228.78 +/- 29.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1349000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=1284.00 +/- 33.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.868   |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 1.78     |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1351000, episode_reward=1291.24 +/- 23.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1351000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1352000, episode_reward=1406.03 +/- 35.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1352000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | 3.66     |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6600     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1352     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1942     |\n",
      "|    total_timesteps | 1352000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1353000, episode_reward=1407.84 +/- 28.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1353000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1354000, episode_reward=1393.71 +/- 35.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1354000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1355000, episode_reward=1404.70 +/- 23.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1355000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1356000, episode_reward=1381.34 +/- 28.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1356000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.83    |\n",
      "|    critic_loss     | 2.13     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 2.52     |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6620     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1356     |\n",
      "|    fps             | 696      |\n",
      "|    time_elapsed    | 1947     |\n",
      "|    total_timesteps | 1356000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1357000, episode_reward=1390.11 +/- 41.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1357000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1358000, episode_reward=1297.22 +/- 32.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1358000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.32    |\n",
      "|    critic_loss     | 2.15     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | 3.05     |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1359000, episode_reward=1304.32 +/- 30.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1359000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=1426.90 +/- 32.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | 0.204    |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6640     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1360     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1954     |\n",
      "|    total_timesteps | 1360000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1361000, episode_reward=1422.93 +/- 32.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1361000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1362000, episode_reward=1385.09 +/- 38.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1362000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.83    |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1363000, episode_reward=1353.15 +/- 38.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1363000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1364000, episode_reward=1340.58 +/- 28.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1364000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.82    |\n",
      "|    critic_loss     | 2.29     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6660     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1364     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1959     |\n",
      "|    total_timesteps | 1364000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1365000, episode_reward=1324.59 +/- 36.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1365000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1366000, episode_reward=1351.14 +/- 32.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1366000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1367000, episode_reward=1335.25 +/- 29.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1367000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.84    |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | 1.64     |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1368000, episode_reward=1318.82 +/- 56.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1368000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1368     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1966     |\n",
      "|    total_timesteps | 1368000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1369000, episode_reward=1390.10 +/- 17.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1369000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1     |\n",
      "|    critic_loss     | 2.17     |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | 0.578    |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=1399.34 +/- 22.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1370000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1371000, episode_reward=1362.43 +/- 32.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1371000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 2.2      |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | 1.95     |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1372000, episode_reward=1425.47 +/- 23.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1372000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1372     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1973     |\n",
      "|    total_timesteps | 1372000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1373000, episode_reward=1370.95 +/- 36.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1373000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.83    |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | -0.902   |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1374000, episode_reward=1368.22 +/- 6.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1374000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1375000, episode_reward=1421.63 +/- 21.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1375000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.58    |\n",
      "|    critic_loss     | 2.19     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | 0.349    |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1376000, episode_reward=1422.14 +/- 20.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1376000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1376     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 1979     |\n",
      "|    total_timesteps | 1376000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1377000, episode_reward=1350.01 +/- 34.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1377000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.66    |\n",
      "|    critic_loss     | 2.02     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -0.269   |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1378000, episode_reward=1351.22 +/- 23.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1378000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1379000, episode_reward=1447.66 +/- 27.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1379000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | -2.07    |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=1450.71 +/- 29.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1380000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1380     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1985     |\n",
      "|    total_timesteps | 1380000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1381000, episode_reward=1484.46 +/- 28.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1381000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.47    |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6740     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1382000, episode_reward=1508.33 +/- 16.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1382000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1383000, episode_reward=1435.11 +/- 37.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1383000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -0.604   |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1384000, episode_reward=1369.10 +/- 23.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1384000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1384     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1991     |\n",
      "|    total_timesteps | 1384000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1385000, episode_reward=1309.76 +/- 28.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1385000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.66    |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -0.414   |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1386000, episode_reward=1272.07 +/- 61.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1386000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1387000, episode_reward=1321.55 +/- 29.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1387000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.78    |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | 0.299    |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1388000, episode_reward=1323.03 +/- 39.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1388000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1388     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 1997     |\n",
      "|    total_timesteps | 1388000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1389000, episode_reward=1299.92 +/- 23.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1389000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.691   |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.732   |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=1254.14 +/- 56.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1390000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391000, episode_reward=1274.21 +/- 54.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1391000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.416   |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | 2.03     |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1392000, episode_reward=1293.80 +/- 38.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1392000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1392     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2003     |\n",
      "|    total_timesteps | 1392000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1393000, episode_reward=1425.95 +/- 23.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1393000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.75    |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -0.156   |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1394000, episode_reward=1435.13 +/- 22.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1394000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1395000, episode_reward=1377.96 +/- 14.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1395000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.69    |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -0.0685  |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1396000, episode_reward=1376.15 +/- 12.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1396000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1396     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2008     |\n",
      "|    total_timesteps | 1396000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1397000, episode_reward=1403.53 +/- 24.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1397000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.22    |\n",
      "|    critic_loss     | 2.36     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -0.224   |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1398000, episode_reward=1427.82 +/- 21.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1398000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1399000, episode_reward=1245.99 +/- 27.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1399000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.787   |\n",
      "|    critic_loss     | 2.47     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | 0.0223   |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=1252.32 +/- 27.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1400     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2014     |\n",
      "|    total_timesteps | 1400000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1401000, episode_reward=1283.33 +/- 43.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1401000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.828   |\n",
      "|    critic_loss     | 2.5      |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | 0.986    |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1402000, episode_reward=1273.75 +/- 36.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1402000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1403000, episode_reward=1324.71 +/- 30.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1403000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.971   |\n",
      "|    critic_loss     | 2.53     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | 0.461    |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1404000, episode_reward=1357.18 +/- 33.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1404000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1404     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2019     |\n",
      "|    total_timesteps | 1404000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1405000, episode_reward=1380.04 +/- 21.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1405000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.11    |\n",
      "|    critic_loss     | 2.41     |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | 0.917    |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1406000, episode_reward=1362.04 +/- 27.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1406000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1407000, episode_reward=1396.02 +/- 28.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1407000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.49    |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.00359  |\n",
      "|    n_updates       | 6870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1408000, episode_reward=1368.17 +/- 23.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1408000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1408     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2025     |\n",
      "|    total_timesteps | 1408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1409000, episode_reward=1415.46 +/- 65.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1409000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=1399.14 +/- 38.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.08    |\n",
      "|    critic_loss     | 2.19     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | 0.515    |\n",
      "|    learning_rate   | 0.00359  |\n",
      "|    n_updates       | 6880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1411000, episode_reward=1403.59 +/- 24.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1411000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1412000, episode_reward=1404.74 +/- 60.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1412000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.55    |\n",
      "|    critic_loss     | 2.29     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | 0.494    |\n",
      "|    learning_rate   | 0.00359  |\n",
      "|    n_updates       | 6890     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1412     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2031     |\n",
      "|    total_timesteps | 1412000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1413000, episode_reward=1403.34 +/- 22.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1413000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1414000, episode_reward=1406.03 +/- 37.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1414000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | -0.798   |\n",
      "|    learning_rate   | 0.00359  |\n",
      "|    n_updates       | 6900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1415000, episode_reward=1386.45 +/- 50.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1415000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1416000, episode_reward=1203.37 +/- 25.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1416000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 2.26     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | 0.488    |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1416     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2037     |\n",
      "|    total_timesteps | 1416000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1417000, episode_reward=1243.43 +/- 46.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1417000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1418000, episode_reward=1198.49 +/- 48.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1418000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0372   |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -0.00402 |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1419000, episode_reward=1229.34 +/- 47.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1419000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=1287.14 +/- 53.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.23     |\n",
      "|    critic_loss     | 2.06     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6930     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1420     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2042     |\n",
      "|    total_timesteps | 1420000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1421000, episode_reward=1255.92 +/- 27.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1421000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1422000, episode_reward=1437.24 +/- 16.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1422000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.212   |\n",
      "|    critic_loss     | 2.22     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1423000, episode_reward=1430.92 +/- 29.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1423000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1424000, episode_reward=1414.72 +/- 9.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1424000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.86    |\n",
      "|    critic_loss     | 2.17     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6950     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1424     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2048     |\n",
      "|    total_timesteps | 1424000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1425000, episode_reward=1408.57 +/- 21.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1425000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1426000, episode_reward=1403.28 +/- 31.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1426000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.57    |\n",
      "|    critic_loss     | 2.19     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | 0.887    |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 6960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1427000, episode_reward=1401.54 +/- 14.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1427000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1428000, episode_reward=1411.85 +/- 24.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1428000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | 0.337    |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 6970     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1428     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2054     |\n",
      "|    total_timesteps | 1428000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1429000, episode_reward=1438.93 +/- 31.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1429000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=1378.12 +/- 32.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 6980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431000, episode_reward=1374.53 +/- 39.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1431000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1432000, episode_reward=1290.48 +/- 30.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1432000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 2.33     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | 0.118    |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 6990     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1432     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2060     |\n",
      "|    total_timesteps | 1432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1433000, episode_reward=1225.29 +/- 68.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1433000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1434000, episode_reward=1437.23 +/- 43.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1434000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.515   |\n",
      "|    critic_loss     | 2.13     |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 7000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1435000, episode_reward=1412.26 +/- 23.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1435000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1436000, episode_reward=1433.76 +/- 23.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1436000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 2.42     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | 0.242    |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7010     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1436     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2066     |\n",
      "|    total_timesteps | 1436000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1437000, episode_reward=1437.78 +/- 30.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1437000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1438000, episode_reward=1450.55 +/- 42.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1438000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.22    |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | 0.0856   |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1439000, episode_reward=1428.83 +/- 37.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1439000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=1367.99 +/- 50.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.42    |\n",
      "|    critic_loss     | 2.41     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | -0.386   |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7030     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1440     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2071     |\n",
      "|    total_timesteps | 1440000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1441000, episode_reward=1354.42 +/- 82.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1441000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1442000, episode_reward=1365.34 +/- 35.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1442000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.914   |\n",
      "|    critic_loss     | 2.41     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | 1.5      |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1443000, episode_reward=1330.03 +/- 46.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1443000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1444000, episode_reward=1298.43 +/- 67.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1444000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.7     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -0.437   |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1444     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2077     |\n",
      "|    total_timesteps | 1444000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1445000, episode_reward=1340.68 +/- 31.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1445000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1446000, episode_reward=1322.95 +/- 31.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1446000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.04    |\n",
      "|    critic_loss     | 2.56     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | 1.26     |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1447000, episode_reward=1349.28 +/- 34.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1447000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1448000, episode_reward=1447.31 +/- 27.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1448000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.983   |\n",
      "|    critic_loss     | 2.29     |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | 0.319    |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1448     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2082     |\n",
      "|    total_timesteps | 1448000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1449000, episode_reward=1416.68 +/- 36.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1449000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=1378.29 +/- 37.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.54    |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 1.85     |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1451000, episode_reward=1379.84 +/- 33.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1451000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1452000, episode_reward=1375.32 +/- 28.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1452000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1452     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2088     |\n",
      "|    total_timesteps | 1452000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1453000, episode_reward=1363.28 +/- 56.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1453000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.21    |\n",
      "|    critic_loss     | 2.5      |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 0.0739   |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1454000, episode_reward=1364.39 +/- 39.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1454000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1455000, episode_reward=1298.84 +/- 55.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1455000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.473   |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1456000, episode_reward=1280.14 +/- 72.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1456000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1456     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2093     |\n",
      "|    total_timesteps | 1456000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1457000, episode_reward=1409.32 +/- 8.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1457000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.00858  |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1458000, episode_reward=1431.41 +/- 40.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1458000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1459000, episode_reward=651.55 +/- 669.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 652      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1459000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.09    |\n",
      "|    critic_loss     | 2.45     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=1217.58 +/- 25.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1460000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1460     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2099     |\n",
      "|    total_timesteps | 1460000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1461000, episode_reward=1271.60 +/- 27.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1461000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.598    |\n",
      "|    critic_loss     | 2.21     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -0.887   |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1462000, episode_reward=1225.73 +/- 50.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1462000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1463000, episode_reward=1423.99 +/- 33.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1463000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.528   |\n",
      "|    critic_loss     | 2.24     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -0.913   |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1464000, episode_reward=1419.21 +/- 36.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1464000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1464     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2105     |\n",
      "|    total_timesteps | 1464000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1465000, episode_reward=1114.16 +/- 593.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1465000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.16    |\n",
      "|    critic_loss     | 2.5      |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -1.42    |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1466000, episode_reward=1124.04 +/- 598.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1466000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1467000, episode_reward=1446.38 +/- 36.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1467000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.16    |\n",
      "|    critic_loss     | 2.3      |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1468000, episode_reward=1454.53 +/- 22.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1468000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1468     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2111     |\n",
      "|    total_timesteps | 1468000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1469000, episode_reward=1375.01 +/- 33.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1469000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.46    |\n",
      "|    critic_loss     | 2.45     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 0.192    |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=1430.44 +/- 36.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1470000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471000, episode_reward=1399.07 +/- 21.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1471000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1472000, episode_reward=1430.07 +/- 45.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1472000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1472     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2117     |\n",
      "|    total_timesteps | 1472000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1473000, episode_reward=1393.65 +/- 26.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1473000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.47    |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | 2.15     |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1474000, episode_reward=1379.99 +/- 14.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1474000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1475000, episode_reward=1400.18 +/- 51.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1475000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1476000, episode_reward=1371.23 +/- 24.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1476000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1476     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2122     |\n",
      "|    total_timesteps | 1476000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1477000, episode_reward=-160.98 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -161     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1477000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.62    |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | -0.787   |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1478000, episode_reward=-160.28 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1478000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1479000, episode_reward=-180.85 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -181     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1479000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.474   |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=-181.13 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -181     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1480000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1480     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2129     |\n",
      "|    total_timesteps | 1480000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1481000, episode_reward=1444.84 +/- 27.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1481000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.7      |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -7.47    |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1482000, episode_reward=1429.14 +/- 30.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1482000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1483000, episode_reward=1388.50 +/- 42.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1483000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.898   |\n",
      "|    critic_loss     | 2.34     |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1484000, episode_reward=1427.53 +/- 27.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1484000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1484     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2135     |\n",
      "|    total_timesteps | 1484000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1485000, episode_reward=1320.00 +/- 18.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1485000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.818   |\n",
      "|    critic_loss     | 2.29     |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -0.254   |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1486000, episode_reward=1344.17 +/- 29.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1486000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1487000, episode_reward=1348.33 +/- 28.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1487000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.339   |\n",
      "|    critic_loss     | 2.33     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.00351  |\n",
      "|    n_updates       | 7260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1488000, episode_reward=1375.65 +/- 42.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1488000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1488     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2141     |\n",
      "|    total_timesteps | 1488000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1489000, episode_reward=1328.54 +/- 51.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1489000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.694   |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | -0.066   |\n",
      "|    learning_rate   | 0.00351  |\n",
      "|    n_updates       | 7270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=1342.09 +/- 47.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1490000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1491000, episode_reward=1331.34 +/- 38.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1491000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.342   |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 0.217    |\n",
      "|    learning_rate   | 0.00351  |\n",
      "|    n_updates       | 7280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1492000, episode_reward=1325.71 +/- 47.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1492000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1492     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2147     |\n",
      "|    total_timesteps | 1492000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1493000, episode_reward=1376.72 +/- 23.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1493000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | 3.14     |\n",
      "|    learning_rate   | 0.00351  |\n",
      "|    n_updates       | 7290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1494000, episode_reward=1337.61 +/- 56.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1494000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1495000, episode_reward=1334.12 +/- 21.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1495000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1496000, episode_reward=1417.75 +/- 52.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1496000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.644   |\n",
      "|    critic_loss     | 2.27     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7300     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1496     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2153     |\n",
      "|    total_timesteps | 1496000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1497000, episode_reward=1407.88 +/- 37.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1497000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1498000, episode_reward=-400.33 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -400     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1498000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.32    |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -0.116   |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1499000, episode_reward=-399.99 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -400     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1499000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=1416.69 +/- 45.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.55     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | -4.42    |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7320     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1500     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2159     |\n",
      "|    total_timesteps | 1500000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1501000, episode_reward=1435.84 +/- 28.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1501000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1502000, episode_reward=1320.32 +/- 48.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1502000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.685   |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 1.64     |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1503000, episode_reward=1312.93 +/- 44.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1503000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1504000, episode_reward=1323.73 +/- 41.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1504000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.45     |\n",
      "|    critic_loss     | 2.47     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 0.577    |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7340     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1504     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2165     |\n",
      "|    total_timesteps | 1504000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1505000, episode_reward=1369.38 +/- 39.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1505000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1506000, episode_reward=1363.26 +/- 34.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1506000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.17     |\n",
      "|    critic_loss     | 2.27     |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -0.423   |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1507000, episode_reward=1405.71 +/- 31.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1507000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1508000, episode_reward=-183.24 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1508000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.419    |\n",
      "|    critic_loss     | 2.36     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | -2.05    |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7360     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1508     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2171     |\n",
      "|    total_timesteps | 1508000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1509000, episode_reward=-182.24 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -182     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1509000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=-322.48 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -322     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.63     |\n",
      "|    critic_loss     | 0.964    |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | -8.22    |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1511000, episode_reward=-322.13 +/- 2.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -322     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1511000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=450.91 +/- 765.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1512000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | -4.01    |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7380     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1512     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2177     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1513000, episode_reward=773.68 +/- 772.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 774      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1513000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1514000, episode_reward=-255.05 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1514000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.35     |\n",
      "|    critic_loss     | 2.34     |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | -5.08    |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1515000, episode_reward=-253.28 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -253     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1515000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1516000, episode_reward=-243.15 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1516000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.99     |\n",
      "|    critic_loss     | 0.771    |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7400     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1516     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2183     |\n",
      "|    total_timesteps | 1516000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1517000, episode_reward=-242.72 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1517000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1518000, episode_reward=506.14 +/- 573.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 506      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1518000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.485    |\n",
      "|    critic_loss     | 2.71     |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | 2.07     |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1519000, episode_reward=-10.53 +/- 384.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1519000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=-182.16 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -182     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.54     |\n",
      "|    critic_loss     | 2.2      |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | 2.81     |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7420     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1520     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2189     |\n",
      "|    total_timesteps | 1520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1521000, episode_reward=-184.35 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -184     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1521000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1522000, episode_reward=1090.82 +/- 21.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1522000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.438    |\n",
      "|    critic_loss     | 2.15     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | 3.01     |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1523000, episode_reward=1084.27 +/- 28.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1523000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1524000, episode_reward=521.55 +/- 708.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 522      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1524000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.33     |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | 5.75     |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7440     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1524     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2194     |\n",
      "|    total_timesteps | 1524000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1525000, episode_reward=398.47 +/- 702.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 398      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1525000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1526000, episode_reward=1406.91 +/- 19.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1526000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.671    |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | 3.05     |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1527000, episode_reward=1353.78 +/- 67.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1527000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1528000, episode_reward=1371.40 +/- 27.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1528000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.37     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7460     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1528     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2200     |\n",
      "|    total_timesteps | 1528000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1529000, episode_reward=1407.31 +/- 39.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1529000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=1359.00 +/- 49.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.386   |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | 3.25     |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1531000, episode_reward=1349.31 +/- 38.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1531000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532000, episode_reward=1348.61 +/- 25.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1532000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0435  |\n",
      "|    critic_loss     | 2.04     |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | 3.37     |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1532     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2206     |\n",
      "|    total_timesteps | 1532000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1533000, episode_reward=1292.87 +/- 30.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1533000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1534000, episode_reward=1343.79 +/- 37.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1534000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.324   |\n",
      "|    critic_loss     | 2.01     |\n",
      "|    ent_coef        | 0.0239   |\n",
      "|    ent_coef_loss   | 3.11     |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1535000, episode_reward=1358.19 +/- 49.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1535000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1536000, episode_reward=1394.24 +/- 16.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1536000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1536     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2211     |\n",
      "|    total_timesteps | 1536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1537000, episode_reward=1364.95 +/- 21.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1537000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.376   |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -0.405   |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1538000, episode_reward=1373.64 +/- 23.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1538000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1539000, episode_reward=1424.86 +/- 17.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1539000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.256    |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 0.0663   |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=1394.56 +/- 36.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1540000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1540     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2217     |\n",
      "|    total_timesteps | 1540000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1541000, episode_reward=1299.21 +/- 60.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1541000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.403   |\n",
      "|    critic_loss     | 2.56     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | 1.73     |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1542000, episode_reward=1304.17 +/- 20.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1542000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1543000, episode_reward=1380.69 +/- 21.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1543000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0142  |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | -0.311   |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1544000, episode_reward=1394.90 +/- 15.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1544000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1544     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2223     |\n",
      "|    total_timesteps | 1544000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1545000, episode_reward=1424.80 +/- 26.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1545000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.22    |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | 1.25     |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1546000, episode_reward=1417.47 +/- 15.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1546000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1547000, episode_reward=1391.36 +/- 22.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1547000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.335   |\n",
      "|    critic_loss     | 2.11     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | 0.683    |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1548000, episode_reward=1350.75 +/- 31.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1548000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1548     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2228     |\n",
      "|    total_timesteps | 1548000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1549000, episode_reward=1422.97 +/- 13.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1549000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.143   |\n",
      "|    critic_loss     | 2.02     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=1431.28 +/- 26.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1550000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1551000, episode_reward=1467.80 +/- 36.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1551000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.427   |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | 1.82     |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552000, episode_reward=1447.00 +/- 46.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1552000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1552     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2234     |\n",
      "|    total_timesteps | 1552000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1553000, episode_reward=1454.98 +/- 24.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1553000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.614   |\n",
      "|    critic_loss     | 2.24     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1554000, episode_reward=1447.22 +/- 47.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1554000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1555000, episode_reward=1454.42 +/- 27.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1555000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.638   |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | 1        |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1556000, episode_reward=1477.46 +/- 12.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1556000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1556     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2240     |\n",
      "|    total_timesteps | 1556000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1557000, episode_reward=1339.26 +/- 63.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1557000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.272   |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | 0.542    |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1558000, episode_reward=1326.96 +/- 14.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1558000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1559000, episode_reward=1455.70 +/- 26.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1559000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.122    |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | 0.148    |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=1446.19 +/- 37.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1560000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1560     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2245     |\n",
      "|    total_timesteps | 1560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1561000, episode_reward=1464.68 +/- 46.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1561000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.606   |\n",
      "|    critic_loss     | 2.04     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | -0.445   |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1562000, episode_reward=1430.58 +/- 16.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1562000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1563000, episode_reward=1386.53 +/- 34.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1563000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.444   |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | -0.123   |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1564000, episode_reward=1374.81 +/- 31.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1564000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1564     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2251     |\n",
      "|    total_timesteps | 1564000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1565000, episode_reward=1482.40 +/- 20.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1565000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.202   |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | -0.112   |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1566000, episode_reward=1505.12 +/- 18.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1566000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1567000, episode_reward=1340.05 +/- 58.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1567000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.758   |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | -0.272   |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1568000, episode_reward=1370.41 +/- 33.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1568000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1568     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2257     |\n",
      "|    total_timesteps | 1568000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1569000, episode_reward=1358.62 +/- 53.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1569000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.208   |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=1331.33 +/- 23.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1570000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1571000, episode_reward=1347.74 +/- 27.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1571000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0545   |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1572000, episode_reward=1353.23 +/- 33.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1572000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1572     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2262     |\n",
      "|    total_timesteps | 1572000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1573000, episode_reward=1350.51 +/- 35.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1573000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.00973  |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | 0.049    |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1574000, episode_reward=1358.71 +/- 45.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1574000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1575000, episode_reward=1396.56 +/- 23.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1575000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0065   |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1576000, episode_reward=1380.89 +/- 22.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1576000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1576     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2268     |\n",
      "|    total_timesteps | 1576000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1577000, episode_reward=1344.05 +/- 18.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1577000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.593   |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | 0.389    |\n",
      "|    learning_rate   | 0.00342  |\n",
      "|    n_updates       | 7700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1578000, episode_reward=1333.17 +/- 25.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1578000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1579000, episode_reward=1331.59 +/- 36.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1579000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=1369.94 +/- 18.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0345   |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | -0.37    |\n",
      "|    learning_rate   | 0.00342  |\n",
      "|    n_updates       | 7710     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1580     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2274     |\n",
      "|    total_timesteps | 1580000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1581000, episode_reward=1384.76 +/- 12.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1581000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1582000, episode_reward=1408.23 +/- 21.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1582000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.34    |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | 0.365    |\n",
      "|    learning_rate   | 0.00342  |\n",
      "|    n_updates       | 7720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1583000, episode_reward=1392.51 +/- 18.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1583000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1584000, episode_reward=1420.03 +/- 25.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1584000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.357   |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | -0.0671  |\n",
      "|    learning_rate   | 0.00342  |\n",
      "|    n_updates       | 7730     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1584     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2279     |\n",
      "|    total_timesteps | 1584000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1585000, episode_reward=1393.84 +/- 48.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1585000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1586000, episode_reward=1372.67 +/- 18.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1586000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.517   |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -0.324   |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1587000, episode_reward=1352.27 +/- 32.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1587000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1588000, episode_reward=1385.66 +/- 19.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1588000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.142   |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | -2.12    |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7750     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1588     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2285     |\n",
      "|    total_timesteps | 1588000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1589000, episode_reward=1368.97 +/- 32.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1589000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=1369.90 +/- 27.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.468   |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | -1.89    |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1591000, episode_reward=1348.20 +/- 22.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1591000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592000, episode_reward=1401.81 +/- 28.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1592000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.372    |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | -1.62    |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7770     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1592     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2291     |\n",
      "|    total_timesteps | 1592000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1593000, episode_reward=1400.10 +/- 47.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1593000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1594000, episode_reward=1381.99 +/- 28.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1594000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.312   |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | -2.54    |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1595000, episode_reward=1366.25 +/- 21.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1595000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1596000, episode_reward=1437.84 +/- 21.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1596000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.286   |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -0.78    |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7790     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1596     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2296     |\n",
      "|    total_timesteps | 1596000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1597000, episode_reward=1427.64 +/- 37.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1597000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1598000, episode_reward=1428.39 +/- 31.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1598000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0469   |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | 0.00959  |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1599000, episode_reward=1425.07 +/- 29.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1599000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=1438.14 +/- 40.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.556   |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | 1.46     |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7810     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1600     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2302     |\n",
      "|    total_timesteps | 1600000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1601000, episode_reward=1447.26 +/- 23.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1601000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1602000, episode_reward=1448.52 +/- 31.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1602000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.9     |\n",
      "|    critic_loss     | 2.04     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | 2.65     |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1603000, episode_reward=1463.96 +/- 43.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1603000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1604000, episode_reward=1453.44 +/- 34.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1604000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.59    |\n",
      "|    critic_loss     | 2.04     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | -0.275   |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7830     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1604     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2307     |\n",
      "|    total_timesteps | 1604000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1605000, episode_reward=1465.25 +/- 19.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1605000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1606000, episode_reward=1368.13 +/- 50.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1606000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.445   |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -0.832   |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1607000, episode_reward=1369.88 +/- 32.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1607000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1608000, episode_reward=1401.29 +/- 45.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1608000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.52    |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7850     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1608     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2313     |\n",
      "|    total_timesteps | 1608000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1609000, episode_reward=1426.33 +/- 20.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1609000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=1439.66 +/- 41.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.131   |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -0.749   |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1611000, episode_reward=1443.45 +/- 14.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1611000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612000, episode_reward=1425.99 +/- 45.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1612000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.548   |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7870     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1612     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2319     |\n",
      "|    total_timesteps | 1612000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1613000, episode_reward=1425.37 +/- 40.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1613000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1614000, episode_reward=1396.78 +/- 62.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1614000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.419   |\n",
      "|    critic_loss     | 2.14     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1615000, episode_reward=1400.55 +/- 43.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1615000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1616000, episode_reward=1412.62 +/- 44.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1616000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.359   |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7890     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1616     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2324     |\n",
      "|    total_timesteps | 1616000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1617000, episode_reward=1470.93 +/- 30.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1617000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1618000, episode_reward=1426.59 +/- 52.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1618000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.115   |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | -0.502   |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1619000, episode_reward=1429.34 +/- 45.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1619000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=1470.80 +/- 46.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.161   |\n",
      "|    critic_loss     | 1.95     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -0.799   |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1620     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2330     |\n",
      "|    total_timesteps | 1620000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1621000, episode_reward=1476.54 +/- 39.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1621000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1622000, episode_reward=1470.33 +/- 25.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1622000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1623000, episode_reward=1463.10 +/- 51.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1623000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.586   |\n",
      "|    critic_loss     | 2.13     |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -0.258   |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1624000, episode_reward=1443.18 +/- 35.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1624000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1624     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2336     |\n",
      "|    total_timesteps | 1624000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1625000, episode_reward=1471.87 +/- 27.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1625000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.384   |\n",
      "|    critic_loss     | 2.13     |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | -0.759   |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1626000, episode_reward=1499.17 +/- 25.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1626000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1627000, episode_reward=1426.91 +/- 20.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1627000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.858   |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | -0.208   |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1628000, episode_reward=1428.43 +/- 46.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1628000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1628     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2341     |\n",
      "|    total_timesteps | 1628000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1629000, episode_reward=1397.40 +/- 43.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1629000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.526   |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -0.435   |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=1423.34 +/- 37.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1630000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1631000, episode_reward=1476.34 +/- 22.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1631000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.111   |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | -0.033   |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632000, episode_reward=1420.56 +/- 50.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1632000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1632     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2347     |\n",
      "|    total_timesteps | 1632000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1633000, episode_reward=1423.16 +/- 32.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1633000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0657   |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 1.52     |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1634000, episode_reward=1439.45 +/- 11.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1634000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1635000, episode_reward=1405.75 +/- 30.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1635000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.254   |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | -0.275   |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1636000, episode_reward=1415.11 +/- 22.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1636000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1636     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2353     |\n",
      "|    total_timesteps | 1636000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1637000, episode_reward=1371.04 +/- 39.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1637000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.102   |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0231   |\n",
      "|    ent_coef_loss   | -0.205   |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 7990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1638000, episode_reward=1371.51 +/- 43.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1638000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1639000, episode_reward=1420.02 +/- 27.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1639000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.251    |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | -1.79    |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=1472.65 +/- 13.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1640000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1640     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2358     |\n",
      "|    total_timesteps | 1640000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1641000, episode_reward=1457.17 +/- 17.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1641000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.589   |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 8010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1642000, episode_reward=1427.06 +/- 10.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1642000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1643000, episode_reward=1352.27 +/- 30.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1643000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.687   |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 8020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1644000, episode_reward=1367.91 +/- 37.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1644000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1644     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2364     |\n",
      "|    total_timesteps | 1644000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1645000, episode_reward=1371.34 +/- 34.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1645000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.242   |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.0231   |\n",
      "|    ent_coef_loss   | 0.909    |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 8030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1646000, episode_reward=1425.26 +/- 31.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1646000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1647000, episode_reward=728.51 +/- 793.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 729      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1647000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.231   |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1648000, episode_reward=396.36 +/- 780.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 396      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1648000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1648     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2370     |\n",
      "|    total_timesteps | 1648000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1649000, episode_reward=-180.78 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -181     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1649000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.937    |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=-179.33 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -179     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1650000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1651000, episode_reward=-188.00 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -188     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1651000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.4     |\n",
      "|    critic_loss     | 0.639    |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1652000, episode_reward=-187.90 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -188     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1652000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1652     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2376     |\n",
      "|    total_timesteps | 1652000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1653000, episode_reward=-266.09 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -266     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1653000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 0.339    |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1654000, episode_reward=-265.33 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1654000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1655000, episode_reward=-481.40 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -481     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1655000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.228    |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1656000, episode_reward=-481.01 +/- 1.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -481     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1656000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1656     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2381     |\n",
      "|    total_timesteps | 1656000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1657000, episode_reward=-335.98 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1657000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.2     |\n",
      "|    critic_loss     | 0.249    |\n",
      "|    ent_coef        | 0.0172   |\n",
      "|    ent_coef_loss   | -0.203   |\n",
      "|    learning_rate   | 0.00334  |\n",
      "|    n_updates       | 8090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1658000, episode_reward=-335.18 +/- 2.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1658000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1659000, episode_reward=-388.68 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -389     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1659000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.261    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.00334  |\n",
      "|    n_updates       | 8100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=-388.53 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -389     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1660000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1660     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2388     |\n",
      "|    total_timesteps | 1660000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1661000, episode_reward=-431.31 +/- 2.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -431     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1661000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 0.156    |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | -2.46    |\n",
      "|    learning_rate   | 0.00334  |\n",
      "|    n_updates       | 8110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1662000, episode_reward=-430.51 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -431     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1662000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1663000, episode_reward=-384.24 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -384     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1663000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 0.0917   |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | -7.77    |\n",
      "|    learning_rate   | 0.00334  |\n",
      "|    n_updates       | 8120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1664000, episode_reward=-383.50 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -384     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1664000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1664     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2393     |\n",
      "|    total_timesteps | 1664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1665000, episode_reward=-383.50 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1665000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1666000, episode_reward=-379.78 +/- 1.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -380     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1666000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.135    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -2.89    |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1667000, episode_reward=-378.93 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -379     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1667000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1668000, episode_reward=-428.10 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -428     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1668000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 0.149    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 989      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1668     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2399     |\n",
      "|    total_timesteps | 1668000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1669000, episode_reward=-428.53 +/- 1.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -429     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1669000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=-362.15 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -362     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.136    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -3.98    |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1671000, episode_reward=-361.36 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -361     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1671000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1672000, episode_reward=-449.84 +/- 2.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -450     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1672000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.0868   |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 8.07     |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8160     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 923      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1672     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2405     |\n",
      "|    total_timesteps | 1672000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673000, episode_reward=-449.19 +/- 3.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -449     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1673000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1674000, episode_reward=-393.57 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -394     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1674000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.115    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -2.44    |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1675000, episode_reward=-393.01 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -393     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1675000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1676000, episode_reward=-483.32 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -483     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1676000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.0956   |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 8.4      |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8180     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 855      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1676     |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 2411     |\n",
      "|    total_timesteps | 1676000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1677000, episode_reward=-480.98 +/- 1.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -481     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1677000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1678000, episode_reward=-381.83 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1678000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.0484   |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1679000, episode_reward=-380.94 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1679000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=-390.64 +/- 0.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -391     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 0.0661   |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | 1.76     |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8200     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 787      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1680     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2417     |\n",
      "|    total_timesteps | 1680000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1681000, episode_reward=-390.97 +/- 1.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -391     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1681000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1682000, episode_reward=-367.23 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -367     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1682000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.0767   |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -4.2     |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1683000, episode_reward=-368.30 +/- 1.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -368     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1683000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1684000, episode_reward=-334.37 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -334     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1684000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.062    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8220     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 722      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1684     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2423     |\n",
      "|    total_timesteps | 1684000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1685000, episode_reward=-335.14 +/- 1.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1685000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1686000, episode_reward=-470.24 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -470     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1686000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.148    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 6.64     |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1687000, episode_reward=-469.99 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -470     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1687000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1688000, episode_reward=-341.90 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -342     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1688000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.062    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -2.16    |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8240     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 656      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1688     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2429     |\n",
      "|    total_timesteps | 1688000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1689000, episode_reward=-339.98 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1689000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=-236.28 +/- 3.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.123    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -6.14    |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1691000, episode_reward=-237.44 +/- 3.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -237     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1691000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1692000, episode_reward=-299.55 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -300     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1692000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.158    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8260     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 595      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1692     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2435     |\n",
      "|    total_timesteps | 1692000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693000, episode_reward=-298.45 +/- 1.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -298     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1693000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1694000, episode_reward=-299.49 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -299     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1694000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.0687   |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1695000, episode_reward=-300.23 +/- 2.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -300     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1695000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1696000, episode_reward=-293.67 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -294     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1696000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.0581   |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | -6.75    |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8280     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 531      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1696     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2441     |\n",
      "|    total_timesteps | 1696000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1697000, episode_reward=-294.75 +/- 1.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -295     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1697000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1698000, episode_reward=-275.83 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -276     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1698000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.0751   |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -5.43    |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1699000, episode_reward=-275.91 +/- 2.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -276     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1699000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=-265.00 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.132    |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | -5.33    |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8300     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 467      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1700     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2447     |\n",
      "|    total_timesteps | 1700000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1701000, episode_reward=-266.09 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -266     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1701000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1702000, episode_reward=-158.79 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1702000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.108    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1703000, episode_reward=-156.06 +/- 3.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1703000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1704000, episode_reward=255.61 +/- 6.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1704000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.237    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8320     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 404      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1704     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2453     |\n",
      "|    total_timesteps | 1704000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1705000, episode_reward=255.30 +/- 4.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 255      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1705000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1706000, episode_reward=-93.07 +/- 1.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -93.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1706000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.3     |\n",
      "|    critic_loss     | 0.839    |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1707000, episode_reward=-97.76 +/- 3.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1707000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1708000, episode_reward=-96.40 +/- 4.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -96.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1708000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 351      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1708     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2459     |\n",
      "|    total_timesteps | 1708000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1709000, episode_reward=-149.42 +/- 1.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -149     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1709000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.574    |\n",
      "|    ent_coef        | 0.0094   |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=-149.55 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -150     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1710000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1711000, episode_reward=-523.71 +/- 6.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -524     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1711000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.317    |\n",
      "|    ent_coef        | 0.00865  |\n",
      "|    ent_coef_loss   | -14.7    |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1712000, episode_reward=-525.22 +/- 8.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -525     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1712000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 300      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1712     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2465     |\n",
      "|    total_timesteps | 1712000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713000, episode_reward=-483.65 +/- 4.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -484     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1713000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.3     |\n",
      "|    critic_loss     | 3.75     |\n",
      "|    ent_coef        | 0.00823  |\n",
      "|    ent_coef_loss   | 49.4     |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1714000, episode_reward=-479.90 +/- 3.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -480     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1714000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1715000, episode_reward=-521.39 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -521     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1715000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 0.646    |\n",
      "|    ent_coef        | 0.00899  |\n",
      "|    ent_coef_loss   | 41.8     |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1716000, episode_reward=-521.24 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -521     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1716000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 226      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1716     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2471     |\n",
      "|    total_timesteps | 1716000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1717000, episode_reward=-496.33 +/- 1.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -496     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1717000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.292    |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 55.1     |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1718000, episode_reward=-495.66 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -496     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1718000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1719000, episode_reward=-468.60 +/- 2.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -469     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1719000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0887   |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | 38.2     |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=-467.55 +/- 2.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -468     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1720000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 154      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1720     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2477     |\n",
      "|    total_timesteps | 1720000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1721000, episode_reward=-465.70 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -466     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1721000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.182    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 21.6     |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1722000, episode_reward=-465.61 +/- 2.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -466     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1722000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1723000, episode_reward=-475.79 +/- 1.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -476     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1723000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 0.0787   |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 15.3     |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1724000, episode_reward=-474.99 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -475     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1724000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 74.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1724     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2483     |\n",
      "|    total_timesteps | 1724000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1725000, episode_reward=-455.31 +/- 2.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -455     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1725000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.1     |\n",
      "|    critic_loss     | 0.0443   |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 13.7     |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1726000, episode_reward=-455.54 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -456     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1726000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1727000, episode_reward=-447.61 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -448     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1727000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 0.0779   |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | 10.4     |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1728000, episode_reward=-447.27 +/- 2.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -447     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1728000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -2.19    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1728     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2489     |\n",
      "|    total_timesteps | 1728000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1729000, episode_reward=-492.92 +/- 2.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -493     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1729000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 0.0812   |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | 9.21     |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=-490.04 +/- 1.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -490     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1730000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1731000, episode_reward=-503.13 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -503     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1731000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.11     |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | 9.75     |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1732000, episode_reward=-502.15 +/- 3.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -502     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1732000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -71.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1732     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2494     |\n",
      "|    total_timesteps | 1732000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1733000, episode_reward=-463.42 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -463     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1733000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.147    |\n",
      "|    ent_coef        | 0.0175   |\n",
      "|    ent_coef_loss   | 5.26     |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1734000, episode_reward=-462.76 +/- 2.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -463     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1734000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1735000, episode_reward=-477.86 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -478     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1735000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.5     |\n",
      "|    critic_loss     | 0.172    |\n",
      "|    ent_coef        | 0.0179   |\n",
      "|    ent_coef_loss   | 4.33     |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1736000, episode_reward=-477.90 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -478     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1736000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -143     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1736     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2500     |\n",
      "|    total_timesteps | 1736000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1737000, episode_reward=-496.07 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -496     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1737000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.3     |\n",
      "|    critic_loss     | 0.103    |\n",
      "|    ent_coef        | 0.0182   |\n",
      "|    ent_coef_loss   | 12.1     |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1738000, episode_reward=-495.87 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -496     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1738000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1739000, episode_reward=-518.51 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -519     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1739000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 0.0891   |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | 12.3     |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=-518.18 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -518     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1740000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1740     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2506     |\n",
      "|    total_timesteps | 1740000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1741000, episode_reward=-522.84 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -523     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1741000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 0.0395   |\n",
      "|    ent_coef        | 0.0194   |\n",
      "|    ent_coef_loss   | 11.7     |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1742000, episode_reward=-522.57 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -523     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1742000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1743000, episode_reward=-503.77 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -504     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1743000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 0.048    |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 9.11     |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1744000, episode_reward=-503.82 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -504     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1744000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -286     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1744     |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 2512     |\n",
      "|    total_timesteps | 1744000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1745000, episode_reward=-469.95 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -470     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1745000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 0.0736   |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 5.72     |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1746000, episode_reward=-470.08 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -470     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1746000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1747000, episode_reward=-462.23 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -462     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1747000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 0.0778   |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | 4.34     |\n",
      "|    learning_rate   | 0.00325  |\n",
      "|    n_updates       | 8530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1748000, episode_reward=-461.74 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -462     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1748000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -352     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1748     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2519     |\n",
      "|    total_timesteps | 1748000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1749000, episode_reward=-440.17 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -440     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1749000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.0502   |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | 1.77     |\n",
      "|    learning_rate   | 0.00325  |\n",
      "|    n_updates       | 8540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=-439.66 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -440     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1750000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1751000, episode_reward=-438.93 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -439     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1751000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1752000, episode_reward=-433.11 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -433     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1752000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 0.0775   |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.00325  |\n",
      "|    n_updates       | 8550     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -371     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1752     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2525     |\n",
      "|    total_timesteps | 1752000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753000, episode_reward=-432.25 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -432     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1753000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1754000, episode_reward=-431.48 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -431     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1754000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.0761   |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.00325  |\n",
      "|    n_updates       | 8560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1755000, episode_reward=-430.05 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -430     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1755000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1756000, episode_reward=-425.38 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -425     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1756000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.0719   |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 2.11     |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8570     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -372     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1756     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2531     |\n",
      "|    total_timesteps | 1756000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1757000, episode_reward=-425.11 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -425     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1757000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1758000, episode_reward=-406.48 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -406     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1758000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.057    |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 0.0395   |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1759000, episode_reward=-405.21 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -405     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1759000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=-375.57 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.0599   |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | -0.71    |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8590     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -372     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1760     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2538     |\n",
      "|    total_timesteps | 1760000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1761000, episode_reward=-376.27 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1761000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1762000, episode_reward=-369.74 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1762000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.0482   |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 0.398    |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1763000, episode_reward=-369.47 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -369     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1763000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1764000, episode_reward=-381.14 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1764000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.0495   |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | -0.38    |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8610     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -370     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1764     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2544     |\n",
      "|    total_timesteps | 1764000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1765000, episode_reward=-381.61 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1765000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1766000, episode_reward=-352.57 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1766000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.0447   |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1767000, episode_reward=-353.09 +/- 1.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1767000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1768000, episode_reward=-339.98 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1768000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.0442   |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8630     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -368     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1768     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2550     |\n",
      "|    total_timesteps | 1768000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1769000, episode_reward=-340.46 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1769000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=-333.90 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -334     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.0493   |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1771000, episode_reward=-332.17 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -332     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1771000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1772000, episode_reward=-350.40 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1772000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.0491   |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8650     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -365     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1772     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2556     |\n",
      "|    total_timesteps | 1772000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1773000, episode_reward=-348.80 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -349     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1773000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774000, episode_reward=-374.53 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -375     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1774000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.0479   |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | 2.3      |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1775000, episode_reward=-373.91 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -374     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1775000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1776000, episode_reward=-350.72 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -351     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1776000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.036    |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | 0.962    |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8670     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -362     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1776     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2562     |\n",
      "|    total_timesteps | 1776000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1777000, episode_reward=-349.62 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1777000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1778000, episode_reward=-336.58 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -337     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1778000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 0.0378   |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 1.84     |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1779000, episode_reward=-335.39 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1779000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=-335.36 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 0.0468   |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8690     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -358     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1780     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2568     |\n",
      "|    total_timesteps | 1780000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1781000, episode_reward=-336.23 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1781000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1782000, episode_reward=-351.54 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -352     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1782000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 0.0444   |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1783000, episode_reward=-352.65 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1783000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1784000, episode_reward=-417.81 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -418     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1784000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.0454   |\n",
      "|    ent_coef        | 0.0231   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8710     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -356     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1784     |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 2574     |\n",
      "|    total_timesteps | 1784000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1785000, episode_reward=-417.57 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -418     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1785000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1786000, episode_reward=-411.71 +/- 1.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -412     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1786000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0363   |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.00321  |\n",
      "|    n_updates       | 8720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1787000, episode_reward=-411.01 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -411     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1787000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1788000, episode_reward=-413.02 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -413     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1788000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0441   |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -0.0886  |\n",
      "|    learning_rate   | 0.00321  |\n",
      "|    n_updates       | 8730     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -354     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1788     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2580     |\n",
      "|    total_timesteps | 1788000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1789000, episode_reward=-413.92 +/- 0.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -414     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1789000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=-413.33 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -413     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.0486   |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -0.623   |\n",
      "|    learning_rate   | 0.00321  |\n",
      "|    n_updates       | 8740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1791000, episode_reward=-412.20 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -412     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1791000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1792000, episode_reward=-412.45 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -412     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1792000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -356     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1792     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2585     |\n",
      "|    total_timesteps | 1792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1793000, episode_reward=-404.93 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -405     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1793000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0435   |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -0.714   |\n",
      "|    learning_rate   | 0.00321  |\n",
      "|    n_updates       | 8750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794000, episode_reward=-404.25 +/- 1.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -404     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1794000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1795000, episode_reward=-402.94 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1795000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.05     |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -0.922   |\n",
      "|    learning_rate   | 0.00321  |\n",
      "|    n_updates       | 8760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1796000, episode_reward=-403.56 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -404     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1796000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -356     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1796     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2591     |\n",
      "|    total_timesteps | 1796000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1797000, episode_reward=-422.19 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -422     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1797000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0372   |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | -1.63    |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1798000, episode_reward=-422.54 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -423     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1798000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1799000, episode_reward=-408.33 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -408     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1799000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.0387   |\n",
      "|    ent_coef        | 0.0231   |\n",
      "|    ent_coef_loss   | -0.627   |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=-407.49 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -407     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -357     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1800     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2597     |\n",
      "|    total_timesteps | 1800000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1801000, episode_reward=-402.82 +/- 1.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1801000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.033    |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | -2.13    |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1802000, episode_reward=-402.54 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1802000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1803000, episode_reward=-408.54 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -409     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1803000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.0474   |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -2.23    |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1804000, episode_reward=-408.64 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -409     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1804000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -361     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1804     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2603     |\n",
      "|    total_timesteps | 1804000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1805000, episode_reward=-399.70 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -400     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1805000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 0.0493   |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | -2.76    |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1806000, episode_reward=-398.24 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -398     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1806000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1807000, episode_reward=-383.69 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -384     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1807000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 0.0446   |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -4.43    |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1808000, episode_reward=-383.60 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -384     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1808000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -373     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1808     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2608     |\n",
      "|    total_timesteps | 1808000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1809000, episode_reward=-370.80 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1809000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.0581   |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | -5.42    |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=-369.59 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1810000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1811000, episode_reward=-352.23 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -352     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1811000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.0852   |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -7.67    |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1812000, episode_reward=-351.66 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -352     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1812000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -383     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1812     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2615     |\n",
      "|    total_timesteps | 1812000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1813000, episode_reward=-213.54 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -214     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1813000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 0.119    |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1814000, episode_reward=-213.63 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -214     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1814000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1815000, episode_reward=836.29 +/- 27.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 836      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1815000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1816000, episode_reward=801.31 +/- 18.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 801      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1816000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -352     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1816     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2621     |\n",
      "|    total_timesteps | 1816000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1817000, episode_reward=46.82 +/- 68.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 46.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1817000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.71     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1818000, episode_reward=38.08 +/- 28.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 38.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1818000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1819000, episode_reward=107.12 +/- 256.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1819000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.34     |\n",
      "|    critic_loss     | 0.551    |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | -9.32    |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=-46.73 +/- 232.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -46.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1820000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -319     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1820     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2626     |\n",
      "|    total_timesteps | 1820000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1821000, episode_reward=56.66 +/- 353.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 56.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1821000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.61     |\n",
      "|    critic_loss     | 0.473    |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | -16.9    |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1822000, episode_reward=92.77 +/- 382.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 92.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1822000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1823000, episode_reward=-23.68 +/- 413.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1823000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6        |\n",
      "|    critic_loss     | 0.367    |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | -16.8    |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1824000, episode_reward=-187.89 +/- 347.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -188     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1824000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -285     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1824     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2632     |\n",
      "|    total_timesteps | 1824000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1825000, episode_reward=-247.93 +/- 2.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1825000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.27     |\n",
      "|    critic_loss     | 0.323    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -14.6    |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1826000, episode_reward=-241.57 +/- 11.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1826000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1827000, episode_reward=-122.45 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1827000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.38     |\n",
      "|    critic_loss     | 0.18     |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -15.6    |\n",
      "|    learning_rate   | 0.00317  |\n",
      "|    n_updates       | 8920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1828000, episode_reward=-112.55 +/- 17.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1828000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -264     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1828     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2638     |\n",
      "|    total_timesteps | 1828000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1829000, episode_reward=-185.29 +/- 4.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -185     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1829000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.18     |\n",
      "|    critic_loss     | 0.157    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.00317  |\n",
      "|    n_updates       | 8930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=-182.91 +/- 4.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1830000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1831000, episode_reward=-143.41 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1831000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.22     |\n",
      "|    critic_loss     | 0.113    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.00317  |\n",
      "|    n_updates       | 8940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1832000, episode_reward=-145.01 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1832000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1832     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2645     |\n",
      "|    total_timesteps | 1832000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1833000, episode_reward=-191.56 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1833000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.15     |\n",
      "|    critic_loss     | 0.12     |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -14.5    |\n",
      "|    learning_rate   | 0.00317  |\n",
      "|    n_updates       | 8950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834000, episode_reward=-190.30 +/- 2.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1834000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1835000, episode_reward=-189.57 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1835000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1836000, episode_reward=-199.18 +/- 59.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -199     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1836000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.08     |\n",
      "|    critic_loss     | 0.0887   |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 8960     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -239     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1836     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2651     |\n",
      "|    total_timesteps | 1836000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1837000, episode_reward=-256.45 +/- 74.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -256     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1837000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1838000, episode_reward=-184.09 +/- 1.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -184     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1838000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.37     |\n",
      "|    critic_loss     | 0.0959   |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | -8.69    |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 8970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1839000, episode_reward=-184.91 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -185     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1839000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=-177.80 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -178     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.07     |\n",
      "|    critic_loss     | 0.0697   |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 8980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -228     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1840     |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 2658     |\n",
      "|    total_timesteps | 1840000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1841000, episode_reward=-178.93 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -179     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1841000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1842000, episode_reward=-184.88 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -185     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1842000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.08     |\n",
      "|    critic_loss     | 0.0615   |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | -9.82    |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 8990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1843000, episode_reward=-184.03 +/- 2.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -184     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1843000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1844000, episode_reward=-214.57 +/- 4.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1844000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.05     |\n",
      "|    critic_loss     | 0.0688   |\n",
      "|    ent_coef        | 0.0112   |\n",
      "|    ent_coef_loss   | -8.22    |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 9000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1844     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2665     |\n",
      "|    total_timesteps | 1844000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1845000, episode_reward=-215.46 +/- 4.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1845000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1846000, episode_reward=-213.17 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1846000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.18     |\n",
      "|    critic_loss     | 0.0722   |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -15.7    |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1847000, episode_reward=-212.85 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1847000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1848000, episode_reward=-451.62 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -452     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1848000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.23     |\n",
      "|    critic_loss     | 0.0867   |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -8.34    |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9020     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1848     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2671     |\n",
      "|    total_timesteps | 1848000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1849000, episode_reward=-450.51 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -451     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1849000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1850000, episode_reward=-476.34 +/- 1.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -476     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.67     |\n",
      "|    critic_loss     | 0.0594   |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1851000, episode_reward=-477.06 +/- 1.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -477     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1851000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1852000, episode_reward=-370.36 +/- 2.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1852000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.93     |\n",
      "|    critic_loss     | 0.0469   |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 15.1     |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9040     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1852     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2677     |\n",
      "|    total_timesteps | 1852000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1853000, episode_reward=-371.23 +/- 2.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1853000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854000, episode_reward=-353.07 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1854000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.5      |\n",
      "|    critic_loss     | 0.0782   |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -2.99    |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1855000, episode_reward=-352.18 +/- 2.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -352     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1855000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1856000, episode_reward=-252.42 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -252     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1856000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.47     |\n",
      "|    critic_loss     | 0.0492   |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 0.49     |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9060     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1856     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2683     |\n",
      "|    total_timesteps | 1856000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1857000, episode_reward=-249.13 +/- 1.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -249     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1857000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1858000, episode_reward=-295.72 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -296     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1858000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.52     |\n",
      "|    critic_loss     | 0.0682   |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | -8.05    |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1859000, episode_reward=-295.39 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -295     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1859000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=-253.47 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -253     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.42     |\n",
      "|    critic_loss     | 0.0506   |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -3.71    |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9080     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -206     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1860     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2689     |\n",
      "|    total_timesteps | 1860000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1861000, episode_reward=-252.21 +/- 2.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -252     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1861000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1862000, episode_reward=-282.92 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -283     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1862000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.48     |\n",
      "|    critic_loss     | 0.0513   |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | -3.89    |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1863000, episode_reward=-283.11 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -283     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1863000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1864000, episode_reward=-278.84 +/- 2.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -279     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1864000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.38     |\n",
      "|    critic_loss     | 0.0467   |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | -2.78    |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9100     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -204     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1864     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2695     |\n",
      "|    total_timesteps | 1864000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1865000, episode_reward=-279.74 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -280     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1865000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1866000, episode_reward=-297.52 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -298     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1866000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.38     |\n",
      "|    critic_loss     | 0.044    |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | -1.72    |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1867000, episode_reward=-296.14 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -296     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1867000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1868000, episode_reward=-290.11 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -290     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1868000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.46     |\n",
      "|    critic_loss     | 0.0438   |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | -3.71    |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9120     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -201     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1868     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2701     |\n",
      "|    total_timesteps | 1868000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1869000, episode_reward=-290.07 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -290     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1869000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=-282.00 +/- 3.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -282     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.32     |\n",
      "|    critic_loss     | 0.0434   |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | -2.68    |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1871000, episode_reward=-281.84 +/- 4.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -282     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1871000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1872000, episode_reward=-269.73 +/- 3.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -270     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1872000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.37     |\n",
      "|    critic_loss     | 0.0337   |\n",
      "|    ent_coef        | 0.00996  |\n",
      "|    ent_coef_loss   | -3.88    |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1872     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2707     |\n",
      "|    total_timesteps | 1872000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1873000, episode_reward=-271.16 +/- 3.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -271     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1873000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874000, episode_reward=-261.46 +/- 4.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -261     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1874000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.51     |\n",
      "|    critic_loss     | 0.0455   |\n",
      "|    ent_coef        | 0.00987  |\n",
      "|    ent_coef_loss   | -3.99    |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1875000, episode_reward=-262.59 +/- 2.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -263     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1875000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1876000, episode_reward=-234.71 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -235     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1876000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.44     |\n",
      "|    critic_loss     | 0.0382   |\n",
      "|    ent_coef        | 0.00976  |\n",
      "|    ent_coef_loss   | -4.53    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9160     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -196     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1876     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2713     |\n",
      "|    total_timesteps | 1876000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1877000, episode_reward=-232.42 +/- 3.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1877000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1878000, episode_reward=-234.43 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1878000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1879000, episode_reward=-219.91 +/- 3.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -220     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1879000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.44     |\n",
      "|    critic_loss     | 0.0403   |\n",
      "|    ent_coef        | 0.00963  |\n",
      "|    ent_coef_loss   | -5.97    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=-222.55 +/- 4.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -223     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1880000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -192     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1880     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2719     |\n",
      "|    total_timesteps | 1880000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1881000, episode_reward=-194.80 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -195     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1881000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.46     |\n",
      "|    critic_loss     | 0.0509   |\n",
      "|    ent_coef        | 0.00949  |\n",
      "|    ent_coef_loss   | -4.39    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1882000, episode_reward=-193.93 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1882000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1883000, episode_reward=-206.96 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -207     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1883000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.36     |\n",
      "|    critic_loss     | 0.0388   |\n",
      "|    ent_coef        | 0.00935  |\n",
      "|    ent_coef_loss   | -6.65    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1884000, episode_reward=-206.75 +/- 3.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -207     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1884000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1884     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2725     |\n",
      "|    total_timesteps | 1884000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1885000, episode_reward=-199.11 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -199     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1885000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.46     |\n",
      "|    critic_loss     | 0.0497   |\n",
      "|    ent_coef        | 0.0092   |\n",
      "|    ent_coef_loss   | -5.08    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1886000, episode_reward=-199.74 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1886000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1887000, episode_reward=-203.00 +/- 2.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1887000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.44     |\n",
      "|    critic_loss     | 0.0361   |\n",
      "|    ent_coef        | 0.00906  |\n",
      "|    ent_coef_loss   | -5.29    |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1888000, episode_reward=-204.63 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -205     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1888000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1888     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2730     |\n",
      "|    total_timesteps | 1888000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1889000, episode_reward=-205.65 +/- 2.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1889000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.45     |\n",
      "|    critic_loss     | 0.0425   |\n",
      "|    ent_coef        | 0.00892  |\n",
      "|    ent_coef_loss   | -5.42    |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=-207.97 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -208     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1890000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1891000, episode_reward=-217.75 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1891000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.41     |\n",
      "|    critic_loss     | 0.044    |\n",
      "|    ent_coef        | 0.00878  |\n",
      "|    ent_coef_loss   | -5.67    |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1892000, episode_reward=-216.50 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -216     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1892000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -178     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1892     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2736     |\n",
      "|    total_timesteps | 1892000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1893000, episode_reward=-196.74 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1893000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.45     |\n",
      "|    critic_loss     | 0.0428   |\n",
      "|    ent_coef        | 0.00864  |\n",
      "|    ent_coef_loss   | -7.19    |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1894000, episode_reward=-196.05 +/- 1.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -196     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1894000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1895000, episode_reward=-172.55 +/- 1.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -173     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1895000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.42     |\n",
      "|    critic_loss     | 0.039    |\n",
      "|    ent_coef        | 0.00848  |\n",
      "|    ent_coef_loss   | -9.39    |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1896000, episode_reward=-172.61 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -173     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1896000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1896     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2742     |\n",
      "|    total_timesteps | 1896000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1897000, episode_reward=-163.04 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1897000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.39     |\n",
      "|    critic_loss     | 0.0524   |\n",
      "|    ent_coef        | 0.00828  |\n",
      "|    ent_coef_loss   | -8.98    |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1898000, episode_reward=-163.39 +/- 1.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1898000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1899000, episode_reward=-145.23 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1899000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.41     |\n",
      "|    critic_loss     | 0.0433   |\n",
      "|    ent_coef        | 0.00807  |\n",
      "|    ent_coef_loss   | -7.79    |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=-146.75 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -168     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1900     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2747     |\n",
      "|    total_timesteps | 1900000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1901000, episode_reward=-151.32 +/- 1.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1901000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.41     |\n",
      "|    critic_loss     | 0.0465   |\n",
      "|    ent_coef        | 0.00789  |\n",
      "|    ent_coef_loss   | -7.33    |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1902000, episode_reward=-149.64 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -150     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1902000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1903000, episode_reward=-145.11 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1903000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.42     |\n",
      "|    critic_loss     | 0.0498   |\n",
      "|    ent_coef        | 0.00773  |\n",
      "|    ent_coef_loss   | -6.11    |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1904000, episode_reward=-144.63 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1904000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -162     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1904     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2753     |\n",
      "|    total_timesteps | 1904000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1905000, episode_reward=-133.15 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1905000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.46     |\n",
      "|    critic_loss     | 0.0489   |\n",
      "|    ent_coef        | 0.00758  |\n",
      "|    ent_coef_loss   | -8.17    |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1906000, episode_reward=-133.86 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1906000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1907000, episode_reward=-138.39 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1907000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.34     |\n",
      "|    critic_loss     | 0.0424   |\n",
      "|    ent_coef        | 0.00742  |\n",
      "|    ent_coef_loss   | -8.11    |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1908000, episode_reward=-140.25 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1908000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -156     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1908     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2759     |\n",
      "|    total_timesteps | 1908000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1909000, episode_reward=-117.14 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1909000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.36     |\n",
      "|    critic_loss     | 0.0419   |\n",
      "|    ent_coef        | 0.00726  |\n",
      "|    ent_coef_loss   | -6.55    |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=-116.38 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1910000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1911000, episode_reward=-125.02 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1911000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.4      |\n",
      "|    critic_loss     | 0.0427   |\n",
      "|    ent_coef        | 0.00711  |\n",
      "|    ent_coef_loss   | -6.93    |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1912000, episode_reward=-123.99 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1912000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -153     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1912     |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 2765     |\n",
      "|    total_timesteps | 1912000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1913000, episode_reward=-130.56 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1913000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.43     |\n",
      "|    critic_loss     | 0.0438   |\n",
      "|    ent_coef        | 0.00698  |\n",
      "|    ent_coef_loss   | -5.37    |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1914000, episode_reward=-129.60 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1914000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915000, episode_reward=-135.83 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1915000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.45     |\n",
      "|    critic_loss     | 0.0419   |\n",
      "|    ent_coef        | 0.00687  |\n",
      "|    ent_coef_loss   | -5.42    |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1916000, episode_reward=-135.85 +/- 1.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1916000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -169     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1916     |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 2772     |\n",
      "|    total_timesteps | 1916000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1917000, episode_reward=-121.83 +/- 1.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1917000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.4      |\n",
      "|    critic_loss     | 0.0392   |\n",
      "|    ent_coef        | 0.00675  |\n",
      "|    ent_coef_loss   | -7.93    |\n",
      "|    learning_rate   | 0.00308  |\n",
      "|    n_updates       | 9360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1918000, episode_reward=-121.16 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1918000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1919000, episode_reward=-108.36 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1919000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.38     |\n",
      "|    critic_loss     | 0.0481   |\n",
      "|    ent_coef        | 0.00662  |\n",
      "|    ent_coef_loss   | -9.65    |\n",
      "|    learning_rate   | 0.00308  |\n",
      "|    n_updates       | 9370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=-109.40 +/- 2.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1920000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1920     |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 2781     |\n",
      "|    total_timesteps | 1920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1921000, episode_reward=-109.06 +/- 1.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1921000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1922000, episode_reward=-98.23 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1922000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.4      |\n",
      "|    critic_loss     | 0.0493   |\n",
      "|    ent_coef        | 0.00646  |\n",
      "|    ent_coef_loss   | -8.09    |\n",
      "|    learning_rate   | 0.00308  |\n",
      "|    n_updates       | 9380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1923000, episode_reward=-97.27 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1923000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1924000, episode_reward=-81.19 +/- 2.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1924000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.31     |\n",
      "|    critic_loss     | 0.0475   |\n",
      "|    ent_coef        | 0.00631  |\n",
      "|    ent_coef_loss   | -8.97    |\n",
      "|    learning_rate   | 0.00308  |\n",
      "|    n_updates       | 9390     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -200     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1924     |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 2788     |\n",
      "|    total_timesteps | 1924000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1925000, episode_reward=-83.60 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -83.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1925000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1926000, episode_reward=-77.96 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -78      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1926000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.24     |\n",
      "|    critic_loss     | 0.0437   |\n",
      "|    ent_coef        | 0.00616  |\n",
      "|    ent_coef_loss   | -9.98    |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1927000, episode_reward=-79.54 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -79.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1927000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1928000, episode_reward=-37.28 +/- 42.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -37.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1928000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.3      |\n",
      "|    critic_loss     | 0.0451   |\n",
      "|    ent_coef        | 0.006    |\n",
      "|    ent_coef_loss   | -9.06    |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9410     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -204     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1928     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2795     |\n",
      "|    total_timesteps | 1928000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1929000, episode_reward=-35.52 +/- 42.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -35.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1929000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=102.75 +/- 72.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 103      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.26     |\n",
      "|    critic_loss     | 0.0557   |\n",
      "|    ent_coef        | 0.00585  |\n",
      "|    ent_coef_loss   | -9.69    |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1931000, episode_reward=30.98 +/- 87.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 31       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1931000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1932000, episode_reward=128.97 +/- 78.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 129      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1932000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.18     |\n",
      "|    critic_loss     | 0.0576   |\n",
      "|    ent_coef        | 0.0057   |\n",
      "|    ent_coef_loss   | -9.8     |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9430     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -201     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1932     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2801     |\n",
      "|    total_timesteps | 1932000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1933000, episode_reward=129.87 +/- 78.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 130      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1933000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1934000, episode_reward=-16.29 +/- 2.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1934000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.17     |\n",
      "|    critic_loss     | 0.0604   |\n",
      "|    ent_coef        | 0.00556  |\n",
      "|    ent_coef_loss   | -4.96    |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1935000, episode_reward=-14.62 +/- 2.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -14.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1935000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1936000, episode_reward=148.32 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 148      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1936000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.13     |\n",
      "|    critic_loss     | 0.0459   |\n",
      "|    ent_coef        | 0.00546  |\n",
      "|    ent_coef_loss   | -4.73    |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9450     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -193     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1936     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2807     |\n",
      "|    total_timesteps | 1936000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1937000, episode_reward=148.34 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 148      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1937000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1938000, episode_reward=-22.80 +/- 2.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1938000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.05     |\n",
      "|    critic_loss     | 0.0718   |\n",
      "|    ent_coef        | 0.00538  |\n",
      "|    ent_coef_loss   | -3.91    |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1939000, episode_reward=-21.91 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1939000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=79.21 +/- 48.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 79.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.19     |\n",
      "|    critic_loss     | 0.0641   |\n",
      "|    ent_coef        | 0.00532  |\n",
      "|    ent_coef_loss   | -6.29    |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9470     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -185     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1940     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2813     |\n",
      "|    total_timesteps | 1940000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1941000, episode_reward=103.40 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 103      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1941000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1942000, episode_reward=-4.31 +/- 11.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.31    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1942000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.21     |\n",
      "|    critic_loss     | 0.0614   |\n",
      "|    ent_coef        | 0.00524  |\n",
      "|    ent_coef_loss   | -5.46    |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1943000, episode_reward=-10.88 +/- 5.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1943000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1944000, episode_reward=228.32 +/- 35.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 228      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1944000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.06     |\n",
      "|    critic_loss     | 0.0696   |\n",
      "|    ent_coef        | 0.00516  |\n",
      "|    ent_coef_loss   | -4.69    |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9490     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -175     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1944     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2819     |\n",
      "|    total_timesteps | 1944000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1945000, episode_reward=227.10 +/- 16.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 227      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1945000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1946000, episode_reward=-9.12 +/- 3.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.12    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1946000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.78     |\n",
      "|    critic_loss     | 0.136    |\n",
      "|    ent_coef        | 0.0051   |\n",
      "|    ent_coef_loss   | 3.28     |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1947000, episode_reward=-11.94 +/- 3.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1947000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1948000, episode_reward=237.02 +/- 9.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 237      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1948000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.19     |\n",
      "|    critic_loss     | 0.0707   |\n",
      "|    ent_coef        | 0.00508  |\n",
      "|    ent_coef_loss   | -5.61    |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9510     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -160     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1948     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2825     |\n",
      "|    total_timesteps | 1948000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1949000, episode_reward=188.97 +/- 84.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 189      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1949000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=-64.12 +/- 1.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -64.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.19     |\n",
      "|    critic_loss     | 0.116    |\n",
      "|    ent_coef        | 0.00505  |\n",
      "|    ent_coef_loss   | -4.23    |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1951000, episode_reward=-61.75 +/- 3.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -61.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1951000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1952000, episode_reward=-32.38 +/- 1.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1952000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.3      |\n",
      "|    critic_loss     | 0.0669   |\n",
      "|    ent_coef        | 0.005    |\n",
      "|    ent_coef_loss   | -7.15    |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9530     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -140     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1952     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2831     |\n",
      "|    total_timesteps | 1952000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1953000, episode_reward=-30.96 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -31      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1953000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1954000, episode_reward=5.65 +/- 8.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.65     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1954000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.23     |\n",
      "|    critic_loss     | 0.0434   |\n",
      "|    ent_coef        | 0.00491  |\n",
      "|    ent_coef_loss   | -9.13    |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955000, episode_reward=0.45 +/- 3.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 0.454    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1955000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1956000, episode_reward=-45.28 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1956000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.06     |\n",
      "|    critic_loss     | 0.0553   |\n",
      "|    ent_coef        | 0.0048   |\n",
      "|    ent_coef_loss   | -8.88    |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9550     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1956     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2837     |\n",
      "|    total_timesteps | 1956000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1957000, episode_reward=-46.86 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -46.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1957000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1958000, episode_reward=-10.68 +/- 33.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1958000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.29     |\n",
      "|    critic_loss     | 0.035    |\n",
      "|    ent_coef        | 0.0047   |\n",
      "|    ent_coef_loss   | -2.21    |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1959000, episode_reward=-30.66 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -30.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1959000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=-26.03 +/- 2.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.19     |\n",
      "|    critic_loss     | 0.0384   |\n",
      "|    ent_coef        | 0.00464  |\n",
      "|    ent_coef_loss   | -0.963   |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9570     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -115     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1960     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2844     |\n",
      "|    total_timesteps | 1960000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1961000, episode_reward=-22.34 +/- 3.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1961000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1962000, episode_reward=9.75 +/- 11.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 9.75     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1962000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.26     |\n",
      "|    critic_loss     | 0.0414   |\n",
      "|    ent_coef        | 0.00461  |\n",
      "|    ent_coef_loss   | -4.26    |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1963000, episode_reward=-6.33 +/- 9.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -6.33    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1963000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1964000, episode_reward=24.33 +/- 32.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 24.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1964000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1964     |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 2850     |\n",
      "|    total_timesteps | 1964000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1965000, episode_reward=366.81 +/- 12.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 367      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1965000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.03     |\n",
      "|    critic_loss     | 0.08     |\n",
      "|    ent_coef        | 0.00457  |\n",
      "|    ent_coef_loss   | -3.81    |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1966000, episode_reward=368.38 +/- 20.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 368      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1966000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1967000, episode_reward=89.21 +/- 86.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 89.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1967000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.59     |\n",
      "|    critic_loss     | 0.0958   |\n",
      "|    ent_coef        | 0.00453  |\n",
      "|    ent_coef_loss   | 1.84     |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1968000, episode_reward=235.38 +/- 87.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 235      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1968000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -74.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1968     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 2856     |\n",
      "|    total_timesteps | 1968000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1969000, episode_reward=503.78 +/- 21.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 504      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1969000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.13     |\n",
      "|    critic_loss     | 0.175    |\n",
      "|    ent_coef        | 0.00454  |\n",
      "|    ent_coef_loss   | 9.9      |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=499.72 +/- 25.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1970000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1971000, episode_reward=461.83 +/- 4.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1971000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 0.169    |\n",
      "|    ent_coef        | 0.00462  |\n",
      "|    ent_coef_loss   | 9.3      |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1972000, episode_reward=462.23 +/- 4.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1972000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -46.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1972     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 2862     |\n",
      "|    total_timesteps | 1972000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1973000, episode_reward=448.66 +/- 8.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 449      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1973000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.47     |\n",
      "|    critic_loss     | 0.121    |\n",
      "|    ent_coef        | 0.00471  |\n",
      "|    ent_coef_loss   | 3.66     |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1974000, episode_reward=441.50 +/- 6.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 442      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1974000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1975000, episode_reward=-267.81 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -268     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1975000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 0.202    |\n",
      "|    ent_coef        | 0.00478  |\n",
      "|    ent_coef_loss   | 3.25     |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1976000, episode_reward=-269.54 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -270     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1976000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -14.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1976     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 2868     |\n",
      "|    total_timesteps | 1976000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1977000, episode_reward=610.27 +/- 24.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 610      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1977000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.84     |\n",
      "|    critic_loss     | 0.32     |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 4.12     |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1978000, episode_reward=588.80 +/- 13.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 589      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1978000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1979000, episode_reward=697.81 +/- 14.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 698      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1979000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.85     |\n",
      "|    critic_loss     | 0.254    |\n",
      "|    ent_coef        | 0.00489  |\n",
      "|    ent_coef_loss   | 5.94     |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=685.87 +/- 40.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 686      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1980000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 18.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1980     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 2874     |\n",
      "|    total_timesteps | 1980000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1981000, episode_reward=702.62 +/- 14.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 703      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1981000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.36     |\n",
      "|    critic_loss     | 0.403    |\n",
      "|    ent_coef        | 0.00498  |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1982000, episode_reward=709.33 +/- 12.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 709      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1982000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1983000, episode_reward=-188.39 +/- 2.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -188     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1983000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.48     |\n",
      "|    critic_loss     | 0.438    |\n",
      "|    ent_coef        | 0.00508  |\n",
      "|    ent_coef_loss   | 4.01     |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1984000, episode_reward=-189.05 +/- 2.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -189     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1984000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 47.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1984     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 2880     |\n",
      "|    total_timesteps | 1984000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1985000, episode_reward=-289.46 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -289     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1985000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.4     |\n",
      "|    critic_loss     | 0.386    |\n",
      "|    ent_coef        | 0.00516  |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1986000, episode_reward=-290.98 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -291     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1986000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1987000, episode_reward=-301.48 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -301     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1987000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 0.096    |\n",
      "|    ent_coef        | 0.00521  |\n",
      "|    ent_coef_loss   | 3.22     |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1988000, episode_reward=-301.83 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -302     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1988000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 45.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1988     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 2888     |\n",
      "|    total_timesteps | 1988000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1989000, episode_reward=564.66 +/- 23.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 565      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1989000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 0.124    |\n",
      "|    ent_coef        | 0.00526  |\n",
      "|    ent_coef_loss   | 6.74     |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=531.62 +/- 18.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 532      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1990000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1991000, episode_reward=-92.48 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -92.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1991000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.34     |\n",
      "|    critic_loss     | 0.807    |\n",
      "|    ent_coef        | 0.00538  |\n",
      "|    ent_coef_loss   | 24.6     |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1992000, episode_reward=-91.94 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -91.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1992000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 75.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1992     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 2894     |\n",
      "|    total_timesteps | 1992000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1993000, episode_reward=-131.26 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1993000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.35     |\n",
      "|    critic_loss     | 0.732    |\n",
      "|    ent_coef        | 0.00564  |\n",
      "|    ent_coef_loss   | 15.8     |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1994000, episode_reward=-131.02 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1994000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995000, episode_reward=-399.28 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -399     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1995000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 0.315    |\n",
      "|    ent_coef        | 0.00585  |\n",
      "|    ent_coef_loss   | -4.58    |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1996000, episode_reward=-400.17 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -400     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1996000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 84.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1996     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 2900     |\n",
      "|    total_timesteps | 1996000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1997000, episode_reward=-261.68 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -262     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1997000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.8     |\n",
      "|    critic_loss     | 0.0631   |\n",
      "|    ent_coef        | 0.00593  |\n",
      "|    ent_coef_loss   | -0.184   |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1998000, episode_reward=-260.55 +/- 1.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -261     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1998000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1999000, episode_reward=-233.15 +/- 1.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1999000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.3     |\n",
      "|    critic_loss     | 0.134    |\n",
      "|    ent_coef        | 0.00592  |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=-232.73 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 80.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2000     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2907     |\n",
      "|    total_timesteps | 2000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2001000, episode_reward=-270.23 +/- 2.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -270     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2001000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 0.107    |\n",
      "|    ent_coef        | 0.00583  |\n",
      "|    ent_coef_loss   | 3.28     |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2002000, episode_reward=-269.41 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -269     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2002000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2003000, episode_reward=-202.81 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2003000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 0.0827   |\n",
      "|    ent_coef        | 0.00581  |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2004000, episode_reward=-200.47 +/- 1.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2004000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 77       |\n",
      "| time/              |          |\n",
      "|    episodes        | 2004     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2914     |\n",
      "|    total_timesteps | 2004000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2005000, episode_reward=-213.04 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2005000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.107    |\n",
      "|    ent_coef        | 0.00567  |\n",
      "|    ent_coef_loss   | -22.3    |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2006000, episode_reward=-212.89 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2006000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2007000, episode_reward=-212.77 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2007000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2008000, episode_reward=-152.42 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2008000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 0.0793   |\n",
      "|    ent_coef        | 0.00541  |\n",
      "|    ent_coef_loss   | -17.8    |\n",
      "|    learning_rate   | 0.00299  |\n",
      "|    n_updates       | 9800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 76.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2008     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2921     |\n",
      "|    total_timesteps | 2008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2009000, episode_reward=-152.66 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2009000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=-113.41 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 0.0852   |\n",
      "|    ent_coef        | 0.00516  |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.00299  |\n",
      "|    n_updates       | 9810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2011000, episode_reward=-113.86 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -114     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2011000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2012000, episode_reward=437.48 +/- 486.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 437      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2012000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 0.105    |\n",
      "|    ent_coef        | 0.00493  |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.00299  |\n",
      "|    n_updates       | 9820     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 85.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2012     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2927     |\n",
      "|    total_timesteps | 2012000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2013000, episode_reward=482.27 +/- 518.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 482      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2013000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2014000, episode_reward=878.16 +/- 27.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 878      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2014000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.46     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0048   |\n",
      "|    ent_coef_loss   | 20.2     |\n",
      "|    learning_rate   | 0.00299  |\n",
      "|    n_updates       | 9830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2015000, episode_reward=911.24 +/- 36.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 911      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2015000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=1066.99 +/- 46.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2016000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.55     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.00489  |\n",
      "|    ent_coef_loss   | 12.5     |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9840     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2016     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2933     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2017000, episode_reward=1088.25 +/- 27.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2017000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2018000, episode_reward=1059.33 +/- 35.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2018000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.47     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.00502  |\n",
      "|    ent_coef_loss   | 11.5     |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2019000, episode_reward=1048.08 +/- 24.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2019000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=462.16 +/- 582.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.16     |\n",
      "|    critic_loss     | 0.896    |\n",
      "|    ent_coef        | 0.00517  |\n",
      "|    ent_coef_loss   | 16.3     |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9860     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 169      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2020     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2939     |\n",
      "|    total_timesteps | 2020000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2021000, episode_reward=241.97 +/- 598.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 242      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2021000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2022000, episode_reward=895.52 +/- 22.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 896      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2022000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.76     |\n",
      "|    critic_loss     | 0.836    |\n",
      "|    ent_coef        | 0.00538  |\n",
      "|    ent_coef_loss   | 18.9     |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2023000, episode_reward=908.70 +/- 21.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 909      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2023000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2024000, episode_reward=937.95 +/- 25.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 938      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2024000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.74     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.00564  |\n",
      "|    ent_coef_loss   | 20.6     |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9880     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 209      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2024     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2944     |\n",
      "|    total_timesteps | 2024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2025000, episode_reward=948.58 +/- 22.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 949      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2025000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2026000, episode_reward=1003.34 +/- 27.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2026000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.95     |\n",
      "|    critic_loss     | 0.828    |\n",
      "|    ent_coef        | 0.00591  |\n",
      "|    ent_coef_loss   | 15.8     |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2027000, episode_reward=1025.30 +/- 23.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2027000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2028000, episode_reward=1065.20 +/- 36.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2028000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.16     |\n",
      "|    critic_loss     | 0.913    |\n",
      "|    ent_coef        | 0.00619  |\n",
      "|    ent_coef_loss   | 19.6     |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9900     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 251      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2028     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2950     |\n",
      "|    total_timesteps | 2028000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2029000, episode_reward=1055.41 +/- 34.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2029000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=979.97 +/- 43.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 980      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.18     |\n",
      "|    critic_loss     | 0.968    |\n",
      "|    ent_coef        | 0.00647  |\n",
      "|    ent_coef_loss   | 7.55     |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2031000, episode_reward=998.54 +/- 31.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 999      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2031000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2032000, episode_reward=1084.73 +/- 20.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2032000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.61     |\n",
      "|    critic_loss     | 0.998    |\n",
      "|    ent_coef        | 0.00668  |\n",
      "|    ent_coef_loss   | 10.8     |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9920     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 290      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2032     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2956     |\n",
      "|    total_timesteps | 2032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2033000, episode_reward=1084.77 +/- 12.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2033000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2034000, episode_reward=1103.76 +/- 16.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2034000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.92     |\n",
      "|    critic_loss     | 0.905    |\n",
      "|    ent_coef        | 0.00687  |\n",
      "|    ent_coef_loss   | 6.98     |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2035000, episode_reward=1112.09 +/- 43.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2035000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036000, episode_reward=1055.62 +/- 19.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2036000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.06     |\n",
      "|    critic_loss     | 0.954    |\n",
      "|    ent_coef        | 0.00703  |\n",
      "|    ent_coef_loss   | 9.29     |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9940     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 331      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2036     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2962     |\n",
      "|    total_timesteps | 2036000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2037000, episode_reward=1073.17 +/- 42.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2037000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2038000, episode_reward=1162.15 +/- 40.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2038000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.33     |\n",
      "|    critic_loss     | 0.911    |\n",
      "|    ent_coef        | 0.0072   |\n",
      "|    ent_coef_loss   | 9.64     |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2039000, episode_reward=1168.42 +/- 28.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2039000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=1263.84 +/- 13.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.39     |\n",
      "|    critic_loss     | 0.955    |\n",
      "|    ent_coef        | 0.00738  |\n",
      "|    ent_coef_loss   | 7.27     |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9960     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 373      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2040     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2967     |\n",
      "|    total_timesteps | 2040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2041000, episode_reward=1269.94 +/- 35.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2041000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2042000, episode_reward=1169.95 +/- 76.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2042000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.34     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.00754  |\n",
      "|    ent_coef_loss   | 5.3      |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2043000, episode_reward=1230.55 +/- 26.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2043000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2044000, episode_reward=1115.84 +/- 45.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2044000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.5      |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.00767  |\n",
      "|    ent_coef_loss   | 7.35     |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 417      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2044     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2973     |\n",
      "|    total_timesteps | 2044000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2045000, episode_reward=1138.70 +/- 25.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2045000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2046000, episode_reward=1240.86 +/- 31.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2046000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.05     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.00783  |\n",
      "|    ent_coef_loss   | 9.63     |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 9990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2047000, episode_reward=1267.85 +/- 16.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2047000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2048000, episode_reward=1249.55 +/- 32.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2048000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 456      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2048     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2979     |\n",
      "|    total_timesteps | 2048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2049000, episode_reward=1101.88 +/- 34.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2049000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.34     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.00801  |\n",
      "|    ent_coef_loss   | 8.22     |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 10000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=1126.55 +/- 26.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2050000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2051000, episode_reward=1182.66 +/- 18.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2051000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.07     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.00819  |\n",
      "|    ent_coef_loss   | 3.83     |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 10010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2052000, episode_reward=1167.43 +/- 32.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2052000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 498      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2052     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2985     |\n",
      "|    total_timesteps | 2052000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2053000, episode_reward=1193.65 +/- 32.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2053000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.99     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.00832  |\n",
      "|    ent_coef_loss   | 4.21     |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 10020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2054000, episode_reward=1218.97 +/- 37.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2054000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2055000, episode_reward=1268.73 +/- 53.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2055000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.82     |\n",
      "|    critic_loss     | 0.893    |\n",
      "|    ent_coef        | 0.00844  |\n",
      "|    ent_coef_loss   | 5.23     |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 10030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2056000, episode_reward=1313.19 +/- 41.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2056000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 542      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2056     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2991     |\n",
      "|    total_timesteps | 2056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2057000, episode_reward=1347.01 +/- 29.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2057000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.41     |\n",
      "|    critic_loss     | 0.995    |\n",
      "|    ent_coef        | 0.00855  |\n",
      "|    ent_coef_loss   | 3.79     |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2058000, episode_reward=1326.23 +/- 52.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2058000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2059000, episode_reward=1295.85 +/- 51.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2059000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.05     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.00867  |\n",
      "|    ent_coef_loss   | 6.49     |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=1259.18 +/- 44.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2060000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 594      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2060     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 2998     |\n",
      "|    total_timesteps | 2060000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2061000, episode_reward=1119.04 +/- 36.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2061000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.23     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0088   |\n",
      "|    ent_coef_loss   | 4.88     |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2062000, episode_reward=1122.45 +/- 16.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2062000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2063000, episode_reward=1210.52 +/- 43.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2063000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.76     |\n",
      "|    critic_loss     | 0.895    |\n",
      "|    ent_coef        | 0.00893  |\n",
      "|    ent_coef_loss   | 6.26     |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2064000, episode_reward=1201.54 +/- 24.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2064000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 636      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2064     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3003     |\n",
      "|    total_timesteps | 2064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2065000, episode_reward=1259.82 +/- 46.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2065000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.65     |\n",
      "|    critic_loss     | 0.981    |\n",
      "|    ent_coef        | 0.0091   |\n",
      "|    ent_coef_loss   | 10       |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2066000, episode_reward=1258.23 +/- 28.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2066000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2067000, episode_reward=1337.90 +/- 21.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2067000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.53     |\n",
      "|    critic_loss     | 0.877    |\n",
      "|    ent_coef        | 0.00932  |\n",
      "|    ent_coef_loss   | 6.87     |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2068000, episode_reward=1377.05 +/- 38.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2068000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 669      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2068     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3009     |\n",
      "|    total_timesteps | 2068000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2069000, episode_reward=1319.31 +/- 39.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2069000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.00953  |\n",
      "|    ent_coef_loss   | 8.25     |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=1333.91 +/- 20.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2070000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2071000, episode_reward=1353.95 +/- 37.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2071000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.26     |\n",
      "|    critic_loss     | 0.98     |\n",
      "|    ent_coef        | 0.00975  |\n",
      "|    ent_coef_loss   | 6.02     |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2072000, episode_reward=1363.18 +/- 17.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2072000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 703      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2072     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3015     |\n",
      "|    total_timesteps | 2072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2073000, episode_reward=1372.72 +/- 28.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2073000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.17     |\n",
      "|    critic_loss     | 0.964    |\n",
      "|    ent_coef        | 0.00996  |\n",
      "|    ent_coef_loss   | 6.88     |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2074000, episode_reward=1376.39 +/- 17.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2074000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2075000, episode_reward=1355.58 +/- 30.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2075000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.12     |\n",
      "|    critic_loss     | 0.985    |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | 3.83     |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076000, episode_reward=1351.33 +/- 23.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2076000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 732      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2076     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3020     |\n",
      "|    total_timesteps | 2076000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2077000, episode_reward=1330.62 +/- 60.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2077000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.31     |\n",
      "|    critic_loss     | 0.993    |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 2.17     |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2078000, episode_reward=1328.27 +/- 37.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2078000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2079000, episode_reward=-393.10 +/- 2.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -393     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2079000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.13     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 2.73     |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=-392.18 +/- 1.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -392     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2080000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 756      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2080     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3027     |\n",
      "|    total_timesteps | 2080000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2081000, episode_reward=-265.06 +/- 2.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2081000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.04     |\n",
      "|    critic_loss     | 0.863    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 5.21     |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2082000, episode_reward=-264.17 +/- 2.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -264     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2082000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2083000, episode_reward=-283.42 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -283     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2083000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.1     |\n",
      "|    critic_loss     | 0.191    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -3.81    |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2084000, episode_reward=-284.37 +/- 2.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -284     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2084000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 725      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2084     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3033     |\n",
      "|    total_timesteps | 2084000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2085000, episode_reward=1392.72 +/- 37.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2085000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 0.178    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2086000, episode_reward=1381.12 +/- 12.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2086000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2087000, episode_reward=1192.49 +/- 25.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2087000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.44     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 2.21     |\n",
      "|    learning_rate   | 0.00291  |\n",
      "|    n_updates       | 10190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2088000, episode_reward=1191.20 +/- 42.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2088000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 772      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2088     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3039     |\n",
      "|    total_timesteps | 2088000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2089000, episode_reward=1358.96 +/- 46.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2089000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.84     |\n",
      "|    critic_loss     | 0.991    |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 1.68     |\n",
      "|    learning_rate   | 0.00291  |\n",
      "|    n_updates       | 10200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=1356.88 +/- 36.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2090000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2091000, episode_reward=1350.05 +/- 23.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2091000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2092000, episode_reward=1307.48 +/- 39.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2092000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.91     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 6.29     |\n",
      "|    learning_rate   | 0.00291  |\n",
      "|    n_updates       | 10210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 800      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2092     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3045     |\n",
      "|    total_timesteps | 2092000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2093000, episode_reward=1316.43 +/- 46.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2093000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2094000, episode_reward=1202.34 +/- 17.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2094000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.15     |\n",
      "|    critic_loss     | 0.987    |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 3.87     |\n",
      "|    learning_rate   | 0.00291  |\n",
      "|    n_updates       | 10220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2095000, episode_reward=1242.74 +/- 12.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2095000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096000, episode_reward=1100.36 +/- 22.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2096000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.67     |\n",
      "|    critic_loss     | 0.993    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | 1.97     |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 846      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2096     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3050     |\n",
      "|    total_timesteps | 2096000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2097000, episode_reward=1115.85 +/- 40.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2097000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2098000, episode_reward=1073.49 +/- 67.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2098000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.14     |\n",
      "|    critic_loss     | 0.841    |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 2.71     |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2099000, episode_reward=1085.31 +/- 34.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2099000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=1268.79 +/- 34.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.15     |\n",
      "|    critic_loss     | 0.934    |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 1.87     |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10250    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 900      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2100     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3056     |\n",
      "|    total_timesteps | 2100000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2101000, episode_reward=1270.00 +/- 35.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2101000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2102000, episode_reward=465.74 +/- 650.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 466      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2102000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.54     |\n",
      "|    critic_loss     | 0.99     |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | 5.19     |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2103000, episode_reward=-66.36 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -66.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2103000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2104000, episode_reward=1352.95 +/- 32.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2104000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.3     |\n",
      "|    critic_loss     | 0.506    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -7.68    |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10270    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 944      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2104     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3061     |\n",
      "|    total_timesteps | 2104000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2105000, episode_reward=1343.59 +/- 28.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2105000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2106000, episode_reward=1153.67 +/- 27.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2106000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.63     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | 3.77     |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2107000, episode_reward=1200.31 +/- 42.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2107000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2108000, episode_reward=1302.28 +/- 26.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2108000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.75     |\n",
      "|    critic_loss     | 0.983    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | 7.04     |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10290    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 996      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2108     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3067     |\n",
      "|    total_timesteps | 2108000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2109000, episode_reward=1309.88 +/- 39.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2109000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=-372.03 +/- 3.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -372     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0111   |\n",
      "|    ent_coef_loss   | 9.48     |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2111000, episode_reward=-370.49 +/- 4.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2111000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2112000, episode_reward=-75.95 +/- 1.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -76      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2112000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.13     |\n",
      "|    critic_loss     | 0.823    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -0.961   |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10310    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2112     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3073     |\n",
      "|    total_timesteps | 2112000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2113000, episode_reward=-70.39 +/- 3.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2113000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2114000, episode_reward=-107.10 +/- 10.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2114000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.8     |\n",
      "|    critic_loss     | 0.252    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -21.2    |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2115000, episode_reward=-106.71 +/- 9.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2115000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116000, episode_reward=-98.89 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2116000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.9     |\n",
      "|    critic_loss     | 0.106    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -20.8    |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10330    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 989      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2116     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3079     |\n",
      "|    total_timesteps | 2116000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2117000, episode_reward=-99.46 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2117000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2118000, episode_reward=-59.02 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -59      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2118000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.9     |\n",
      "|    critic_loss     | 0.0982   |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | -21.9    |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2119000, episode_reward=-60.13 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -60.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2119000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=-51.71 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -51.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 0.0674   |\n",
      "|    ent_coef        | 0.00969  |\n",
      "|    ent_coef_loss   | -20.6    |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10350    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 945      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2120     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3089     |\n",
      "|    total_timesteps | 2120000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2121000, episode_reward=-51.73 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -51.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2121000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2122000, episode_reward=-118.63 +/- 1.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2122000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.1     |\n",
      "|    critic_loss     | 0.0767   |\n",
      "|    ent_coef        | 0.00912  |\n",
      "|    ent_coef_loss   | -20.9    |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2123000, episode_reward=-119.61 +/- 2.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2123000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2124000, episode_reward=-195.87 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -196     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2124000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 0.0445   |\n",
      "|    ent_coef        | 0.00862  |\n",
      "|    ent_coef_loss   | -16.5    |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10370    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 905      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2124     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3095     |\n",
      "|    total_timesteps | 2124000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2125000, episode_reward=-196.22 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -196     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2125000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2126000, episode_reward=-340.35 +/- 1.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2126000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 0.0411   |\n",
      "|    ent_coef        | 0.00821  |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2127000, episode_reward=-338.79 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -339     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2127000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2128000, episode_reward=-378.18 +/- 1.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -378     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2128000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 0.0666   |\n",
      "|    ent_coef        | 0.00796  |\n",
      "|    ent_coef_loss   | -0.83    |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 857      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2128     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3100     |\n",
      "|    total_timesteps | 2128000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2129000, episode_reward=-378.55 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -379     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2129000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=-98.38 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.5     |\n",
      "|    critic_loss     | 0.447    |\n",
      "|    ent_coef        | 0.00784  |\n",
      "|    ent_coef_loss   | -4.97    |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2131000, episode_reward=-98.58 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2131000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2132000, episode_reward=-408.96 +/- 2.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -409     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2132000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.3     |\n",
      "|    critic_loss     | 0.253    |\n",
      "|    ent_coef        | 0.00771  |\n",
      "|    ent_coef_loss   | -8.92    |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 812      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2132     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3106     |\n",
      "|    total_timesteps | 2132000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2133000, episode_reward=-409.54 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -410     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2133000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2134000, episode_reward=-408.75 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -409     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2134000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2135000, episode_reward=-93.87 +/- 2.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -93.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2135000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 0.162    |\n",
      "|    ent_coef        | 0.00756  |\n",
      "|    ent_coef_loss   | -9.57    |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2136000, episode_reward=-95.13 +/- 1.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2136000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 764      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2136     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3112     |\n",
      "|    total_timesteps | 2136000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2137000, episode_reward=-125.10 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2137000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19       |\n",
      "|    critic_loss     | 0.521    |\n",
      "|    ent_coef        | 0.00739  |\n",
      "|    ent_coef_loss   | -2.45    |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2138000, episode_reward=-125.38 +/- 2.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2138000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2139000, episode_reward=1139.77 +/- 24.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2139000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.158    |\n",
      "|    ent_coef        | 0.00727  |\n",
      "|    ent_coef_loss   | -8.36    |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=1159.08 +/- 31.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2140000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 739      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2140     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3117     |\n",
      "|    total_timesteps | 2140000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2141000, episode_reward=1398.41 +/- 26.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2141000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.68     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.00719  |\n",
      "|    ent_coef_loss   | 11.3     |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2142000, episode_reward=1336.82 +/- 29.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2142000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2143000, episode_reward=1236.82 +/- 14.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2143000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.742    |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.00731  |\n",
      "|    ent_coef_loss   | 21       |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2144000, episode_reward=1224.98 +/- 19.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2144000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 743      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2144     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3123     |\n",
      "|    total_timesteps | 2144000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2145000, episode_reward=1378.92 +/- 60.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2145000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.96     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0076   |\n",
      "|    ent_coef_loss   | 16.5     |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2146000, episode_reward=1392.25 +/- 34.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2146000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2147000, episode_reward=1317.90 +/- 49.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2147000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.9      |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.00793  |\n",
      "|    ent_coef_loss   | 17.5     |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2148000, episode_reward=1362.75 +/- 32.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2148000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 750      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2148     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3128     |\n",
      "|    total_timesteps | 2148000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2149000, episode_reward=1322.04 +/- 56.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2149000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.36     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.00827  |\n",
      "|    ent_coef_loss   | 12.6     |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=1366.48 +/- 19.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2150000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2151000, episode_reward=1356.74 +/- 44.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2151000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.16     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.00858  |\n",
      "|    ent_coef_loss   | 12.2     |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2152000, episode_reward=1382.38 +/- 31.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2152000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 759      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2152     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3134     |\n",
      "|    total_timesteps | 2152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2153000, episode_reward=1133.21 +/- 53.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2153000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.17     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.00889  |\n",
      "|    ent_coef_loss   | 17.8     |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2154000, episode_reward=1110.78 +/- 11.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2154000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2155000, episode_reward=1227.28 +/- 30.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2155000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.24     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.00926  |\n",
      "|    ent_coef_loss   | 12.4     |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2156000, episode_reward=1283.87 +/- 42.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2156000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 762      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2156     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3140     |\n",
      "|    total_timesteps | 2156000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157000, episode_reward=1306.54 +/- 15.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2157000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.56     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0096   |\n",
      "|    ent_coef_loss   | 11.2     |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2158000, episode_reward=1312.87 +/- 50.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2158000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2159000, episode_reward=1326.55 +/- 34.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2159000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.19     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.00993  |\n",
      "|    ent_coef_loss   | 14.2     |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=1357.67 +/- 29.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2160000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 762      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2160     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3145     |\n",
      "|    total_timesteps | 2160000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2161000, episode_reward=1371.88 +/- 26.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2161000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.37     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 6.61     |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2162000, episode_reward=1382.33 +/- 24.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2162000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2163000, episode_reward=1418.85 +/- 36.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2163000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.447    |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 10.6     |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2164000, episode_reward=1405.76 +/- 29.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2164000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 770      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2164     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3151     |\n",
      "|    total_timesteps | 2164000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2165000, episode_reward=1398.22 +/- 29.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2165000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.618    |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | 8.42     |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2166000, episode_reward=1405.83 +/- 47.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2166000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2167000, episode_reward=1420.53 +/- 29.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2167000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.807    |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0111   |\n",
      "|    ent_coef_loss   | 8.53     |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2168000, episode_reward=1431.39 +/- 25.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2168000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 776      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2168     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3156     |\n",
      "|    total_timesteps | 2168000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2169000, episode_reward=1369.33 +/- 31.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2169000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.613    |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | 6.89     |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=1398.83 +/- 46.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2170000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2171000, episode_reward=1337.06 +/- 29.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2171000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.07     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2172000, episode_reward=1318.07 +/- 34.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2172000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 778      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2172     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3162     |\n",
      "|    total_timesteps | 2172000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2173000, episode_reward=1208.27 +/- 61.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2173000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.57     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | 9.99     |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2174000, episode_reward=1221.48 +/- 48.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2174000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2175000, episode_reward=1376.98 +/- 51.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2175000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.7      |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 8.93     |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2176000, episode_reward=1347.75 +/- 60.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2176000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 776      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2176     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3168     |\n",
      "|    total_timesteps | 2176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2177000, episode_reward=1323.45 +/- 43.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2177000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2178000, episode_reward=1424.49 +/- 53.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2178000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.27     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 8.38     |\n",
      "|    learning_rate   | 0.00282  |\n",
      "|    n_updates       | 10630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2179000, episode_reward=1387.68 +/- 19.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2179000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=1335.46 +/- 38.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.514    |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | 7.04     |\n",
      "|    learning_rate   | 0.00282  |\n",
      "|    n_updates       | 10640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 781      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2180     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3173     |\n",
      "|    total_timesteps | 2180000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2181000, episode_reward=1347.16 +/- 38.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2181000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2182000, episode_reward=1383.54 +/- 30.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2182000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.58     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 4.69     |\n",
      "|    learning_rate   | 0.00282  |\n",
      "|    n_updates       | 10650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2183000, episode_reward=1384.91 +/- 52.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2183000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2184000, episode_reward=1406.89 +/- 20.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2184000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 5.78     |\n",
      "|    learning_rate   | 0.00282  |\n",
      "|    n_updates       | 10660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 844      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2184     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3179     |\n",
      "|    total_timesteps | 2184000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2185000, episode_reward=1423.61 +/- 35.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2185000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2186000, episode_reward=1413.00 +/- 41.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2186000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.959    |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 3.16     |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2187000, episode_reward=1424.48 +/- 39.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2187000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2188000, episode_reward=1355.97 +/- 74.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2188000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.761    |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | 1.13     |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10680    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 861      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2188     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3184     |\n",
      "|    total_timesteps | 2188000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2189000, episode_reward=1414.29 +/- 35.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2189000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=1351.84 +/- 52.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.718    |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 2.41     |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2191000, episode_reward=1364.91 +/- 41.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2191000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2192000, episode_reward=1339.52 +/- 56.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2192000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.753    |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | 2.34     |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10700    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 866      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2192     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3190     |\n",
      "|    total_timesteps | 2192000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2193000, episode_reward=1381.08 +/- 35.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2193000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2194000, episode_reward=1313.50 +/- 55.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2194000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 3.27     |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2195000, episode_reward=1318.21 +/- 47.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2195000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2196000, episode_reward=1324.90 +/- 24.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2196000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.58     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | 1.72     |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10720    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 870      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2196     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3196     |\n",
      "|    total_timesteps | 2196000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197000, episode_reward=1294.06 +/- 47.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2197000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2198000, episode_reward=1381.01 +/- 29.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2198000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.64     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 2.21     |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2199000, episode_reward=1425.47 +/- 30.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2199000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=1455.34 +/- 46.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.942    |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 1.71     |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10740    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 878      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2200     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3201     |\n",
      "|    total_timesteps | 2200000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2201000, episode_reward=1463.55 +/- 14.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2201000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2202000, episode_reward=1356.53 +/- 16.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2202000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.31     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -0.396   |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2203000, episode_reward=1325.92 +/- 18.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2203000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2204000, episode_reward=1379.03 +/- 42.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2204000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.6      |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 2.47     |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10760    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 894      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2204     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3207     |\n",
      "|    total_timesteps | 2204000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2205000, episode_reward=1385.00 +/- 47.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2205000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2206000, episode_reward=1424.20 +/- 32.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2206000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2207000, episode_reward=1373.74 +/- 34.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2207000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2208000, episode_reward=1362.15 +/- 16.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2208000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.15     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10780    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 899      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2208     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3212     |\n",
      "|    total_timesteps | 2208000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2209000, episode_reward=1372.11 +/- 42.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2209000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=1460.86 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.64     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -2.57    |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2211000, episode_reward=1443.38 +/- 41.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2211000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2212000, episode_reward=1376.82 +/- 24.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2212000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.34     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -0.021   |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 916      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2212     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3218     |\n",
      "|    total_timesteps | 2212000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2213000, episode_reward=1360.54 +/- 43.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2213000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2214000, episode_reward=1450.06 +/- 30.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2214000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.38     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 1.35     |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2215000, episode_reward=1430.07 +/- 52.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2215000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2216000, episode_reward=1409.53 +/- 32.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2216000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.02     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -0.637   |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 973      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2216     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3223     |\n",
      "|    total_timesteps | 2216000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2217000, episode_reward=1384.32 +/- 30.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2217000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2218000, episode_reward=1450.02 +/- 45.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2218000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.33     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.0386   |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2219000, episode_reward=1488.69 +/- 15.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2219000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=1498.42 +/- 39.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2220000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2220     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3229     |\n",
      "|    total_timesteps | 2220000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2221000, episode_reward=1464.47 +/- 32.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2221000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.19     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2222000, episode_reward=1485.67 +/- 32.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2222000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2223000, episode_reward=1341.47 +/- 55.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2223000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.89     |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 4.53     |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2224000, episode_reward=1307.15 +/- 31.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2224000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2224     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3234     |\n",
      "|    total_timesteps | 2224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2225000, episode_reward=1268.51 +/- 39.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2225000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.5      |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 1.88     |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2226000, episode_reward=1278.64 +/- 30.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2226000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2227000, episode_reward=1386.41 +/- 50.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2227000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.4      |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 1.82     |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2228000, episode_reward=1359.80 +/- 56.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2228000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2228     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3240     |\n",
      "|    total_timesteps | 2228000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2229000, episode_reward=1402.24 +/- 29.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2229000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.08     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 0.623    |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=1397.29 +/- 43.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2230000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2231000, episode_reward=1308.06 +/- 33.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2231000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.22     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2232000, episode_reward=1342.64 +/- 38.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2232000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2232     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3246     |\n",
      "|    total_timesteps | 2232000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2233000, episode_reward=1421.87 +/- 30.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2233000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.49     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 1.82     |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2234000, episode_reward=1433.12 +/- 26.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2234000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2235000, episode_reward=1408.38 +/- 29.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2235000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.11     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | 0.0207   |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2236000, episode_reward=1415.21 +/- 31.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2236000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2236     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3251     |\n",
      "|    total_timesteps | 2236000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237000, episode_reward=1449.54 +/- 48.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2237000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.45     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -0.318   |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2238000, episode_reward=1468.53 +/- 36.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2238000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2239000, episode_reward=1420.52 +/- 28.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2239000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.877    |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | 1.97     |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=1400.77 +/- 38.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2240     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3257     |\n",
      "|    total_timesteps | 2240000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2241000, episode_reward=1428.59 +/- 45.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2241000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.27     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 1.71     |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2242000, episode_reward=1448.29 +/- 35.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2242000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2243000, episode_reward=1425.33 +/- 26.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2243000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.12     |\n",
      "|    critic_loss     | 1.59     |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | -1.89    |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2244000, episode_reward=1443.43 +/- 15.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2244000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2244     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3262     |\n",
      "|    total_timesteps | 2244000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2245000, episode_reward=1460.59 +/- 60.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2245000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.2      |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 1.25     |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2246000, episode_reward=1450.25 +/- 44.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2246000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2247000, episode_reward=1484.39 +/- 21.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2247000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.71     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 1.71     |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 10970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2248000, episode_reward=1509.86 +/- 32.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2248000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2248     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3268     |\n",
      "|    total_timesteps | 2248000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2249000, episode_reward=1388.18 +/- 27.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2249000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.178    |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 4.78     |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 10980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=1442.17 +/- 28.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2250000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2251000, episode_reward=1302.98 +/- 24.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2251000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.08     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 3.56     |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 10990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2252000, episode_reward=1397.63 +/- 17.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2252000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2252     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3274     |\n",
      "|    total_timesteps | 2252000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2253000, episode_reward=1338.66 +/- 54.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2253000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.25     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 11000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2254000, episode_reward=1310.35 +/- 26.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2254000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2255000, episode_reward=1293.60 +/- 27.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2255000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.18     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -0.369   |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 11010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2256000, episode_reward=1332.62 +/- 33.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2256000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2256     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3279     |\n",
      "|    total_timesteps | 2256000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2257000, episode_reward=1357.03 +/- 34.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2257000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.87     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | 2.11     |\n",
      "|    learning_rate   | 0.00274  |\n",
      "|    n_updates       | 11020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2258000, episode_reward=1339.88 +/- 37.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2258000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2259000, episode_reward=1294.05 +/- 44.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2259000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.19     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | 4.08     |\n",
      "|    learning_rate   | 0.00274  |\n",
      "|    n_updates       | 11030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=1362.44 +/- 39.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2260000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2260     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3285     |\n",
      "|    total_timesteps | 2260000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2261000, episode_reward=1376.83 +/- 42.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2261000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.45     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | 0.751    |\n",
      "|    learning_rate   | 0.00274  |\n",
      "|    n_updates       | 11040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2262000, episode_reward=1376.30 +/- 42.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2262000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2263000, episode_reward=1398.68 +/- 29.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2263000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2264000, episode_reward=1319.89 +/- 24.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2264000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.12     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -0.893   |\n",
      "|    learning_rate   | 0.00274  |\n",
      "|    n_updates       | 11050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2264     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3290     |\n",
      "|    total_timesteps | 2264000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2265000, episode_reward=1290.64 +/- 39.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2265000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2266000, episode_reward=1388.74 +/- 40.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2266000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.44     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | 1.44     |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2267000, episode_reward=1385.12 +/- 20.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2267000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2268000, episode_reward=1419.01 +/- 15.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2268000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.36     |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 1.8      |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2268     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 3296     |\n",
      "|    total_timesteps | 2268000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2269000, episode_reward=1412.36 +/- 31.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2269000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=1434.52 +/- 40.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.51     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 2.06     |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2271000, episode_reward=1451.69 +/- 41.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2271000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2272000, episode_reward=1432.37 +/- 26.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2272000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.64     |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11090    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2272     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 3302     |\n",
      "|    total_timesteps | 2272000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2273000, episode_reward=1465.93 +/- 27.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2273000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2274000, episode_reward=1117.24 +/- 630.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2274000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.23     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 0.383    |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2275000, episode_reward=1446.12 +/- 27.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2275000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2276000, episode_reward=-237.93 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2276000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.27     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | 1.07     |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11110    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2276     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 3307     |\n",
      "|    total_timesteps | 2276000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2277000, episode_reward=-237.18 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -237     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2277000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278000, episode_reward=1384.38 +/- 44.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2278000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.9     |\n",
      "|    critic_loss     | 0.621    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2279000, episode_reward=801.47 +/- 747.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 801      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2279000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=1389.60 +/- 34.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.52     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -5.66    |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11130    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2280     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 3313     |\n",
      "|    total_timesteps | 2280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2281000, episode_reward=1343.19 +/- 28.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2281000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2282000, episode_reward=-156.99 +/- 2.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2282000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.9      |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2283000, episode_reward=-158.45 +/- 2.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -158     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2283000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2284000, episode_reward=-243.05 +/- 2.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2284000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 0.563    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11150    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2284     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 3318     |\n",
      "|    total_timesteps | 2284000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2285000, episode_reward=-244.65 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2285000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2286000, episode_reward=-124.60 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2286000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.0915   |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -19      |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2287000, episode_reward=-123.71 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2287000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2288000, episode_reward=-43.80 +/- 1.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -43.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2288000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.106    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -22.3    |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11170    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2288     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 3324     |\n",
      "|    total_timesteps | 2288000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2289000, episode_reward=-43.03 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -43      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2289000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=1304.25 +/- 41.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.0859   |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | -24      |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2291000, episode_reward=1286.55 +/- 21.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2291000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2292000, episode_reward=1313.82 +/- 29.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2292000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.37     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | -5.09    |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11190    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2292     |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 3331     |\n",
      "|    total_timesteps | 2292000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2293000, episode_reward=1294.54 +/- 31.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2293000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2294000, episode_reward=1349.05 +/- 20.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2294000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.07     |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2295000, episode_reward=1342.15 +/- 39.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2295000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2296000, episode_reward=-135.66 +/- 1.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2296000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.68     |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | 0.512    |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2296     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3338     |\n",
      "|    total_timesteps | 2296000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2297000, episode_reward=-136.59 +/- 2.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2297000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2298000, episode_reward=-136.75 +/- 2.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2298000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.7     |\n",
      "|    critic_loss     | 0.27     |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2299000, episode_reward=-136.03 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2299000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=-87.36 +/- 45.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.132    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -19.2    |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2300     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3345     |\n",
      "|    total_timesteps | 2300000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2301000, episode_reward=-87.77 +/- 46.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2301000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2302000, episode_reward=-155.64 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2302000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.147    |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | -20.1    |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2303000, episode_reward=-154.31 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2303000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2304000, episode_reward=-156.15 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2304000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2304     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3351     |\n",
      "|    total_timesteps | 2304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2305000, episode_reward=-183.12 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2305000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.186    |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | -24      |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2306000, episode_reward=-181.30 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -181     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2306000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2307000, episode_reward=-282.72 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -283     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2307000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.185    |\n",
      "|    ent_coef        | 0.00967  |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2308000, episode_reward=-282.22 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -282     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2308000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 990      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2308     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3357     |\n",
      "|    total_timesteps | 2308000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2309000, episode_reward=-155.60 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2309000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.0869   |\n",
      "|    ent_coef        | 0.00925  |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310000, episode_reward=-154.88 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2310000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2311000, episode_reward=-86.41 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2311000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.0745   |\n",
      "|    ent_coef        | 0.00891  |\n",
      "|    ent_coef_loss   | -19      |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2312000, episode_reward=-86.33 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2312000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 934      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2312     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3363     |\n",
      "|    total_timesteps | 2312000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2313000, episode_reward=-93.81 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -93.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2313000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 0.117    |\n",
      "|    ent_coef        | 0.00853  |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2314000, episode_reward=-92.82 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2314000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2315000, episode_reward=-84.44 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2315000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0886   |\n",
      "|    ent_coef        | 0.00821  |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2316000, episode_reward=-83.76 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -83.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2316000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 878      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2316     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3370     |\n",
      "|    total_timesteps | 2316000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2317000, episode_reward=-79.60 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -79.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2317000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0722   |\n",
      "|    ent_coef        | 0.00793  |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318000, episode_reward=-80.18 +/- 0.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2318000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2319000, episode_reward=-125.45 +/- 0.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2319000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0763   |\n",
      "|    ent_coef        | 0.00769  |\n",
      "|    ent_coef_loss   | -9.93    |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=-124.50 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2320000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 840      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2320     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3376     |\n",
      "|    total_timesteps | 2320000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2321000, episode_reward=1325.04 +/- 41.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2321000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.7      |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0075   |\n",
      "|    ent_coef_loss   | -1.89    |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2322000, episode_reward=1296.18 +/- 29.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2322000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2323000, episode_reward=411.39 +/- 299.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 411      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2323000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.17     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.00743  |\n",
      "|    ent_coef_loss   | 6.02     |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2324000, episode_reward=436.00 +/- 153.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 436      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2324000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 833      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2324     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3381     |\n",
      "|    total_timesteps | 2324000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2325000, episode_reward=96.18 +/- 15.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2325000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.58     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.00746  |\n",
      "|    ent_coef_loss   | 5.95     |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2326000, episode_reward=134.87 +/- 65.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 135      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2326000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2327000, episode_reward=185.35 +/- 5.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 185      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2327000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.667    |\n",
      "|    ent_coef        | 0.00754  |\n",
      "|    ent_coef_loss   | 4.24     |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2328000, episode_reward=184.24 +/- 6.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 184      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2328000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 794      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2328     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3387     |\n",
      "|    total_timesteps | 2328000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2329000, episode_reward=50.20 +/- 585.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 50.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2329000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 0.367    |\n",
      "|    ent_coef        | 0.00763  |\n",
      "|    ent_coef_loss   | 4.67     |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=66.15 +/- 618.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 66.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2330000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2331000, episode_reward=918.40 +/- 429.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 918      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2331000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.93     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.00773  |\n",
      "|    ent_coef_loss   | 7        |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2332000, episode_reward=983.03 +/- 268.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 983      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2332000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 787      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2332     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3393     |\n",
      "|    total_timesteps | 2332000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2333000, episode_reward=1289.44 +/- 36.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2333000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.13     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.00785  |\n",
      "|    ent_coef_loss   | 10.1     |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2334000, episode_reward=1276.85 +/- 7.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2334000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2335000, episode_reward=1090.27 +/- 344.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2335000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.32     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.00802  |\n",
      "|    ent_coef_loss   | 9.57     |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2336000, episode_reward=754.79 +/- 405.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 755      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2336000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 783      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2336     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3398     |\n",
      "|    total_timesteps | 2336000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2337000, episode_reward=226.99 +/- 7.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 227      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2337000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.86     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.00821  |\n",
      "|    ent_coef_loss   | 7.16     |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2338000, episode_reward=224.93 +/- 9.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 225      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2338000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2339000, episode_reward=283.43 +/- 447.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 283      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2339000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 0.922    |\n",
      "|    ent_coef        | 0.00835  |\n",
      "|    ent_coef_loss   | -2.55    |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=59.51 +/- 1.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 59.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2340000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 751      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2340     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3404     |\n",
      "|    total_timesteps | 2340000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2341000, episode_reward=-224.29 +/- 2.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2341000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.1     |\n",
      "|    critic_loss     | 0.804    |\n",
      "|    ent_coef        | 0.00838  |\n",
      "|    ent_coef_loss   | -2.2     |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2342000, episode_reward=-224.75 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2342000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2343000, episode_reward=1071.78 +/- 189.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2343000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.1     |\n",
      "|    critic_loss     | 0.127    |\n",
      "|    ent_coef        | 0.00834  |\n",
      "|    ent_coef_loss   | -6.23    |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2344000, episode_reward=1187.22 +/- 69.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2344000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 717      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2344     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3410     |\n",
      "|    total_timesteps | 2344000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2345000, episode_reward=1331.53 +/- 56.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2345000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.9      |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0083   |\n",
      "|    ent_coef_loss   | 12.3     |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2346000, episode_reward=1330.71 +/- 65.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2346000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2347000, episode_reward=1309.93 +/- 26.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2347000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2348000, episode_reward=1334.24 +/- 41.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2348000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.13     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.00844  |\n",
      "|    ent_coef_loss   | 11.4     |\n",
      "|    learning_rate   | 0.00265  |\n",
      "|    n_updates       | 11460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 711      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2348     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3415     |\n",
      "|    total_timesteps | 2348000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2349000, episode_reward=1326.98 +/- 25.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2349000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=1392.78 +/- 51.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.43     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.00864  |\n",
      "|    ent_coef_loss   | 9.66     |\n",
      "|    learning_rate   | 0.00265  |\n",
      "|    n_updates       | 11470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2351000, episode_reward=1385.32 +/- 13.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2351000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2352000, episode_reward=1306.71 +/- 29.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2352000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.79     |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.00886  |\n",
      "|    ent_coef_loss   | 8.52     |\n",
      "|    learning_rate   | 0.00265  |\n",
      "|    n_updates       | 11480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 710      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2352     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3421     |\n",
      "|    total_timesteps | 2352000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2353000, episode_reward=1253.36 +/- 48.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2353000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2354000, episode_reward=-107.25 +/- 1.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2354000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.00907  |\n",
      "|    ent_coef_loss   | 9.82     |\n",
      "|    learning_rate   | 0.00265  |\n",
      "|    n_updates       | 11490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2355000, episode_reward=-109.68 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2355000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2356000, episode_reward=-127.02 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2356000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 0.763    |\n",
      "|    ent_coef        | 0.00925  |\n",
      "|    ent_coef_loss   | -3.15    |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 684      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2356     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3427     |\n",
      "|    total_timesteps | 2356000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2357000, episode_reward=-126.58 +/- 1.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2357000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358000, episode_reward=-137.01 +/- 1.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2358000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 0.137    |\n",
      "|    ent_coef        | 0.00927  |\n",
      "|    ent_coef_loss   | -9.04    |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2359000, episode_reward=-136.48 +/- 2.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2359000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=-245.38 +/- 2.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.129    |\n",
      "|    ent_coef        | 0.00912  |\n",
      "|    ent_coef_loss   | -19.5    |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11520    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 629      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2360     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3433     |\n",
      "|    total_timesteps | 2360000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2361000, episode_reward=-245.92 +/- 3.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -246     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2361000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2362000, episode_reward=-449.21 +/- 3.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -449     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2362000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 0.15     |\n",
      "|    ent_coef        | 0.00881  |\n",
      "|    ent_coef_loss   | -3.78    |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2363000, episode_reward=-450.80 +/- 3.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -451     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2363000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2364000, episode_reward=-420.81 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -421     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2364000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.3     |\n",
      "|    critic_loss     | 0.061    |\n",
      "|    ent_coef        | 0.00867  |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11540    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 563      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2364     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3439     |\n",
      "|    total_timesteps | 2364000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2365000, episode_reward=-419.48 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -419     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2365000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2366000, episode_reward=-345.39 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -345     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2366000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.2     |\n",
      "|    critic_loss     | 0.0327   |\n",
      "|    ent_coef        | 0.00877  |\n",
      "|    ent_coef_loss   | 8.39     |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2367000, episode_reward=-344.08 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -344     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2367000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2368000, episode_reward=-276.62 +/- 2.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -277     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2368000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.2     |\n",
      "|    critic_loss     | 0.0637   |\n",
      "|    ent_coef        | 0.00888  |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11560    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 498      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2368     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3445     |\n",
      "|    total_timesteps | 2368000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2369000, episode_reward=-277.46 +/- 1.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -277     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2369000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=-150.11 +/- 1.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -150     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 0.0907   |\n",
      "|    ent_coef        | 0.00877  |\n",
      "|    ent_coef_loss   | -17.7    |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2371000, episode_reward=-147.88 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2371000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2372000, episode_reward=-138.09 +/- 2.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2372000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.0924   |\n",
      "|    ent_coef        | 0.00848  |\n",
      "|    ent_coef_loss   | -19.3    |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11580    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 437      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2372     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3451     |\n",
      "|    total_timesteps | 2372000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2373000, episode_reward=-137.48 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2373000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2374000, episode_reward=-15.27 +/- 2.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -15.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2374000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.4     |\n",
      "|    critic_loss     | 0.05     |\n",
      "|    ent_coef        | 0.00812  |\n",
      "|    ent_coef_loss   | -18.1    |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2375000, episode_reward=-14.05 +/- 1.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -14.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2375000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2376000, episode_reward=29.41 +/- 2.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 29.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2376000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.142    |\n",
      "|    ent_coef        | 0.00778  |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11600    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 382      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2376     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3456     |\n",
      "|    total_timesteps | 2376000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2377000, episode_reward=26.28 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 26.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2377000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2378000, episode_reward=-52.65 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2378000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.128    |\n",
      "|    ent_coef        | 0.00749  |\n",
      "|    ent_coef_loss   | -17.4    |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2379000, episode_reward=-53.43 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2379000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=9.09 +/- 58.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 9.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.081    |\n",
      "|    ent_coef        | 0.0072   |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11620    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 354      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2380     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3462     |\n",
      "|    total_timesteps | 2380000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2381000, episode_reward=60.79 +/- 2.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 60.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2381000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2382000, episode_reward=21.89 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 21.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2382000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 0.143    |\n",
      "|    ent_coef        | 0.00695  |\n",
      "|    ent_coef_loss   | -16      |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2383000, episode_reward=19.49 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 19.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2383000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2384000, episode_reward=1069.93 +/- 31.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2384000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.222    |\n",
      "|    ent_coef        | 0.00671  |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 333      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2384     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3468     |\n",
      "|    total_timesteps | 2384000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2385000, episode_reward=1084.91 +/- 50.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2385000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2386000, episode_reward=-54.04 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -54      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2386000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 0.175    |\n",
      "|    ent_coef        | 0.00649  |\n",
      "|    ent_coef_loss   | -16.8    |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2387000, episode_reward=-52.04 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2387000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2388000, episode_reward=-45.24 +/- 1.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2388000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 0.131    |\n",
      "|    ent_coef        | 0.00625  |\n",
      "|    ent_coef_loss   | -18.8    |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 338      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2388     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3474     |\n",
      "|    total_timesteps | 2388000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2389000, episode_reward=-45.82 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2389000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=-45.74 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2390000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2391000, episode_reward=-59.49 +/- 2.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -59.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2391000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.165    |\n",
      "|    ent_coef        | 0.00601  |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2392000, episode_reward=-60.46 +/- 1.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -60.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2392000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 318      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2392     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3480     |\n",
      "|    total_timesteps | 2392000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2393000, episode_reward=-128.71 +/- 1.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2393000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 0.197    |\n",
      "|    ent_coef        | 0.00579  |\n",
      "|    ent_coef_loss   | -16.6    |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2394000, episode_reward=-128.31 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2394000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2395000, episode_reward=-230.72 +/- 4.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2395000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 0.096    |\n",
      "|    ent_coef        | 0.00559  |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2396000, episode_reward=-230.19 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -230     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2396000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 262      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2396     |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 3486     |\n",
      "|    total_timesteps | 2396000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2397000, episode_reward=-75.83 +/- 3.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -75.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2397000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.103    |\n",
      "|    ent_coef        | 0.00543  |\n",
      "|    ent_coef_loss   | -17.5    |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2398000, episode_reward=-77.16 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2398000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399000, episode_reward=-18.43 +/- 3.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2399000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.163    |\n",
      "|    ent_coef        | 0.00525  |\n",
      "|    ent_coef_loss   | -14.7    |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=-19.20 +/- 3.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -19.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2400000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 262      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2400     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3493     |\n",
      "|    total_timesteps | 2400000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2401000, episode_reward=-45.28 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2401000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.251    |\n",
      "|    ent_coef        | 0.00509  |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2402000, episode_reward=-44.91 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -44.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2402000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2403000, episode_reward=-42.87 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2403000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 0.108    |\n",
      "|    ent_coef        | 0.00495  |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2404000, episode_reward=-43.49 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -43.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2404000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 265      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2404     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3499     |\n",
      "|    total_timesteps | 2404000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2405000, episode_reward=-52.58 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2405000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 0.0908   |\n",
      "|    ent_coef        | 0.00478  |\n",
      "|    ent_coef_loss   | -17.7    |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2406000, episode_reward=-52.86 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2406000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2407000, episode_reward=-64.94 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -64.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2407000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.4     |\n",
      "|    critic_loss     | 0.0528   |\n",
      "|    ent_coef        | 0.00462  |\n",
      "|    ent_coef_loss   | -17.7    |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2408000, episode_reward=-65.09 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -65.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2408000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 268      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2408     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3505     |\n",
      "|    total_timesteps | 2408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2409000, episode_reward=-125.59 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2409000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.0395   |\n",
      "|    ent_coef        | 0.00446  |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=-125.86 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2410000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2411000, episode_reward=-226.67 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -227     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2411000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.7     |\n",
      "|    critic_loss     | 0.0242   |\n",
      "|    ent_coef        | 0.00432  |\n",
      "|    ent_coef_loss   | -5.95    |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2412000, episode_reward=-227.63 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2412000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 267      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2412     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3511     |\n",
      "|    total_timesteps | 2412000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2413000, episode_reward=-234.99 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -235     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2413000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 0.0188   |\n",
      "|    ent_coef        | 0.00425  |\n",
      "|    ent_coef_loss   | -1.75    |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2414000, episode_reward=-235.01 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -235     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2414000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2415000, episode_reward=-211.31 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2415000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19       |\n",
      "|    critic_loss     | 0.0109   |\n",
      "|    ent_coef        | 0.00421  |\n",
      "|    ent_coef_loss   | -0.296   |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2416000, episode_reward=-213.30 +/- 1.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2416000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 261      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2416     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3517     |\n",
      "|    total_timesteps | 2416000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2417000, episode_reward=-256.10 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -256     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2417000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 0.02     |\n",
      "|    ent_coef        | 0.00419  |\n",
      "|    ent_coef_loss   | -5.66    |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2418000, episode_reward=-254.62 +/- 2.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2418000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2419000, episode_reward=-352.53 +/- 6.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2419000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 0.0232   |\n",
      "|    ent_coef        | 0.00416  |\n",
      "|    ent_coef_loss   | -2.88    |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=-346.90 +/- 13.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2420000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 236      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2420     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3523     |\n",
      "|    total_timesteps | 2420000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2421000, episode_reward=898.72 +/- 614.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 899      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2421000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.6     |\n",
      "|    critic_loss     | 0.046    |\n",
      "|    ent_coef        | 0.00414  |\n",
      "|    ent_coef_loss   | 8.4      |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2422000, episode_reward=644.04 +/- 646.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 644      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2422000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2423000, episode_reward=1207.60 +/- 21.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2423000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.64     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.00418  |\n",
      "|    ent_coef_loss   | 14.1     |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2424000, episode_reward=1176.48 +/- 67.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2424000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 224      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2424     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3529     |\n",
      "|    total_timesteps | 2424000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2425000, episode_reward=1274.02 +/- 26.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2425000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.33     |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.00428  |\n",
      "|    ent_coef_loss   | 23.8     |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2426000, episode_reward=1248.44 +/- 25.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2426000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2427000, episode_reward=-155.69 +/- 0.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2427000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.12     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.00444  |\n",
      "|    ent_coef_loss   | 16       |\n",
      "|    learning_rate   | 0.00257  |\n",
      "|    n_updates       | 11850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2428000, episode_reward=-156.56 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2428000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 238      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2428     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3534     |\n",
      "|    total_timesteps | 2428000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2429000, episode_reward=-46.76 +/- 2.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -46.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2429000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.3     |\n",
      "|    critic_loss     | 0.152    |\n",
      "|    ent_coef        | 0.00456  |\n",
      "|    ent_coef_loss   | -3.91    |\n",
      "|    learning_rate   | 0.00257  |\n",
      "|    n_updates       | 11860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=-49.64 +/- 5.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2430000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2431000, episode_reward=-62.03 +/- 2.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -62      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2431000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.0837   |\n",
      "|    ent_coef        | 0.0046   |\n",
      "|    ent_coef_loss   | -6.81    |\n",
      "|    learning_rate   | 0.00257  |\n",
      "|    n_updates       | 11870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2432000, episode_reward=-62.34 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -62.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2432000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 194      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2432     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3540     |\n",
      "|    total_timesteps | 2432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2433000, episode_reward=-61.18 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -61.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2433000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2434000, episode_reward=-7.00 +/- 30.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2434000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.0827   |\n",
      "|    ent_coef        | 0.00457  |\n",
      "|    ent_coef_loss   | -8.42    |\n",
      "|    learning_rate   | 0.00257  |\n",
      "|    n_updates       | 11880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2435000, episode_reward=-0.17 +/- 46.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.165   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2435000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2436000, episode_reward=-82.41 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -82.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2436000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.0646   |\n",
      "|    ent_coef        | 0.00451  |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 146      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2436     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3546     |\n",
      "|    total_timesteps | 2436000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2437000, episode_reward=-81.54 +/- 1.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2437000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2438000, episode_reward=-65.02 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -65      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2438000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 0.0257   |\n",
      "|    ent_coef        | 0.0044   |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439000, episode_reward=-65.37 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -65.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2439000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=-78.43 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -78.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 0.0254   |\n",
      "|    ent_coef        | 0.00428  |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 122      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2440     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3552     |\n",
      "|    total_timesteps | 2440000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2441000, episode_reward=-76.54 +/- 2.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -76.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2441000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2442000, episode_reward=-87.35 +/- 2.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2442000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.0225   |\n",
      "|    ent_coef        | 0.00416  |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2443000, episode_reward=-87.35 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2443000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2444000, episode_reward=-66.21 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -66.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2444000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.0222   |\n",
      "|    ent_coef        | 0.00406  |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11930    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2444     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3557     |\n",
      "|    total_timesteps | 2444000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2445000, episode_reward=-67.61 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -67.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2445000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2446000, episode_reward=-84.98 +/- 2.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2446000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 0.0206   |\n",
      "|    ent_coef        | 0.00398  |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2447000, episode_reward=-85.05 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2447000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2448000, episode_reward=-86.99 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2448000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.5     |\n",
      "|    critic_loss     | 0.0155   |\n",
      "|    ent_coef        | 0.00389  |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11950    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 50.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2448     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3563     |\n",
      "|    total_timesteps | 2448000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2449000, episode_reward=-86.37 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2449000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2450000, episode_reward=-86.82 +/- 1.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.4     |\n",
      "|    critic_loss     | 0.0127   |\n",
      "|    ent_coef        | 0.00382  |\n",
      "|    ent_coef_loss   | -6.43    |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2451000, episode_reward=-86.59 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2451000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2452000, episode_reward=-89.84 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -89.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2452000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 0.0164   |\n",
      "|    ent_coef        | 0.00376  |\n",
      "|    ent_coef_loss   | -5.02    |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11970    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -3.93    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2452     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3570     |\n",
      "|    total_timesteps | 2452000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2453000, episode_reward=-90.51 +/- 2.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2453000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2454000, episode_reward=95.76 +/- 2.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2454000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.4     |\n",
      "|    critic_loss     | 0.0223   |\n",
      "|    ent_coef        | 0.00371  |\n",
      "|    ent_coef_loss   | -2.55    |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2455000, episode_reward=95.16 +/- 2.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2455000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2456000, episode_reward=62.77 +/- 7.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 62.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2456000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 0.0257   |\n",
      "|    ent_coef        | 0.00369  |\n",
      "|    ent_coef_loss   | 0.142    |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 11990    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -27.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2456     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3576     |\n",
      "|    total_timesteps | 2456000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2457000, episode_reward=68.09 +/- 7.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 68.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2457000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2458000, episode_reward=55.16 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 55.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2458000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.0515   |\n",
      "|    ent_coef        | 0.00368  |\n",
      "|    ent_coef_loss   | -2.28    |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2459000, episode_reward=57.52 +/- 2.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 57.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2459000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=85.76 +/- 5.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 85.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.0406   |\n",
      "|    ent_coef        | 0.00366  |\n",
      "|    ent_coef_loss   | -1.67    |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 12010    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -21.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2460     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3582     |\n",
      "|    total_timesteps | 2460000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2461000, episode_reward=48.07 +/- 73.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2461000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2462000, episode_reward=-28.83 +/- 19.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -28.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2462000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.0252   |\n",
      "|    ent_coef        | 0.00365  |\n",
      "|    ent_coef_loss   | -0.702   |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 12020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2463000, episode_reward=-39.23 +/- 16.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -39.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2463000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2464000, episode_reward=294.13 +/- 1.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 294      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2464000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.0245   |\n",
      "|    ent_coef        | 0.00364  |\n",
      "|    ent_coef_loss   | -0.934   |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 12030    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -6.03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2464     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3588     |\n",
      "|    total_timesteps | 2464000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2465000, episode_reward=291.50 +/- 2.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2465000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2466000, episode_reward=217.04 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 217      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2466000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.3     |\n",
      "|    critic_loss     | 0.0458   |\n",
      "|    ent_coef        | 0.00364  |\n",
      "|    ent_coef_loss   | -3.94    |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2467000, episode_reward=217.61 +/- 3.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 218      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2467000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2468000, episode_reward=77.22 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 77.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2468000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.3     |\n",
      "|    critic_loss     | 0.0411   |\n",
      "|    ent_coef        | 0.00361  |\n",
      "|    ent_coef_loss   | -6.74    |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 13.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2468     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3594     |\n",
      "|    total_timesteps | 2468000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2469000, episode_reward=79.40 +/- 1.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 79.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2469000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=52.93 +/- 12.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 52.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0312   |\n",
      "|    ent_coef        | 0.00357  |\n",
      "|    ent_coef_loss   | -9.18    |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2471000, episode_reward=37.61 +/- 3.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 37.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2471000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2472000, episode_reward=78.37 +/- 3.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 78.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2472000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.0339   |\n",
      "|    ent_coef        | 0.00352  |\n",
      "|    ent_coef_loss   | -8.95    |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2472     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3600     |\n",
      "|    total_timesteps | 2472000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2473000, episode_reward=76.96 +/- 7.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 77       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2473000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2474000, episode_reward=179.91 +/- 26.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 180      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2474000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 0.0453   |\n",
      "|    ent_coef        | 0.00346  |\n",
      "|    ent_coef_loss   | -7.42    |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2475000, episode_reward=167.88 +/- 43.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 168      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2475000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2476000, episode_reward=109.59 +/- 17.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2476000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 31.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2476     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3606     |\n",
      "|    total_timesteps | 2476000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2477000, episode_reward=12.97 +/- 6.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 13       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2477000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 0.0421   |\n",
      "|    ent_coef        | 0.0034   |\n",
      "|    ent_coef_loss   | -6.4     |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2478000, episode_reward=16.59 +/- 5.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 16.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2478000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479000, episode_reward=58.35 +/- 7.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 58.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2479000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.3     |\n",
      "|    critic_loss     | 0.0466   |\n",
      "|    ent_coef        | 0.00336  |\n",
      "|    ent_coef_loss   | -7.2     |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=50.61 +/- 6.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 50.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 37.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2480     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3612     |\n",
      "|    total_timesteps | 2480000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2481000, episode_reward=-0.27 +/- 37.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.267   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2481000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 0.0565   |\n",
      "|    ent_coef        | 0.00331  |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2482000, episode_reward=18.15 +/- 47.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 18.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2482000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2483000, episode_reward=163.53 +/- 4.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 164      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2483000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 0.0479   |\n",
      "|    ent_coef        | 0.00325  |\n",
      "|    ent_coef_loss   | -8.89    |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2484000, episode_reward=160.33 +/- 6.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2484000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 41.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2484     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3618     |\n",
      "|    total_timesteps | 2484000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2485000, episode_reward=58.69 +/- 8.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 58.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2485000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.0867   |\n",
      "|    ent_coef        | 0.0032   |\n",
      "|    ent_coef_loss   | 2.87     |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2486000, episode_reward=55.54 +/- 6.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 55.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2486000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2487000, episode_reward=-26.22 +/- 6.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2487000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.0832   |\n",
      "|    ent_coef        | 0.0032   |\n",
      "|    ent_coef_loss   | 13.9     |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2488000, episode_reward=-26.37 +/- 7.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2488000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 43.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2488     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3623     |\n",
      "|    total_timesteps | 2488000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2489000, episode_reward=-130.80 +/- 1.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2489000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.101    |\n",
      "|    ent_coef        | 0.00325  |\n",
      "|    ent_coef_loss   | 11.4     |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=-129.23 +/- 2.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2490000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2491000, episode_reward=-18.70 +/- 5.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2491000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.0679   |\n",
      "|    ent_coef        | 0.00331  |\n",
      "|    ent_coef_loss   | 0.838    |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2492000, episode_reward=-10.65 +/- 19.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2492000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 44       |\n",
      "| time/              |          |\n",
      "|    episodes        | 2492     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3629     |\n",
      "|    total_timesteps | 2492000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2493000, episode_reward=-77.19 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2493000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 0.0822   |\n",
      "|    ent_coef        | 0.00333  |\n",
      "|    ent_coef_loss   | -9.29    |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2494000, episode_reward=-75.14 +/- 1.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -75.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2494000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2495000, episode_reward=-156.82 +/- 2.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2495000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.0429   |\n",
      "|    ent_coef        | 0.0033   |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2496000, episode_reward=-156.52 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2496000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 48.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2496     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3635     |\n",
      "|    total_timesteps | 2496000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2497000, episode_reward=78.85 +/- 4.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 78.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2497000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.0545   |\n",
      "|    ent_coef        | 0.00324  |\n",
      "|    ent_coef_loss   | -8.07    |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2498000, episode_reward=81.49 +/- 6.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 81.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2498000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2499000, episode_reward=112.27 +/- 6.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 112      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2499000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 0.0854   |\n",
      "|    ent_coef        | 0.00319  |\n",
      "|    ent_coef_loss   | -9.7     |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=112.92 +/- 3.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 113      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 51.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2500     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3641     |\n",
      "|    total_timesteps | 2500000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2501000, episode_reward=67.25 +/- 7.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 67.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2501000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.099    |\n",
      "|    ent_coef        | 0.00313  |\n",
      "|    ent_coef_loss   | -5.53    |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2502000, episode_reward=70.01 +/- 6.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 70       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2502000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2503000, episode_reward=319.20 +/- 27.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 319      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2503000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.17     |\n",
      "|    ent_coef        | 0.00309  |\n",
      "|    ent_coef_loss   | 1.83     |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2504000, episode_reward=290.81 +/- 55.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 291      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2504000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 56.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2504     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3648     |\n",
      "|    total_timesteps | 2504000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2505000, episode_reward=191.63 +/- 49.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 192      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2505000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 0.181    |\n",
      "|    ent_coef        | 0.00308  |\n",
      "|    ent_coef_loss   | -0.609   |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2506000, episode_reward=166.87 +/- 6.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 167      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2506000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2507000, episode_reward=102.19 +/- 7.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 102      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2507000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 0.0976   |\n",
      "|    ent_coef        | 0.00307  |\n",
      "|    ent_coef_loss   | -7.6     |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2508000, episode_reward=101.80 +/- 2.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 102      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2508000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 63.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2508     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3654     |\n",
      "|    total_timesteps | 2508000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2509000, episode_reward=61.59 +/- 4.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 61.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2509000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 0.102    |\n",
      "|    ent_coef        | 0.00304  |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=61.96 +/- 4.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 62       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2510000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2511000, episode_reward=-81.83 +/- 8.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2511000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.3     |\n",
      "|    critic_loss     | 0.0933   |\n",
      "|    ent_coef        | 0.00303  |\n",
      "|    ent_coef_loss   | 7.95     |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2512000, episode_reward=-85.57 +/- 12.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2512000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 71.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2512     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3660     |\n",
      "|    total_timesteps | 2512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2513000, episode_reward=1.67 +/- 6.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2513000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 0.106    |\n",
      "|    ent_coef        | 0.00306  |\n",
      "|    ent_coef_loss   | 15.6     |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2514000, episode_reward=-3.22 +/- 5.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.22    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2514000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2515000, episode_reward=143.26 +/- 9.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 143      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2515000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 0.08     |\n",
      "|    ent_coef        | 0.00312  |\n",
      "|    ent_coef_loss   | 6.17     |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2516000, episode_reward=160.15 +/- 14.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2516000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 80.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2516     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3666     |\n",
      "|    total_timesteps | 2516000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2517000, episode_reward=-11.73 +/- 19.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2517000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.1     |\n",
      "|    critic_loss     | 0.124    |\n",
      "|    ent_coef        | 0.00318  |\n",
      "|    ent_coef_loss   | 3.79     |\n",
      "|    learning_rate   | 0.00248  |\n",
      "|    n_updates       | 12290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2518000, episode_reward=-26.61 +/- 20.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2518000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2519000, episode_reward=-25.13 +/- 13.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -25.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2519000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=39.92 +/- 8.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 39.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.117    |\n",
      "|    ent_coef        | 0.00321  |\n",
      "|    ent_coef_loss   | 0.561    |\n",
      "|    learning_rate   | 0.00248  |\n",
      "|    n_updates       | 12300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 91.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2520     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3672     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2521000, episode_reward=38.69 +/- 9.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 38.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2521000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2522000, episode_reward=36.24 +/- 7.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2522000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.138    |\n",
      "|    ent_coef        | 0.00322  |\n",
      "|    ent_coef_loss   | -2.26    |\n",
      "|    learning_rate   | 0.00248  |\n",
      "|    n_updates       | 12310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2523000, episode_reward=41.55 +/- 14.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 41.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2523000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2524000, episode_reward=-26.94 +/- 5.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2524000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.119    |\n",
      "|    ent_coef        | 0.00322  |\n",
      "|    ent_coef_loss   | -0.628   |\n",
      "|    learning_rate   | 0.00248  |\n",
      "|    n_updates       | 12320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 56.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2524     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3678     |\n",
      "|    total_timesteps | 2524000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2525000, episode_reward=-24.29 +/- 2.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2525000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2526000, episode_reward=15.03 +/- 2.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 15       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2526000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 0.255    |\n",
      "|    ent_coef        | 0.00321  |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2527000, episode_reward=12.21 +/- 6.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 12.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2527000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2528000, episode_reward=-89.03 +/- 4.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -89      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2528000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.4     |\n",
      "|    critic_loss     | 0.245    |\n",
      "|    ent_coef        | 0.00321  |\n",
      "|    ent_coef_loss   | 5.71     |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12340    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2528     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3683     |\n",
      "|    total_timesteps | 2528000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2529000, episode_reward=-91.44 +/- 7.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -91.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2529000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=1288.27 +/- 12.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.228    |\n",
      "|    ent_coef        | 0.00324  |\n",
      "|    ent_coef_loss   | 12.9     |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2531000, episode_reward=1296.31 +/- 18.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2531000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2532000, episode_reward=1278.71 +/- 45.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2532000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.88     |\n",
      "|    critic_loss     | 1.6      |\n",
      "|    ent_coef        | 0.00332  |\n",
      "|    ent_coef_loss   | 32.9     |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12360    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2532     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3689     |\n",
      "|    total_timesteps | 2532000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2533000, episode_reward=1249.42 +/- 38.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2533000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2534000, episode_reward=1216.97 +/- 36.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2534000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.52     |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.00349  |\n",
      "|    ent_coef_loss   | 40.2     |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2535000, episode_reward=1213.73 +/- 34.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2535000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2536000, episode_reward=1288.03 +/- 21.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2536000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.93     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.00373  |\n",
      "|    ent_coef_loss   | 37.1     |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12380    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2536     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3694     |\n",
      "|    total_timesteps | 2536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2537000, episode_reward=1289.55 +/- 32.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2537000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2538000, episode_reward=1038.86 +/- 14.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2538000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.55     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.004    |\n",
      "|    ent_coef_loss   | 34.7     |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2539000, episode_reward=1041.39 +/- 40.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2539000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=1009.44 +/- 33.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.8      |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.00426  |\n",
      "|    ent_coef_loss   | 25.5     |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12400    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 154      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2540     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3701     |\n",
      "|    total_timesteps | 2540000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2541000, episode_reward=994.70 +/- 38.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 995      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2541000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2542000, episode_reward=719.63 +/- 69.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 720      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2542000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.48     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.00449  |\n",
      "|    ent_coef_loss   | 23.8     |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2543000, episode_reward=368.20 +/- 69.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 368      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2543000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2544000, episode_reward=1061.61 +/- 32.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2544000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.79     |\n",
      "|    critic_loss     | 0.764    |\n",
      "|    ent_coef        | 0.0047   |\n",
      "|    ent_coef_loss   | 16.5     |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12420    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 190      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2544     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3706     |\n",
      "|    total_timesteps | 2544000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2545000, episode_reward=1008.67 +/- 45.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2545000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2546000, episode_reward=1064.63 +/- 21.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2546000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.02     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.00489  |\n",
      "|    ent_coef_loss   | 22.6     |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2547000, episode_reward=1068.95 +/- 39.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2547000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2548000, episode_reward=1055.49 +/- 31.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2548000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.79     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.00509  |\n",
      "|    ent_coef_loss   | 33.4     |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12440    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 227      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2548     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3712     |\n",
      "|    total_timesteps | 2548000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2549000, episode_reward=1054.57 +/- 29.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2549000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=986.75 +/- 32.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 987      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.96     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.00535  |\n",
      "|    ent_coef_loss   | 20.8     |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2551000, episode_reward=1010.65 +/- 28.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2551000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2552000, episode_reward=1164.98 +/- 24.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2552000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.97     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.00558  |\n",
      "|    ent_coef_loss   | 16.1     |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 268      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2552     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3718     |\n",
      "|    total_timesteps | 2552000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2553000, episode_reward=1167.22 +/- 37.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2553000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2554000, episode_reward=1205.14 +/- 35.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2554000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.38     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.00578  |\n",
      "|    ent_coef_loss   | 23.3     |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2555000, episode_reward=1222.31 +/- 44.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2555000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2556000, episode_reward=1020.58 +/- 19.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2556000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.97     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.00602  |\n",
      "|    ent_coef_loss   | 23.3     |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 313      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2556     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3723     |\n",
      "|    total_timesteps | 2556000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2557000, episode_reward=1024.59 +/- 20.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2557000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2558000, episode_reward=1061.67 +/- 26.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2558000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.22     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.00626  |\n",
      "|    ent_coef_loss   | 17       |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2559000, episode_reward=1060.12 +/- 34.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2559000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=843.07 +/- 426.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 843      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2560000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 353      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2560     |\n",
      "|    fps             | 686      |\n",
      "|    time_elapsed    | 3729     |\n",
      "|    total_timesteps | 2560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2561000, episode_reward=1088.77 +/- 26.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2561000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.45     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.00648  |\n",
      "|    ent_coef_loss   | 16.2     |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2562000, episode_reward=1094.97 +/- 29.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2562000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2563000, episode_reward=1139.43 +/- 22.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2563000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.54     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.00669  |\n",
      "|    ent_coef_loss   | 20.3     |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2564000, episode_reward=1117.87 +/- 14.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2564000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 397      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2564     |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 3744     |\n",
      "|    total_timesteps | 2564000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2565000, episode_reward=1280.21 +/- 18.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2565000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.43     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.00693  |\n",
      "|    ent_coef_loss   | 24.2     |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2566000, episode_reward=1045.45 +/- 529.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2566000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2567000, episode_reward=1356.77 +/- 24.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2567000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.11     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.00722  |\n",
      "|    ent_coef_loss   | 27.6     |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2568000, episode_reward=774.77 +/- 644.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 775      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2568000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 443      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2568     |\n",
      "|    fps             | 683      |\n",
      "|    time_elapsed    | 3754     |\n",
      "|    total_timesteps | 2568000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2569000, episode_reward=1432.13 +/- 18.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2569000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.38     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.00756  |\n",
      "|    ent_coef_loss   | 22.4     |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=1403.41 +/- 38.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2570000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2571000, episode_reward=1277.88 +/- 48.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2571000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.1      |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.00788  |\n",
      "|    ent_coef_loss   | 16.1     |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2572000, episode_reward=1263.94 +/- 32.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2572000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 492      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2572     |\n",
      "|    fps             | 683      |\n",
      "|    time_elapsed    | 3760     |\n",
      "|    total_timesteps | 2572000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2573000, episode_reward=1211.97 +/- 72.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2573000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.01     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.00815  |\n",
      "|    ent_coef_loss   | 19.4     |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2574000, episode_reward=1267.43 +/- 28.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2574000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2575000, episode_reward=1373.63 +/- 43.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2575000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.72     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.00843  |\n",
      "|    ent_coef_loss   | 16       |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2576000, episode_reward=1392.34 +/- 47.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2576000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 539      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2576     |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 3765     |\n",
      "|    total_timesteps | 2576000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2577000, episode_reward=1315.76 +/- 27.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2577000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.55     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0087   |\n",
      "|    ent_coef_loss   | 15.4     |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2578000, episode_reward=1270.38 +/- 54.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2578000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2579000, episode_reward=1211.39 +/- 47.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2579000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.6      |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.00895  |\n",
      "|    ent_coef_loss   | 16.9     |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=1176.43 +/- 33.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2580000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 583      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2580     |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 3771     |\n",
      "|    total_timesteps | 2580000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2581000, episode_reward=1360.86 +/- 38.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2581000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.00921  |\n",
      "|    ent_coef_loss   | 13.5     |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2582000, episode_reward=1335.47 +/- 35.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2582000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2583000, episode_reward=1279.07 +/- 42.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2583000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.58     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.00947  |\n",
      "|    ent_coef_loss   | 16       |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2584000, episode_reward=1303.12 +/- 23.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2584000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 630      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2584     |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 3776     |\n",
      "|    total_timesteps | 2584000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2585000, episode_reward=1329.51 +/- 45.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2585000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.54     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.00974  |\n",
      "|    ent_coef_loss   | 12.2     |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2586000, episode_reward=1317.68 +/- 29.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2586000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2587000, episode_reward=1321.17 +/- 33.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2587000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.66     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.00999  |\n",
      "|    ent_coef_loss   | 14.5     |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2588000, episode_reward=1320.49 +/- 47.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2588000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 680      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2588     |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 3782     |\n",
      "|    total_timesteps | 2588000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2589000, episode_reward=1263.90 +/- 16.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2589000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.92     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | 9.8      |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=1234.88 +/- 39.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2590000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2591000, episode_reward=1295.09 +/- 24.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2591000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.94     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 10.1     |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2592000, episode_reward=1283.97 +/- 29.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2592000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 728      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2592     |\n",
      "|    fps             | 684      |\n",
      "|    time_elapsed    | 3788     |\n",
      "|    total_timesteps | 2592000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2593000, episode_reward=1301.43 +/- 36.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2593000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.01     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2594000, episode_reward=1315.03 +/- 63.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2594000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2595000, episode_reward=1333.86 +/- 61.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2595000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.37     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | 11.3     |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2596000, episode_reward=1355.68 +/- 47.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2596000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 778      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2596     |\n",
      "|    fps             | 682      |\n",
      "|    time_elapsed    | 3802     |\n",
      "|    total_timesteps | 2596000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2597000, episode_reward=1338.56 +/- 14.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2597000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.65     |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0111   |\n",
      "|    ent_coef_loss   | 10.2     |\n",
      "|    learning_rate   | 0.0024   |\n",
      "|    n_updates       | 12680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2598000, episode_reward=1352.92 +/- 63.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2598000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2599000, episode_reward=1280.10 +/- 15.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2599000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.81     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | 9.58     |\n",
      "|    learning_rate   | 0.0024   |\n",
      "|    n_updates       | 12690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=1323.45 +/- 13.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2600000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 829      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2600     |\n",
      "|    fps             | 681      |\n",
      "|    time_elapsed    | 3817     |\n",
      "|    total_timesteps | 2600000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2601000, episode_reward=1228.33 +/- 44.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2601000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.01     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | 5.63     |\n",
      "|    learning_rate   | 0.0024   |\n",
      "|    n_updates       | 12700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2602000, episode_reward=1244.46 +/- 34.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2602000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2603000, episode_reward=1227.00 +/- 15.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2603000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2604000, episode_reward=1274.82 +/- 60.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2604000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.33     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | 4.79     |\n",
      "|    learning_rate   | 0.0024   |\n",
      "|    n_updates       | 12710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 871      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2604     |\n",
      "|    fps             | 681      |\n",
      "|    time_elapsed    | 3823     |\n",
      "|    total_timesteps | 2604000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2605000, episode_reward=1240.39 +/- 57.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2605000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2606000, episode_reward=1273.49 +/- 30.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2606000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.67     |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | 6.43     |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2607000, episode_reward=1297.03 +/- 27.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2607000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2608000, episode_reward=1281.76 +/- 32.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2608000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.98     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 8.08     |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 916      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2608     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3833     |\n",
      "|    total_timesteps | 2608000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2609000, episode_reward=1315.15 +/- 24.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2609000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=1343.24 +/- 39.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.9      |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | 7.64     |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2611000, episode_reward=1284.00 +/- 43.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2611000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2612000, episode_reward=1346.30 +/- 31.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2612000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.08     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | 6.28     |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12750    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 964      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2612     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3839     |\n",
      "|    total_timesteps | 2612000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2613000, episode_reward=1348.50 +/- 58.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2613000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2614000, episode_reward=1260.75 +/- 68.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2614000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.98     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 10       |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2615000, episode_reward=1242.56 +/- 29.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2615000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2616000, episode_reward=1282.29 +/- 80.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2616000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.23     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | 7.26     |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12770    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2616     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3844     |\n",
      "|    total_timesteps | 2616000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2617000, episode_reward=1284.39 +/- 34.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2617000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2618000, episode_reward=1285.54 +/- 17.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2618000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.78     |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | 10.6     |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2619000, episode_reward=1264.02 +/- 48.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2619000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=1257.19 +/- 28.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.04     |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | 6.24     |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12790    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2620     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3850     |\n",
      "|    total_timesteps | 2620000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2621000, episode_reward=1299.14 +/- 21.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2621000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2622000, episode_reward=1163.05 +/- 7.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2622000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.58     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 5.26     |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2623000, episode_reward=1154.23 +/- 42.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2623000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2624000, episode_reward=1214.96 +/- 33.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2624000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.66     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12810    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2624     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3855     |\n",
      "|    total_timesteps | 2624000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2625000, episode_reward=1230.11 +/- 34.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2625000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2626000, episode_reward=930.57 +/- 501.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 931      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2626000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.164   |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2627000, episode_reward=681.00 +/- 610.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 681      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2627000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2628000, episode_reward=361.92 +/- 527.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 362      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2628000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.63     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 1.96     |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12830    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2628     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3861     |\n",
      "|    total_timesteps | 2628000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2629000, episode_reward=579.76 +/- 529.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 580      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2629000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2630000, episode_reward=-81.55 +/- 1.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.45     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -0.76    |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2631000, episode_reward=-80.84 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2631000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2632000, episode_reward=-79.62 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -79.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2632000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 0.535    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -6.53    |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12850    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2632     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3866     |\n",
      "|    total_timesteps | 2632000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2633000, episode_reward=-79.10 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -79.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2633000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2634000, episode_reward=460.15 +/- 659.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 460      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2634000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.15     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -1.67    |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2635000, episode_reward=-76.99 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2635000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2636000, episode_reward=-44.17 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -44.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2636000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.3      |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.145    |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12870    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2636     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3872     |\n",
      "|    total_timesteps | 2636000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2637000, episode_reward=-44.40 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -44.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2637000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2638000, episode_reward=-37.09 +/- 0.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -37.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2638000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2639000, episode_reward=-36.57 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2639000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=-23.00 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.6     |\n",
      "|    critic_loss     | 0.113    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | -17.6    |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2640     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3877     |\n",
      "|    total_timesteps | 2640000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2641000, episode_reward=-21.80 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2641000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2642000, episode_reward=-49.59 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2642000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.5     |\n",
      "|    critic_loss     | 0.148    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -20.3    |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2643000, episode_reward=-48.87 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2643000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2644000, episode_reward=-100.00 +/- 41.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2644000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.7     |\n",
      "|    critic_loss     | 0.145    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -20.2    |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2644     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3883     |\n",
      "|    total_timesteps | 2644000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2645000, episode_reward=-117.19 +/- 47.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2645000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2646000, episode_reward=-100.56 +/- 41.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2646000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2647000, episode_reward=-158.63 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2647000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.3     |\n",
      "|    critic_loss     | 0.106    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | -18.6    |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2648000, episode_reward=-158.53 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2648000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2648     |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 3889     |\n",
      "|    total_timesteps | 2648000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2649000, episode_reward=-56.33 +/- 3.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2649000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20       |\n",
      "|    critic_loss     | 0.105    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -19.2    |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=-58.76 +/- 2.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -58.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2650000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2651000, episode_reward=-70.97 +/- 2.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -71      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2651000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.6     |\n",
      "|    critic_loss     | 0.101    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -21      |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2652000, episode_reward=-70.99 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -71      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2652000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 986      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2652     |\n",
      "|    fps             | 679      |\n",
      "|    time_elapsed    | 3902     |\n",
      "|    total_timesteps | 2652000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2653000, episode_reward=-103.01 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2653000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.9     |\n",
      "|    critic_loss     | 0.0687   |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -20.2    |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2654000, episode_reward=-102.67 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2654000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2655000, episode_reward=16.13 +/- 6.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 16.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2655000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.8     |\n",
      "|    critic_loss     | 0.0714   |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | -19.9    |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2656000, episode_reward=16.67 +/- 5.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 16.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2656000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 941      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2656     |\n",
      "|    fps             | 678      |\n",
      "|    time_elapsed    | 3912     |\n",
      "|    total_timesteps | 2656000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2657000, episode_reward=334.10 +/- 18.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 334      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2657000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.4     |\n",
      "|    critic_loss     | 0.0996   |\n",
      "|    ent_coef        | 0.00976  |\n",
      "|    ent_coef_loss   | -20.2    |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 12970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2658000, episode_reward=345.04 +/- 38.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 345      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2658000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2659000, episode_reward=266.18 +/- 9.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 266      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2659000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19       |\n",
      "|    critic_loss     | 0.128    |\n",
      "|    ent_coef        | 0.00942  |\n",
      "|    ent_coef_loss   | -19.3    |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 12980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=262.86 +/- 10.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 263      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2660000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 906      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2660     |\n",
      "|    fps             | 678      |\n",
      "|    time_elapsed    | 3918     |\n",
      "|    total_timesteps | 2660000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2661000, episode_reward=169.35 +/- 7.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 169      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2661000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 0.156    |\n",
      "|    ent_coef        | 0.0091   |\n",
      "|    ent_coef_loss   | -17.2    |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 12990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2662000, episode_reward=171.70 +/- 6.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 172      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2662000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2663000, episode_reward=408.59 +/- 6.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 409      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2663000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 0.15     |\n",
      "|    ent_coef        | 0.00881  |\n",
      "|    ent_coef_loss   | -18      |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2664000, episode_reward=404.11 +/- 13.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 404      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2664000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 869      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2664     |\n",
      "|    fps             | 678      |\n",
      "|    time_elapsed    | 3923     |\n",
      "|    total_timesteps | 2664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2665000, episode_reward=35.02 +/- 11.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 35       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2665000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.176    |\n",
      "|    ent_coef        | 0.00855  |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 13010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2666000, episode_reward=45.72 +/- 29.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 45.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2666000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2667000, episode_reward=65.26 +/- 7.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 65.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2667000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.7     |\n",
      "|    critic_loss     | 0.131    |\n",
      "|    ent_coef        | 0.00833  |\n",
      "|    ent_coef_loss   | -17      |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2668000, episode_reward=66.27 +/- 3.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 66.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2668000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 825      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2668     |\n",
      "|    fps             | 678      |\n",
      "|    time_elapsed    | 3929     |\n",
      "|    total_timesteps | 2668000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2669000, episode_reward=312.35 +/- 11.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 312      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2669000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 0.121    |\n",
      "|    ent_coef        | 0.0081   |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=316.00 +/- 8.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 316      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2670000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2671000, episode_reward=136.79 +/- 3.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 137      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2671000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 0.182    |\n",
      "|    ent_coef        | 0.00788  |\n",
      "|    ent_coef_loss   | -17.5    |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2672000, episode_reward=139.95 +/- 4.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 140      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2672000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 781      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2672     |\n",
      "|    fps             | 679      |\n",
      "|    time_elapsed    | 3935     |\n",
      "|    total_timesteps | 2672000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2673000, episode_reward=2.39 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.39     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2673000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.3     |\n",
      "|    critic_loss     | 0.171    |\n",
      "|    ent_coef        | 0.00766  |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2674000, episode_reward=-0.56 +/- 2.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.558   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2674000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2675000, episode_reward=49.29 +/- 2.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 49.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2675000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.6     |\n",
      "|    critic_loss     | 0.164    |\n",
      "|    ent_coef        | 0.00745  |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2676000, episode_reward=48.24 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2676000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 732      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2676     |\n",
      "|    fps             | 679      |\n",
      "|    time_elapsed    | 3940     |\n",
      "|    total_timesteps | 2676000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2677000, episode_reward=163.66 +/- 4.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 164      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2677000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 0.108    |\n",
      "|    ent_coef        | 0.00728  |\n",
      "|    ent_coef_loss   | -14.6    |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2678000, episode_reward=161.59 +/- 7.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 162      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2678000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2679000, episode_reward=284.46 +/- 16.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 284      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2679000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 0.131    |\n",
      "|    ent_coef        | 0.00711  |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=287.35 +/- 34.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 287      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2680000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 688      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2680     |\n",
      "|    fps             | 679      |\n",
      "|    time_elapsed    | 3946     |\n",
      "|    total_timesteps | 2680000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681000, episode_reward=63.36 +/- 4.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 63.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2681000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 0.132    |\n",
      "|    ent_coef        | 0.00694  |\n",
      "|    ent_coef_loss   | -16.2    |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2682000, episode_reward=68.51 +/- 5.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 68.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2682000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2683000, episode_reward=351.79 +/- 11.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 352      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2683000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19       |\n",
      "|    critic_loss     | 0.147    |\n",
      "|    ent_coef        | 0.00677  |\n",
      "|    ent_coef_loss   | -14.7    |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2684000, episode_reward=339.78 +/- 8.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 340      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2684000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 645      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2684     |\n",
      "|    fps             | 678      |\n",
      "|    time_elapsed    | 3956     |\n",
      "|    total_timesteps | 2684000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2685000, episode_reward=447.18 +/- 10.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 447      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2685000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.1     |\n",
      "|    critic_loss     | 0.235    |\n",
      "|    ent_coef        | 0.00661  |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2686000, episode_reward=446.99 +/- 18.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 447      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2686000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2687000, episode_reward=319.65 +/- 17.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 320      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2687000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 0.253    |\n",
      "|    ent_coef        | 0.00647  |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.00231  |\n",
      "|    n_updates       | 13120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2688000, episode_reward=321.24 +/- 13.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2688000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 608      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2688     |\n",
      "|    fps             | 677      |\n",
      "|    time_elapsed    | 3967     |\n",
      "|    total_timesteps | 2688000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2689000, episode_reward=310.47 +/- 10.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 310      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2689000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=481.01 +/- 16.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 481      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 0.198    |\n",
      "|    ent_coef        | 0.00633  |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.00231  |\n",
      "|    n_updates       | 13130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2691000, episode_reward=473.17 +/- 8.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 473      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2691000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2692000, episode_reward=522.07 +/- 7.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 522      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2692000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.187    |\n",
      "|    ent_coef        | 0.0062   |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.00231  |\n",
      "|    n_updates       | 13140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 575      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2692     |\n",
      "|    fps             | 677      |\n",
      "|    time_elapsed    | 3972     |\n",
      "|    total_timesteps | 2692000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2693000, episode_reward=513.91 +/- 7.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 514      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2693000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2694000, episode_reward=284.86 +/- 22.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 285      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2694000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.171    |\n",
      "|    ent_coef        | 0.00607  |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.00231  |\n",
      "|    n_updates       | 13150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2695000, episode_reward=281.35 +/- 28.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 281      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2695000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2696000, episode_reward=332.81 +/- 9.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2696000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.4     |\n",
      "|    critic_loss     | 0.219    |\n",
      "|    ent_coef        | 0.00596  |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 541      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2696     |\n",
      "|    fps             | 677      |\n",
      "|    time_elapsed    | 3981     |\n",
      "|    total_timesteps | 2696000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2697000, episode_reward=316.63 +/- 18.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 317      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2697000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2698000, episode_reward=564.85 +/- 14.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 565      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2698000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.7     |\n",
      "|    critic_loss     | 0.246    |\n",
      "|    ent_coef        | 0.00584  |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2699000, episode_reward=566.72 +/- 17.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 567      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2699000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=403.40 +/- 14.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 403      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.313    |\n",
      "|    ent_coef        | 0.00575  |\n",
      "|    ent_coef_loss   | -4.82    |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13180    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 505      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2700     |\n",
      "|    fps             | 677      |\n",
      "|    time_elapsed    | 3987     |\n",
      "|    total_timesteps | 2700000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2701000, episode_reward=406.99 +/- 10.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 407      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2701000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2702000, episode_reward=354.30 +/- 6.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 354      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2702000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.193    |\n",
      "|    ent_coef        | 0.00567  |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2703000, episode_reward=359.73 +/- 16.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 360      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2703000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2704000, episode_reward=353.93 +/- 20.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 354      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2704000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 0.17     |\n",
      "|    ent_coef        | 0.00558  |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13200    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 472      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2704     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 3995     |\n",
      "|    total_timesteps | 2704000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2705000, episode_reward=328.25 +/- 21.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 328      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2705000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2706000, episode_reward=471.58 +/- 9.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 472      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2706000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 0.202    |\n",
      "|    ent_coef        | 0.00547  |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2707000, episode_reward=476.76 +/- 7.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 477      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2707000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2708000, episode_reward=447.49 +/- 7.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 447      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2708000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.187    |\n",
      "|    ent_coef        | 0.00538  |\n",
      "|    ent_coef_loss   | -5       |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13220    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 438      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2708     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 4002     |\n",
      "|    total_timesteps | 2708000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2709000, episode_reward=437.19 +/- 7.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 437      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2709000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=299.73 +/- 12.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 300      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.169    |\n",
      "|    ent_coef        | 0.00532  |\n",
      "|    ent_coef_loss   | -6.2     |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2711000, episode_reward=293.26 +/- 6.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 293      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2711000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2712000, episode_reward=287.08 +/- 16.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 287      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2712000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.00526  |\n",
      "|    ent_coef_loss   | -7.78    |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13240    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 401      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2712     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 4011     |\n",
      "|    total_timesteps | 2712000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2713000, episode_reward=317.81 +/- 9.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 318      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2713000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2714000, episode_reward=384.41 +/- 16.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 384      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2714000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 0.201    |\n",
      "|    ent_coef        | 0.0052   |\n",
      "|    ent_coef_loss   | -9.9     |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2715000, episode_reward=382.80 +/- 3.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 383      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2715000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2716000, episode_reward=498.92 +/- 3.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 499      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2716000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 0.18     |\n",
      "|    ent_coef        | 0.00513  |\n",
      "|    ent_coef_loss   | -6.49    |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13260    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 365      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2716     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 4021     |\n",
      "|    total_timesteps | 2716000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2717000, episode_reward=498.23 +/- 12.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 498      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2717000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2718000, episode_reward=397.86 +/- 4.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 398      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2718000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.1     |\n",
      "|    critic_loss     | 0.189    |\n",
      "|    ent_coef        | 0.00507  |\n",
      "|    ent_coef_loss   | -7.32    |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2719000, episode_reward=404.68 +/- 11.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 405      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2719000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=464.99 +/- 6.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 0.136    |\n",
      "|    ent_coef        | 0.00501  |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13280    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 329      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2720     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4030     |\n",
      "|    total_timesteps | 2720000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721000, episode_reward=463.14 +/- 13.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 463      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2721000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2722000, episode_reward=507.18 +/- 16.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 507      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2722000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 0.133    |\n",
      "|    ent_coef        | 0.00494  |\n",
      "|    ent_coef_loss   | -8.74    |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2723000, episode_reward=519.56 +/- 2.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 520      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2723000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2724000, episode_reward=579.86 +/- 17.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 580      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2724000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 0.183    |\n",
      "|    ent_coef        | 0.00487  |\n",
      "|    ent_coef_loss   | -3.47    |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 300      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2724     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4035     |\n",
      "|    total_timesteps | 2724000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2725000, episode_reward=589.92 +/- 13.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 590      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2725000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2726000, episode_reward=548.05 +/- 14.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 548      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2726000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 0.208    |\n",
      "|    ent_coef        | 0.00483  |\n",
      "|    ent_coef_loss   | -5.93    |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2727000, episode_reward=560.00 +/- 26.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 560      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2727000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2728000, episode_reward=558.52 +/- 14.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 559      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2728000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 0.165    |\n",
      "|    ent_coef        | 0.00478  |\n",
      "|    ent_coef_loss   | -4.93    |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 276      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2728     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 4041     |\n",
      "|    total_timesteps | 2728000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2729000, episode_reward=544.77 +/- 25.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 545      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2729000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=643.69 +/- 9.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 644      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.8     |\n",
      "|    critic_loss     | 0.178    |\n",
      "|    ent_coef        | 0.00475  |\n",
      "|    ent_coef_loss   | 2.66     |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2731000, episode_reward=629.00 +/- 14.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 629      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2731000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2732000, episode_reward=641.95 +/- 18.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 642      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2732000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 273      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2732     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 4047     |\n",
      "|    total_timesteps | 2732000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2733000, episode_reward=629.46 +/- 12.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 629      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2733000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 0.179    |\n",
      "|    ent_coef        | 0.00475  |\n",
      "|    ent_coef_loss   | -2.58    |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2734000, episode_reward=648.34 +/- 18.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 648      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2734000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2735000, episode_reward=560.73 +/- 5.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 561      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2735000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.7     |\n",
      "|    critic_loss     | 0.156    |\n",
      "|    ent_coef        | 0.00473  |\n",
      "|    ent_coef_loss   | -4.9     |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2736000, episode_reward=567.42 +/- 8.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 567      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2736000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 251      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2736     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 4052     |\n",
      "|    total_timesteps | 2736000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2737000, episode_reward=550.60 +/- 9.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 551      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2737000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 0.204    |\n",
      "|    ent_coef        | 0.00471  |\n",
      "|    ent_coef_loss   | -2.7     |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2738000, episode_reward=562.61 +/- 16.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 563      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2738000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2739000, episode_reward=536.61 +/- 10.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 537      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2739000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.7     |\n",
      "|    critic_loss     | 0.178    |\n",
      "|    ent_coef        | 0.00469  |\n",
      "|    ent_coef_loss   | 0.0271   |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=534.26 +/- 8.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 534      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2740000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 272      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2740     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4060     |\n",
      "|    total_timesteps | 2740000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2741000, episode_reward=618.72 +/- 7.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 619      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2741000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 0.179    |\n",
      "|    ent_coef        | 0.00468  |\n",
      "|    ent_coef_loss   | 0.298    |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2742000, episode_reward=610.23 +/- 12.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 610      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2742000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2743000, episode_reward=556.33 +/- 11.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 556      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2743000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.5     |\n",
      "|    critic_loss     | 0.185    |\n",
      "|    ent_coef        | 0.00467  |\n",
      "|    ent_coef_loss   | -3.18    |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2744000, episode_reward=558.42 +/- 12.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 558      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2744000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 292      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2744     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4066     |\n",
      "|    total_timesteps | 2744000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2745000, episode_reward=524.51 +/- 16.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 525      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2745000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.7     |\n",
      "|    critic_loss     | 0.235    |\n",
      "|    ent_coef        | 0.00466  |\n",
      "|    ent_coef_loss   | -4.52    |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2746000, episode_reward=525.68 +/- 7.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 526      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2746000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2747000, episode_reward=654.44 +/- 23.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 654      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2747000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.8     |\n",
      "|    critic_loss     | 0.194    |\n",
      "|    ent_coef        | 0.00463  |\n",
      "|    ent_coef_loss   | 3.19     |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2748000, episode_reward=659.53 +/- 20.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 660      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2748000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 318      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2748     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4071     |\n",
      "|    total_timesteps | 2748000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2749000, episode_reward=425.64 +/- 7.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 426      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2749000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 0.295    |\n",
      "|    ent_coef        | 0.00465  |\n",
      "|    ent_coef_loss   | 9.78     |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2750000, episode_reward=431.26 +/- 5.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2750000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2751000, episode_reward=571.76 +/- 3.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 572      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2751000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.1     |\n",
      "|    critic_loss     | 0.184    |\n",
      "|    ent_coef        | 0.00468  |\n",
      "|    ent_coef_loss   | -2.84    |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2752000, episode_reward=594.00 +/- 10.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 594      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2752000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 338      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2752     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4077     |\n",
      "|    total_timesteps | 2752000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2753000, episode_reward=383.20 +/- 6.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 383      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2753000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 0.21     |\n",
      "|    ent_coef        | 0.0047   |\n",
      "|    ent_coef_loss   | 3.53     |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2754000, episode_reward=383.81 +/- 8.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 384      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2754000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2755000, episode_reward=813.47 +/- 29.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 813      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2755000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 0.173    |\n",
      "|    ent_coef        | 0.00471  |\n",
      "|    ent_coef_loss   | 0.55     |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2756000, episode_reward=825.12 +/- 18.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 825      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2756000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 360      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2756     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 4082     |\n",
      "|    total_timesteps | 2756000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2757000, episode_reward=464.33 +/- 17.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 464      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2757000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 0.255    |\n",
      "|    ent_coef        | 0.00473  |\n",
      "|    ent_coef_loss   | 12.7     |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2758000, episode_reward=462.15 +/- 13.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2758000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2759000, episode_reward=696.43 +/- 4.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 696      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2759000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.1     |\n",
      "|    critic_loss     | 0.252    |\n",
      "|    ent_coef        | 0.00479  |\n",
      "|    ent_coef_loss   | 2.86     |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=678.17 +/- 25.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 678      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2760000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 375      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2760     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 4088     |\n",
      "|    total_timesteps | 2760000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761000, episode_reward=608.32 +/- 5.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 608      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2761000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 0.276    |\n",
      "|    ent_coef        | 0.00483  |\n",
      "|    ent_coef_loss   | 3.4      |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2762000, episode_reward=612.58 +/- 8.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 613      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2762000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2763000, episode_reward=562.34 +/- 11.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 562      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2763000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 0.253    |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2764000, episode_reward=558.24 +/- 15.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 558      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2764000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 390      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2764     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4096     |\n",
      "|    total_timesteps | 2764000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2765000, episode_reward=587.87 +/- 14.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 588      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2765000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 0.288    |\n",
      "|    ent_coef        | 0.00488  |\n",
      "|    ent_coef_loss   | 5.23     |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2766000, episode_reward=569.34 +/- 7.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 569      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2766000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2767000, episode_reward=626.09 +/- 12.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 626      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2767000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 0.247    |\n",
      "|    ent_coef        | 0.00491  |\n",
      "|    ent_coef_loss   | 1.76     |\n",
      "|    learning_rate   | 0.00223  |\n",
      "|    n_updates       | 13510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2768000, episode_reward=625.41 +/- 16.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 625      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2768000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 405      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2768     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4104     |\n",
      "|    total_timesteps | 2768000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2769000, episode_reward=718.40 +/- 20.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 718      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2769000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.5     |\n",
      "|    critic_loss     | 0.314    |\n",
      "|    ent_coef        | 0.00493  |\n",
      "|    ent_coef_loss   | 1.39     |\n",
      "|    learning_rate   | 0.00223  |\n",
      "|    n_updates       | 13520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=713.96 +/- 7.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 714      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2770000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2771000, episode_reward=559.37 +/- 11.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 559      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2771000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.3     |\n",
      "|    critic_loss     | 0.371    |\n",
      "|    ent_coef        | 0.00495  |\n",
      "|    ent_coef_loss   | 5.68     |\n",
      "|    learning_rate   | 0.00223  |\n",
      "|    n_updates       | 13530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2772000, episode_reward=560.15 +/- 17.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 560      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2772000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 422      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2772     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4111     |\n",
      "|    total_timesteps | 2772000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2773000, episode_reward=489.99 +/- 9.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 490      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2773000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.5     |\n",
      "|    critic_loss     | 0.345    |\n",
      "|    ent_coef        | 0.00498  |\n",
      "|    ent_coef_loss   | 5.97     |\n",
      "|    learning_rate   | 0.00223  |\n",
      "|    n_updates       | 13540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2774000, episode_reward=497.02 +/- 22.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 497      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2774000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2775000, episode_reward=474.32 +/- 15.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 474      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2775000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2776000, episode_reward=666.88 +/- 18.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 667      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2776000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.9     |\n",
      "|    critic_loss     | 0.313    |\n",
      "|    ent_coef        | 0.00502  |\n",
      "|    ent_coef_loss   | 0.942    |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 441      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2776     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4122     |\n",
      "|    total_timesteps | 2776000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2777000, episode_reward=662.88 +/- 13.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 663      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2777000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2778000, episode_reward=662.44 +/- 28.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 662      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2778000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 0.399    |\n",
      "|    ent_coef        | 0.00505  |\n",
      "|    ent_coef_loss   | 5.28     |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2779000, episode_reward=671.26 +/- 28.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 671      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2779000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=597.34 +/- 19.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 597      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 0.38     |\n",
      "|    ent_coef        | 0.00509  |\n",
      "|    ent_coef_loss   | 4.37     |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 459      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2780     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4127     |\n",
      "|    total_timesteps | 2780000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2781000, episode_reward=608.85 +/- 20.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 609      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2781000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2782000, episode_reward=575.16 +/- 10.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 575      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2782000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 0.317    |\n",
      "|    ent_coef        | 0.00512  |\n",
      "|    ent_coef_loss   | 1.91     |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2783000, episode_reward=583.24 +/- 7.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 583      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2783000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2784000, episode_reward=650.85 +/- 20.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 651      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2784000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.3     |\n",
      "|    critic_loss     | 0.328    |\n",
      "|    ent_coef        | 0.00515  |\n",
      "|    ent_coef_loss   | -0.299   |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13590    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 474      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2784     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4133     |\n",
      "|    total_timesteps | 2784000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2785000, episode_reward=665.73 +/- 48.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 666      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2785000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2786000, episode_reward=767.36 +/- 24.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 767      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2786000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 0.384    |\n",
      "|    ent_coef        | 0.00515  |\n",
      "|    ent_coef_loss   | -1.96    |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2787000, episode_reward=767.71 +/- 6.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 768      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2787000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2788000, episode_reward=666.22 +/- 9.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 666      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2788000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 0.336    |\n",
      "|    ent_coef        | 0.00515  |\n",
      "|    ent_coef_loss   | 3.11     |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13610    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 488      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2788     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4138     |\n",
      "|    total_timesteps | 2788000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2789000, episode_reward=663.07 +/- 24.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 663      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2789000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=698.15 +/- 13.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 698      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16       |\n",
      "|    critic_loss     | 0.386    |\n",
      "|    ent_coef        | 0.00517  |\n",
      "|    ent_coef_loss   | 7.09     |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2791000, episode_reward=691.91 +/- 22.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 692      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2791000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2792000, episode_reward=723.86 +/- 13.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 724      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2792000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 0.388    |\n",
      "|    ent_coef        | 0.00521  |\n",
      "|    ent_coef_loss   | 4.32     |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13630    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 498      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2792     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4144     |\n",
      "|    total_timesteps | 2792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2793000, episode_reward=713.38 +/- 25.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 713      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2793000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2794000, episode_reward=635.94 +/- 14.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 636      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2794000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 0.36     |\n",
      "|    ent_coef        | 0.00525  |\n",
      "|    ent_coef_loss   | 2.05     |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2795000, episode_reward=638.97 +/- 31.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 639      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2795000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2796000, episode_reward=655.04 +/- 27.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 655      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2796000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 0.356    |\n",
      "|    ent_coef        | 0.00528  |\n",
      "|    ent_coef_loss   | 1.78     |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13650    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 511      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2796     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4150     |\n",
      "|    total_timesteps | 2796000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2797000, episode_reward=653.23 +/- 16.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 653      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2797000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2798000, episode_reward=436.61 +/- 20.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 437      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2798000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 0.318    |\n",
      "|    ent_coef        | 0.0053   |\n",
      "|    ent_coef_loss   | 4.46     |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2799000, episode_reward=448.14 +/- 10.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 448      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2799000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=736.72 +/- 24.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 737      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 0.305    |\n",
      "|    ent_coef        | 0.00533  |\n",
      "|    ent_coef_loss   | -0.325   |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13670    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 514      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2800     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4155     |\n",
      "|    total_timesteps | 2800000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2801000, episode_reward=714.83 +/- 5.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 715      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2801000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802000, episode_reward=700.01 +/- 14.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 700      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2802000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16       |\n",
      "|    critic_loss     | 0.466    |\n",
      "|    ent_coef        | 0.00534  |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2803000, episode_reward=686.39 +/- 6.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 686      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2803000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2804000, episode_reward=603.28 +/- 17.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 603      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2804000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16       |\n",
      "|    critic_loss     | 0.362    |\n",
      "|    ent_coef        | 0.00534  |\n",
      "|    ent_coef_loss   | -0.0698  |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13690    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 526      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2804     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4161     |\n",
      "|    total_timesteps | 2804000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2805000, episode_reward=614.80 +/- 34.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 615      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2805000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2806000, episode_reward=674.99 +/- 11.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 675      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2806000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 0.249    |\n",
      "|    ent_coef        | 0.00533  |\n",
      "|    ent_coef_loss   | -3.64    |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2807000, episode_reward=661.27 +/- 11.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 661      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2807000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2808000, episode_reward=715.09 +/- 20.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 715      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2808000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 0.209    |\n",
      "|    ent_coef        | 0.00531  |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 531      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2808     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4167     |\n",
      "|    total_timesteps | 2808000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2809000, episode_reward=733.61 +/- 19.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 734      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2809000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=621.76 +/- 16.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 622      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 0.304    |\n",
      "|    ent_coef        | 0.0053   |\n",
      "|    ent_coef_loss   | 3.31     |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2811000, episode_reward=618.81 +/- 14.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 619      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2811000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2812000, episode_reward=482.61 +/- 16.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 483      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2812000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.3     |\n",
      "|    critic_loss     | 0.283    |\n",
      "|    ent_coef        | 0.00531  |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 543      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2812     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4172     |\n",
      "|    total_timesteps | 2812000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2813000, episode_reward=487.07 +/- 8.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 487      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2813000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2814000, episode_reward=564.91 +/- 8.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 565      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2814000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 0.16     |\n",
      "|    ent_coef        | 0.0053   |\n",
      "|    ent_coef_loss   | -5.4     |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2815000, episode_reward=569.99 +/- 23.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 570      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2815000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2816000, episode_reward=556.78 +/- 17.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 557      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2816000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 550      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2816     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4180     |\n",
      "|    total_timesteps | 2816000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2817000, episode_reward=689.91 +/- 26.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 690      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2817000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 0.251    |\n",
      "|    ent_coef        | 0.00527  |\n",
      "|    ent_coef_loss   | 0.411    |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2818000, episode_reward=675.01 +/- 16.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 675      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2818000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2819000, episode_reward=668.79 +/- 23.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 669      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2819000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 0.363    |\n",
      "|    ent_coef        | 0.00527  |\n",
      "|    ent_coef_loss   | 4.59     |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2820000, episode_reward=659.08 +/- 16.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 659      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2820000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 560      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2820     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4187     |\n",
      "|    total_timesteps | 2820000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2821000, episode_reward=729.99 +/- 27.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 730      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2821000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 0.275    |\n",
      "|    ent_coef        | 0.00529  |\n",
      "|    ent_coef_loss   | 4.96     |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2822000, episode_reward=742.06 +/- 20.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 742      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2822000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2823000, episode_reward=781.00 +/- 21.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 781      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2823000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 0.325    |\n",
      "|    ent_coef        | 0.00534  |\n",
      "|    ent_coef_loss   | 5.49     |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2824000, episode_reward=768.85 +/- 34.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 769      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2824000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 570      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2824     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4194     |\n",
      "|    total_timesteps | 2824000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2825000, episode_reward=815.28 +/- 13.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 815      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2825000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.327    |\n",
      "|    ent_coef        | 0.00538  |\n",
      "|    ent_coef_loss   | 3.55     |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2826000, episode_reward=828.40 +/- 13.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 828      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2826000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2827000, episode_reward=873.10 +/- 19.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 873      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2827000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.2     |\n",
      "|    critic_loss     | 0.324    |\n",
      "|    ent_coef        | 0.00543  |\n",
      "|    ent_coef_loss   | 4.71     |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2828000, episode_reward=850.10 +/- 26.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 850      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2828000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 581      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2828     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4200     |\n",
      "|    total_timesteps | 2828000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2829000, episode_reward=826.93 +/- 16.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 827      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2829000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.356    |\n",
      "|    ent_coef        | 0.00547  |\n",
      "|    ent_coef_loss   | 4.55     |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2830000, episode_reward=843.95 +/- 14.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 844      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2830000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2831000, episode_reward=861.85 +/- 19.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 862      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2831000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 0.349    |\n",
      "|    ent_coef        | 0.00552  |\n",
      "|    ent_coef_loss   | 3.37     |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2832000, episode_reward=871.48 +/- 30.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 871      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2832000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 591      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2832     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4206     |\n",
      "|    total_timesteps | 2832000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2833000, episode_reward=935.92 +/- 9.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 936      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2833000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.353    |\n",
      "|    ent_coef        | 0.00556  |\n",
      "|    ent_coef_loss   | 4.28     |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2834000, episode_reward=910.83 +/- 10.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 911      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2834000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2835000, episode_reward=839.37 +/- 19.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 839      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2835000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.364    |\n",
      "|    ent_coef        | 0.0056   |\n",
      "|    ent_coef_loss   | 3.2      |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2836000, episode_reward=830.08 +/- 11.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 830      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2836000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 602      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2836     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4214     |\n",
      "|    total_timesteps | 2836000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2837000, episode_reward=812.48 +/- 22.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 812      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2837000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 0.382    |\n",
      "|    ent_coef        | 0.00564  |\n",
      "|    ent_coef_loss   | 4.58     |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2838000, episode_reward=817.69 +/- 26.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 818      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2838000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2839000, episode_reward=840.06 +/- 45.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 840      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2839000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.416    |\n",
      "|    ent_coef        | 0.00568  |\n",
      "|    ent_coef_loss   | 2.06     |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2840000, episode_reward=876.73 +/- 29.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 877      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2840000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 612      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2840     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4220     |\n",
      "|    total_timesteps | 2840000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2841000, episode_reward=884.72 +/- 12.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 885      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2841000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 0.403    |\n",
      "|    ent_coef        | 0.00572  |\n",
      "|    ent_coef_loss   | 2.48     |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842000, episode_reward=850.06 +/- 22.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 850      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2842000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2843000, episode_reward=844.37 +/- 21.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 844      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2843000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.368    |\n",
      "|    ent_coef        | 0.00574  |\n",
      "|    ent_coef_loss   | 2.12     |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2844000, episode_reward=847.60 +/- 18.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 848      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2844000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 622      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2844     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4226     |\n",
      "|    total_timesteps | 2844000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2845000, episode_reward=879.96 +/- 16.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 880      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2845000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.39     |\n",
      "|    ent_coef        | 0.00577  |\n",
      "|    ent_coef_loss   | 2.42     |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2846000, episode_reward=863.08 +/- 36.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 863      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2846000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2847000, episode_reward=810.72 +/- 28.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 811      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2847000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.346    |\n",
      "|    ent_coef        | 0.0058   |\n",
      "|    ent_coef_loss   | 3.46     |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2848000, episode_reward=824.10 +/- 27.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 824      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2848000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 630      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2848     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4232     |\n",
      "|    total_timesteps | 2848000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2849000, episode_reward=803.92 +/- 23.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 804      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2849000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.395    |\n",
      "|    ent_coef        | 0.00583  |\n",
      "|    ent_coef_loss   | 2.11     |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=805.53 +/- 32.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 806      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2850000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2851000, episode_reward=746.86 +/- 42.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 747      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2851000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.2     |\n",
      "|    critic_loss     | 0.414    |\n",
      "|    ent_coef        | 0.00586  |\n",
      "|    ent_coef_loss   | 1.84     |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2852000, episode_reward=744.76 +/- 39.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 745      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2852000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 639      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2852     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4238     |\n",
      "|    total_timesteps | 2852000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2853000, episode_reward=770.91 +/- 8.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 771      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2853000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.412    |\n",
      "|    ent_coef        | 0.00588  |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2854000, episode_reward=771.54 +/- 23.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 772      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2854000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2855000, episode_reward=911.73 +/- 41.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 912      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2855000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 0.421    |\n",
      "|    ent_coef        | 0.00589  |\n",
      "|    ent_coef_loss   | 4.59     |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2856000, episode_reward=898.85 +/- 11.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 899      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2856000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 647      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2856     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4244     |\n",
      "|    total_timesteps | 2856000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2857000, episode_reward=764.84 +/- 29.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 765      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2857000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 0.549    |\n",
      "|    ent_coef        | 0.00592  |\n",
      "|    ent_coef_loss   | 6.22     |\n",
      "|    learning_rate   | 0.00214  |\n",
      "|    n_updates       | 13950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2858000, episode_reward=771.09 +/- 36.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 771      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2858000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2859000, episode_reward=750.59 +/- 36.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 751      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2859000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=693.67 +/- 59.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 694      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.2     |\n",
      "|    critic_loss     | 0.54     |\n",
      "|    ent_coef        | 0.00598  |\n",
      "|    ent_coef_loss   | 3.19     |\n",
      "|    learning_rate   | 0.00214  |\n",
      "|    n_updates       | 13960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 654      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2860     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4250     |\n",
      "|    total_timesteps | 2860000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2861000, episode_reward=671.51 +/- 45.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 672      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2861000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2862000, episode_reward=794.67 +/- 34.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 795      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2862000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.466    |\n",
      "|    ent_coef        | 0.00603  |\n",
      "|    ent_coef_loss   | 4.9      |\n",
      "|    learning_rate   | 0.00214  |\n",
      "|    n_updates       | 13970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2863000, episode_reward=820.35 +/- 28.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 820      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2863000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2864000, episode_reward=706.10 +/- 16.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 706      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2864000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.482    |\n",
      "|    ent_coef        | 0.00609  |\n",
      "|    ent_coef_loss   | 9.45     |\n",
      "|    learning_rate   | 0.00214  |\n",
      "|    n_updates       | 13980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 659      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2864     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4256     |\n",
      "|    total_timesteps | 2864000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2865000, episode_reward=715.26 +/- 17.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 715      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2865000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2866000, episode_reward=552.67 +/- 181.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 553      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2866000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.2     |\n",
      "|    critic_loss     | 0.454    |\n",
      "|    ent_coef        | 0.00618  |\n",
      "|    ent_coef_loss   | 3.08     |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 13990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2867000, episode_reward=444.01 +/- 168.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 444      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2867000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2868000, episode_reward=644.78 +/- 14.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 645      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2868000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 0.714    |\n",
      "|    ent_coef        | 0.00625  |\n",
      "|    ent_coef_loss   | 6.66     |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 658      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2868     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4262     |\n",
      "|    total_timesteps | 2868000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2869000, episode_reward=636.39 +/- 20.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 636      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2869000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=639.43 +/- 30.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 639      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 0.513    |\n",
      "|    ent_coef        | 0.00632  |\n",
      "|    ent_coef_loss   | 5.05     |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 14010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2871000, episode_reward=643.11 +/- 32.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 643      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2871000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2872000, episode_reward=700.26 +/- 28.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 700      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2872000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 0.52     |\n",
      "|    ent_coef        | 0.00639  |\n",
      "|    ent_coef_loss   | 4.14     |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 14020    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 657      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2872     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4268     |\n",
      "|    total_timesteps | 2872000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2873000, episode_reward=664.19 +/- 50.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 664      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2873000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2874000, episode_reward=888.74 +/- 32.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 889      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2874000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 0.546    |\n",
      "|    ent_coef        | 0.00646  |\n",
      "|    ent_coef_loss   | 6.88     |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 14030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2875000, episode_reward=851.64 +/- 37.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 852      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2875000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2876000, episode_reward=840.57 +/- 18.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 841      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2876000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 0.529    |\n",
      "|    ent_coef        | 0.00654  |\n",
      "|    ent_coef_loss   | 5.97     |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14040    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 666      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2876     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4274     |\n",
      "|    total_timesteps | 2876000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2877000, episode_reward=833.47 +/- 35.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 833      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2877000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2878000, episode_reward=820.14 +/- 10.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 820      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2878000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.521    |\n",
      "|    ent_coef        | 0.00662  |\n",
      "|    ent_coef_loss   | 5.3      |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2879000, episode_reward=796.87 +/- 39.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 797      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2879000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=745.28 +/- 232.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 745      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 0.69     |\n",
      "|    ent_coef        | 0.00669  |\n",
      "|    ent_coef_loss   | 3.36     |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14060    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 669      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2880     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4279     |\n",
      "|    total_timesteps | 2880000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2881000, episode_reward=679.65 +/- 46.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 680      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2881000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882000, episode_reward=551.23 +/- 394.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 551      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2882000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 0.764    |\n",
      "|    ent_coef        | 0.00676  |\n",
      "|    ent_coef_loss   | 9.52     |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2883000, episode_reward=626.45 +/- 262.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 626      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2883000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2884000, episode_reward=-254.42 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -254     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2884000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.515    |\n",
      "|    ent_coef        | 0.00687  |\n",
      "|    ent_coef_loss   | 8.51     |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14080    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 671      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2884     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4286     |\n",
      "|    total_timesteps | 2884000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2885000, episode_reward=-253.16 +/- 2.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -253     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2885000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2886000, episode_reward=730.86 +/- 26.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 731      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2886000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 0.743    |\n",
      "|    ent_coef        | 0.00698  |\n",
      "|    ent_coef_loss   | 5.27     |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2887000, episode_reward=734.80 +/- 47.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 735      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2887000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2888000, episode_reward=781.07 +/- 20.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 781      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2888000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 0.783    |\n",
      "|    ent_coef        | 0.00708  |\n",
      "|    ent_coef_loss   | 5.28     |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14100    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 669      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2888     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4291     |\n",
      "|    total_timesteps | 2888000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2889000, episode_reward=809.45 +/- 21.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 809      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2889000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=648.44 +/- 62.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 648      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.674    |\n",
      "|    ent_coef        | 0.00717  |\n",
      "|    ent_coef_loss   | 9.77     |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2891000, episode_reward=700.93 +/- 254.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 701      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2891000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2892000, episode_reward=697.43 +/- 45.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 697      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2892000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 0.927    |\n",
      "|    ent_coef        | 0.0073   |\n",
      "|    ent_coef_loss   | 8.12     |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14120    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 673      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2892     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4297     |\n",
      "|    total_timesteps | 2892000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2893000, episode_reward=684.35 +/- 26.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 684      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2893000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2894000, episode_reward=811.73 +/- 25.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 812      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2894000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.75     |\n",
      "|    ent_coef        | 0.00743  |\n",
      "|    ent_coef_loss   | 6.37     |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2895000, episode_reward=839.68 +/- 27.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 840      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2895000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2896000, episode_reward=751.23 +/- 37.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 751      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2896000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.648    |\n",
      "|    ent_coef        | 0.00754  |\n",
      "|    ent_coef_loss   | 4.99     |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 676      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2896     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4303     |\n",
      "|    total_timesteps | 2896000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2897000, episode_reward=757.84 +/- 25.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 758      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2897000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2898000, episode_reward=895.79 +/- 24.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 896      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2898000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 0.634    |\n",
      "|    ent_coef        | 0.00763  |\n",
      "|    ent_coef_loss   | 1.61     |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2899000, episode_reward=897.86 +/- 31.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 898      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2899000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=825.14 +/- 32.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 825      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 0.59     |\n",
      "|    ent_coef        | 0.0077   |\n",
      "|    ent_coef_loss   | 7.69     |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 687      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2900     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4309     |\n",
      "|    total_timesteps | 2900000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2901000, episode_reward=808.21 +/- 8.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 808      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2901000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2902000, episode_reward=866.76 +/- 8.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 867      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2902000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2903000, episode_reward=846.42 +/- 21.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 846      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2903000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.741    |\n",
      "|    ent_coef        | 0.0078   |\n",
      "|    ent_coef_loss   | 7.47     |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2904000, episode_reward=852.74 +/- 33.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 853      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2904000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 692      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2904     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4315     |\n",
      "|    total_timesteps | 2904000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2905000, episode_reward=-205.99 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2905000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.675    |\n",
      "|    ent_coef        | 0.00792  |\n",
      "|    ent_coef_loss   | 4.61     |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2906000, episode_reward=-206.15 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2906000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2907000, episode_reward=158.49 +/- 488.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 158      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2907000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.8     |\n",
      "|    critic_loss     | 0.634    |\n",
      "|    ent_coef        | 0.008    |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2908000, episode_reward=315.92 +/- 412.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 316      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2908000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 697      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2908     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4321     |\n",
      "|    total_timesteps | 2908000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2909000, episode_reward=862.65 +/- 104.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 863      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2909000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 0.817    |\n",
      "|    ent_coef        | 0.00803  |\n",
      "|    ent_coef_loss   | -0.0726  |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2910000, episode_reward=726.52 +/- 344.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 727      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2910000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2911000, episode_reward=817.28 +/- 50.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 817      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2911000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.5     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.00803  |\n",
      "|    ent_coef_loss   | -1.61    |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2912000, episode_reward=851.61 +/- 42.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 852      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2912000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 703      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2912     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4327     |\n",
      "|    total_timesteps | 2912000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2913000, episode_reward=811.85 +/- 31.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 812      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2913000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.607    |\n",
      "|    ent_coef        | 0.00802  |\n",
      "|    ent_coef_loss   | 5.19     |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2914000, episode_reward=880.52 +/- 75.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 881      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2914000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2915000, episode_reward=588.84 +/- 128.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 589      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2915000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.2     |\n",
      "|    critic_loss     | 0.719    |\n",
      "|    ent_coef        | 0.00807  |\n",
      "|    ent_coef_loss   | 2.92     |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2916000, episode_reward=715.29 +/- 55.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 715      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2916000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 711      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2916     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4333     |\n",
      "|    total_timesteps | 2916000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2917000, episode_reward=893.83 +/- 19.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 894      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2917000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.00812  |\n",
      "|    ent_coef_loss   | -0.196   |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2918000, episode_reward=810.25 +/- 36.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 810      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2918000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2919000, episode_reward=883.72 +/- 56.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 884      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2919000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.889    |\n",
      "|    ent_coef        | 0.00815  |\n",
      "|    ent_coef_loss   | 4.27     |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=888.85 +/- 39.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 889      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2920000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 715      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2920     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4339     |\n",
      "|    total_timesteps | 2920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2921000, episode_reward=748.02 +/- 65.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 748      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2921000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 0.719    |\n",
      "|    ent_coef        | 0.00821  |\n",
      "|    ent_coef_loss   | 3.44     |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2922000, episode_reward=784.95 +/- 65.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 785      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2922000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923000, episode_reward=926.78 +/- 108.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 927      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2923000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.2     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.00826  |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2924000, episode_reward=918.27 +/- 118.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 918      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2924000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 719      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2924     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4345     |\n",
      "|    total_timesteps | 2924000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2925000, episode_reward=943.04 +/- 39.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 943      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2925000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.44     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.00831  |\n",
      "|    ent_coef_loss   | 5.11     |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2926000, episode_reward=926.00 +/- 44.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 926      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2926000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2927000, episode_reward=626.88 +/- 23.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 627      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2927000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11       |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.00837  |\n",
      "|    ent_coef_loss   | 0.0302   |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2928000, episode_reward=645.45 +/- 28.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 645      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2928000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 719      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2928     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4352     |\n",
      "|    total_timesteps | 2928000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2929000, episode_reward=672.29 +/- 63.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 672      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2929000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 0.578    |\n",
      "|    ent_coef        | 0.00839  |\n",
      "|    ent_coef_loss   | -3.38    |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=624.75 +/- 53.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 625      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2930000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2931000, episode_reward=1007.59 +/- 53.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2931000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.7     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.00836  |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2932000, episode_reward=984.28 +/- 129.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 984      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2932000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 716      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2932     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4358     |\n",
      "|    total_timesteps | 2932000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2933000, episode_reward=1010.54 +/- 116.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2933000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6        |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.00835  |\n",
      "|    ent_coef_loss   | 7.38     |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2934000, episode_reward=1065.46 +/- 43.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2934000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2935000, episode_reward=937.18 +/- 128.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 937      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2935000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.58     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.00843  |\n",
      "|    ent_coef_loss   | 5.44     |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2936000, episode_reward=1035.71 +/- 123.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2936000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 723      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2936     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4364     |\n",
      "|    total_timesteps | 2936000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2937000, episode_reward=733.81 +/- 37.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 734      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2937000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.00853  |\n",
      "|    ent_coef_loss   | 5.64     |\n",
      "|    learning_rate   | 0.00206  |\n",
      "|    n_updates       | 14340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2938000, episode_reward=846.05 +/- 114.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 846      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2938000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2939000, episode_reward=911.25 +/- 46.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 911      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2939000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13       |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.00864  |\n",
      "|    ent_coef_loss   | 3.02     |\n",
      "|    learning_rate   | 0.00206  |\n",
      "|    n_updates       | 14350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=862.16 +/- 54.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 862      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2940000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 725      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2940     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4370     |\n",
      "|    total_timesteps | 2940000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2941000, episode_reward=961.36 +/- 49.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 961      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2941000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.79     |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.00872  |\n",
      "|    ent_coef_loss   | 6.16     |\n",
      "|    learning_rate   | 0.00206  |\n",
      "|    n_updates       | 14360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2942000, episode_reward=921.25 +/- 87.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 921      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2942000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2943000, episode_reward=822.74 +/- 74.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 823      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2943000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.7      |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.00883  |\n",
      "|    ent_coef_loss   | 4.65     |\n",
      "|    learning_rate   | 0.00206  |\n",
      "|    n_updates       | 14370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2944000, episode_reward=838.28 +/- 91.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 838      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2944000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 728      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2944     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4376     |\n",
      "|    total_timesteps | 2944000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2945000, episode_reward=824.57 +/- 112.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 825      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2945000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2946000, episode_reward=938.07 +/- 76.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 938      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2946000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.725    |\n",
      "|    ent_coef        | 0.00893  |\n",
      "|    ent_coef_loss   | 2.93     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2947000, episode_reward=985.72 +/- 59.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 986      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2947000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2948000, episode_reward=1130.25 +/- 29.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2948000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.00902  |\n",
      "|    ent_coef_loss   | 6.47     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 734      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2948     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4382     |\n",
      "|    total_timesteps | 2948000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2949000, episode_reward=1179.63 +/- 51.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2949000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2950000, episode_reward=1024.98 +/- 34.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.1      |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.00915  |\n",
      "|    ent_coef_loss   | 8.81     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2951000, episode_reward=1003.99 +/- 58.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2951000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2952000, episode_reward=743.64 +/- 29.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 744      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2952000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.81     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.00931  |\n",
      "|    ent_coef_loss   | 6.32     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 744      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2952     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4388     |\n",
      "|    total_timesteps | 2952000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2953000, episode_reward=763.45 +/- 19.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 763      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2953000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2954000, episode_reward=438.69 +/- 38.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 439      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2954000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 0.734    |\n",
      "|    ent_coef        | 0.00948  |\n",
      "|    ent_coef_loss   | 12.6     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2955000, episode_reward=453.14 +/- 31.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 453      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2955000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2956000, episode_reward=704.47 +/- 56.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 704      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2956000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.8     |\n",
      "|    critic_loss     | 0.88     |\n",
      "|    ent_coef        | 0.00972  |\n",
      "|    ent_coef_loss   | 9.46     |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14430    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 739      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2956     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4395     |\n",
      "|    total_timesteps | 2956000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2957000, episode_reward=645.25 +/- 33.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 645      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2957000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2958000, episode_reward=769.06 +/- 25.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 769      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2958000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.00995  |\n",
      "|    ent_coef_loss   | 3.71     |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2959000, episode_reward=764.50 +/- 28.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 765      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2959000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2960000, episode_reward=763.89 +/- 46.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 764      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.884    |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | 4.68     |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14450    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 740      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2960     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4401     |\n",
      "|    total_timesteps | 2960000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2961000, episode_reward=797.12 +/- 27.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 797      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2961000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2962000, episode_reward=941.41 +/- 140.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 941      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2962000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.5     |\n",
      "|    critic_loss     | 1.59     |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | 6.26     |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963000, episode_reward=970.01 +/- 181.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 970      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2963000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2964000, episode_reward=544.13 +/- 53.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 544      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2964000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.1     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 7.09     |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14470    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 743      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2964     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4407     |\n",
      "|    total_timesteps | 2964000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2965000, episode_reward=555.98 +/- 30.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 556      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2965000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2966000, episode_reward=-279.05 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -279     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2966000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 0.755    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 5.51     |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2967000, episode_reward=-277.14 +/- 1.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -277     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2967000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2968000, episode_reward=-278.11 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -278     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2968000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.8     |\n",
      "|    critic_loss     | 0.769    |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 4.53     |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14490    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 728      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2968     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4413     |\n",
      "|    total_timesteps | 2968000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2969000, episode_reward=-277.40 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -277     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2969000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=727.26 +/- 56.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 727      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.5     |\n",
      "|    critic_loss     | 0.711    |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | 7.11     |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2971000, episode_reward=753.36 +/- 79.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 753      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2971000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2972000, episode_reward=853.96 +/- 37.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 854      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2972000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | 21.3     |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14510    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 716      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2972     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4420     |\n",
      "|    total_timesteps | 2972000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2973000, episode_reward=898.27 +/- 22.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 898      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2973000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2974000, episode_reward=914.11 +/- 66.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 914      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2974000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | 13.6     |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2975000, episode_reward=923.38 +/- 49.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 923      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2975000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2976000, episode_reward=840.53 +/- 32.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 841      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2976000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | 10.8     |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14530    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 722      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2976     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4426     |\n",
      "|    total_timesteps | 2976000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2977000, episode_reward=878.21 +/- 16.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 878      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2977000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2978000, episode_reward=767.08 +/- 30.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 767      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2978000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 0.868    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 11.1     |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2979000, episode_reward=748.05 +/- 26.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 748      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2979000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=863.78 +/- 29.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 864      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.713    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 9.02     |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 727      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2980     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4432     |\n",
      "|    total_timesteps | 2980000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2981000, episode_reward=850.06 +/- 30.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 850      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2981000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2982000, episode_reward=810.64 +/- 31.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 811      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2982000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.875    |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | 7.04     |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2983000, episode_reward=789.15 +/- 36.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 789      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2983000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2984000, episode_reward=816.11 +/- 19.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 816      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2984000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.79     |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 3.54     |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 733      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2984     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4438     |\n",
      "|    total_timesteps | 2984000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2985000, episode_reward=792.57 +/- 39.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 793      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2985000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2986000, episode_reward=929.24 +/- 61.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 929      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2986000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 4.39     |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2987000, episode_reward=901.45 +/- 41.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 901      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2987000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2988000, episode_reward=960.77 +/- 63.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 961      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2988000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 742      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2988     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4444     |\n",
      "|    total_timesteps | 2988000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2989000, episode_reward=872.41 +/- 15.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 872      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2989000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.954    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 1.42     |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=885.05 +/- 41.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 885      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2990000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2991000, episode_reward=873.59 +/- 25.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 874      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2991000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.768    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 2.25     |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2992000, episode_reward=942.49 +/- 83.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 942      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2992000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 747      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2992     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4450     |\n",
      "|    total_timesteps | 2992000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2993000, episode_reward=897.23 +/- 37.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 897      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2993000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 3.53     |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2994000, episode_reward=886.16 +/- 32.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 886      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2994000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2995000, episode_reward=776.77 +/- 32.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 777      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2995000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 0.996    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | 0.163    |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2996000, episode_reward=783.94 +/- 18.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 784      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2996000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 748      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2996     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4456     |\n",
      "|    total_timesteps | 2996000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2997000, episode_reward=721.31 +/- 178.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 721      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2997000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.846    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 2.15     |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2998000, episode_reward=708.83 +/- 155.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 709      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2998000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2999000, episode_reward=868.81 +/- 99.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 869      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2999000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.7     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 0.0126   |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=799.49 +/- 9.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 799      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 743      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3000     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4463     |\n",
      "|    total_timesteps | 3000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3001000, episode_reward=871.42 +/- 102.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 871      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3001000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 0.994    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 2.05     |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3002000, episode_reward=762.87 +/- 49.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 763      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3002000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003000, episode_reward=925.25 +/- 42.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 925      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3003000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | 2.41     |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3004000, episode_reward=764.94 +/- 26.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 765      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3004000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 743      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3004     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4469     |\n",
      "|    total_timesteps | 3004000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3005000, episode_reward=1024.22 +/- 57.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3005000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.5     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 0.374    |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3006000, episode_reward=1050.44 +/- 86.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3006000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3007000, episode_reward=879.02 +/- 33.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 879      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3007000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.4      |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -2.32    |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3008000, episode_reward=959.42 +/- 90.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 959      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3008000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 756      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3008     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4475     |\n",
      "|    total_timesteps | 3008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3009000, episode_reward=812.21 +/- 19.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 812      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3009000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3010000, episode_reward=895.83 +/- 120.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 896      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3010000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3011000, episode_reward=545.31 +/- 8.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 545      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3011000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.937    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3012000, episode_reward=629.53 +/- 122.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 630      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3012000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 754      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3012     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4481     |\n",
      "|    total_timesteps | 3012000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3013000, episode_reward=878.43 +/- 141.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 878      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3013000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.3     |\n",
      "|    critic_loss     | 0.843    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 0.261    |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3014000, episode_reward=901.96 +/- 193.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 902      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3014000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3015000, episode_reward=1084.13 +/- 137.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3015000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.5     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 0.403    |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3016000, episode_reward=1139.79 +/- 139.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3016000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 763      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3016     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4487     |\n",
      "|    total_timesteps | 3016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3017000, episode_reward=810.31 +/- 34.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 810      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3017000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.93     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -0.878   |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3018000, episode_reward=809.86 +/- 69.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 810      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3018000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3019000, episode_reward=606.12 +/- 12.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 606      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3019000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 3.24     |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3020000, episode_reward=627.90 +/- 30.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 628      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3020000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 765      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3020     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 4494     |\n",
      "|    total_timesteps | 3020000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3021000, episode_reward=650.25 +/- 20.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 650      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3021000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16       |\n",
      "|    critic_loss     | 0.778    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | 0.706    |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3022000, episode_reward=614.98 +/- 39.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 615      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3022000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3023000, episode_reward=649.65 +/- 41.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 650      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3023000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 0.8      |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 1.95     |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3024000, episode_reward=672.52 +/- 23.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 673      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3024000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 759      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3024     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 4500     |\n",
      "|    total_timesteps | 3024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3025000, episode_reward=730.57 +/- 14.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 731      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3025000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.7     |\n",
      "|    critic_loss     | 0.965    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 0.913    |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3026000, episode_reward=705.00 +/- 34.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 705      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3026000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3027000, episode_reward=761.53 +/- 38.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 762      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3027000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.8     |\n",
      "|    critic_loss     | 0.988    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.322    |\n",
      "|    learning_rate   | 0.00197  |\n",
      "|    n_updates       | 14780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3028000, episode_reward=728.83 +/- 44.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 729      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3028000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 755      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3028     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 4506     |\n",
      "|    total_timesteps | 3028000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3029000, episode_reward=759.25 +/- 16.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 759      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3029000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 0.966    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.00197  |\n",
      "|    n_updates       | 14790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3030000, episode_reward=781.42 +/- 33.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 781      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3030000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3031000, episode_reward=783.65 +/- 28.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 784      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3031000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3032000, episode_reward=782.38 +/- 39.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 782      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3032000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 0.854    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.00197  |\n",
      "|    n_updates       | 14800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 753      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3032     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 4512     |\n",
      "|    total_timesteps | 3032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3033000, episode_reward=764.46 +/- 21.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 764      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3033000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3034000, episode_reward=759.54 +/- 46.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 760      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3034000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.8     |\n",
      "|    critic_loss     | 0.857    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.205    |\n",
      "|    learning_rate   | 0.00197  |\n",
      "|    n_updates       | 14810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3035000, episode_reward=771.40 +/- 51.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 771      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3035000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3036000, episode_reward=744.06 +/- 83.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 744      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3036000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 0.899    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.29     |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 744      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3036     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 4518     |\n",
      "|    total_timesteps | 3036000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3037000, episode_reward=818.70 +/- 53.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 819      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3037000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3038000, episode_reward=682.20 +/- 24.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 682      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3038000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 0.812    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -0.833   |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3039000, episode_reward=690.58 +/- 42.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 691      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3039000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=634.92 +/- 37.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 635      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.428    |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14840    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 738      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3040     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 4523     |\n",
      "|    total_timesteps | 3040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3041000, episode_reward=634.79 +/- 26.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 635      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3041000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3042000, episode_reward=681.23 +/- 28.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 681      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3042000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 0.811    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -0.613   |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3043000, episode_reward=706.65 +/- 32.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 707      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3043000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3044000, episode_reward=691.98 +/- 6.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 692      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3044000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 0.868    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -0.91    |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14860    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 728      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3044     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 4529     |\n",
      "|    total_timesteps | 3044000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3045000, episode_reward=686.31 +/- 22.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 686      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3045000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3046000, episode_reward=698.46 +/- 16.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 698      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3046000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 0.799    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 2.14     |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3047000, episode_reward=648.61 +/- 30.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 649      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3047000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3048000, episode_reward=738.78 +/- 24.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 739      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3048000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 0.734    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -0.364   |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14880    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 719      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3048     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 4535     |\n",
      "|    total_timesteps | 3048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3049000, episode_reward=699.08 +/- 45.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 699      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3049000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3050000, episode_reward=785.87 +/- 44.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 786      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 0.849    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.568    |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3051000, episode_reward=766.66 +/- 18.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 767      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3051000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3052000, episode_reward=786.60 +/- 38.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 787      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3052000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 0.896    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14900    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 711      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3052     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 4541     |\n",
      "|    total_timesteps | 3052000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3053000, episode_reward=807.24 +/- 19.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 807      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3053000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3054000, episode_reward=729.47 +/- 7.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 729      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3054000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.889    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.061    |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3055000, episode_reward=747.14 +/- 11.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 747      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3055000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3056000, episode_reward=722.81 +/- 45.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 723      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3056000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 0.805    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.0844   |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14920    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 715      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3056     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4547     |\n",
      "|    total_timesteps | 3056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3057000, episode_reward=726.34 +/- 29.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 726      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3057000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3058000, episode_reward=778.16 +/- 29.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 778      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3058000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.7     |\n",
      "|    critic_loss     | 0.939    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3059000, episode_reward=762.09 +/- 34.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 762      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3059000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060000, episode_reward=701.41 +/- 29.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 701      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.817    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -1.58    |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14940    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 714      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3060     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4553     |\n",
      "|    total_timesteps | 3060000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3061000, episode_reward=671.95 +/- 25.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 672      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3061000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3062000, episode_reward=692.38 +/- 52.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 692      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3062000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16       |\n",
      "|    critic_loss     | 0.912    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 2.04     |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3063000, episode_reward=730.11 +/- 64.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 730      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3063000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3064000, episode_reward=647.09 +/- 62.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 647      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3064000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.8     |\n",
      "|    critic_loss     | 0.822    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -0.0903  |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 711      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3064     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4559     |\n",
      "|    total_timesteps | 3064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3065000, episode_reward=675.81 +/- 50.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 676      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3065000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3066000, episode_reward=774.46 +/- 34.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 774      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3066000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.3     |\n",
      "|    critic_loss     | 0.887    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.0533  |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 14970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3067000, episode_reward=750.80 +/- 30.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 751      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3067000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3068000, episode_reward=735.61 +/- 55.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 736      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3068000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.8     |\n",
      "|    critic_loss     | 0.831    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.0358  |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 14980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 732      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3068     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4564     |\n",
      "|    total_timesteps | 3068000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3069000, episode_reward=732.74 +/- 46.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 733      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3069000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3070000, episode_reward=670.63 +/- 37.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 671      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 0.777    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.419   |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 14990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3071000, episode_reward=650.87 +/- 21.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 651      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3071000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3072000, episode_reward=641.93 +/- 26.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 642      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3072000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 743      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3072     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4570     |\n",
      "|    total_timesteps | 3072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3073000, episode_reward=855.72 +/- 27.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 856      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3073000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.7     |\n",
      "|    critic_loss     | 0.807    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.73     |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 15000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3074000, episode_reward=880.65 +/- 25.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 881      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3074000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3075000, episode_reward=814.28 +/- 34.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 814      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3075000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.828    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 0.718    |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 15010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3076000, episode_reward=800.40 +/- 39.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 800      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3076000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 738      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3076     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4576     |\n",
      "|    total_timesteps | 3076000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3077000, episode_reward=726.37 +/- 62.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 726      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3077000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 0.763    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3078000, episode_reward=695.28 +/- 23.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 695      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3078000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3079000, episode_reward=841.62 +/- 28.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 842      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3079000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 0.808    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 4.4      |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3080000, episode_reward=873.41 +/- 41.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 873      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3080000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 734      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3080     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4582     |\n",
      "|    total_timesteps | 3080000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3081000, episode_reward=858.88 +/- 21.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 859      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3081000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.753    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 2.48     |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3082000, episode_reward=884.94 +/- 15.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 885      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3082000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3083000, episode_reward=755.52 +/- 51.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 756      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3083000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.69     |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3084000, episode_reward=791.44 +/- 56.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 791      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3084000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 732      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3084     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4587     |\n",
      "|    total_timesteps | 3084000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3085000, episode_reward=829.77 +/- 24.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 830      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3085000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3086000, episode_reward=879.61 +/- 36.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 880      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3086000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3087000, episode_reward=956.60 +/- 20.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 957      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3087000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.724    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3088000, episode_reward=946.95 +/- 12.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 947      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3088000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 730      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3088     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4593     |\n",
      "|    total_timesteps | 3088000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3089000, episode_reward=837.07 +/- 24.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 837      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3089000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 0.633    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090000, episode_reward=865.00 +/- 20.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 865      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3090000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3091000, episode_reward=840.60 +/- 46.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 841      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3091000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3092000, episode_reward=847.53 +/- 29.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 848      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3092000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 728      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3092     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4599     |\n",
      "|    total_timesteps | 3092000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3093000, episode_reward=658.59 +/- 13.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 659      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3093000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.805    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 4.68     |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3094000, episode_reward=672.55 +/- 14.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 673      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3094000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3095000, episode_reward=762.20 +/- 38.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 762      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3095000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.8     |\n",
      "|    critic_loss     | 0.684    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 0.341    |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3096000, episode_reward=757.88 +/- 26.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 758      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3096000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 725      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3096     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4605     |\n",
      "|    total_timesteps | 3096000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3097000, episode_reward=664.62 +/- 32.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 665      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3097000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 0.729    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3098000, episode_reward=654.19 +/- 32.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 654      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3098000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3099000, episode_reward=708.01 +/- 11.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 708      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3099000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 0.788    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.795   |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3100000, episode_reward=715.39 +/- 40.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 715      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3100000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 726      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3100     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4610     |\n",
      "|    total_timesteps | 3100000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3101000, episode_reward=752.39 +/- 38.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 752      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3101000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 0.701    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 0.313    |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3102000, episode_reward=758.31 +/- 23.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 758      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3102000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3103000, episode_reward=785.85 +/- 30.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 786      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3103000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 0.62     |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 0.436    |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3104000, episode_reward=798.84 +/- 28.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 799      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3104000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 725      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3104     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4616     |\n",
      "|    total_timesteps | 3104000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3105000, episode_reward=800.75 +/- 25.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 801      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3105000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 0.828    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3106000, episode_reward=766.58 +/- 15.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 767      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3106000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3107000, episode_reward=843.22 +/- 35.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 843      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3107000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.2     |\n",
      "|    critic_loss     | 0.667    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -0.0808  |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    n_updates       | 15170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3108000, episode_reward=805.14 +/- 25.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 805      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3108000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 716      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3108     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4622     |\n",
      "|    total_timesteps | 3108000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3109000, episode_reward=849.85 +/- 30.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 850      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3109000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 0.99     |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | 0.992    |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    n_updates       | 15180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3110000, episode_reward=817.76 +/- 7.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 818      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3110000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3111000, episode_reward=860.29 +/- 11.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 860      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3111000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 0.703    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    n_updates       | 15190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3112000, episode_reward=856.22 +/- 16.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 856      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3112000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 720      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3112     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4627     |\n",
      "|    total_timesteps | 3112000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3113000, episode_reward=852.33 +/- 39.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 852      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3113000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 0.816    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 1.61     |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    n_updates       | 15200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3114000, episode_reward=849.28 +/- 14.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 849      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3114000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3115000, episode_reward=832.01 +/- 18.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 832      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3115000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3116000, episode_reward=853.86 +/- 33.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 854      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3116000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 0.992    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 2.55     |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 716      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3116     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4633     |\n",
      "|    total_timesteps | 3116000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3117000, episode_reward=840.53 +/- 22.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 841      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3117000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3118000, episode_reward=829.34 +/- 28.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 829      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3118000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.747    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 0.14     |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3119000, episode_reward=806.06 +/- 14.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 806      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3119000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3120000, episode_reward=899.22 +/- 24.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 899      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 0.79     |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 2.34     |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 715      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3120     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4639     |\n",
      "|    total_timesteps | 3120000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3121000, episode_reward=842.58 +/- 16.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 843      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3121000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3122000, episode_reward=893.88 +/- 34.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 894      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3122000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.82     |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 0.929    |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3123000, episode_reward=916.33 +/- 20.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 916      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3123000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3124000, episode_reward=813.17 +/- 25.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 813      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3124000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.762    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 0.00598  |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15250    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 724      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3124     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4644     |\n",
      "|    total_timesteps | 3124000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3125000, episode_reward=830.47 +/- 53.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 830      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3125000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3126000, episode_reward=896.61 +/- 8.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 897      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3126000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 2.02     |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3127000, episode_reward=897.26 +/- 24.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 897      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3127000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3128000, episode_reward=865.02 +/- 11.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 865      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3128000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 0.721    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -1.85    |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15270    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 729      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3128     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4650     |\n",
      "|    total_timesteps | 3128000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3129000, episode_reward=852.82 +/- 19.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 853      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3129000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3130000, episode_reward=912.88 +/- 19.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 913      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.638    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -1.68    |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3131000, episode_reward=894.08 +/- 8.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 894      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3131000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3132000, episode_reward=967.45 +/- 15.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 967      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3132000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 0.669    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 0.404    |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15290    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 737      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3132     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4656     |\n",
      "|    total_timesteps | 3132000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3133000, episode_reward=974.55 +/- 8.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 975      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3133000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3134000, episode_reward=902.07 +/- 13.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 902      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3134000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.3     |\n",
      "|    critic_loss     | 0.726    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -0.212   |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3135000, episode_reward=922.98 +/- 22.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 923      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3135000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3136000, episode_reward=943.90 +/- 22.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 944      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3136000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 0.68     |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 0.529    |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15310    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 742      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3136     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4662     |\n",
      "|    total_timesteps | 3136000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3137000, episode_reward=930.55 +/- 14.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 931      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3137000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3138000, episode_reward=904.63 +/- 31.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 905      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3138000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 0.762    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -0.735   |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3139000, episode_reward=903.13 +/- 14.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 903      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3139000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3140000, episode_reward=965.65 +/- 16.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 966      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 0.724    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 1.67     |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15330    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 752      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3140     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4668     |\n",
      "|    total_timesteps | 3140000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3141000, episode_reward=947.90 +/- 22.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 948      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3141000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3142000, episode_reward=848.19 +/- 19.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 848      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3142000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.734    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 0.239    |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3143000, episode_reward=848.07 +/- 19.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 848      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3143000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3144000, episode_reward=981.62 +/- 20.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 982      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3144000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 0.685    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.873    |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15350    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 760      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3144     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4674     |\n",
      "|    total_timesteps | 3144000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3145000, episode_reward=975.38 +/- 12.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 975      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3145000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3146000, episode_reward=842.81 +/- 13.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 843      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3146000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.397    |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3147000, episode_reward=843.78 +/- 5.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 844      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3147000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3148000, episode_reward=962.25 +/- 31.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 962      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3148000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 0.688    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.621    |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15370    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 767      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3148     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4680     |\n",
      "|    total_timesteps | 3148000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3149000, episode_reward=942.44 +/- 37.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 942      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3149000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3150000, episode_reward=901.36 +/- 11.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 901      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 0.668    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 3.61     |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3151000, episode_reward=925.30 +/- 25.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 925      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3151000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3152000, episode_reward=868.81 +/- 18.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 869      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3152000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.745    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | 2.83     |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 773      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3152     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4685     |\n",
      "|    total_timesteps | 3152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3153000, episode_reward=879.01 +/- 10.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 879      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3153000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3154000, episode_reward=904.97 +/- 12.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 905      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3154000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 0.582    |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3155000, episode_reward=911.05 +/- 11.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 911      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3155000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3156000, episode_reward=932.70 +/- 17.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 933      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3156000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 0.555    |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 778      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3156     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4691     |\n",
      "|    total_timesteps | 3156000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3157000, episode_reward=935.47 +/- 8.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 935      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3157000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3158000, episode_reward=947.15 +/- 16.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 947      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3158000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3159000, episode_reward=927.12 +/- 14.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 927      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3159000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 0.647    |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -2.06    |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3160000, episode_reward=931.40 +/- 9.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 931      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3160000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 785      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3160     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4697     |\n",
      "|    total_timesteps | 3160000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3161000, episode_reward=999.54 +/- 17.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3161000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 0.602    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3162000, episode_reward=1003.07 +/- 11.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3162000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3163000, episode_reward=940.15 +/- 18.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 940      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3163000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 0.7      |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.894   |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3164000, episode_reward=951.32 +/- 11.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 951      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3164000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 795      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3164     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4703     |\n",
      "|    total_timesteps | 3164000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3165000, episode_reward=942.71 +/- 15.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 943      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3165000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 0.66     |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 1.35     |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3166000, episode_reward=947.42 +/- 15.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 947      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3166000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3167000, episode_reward=944.94 +/- 8.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 945      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3167000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 0.67     |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 0.586    |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3168000, episode_reward=941.15 +/- 16.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 941      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3168000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 803      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3168     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4709     |\n",
      "|    total_timesteps | 3168000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3169000, episode_reward=909.21 +/- 8.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 909      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3169000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.3     |\n",
      "|    critic_loss     | 0.569    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3170000, episode_reward=919.86 +/- 11.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 920      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3170000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3171000, episode_reward=954.52 +/- 17.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 955      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3171000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 0.61     |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.383   |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3172000, episode_reward=965.97 +/- 14.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 966      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3172000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 814      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3172     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4715     |\n",
      "|    total_timesteps | 3172000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3173000, episode_reward=983.56 +/- 9.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 984      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3173000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 0.612    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.292    |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3174000, episode_reward=974.01 +/- 13.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 974      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3174000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3175000, episode_reward=941.61 +/- 11.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 942      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3175000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 0.744    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3176000, episode_reward=924.06 +/- 47.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 924      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3176000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 819      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3176     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4721     |\n",
      "|    total_timesteps | 3176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3177000, episode_reward=962.85 +/- 15.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 963      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3177000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 0.742    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.553    |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3178000, episode_reward=963.94 +/- 17.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 964      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3178000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3179000, episode_reward=977.38 +/- 12.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 977      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3179000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.8     |\n",
      "|    critic_loss     | 0.679    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.914    |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3180000, episode_reward=976.55 +/- 10.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 977      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3180000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 826      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3180     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4726     |\n",
      "|    total_timesteps | 3180000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3181000, episode_reward=1002.78 +/- 11.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3181000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.7     |\n",
      "|    critic_loss     | 0.555    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3182000, episode_reward=1002.42 +/- 22.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3182000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3183000, episode_reward=1004.07 +/- 9.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3183000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.7     |\n",
      "|    critic_loss     | 0.662    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.136   |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3184000, episode_reward=990.97 +/- 21.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 991      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3184000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 835      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3184     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4732     |\n",
      "|    total_timesteps | 3184000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3185000, episode_reward=978.68 +/- 12.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 979      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3185000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 0.708    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.633   |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3186000, episode_reward=982.87 +/- 8.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 983      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3186000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3187000, episode_reward=1057.52 +/- 22.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3187000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.7     |\n",
      "|    critic_loss     | 0.725    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.646   |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3188000, episode_reward=1016.45 +/- 20.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3188000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 842      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3188     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4738     |\n",
      "|    total_timesteps | 3188000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3189000, episode_reward=1099.01 +/- 8.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3189000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.8     |\n",
      "|    critic_loss     | 0.785    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 0.913    |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3190000, episode_reward=1099.33 +/- 16.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3190000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3191000, episode_reward=1014.18 +/- 21.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3191000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.742    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 2.28     |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3192000, episode_reward=1014.20 +/- 14.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3192000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 848      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3192     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4743     |\n",
      "|    total_timesteps | 3192000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3193000, episode_reward=1047.21 +/- 8.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3193000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.8     |\n",
      "|    critic_loss     | 0.72     |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -0.402   |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3194000, episode_reward=1046.26 +/- 17.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3194000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3195000, episode_reward=1025.88 +/- 6.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3195000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.725    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -0.252   |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3196000, episode_reward=1027.43 +/- 11.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3196000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 859      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3196     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4749     |\n",
      "|    total_timesteps | 3196000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3197000, episode_reward=1028.69 +/- 10.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3197000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.7     |\n",
      "|    critic_loss     | 0.695    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | 0.701    |\n",
      "|    learning_rate   | 0.0018   |\n",
      "|    n_updates       | 15610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3198000, episode_reward=1050.14 +/- 10.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3198000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3199000, episode_reward=1014.04 +/- 15.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3199000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 0.708    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -0.323   |\n",
      "|    learning_rate   | 0.0018   |\n",
      "|    n_updates       | 15620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=1010.94 +/- 21.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3200000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 873      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3200     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4755     |\n",
      "|    total_timesteps | 3200000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3201000, episode_reward=1015.67 +/- 10.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3201000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3202000, episode_reward=983.73 +/- 18.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 984      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3202000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 0.691    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0018   |\n",
      "|    n_updates       | 15630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3203000, episode_reward=984.71 +/- 23.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 985      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3203000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3204000, episode_reward=991.69 +/- 9.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 992      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3204000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.9     |\n",
      "|    critic_loss     | 0.677    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0018   |\n",
      "|    n_updates       | 15640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 882      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3204     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4761     |\n",
      "|    total_timesteps | 3204000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3205000, episode_reward=986.69 +/- 17.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 987      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3205000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3206000, episode_reward=916.51 +/- 10.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 917      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3206000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.7     |\n",
      "|    critic_loss     | 0.602    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3207000, episode_reward=928.96 +/- 11.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 929      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3207000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3208000, episode_reward=963.47 +/- 11.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 963      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3208000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 0.67     |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.57    |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 887      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3208     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4767     |\n",
      "|    total_timesteps | 3208000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3209000, episode_reward=960.36 +/- 12.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 960      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3209000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210000, episode_reward=983.84 +/- 24.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 984      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.9     |\n",
      "|    critic_loss     | 0.661    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.547    |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3211000, episode_reward=974.16 +/- 14.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 974      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3211000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3212000, episode_reward=998.27 +/- 7.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 998      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3212000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 0.744    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.569    |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15680    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 893      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3212     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 4772     |\n",
      "|    total_timesteps | 3212000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3213000, episode_reward=994.00 +/- 12.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 994      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3213000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3214000, episode_reward=968.62 +/- 18.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 969      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3214000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.8     |\n",
      "|    critic_loss     | 0.662    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.239   |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3215000, episode_reward=957.52 +/- 18.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 958      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3215000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3216000, episode_reward=984.18 +/- 13.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 984      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3216000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 0.569    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.0106   |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15700    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 898      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3216     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4778     |\n",
      "|    total_timesteps | 3216000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3217000, episode_reward=978.67 +/- 12.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 979      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3217000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3218000, episode_reward=1010.09 +/- 20.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3218000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 0.604    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.678    |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3219000, episode_reward=995.36 +/- 21.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 995      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3219000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3220000, episode_reward=979.64 +/- 11.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 980      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.8     |\n",
      "|    critic_loss     | 0.719    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -1.99    |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15720    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 904      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3220     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4784     |\n",
      "|    total_timesteps | 3220000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3221000, episode_reward=984.00 +/- 14.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 984      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3221000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3222000, episode_reward=992.89 +/- 13.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 993      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3222000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.9     |\n",
      "|    critic_loss     | 0.661    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3223000, episode_reward=976.25 +/- 21.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 976      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3223000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3224000, episode_reward=1001.84 +/- 18.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3224000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 0.591    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -0.172   |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15740    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 909      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3224     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4789     |\n",
      "|    total_timesteps | 3224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3225000, episode_reward=989.84 +/- 8.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 990      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3225000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3226000, episode_reward=1018.26 +/- 9.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3226000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 0.693    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 1.47     |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3227000, episode_reward=1016.15 +/- 12.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3227000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3228000, episode_reward=1018.49 +/- 5.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3228000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.669    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15760    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 915      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3228     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4795     |\n",
      "|    total_timesteps | 3228000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3229000, episode_reward=1015.81 +/- 11.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3229000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3230000, episode_reward=1014.32 +/- 7.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 0.611    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.822    |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3231000, episode_reward=1018.59 +/- 12.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3231000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3232000, episode_reward=1039.12 +/- 21.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3232000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.622    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.459    |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15780    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 920      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3232     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4801     |\n",
      "|    total_timesteps | 3232000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3233000, episode_reward=1026.31 +/- 21.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3233000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3234000, episode_reward=1041.51 +/- 11.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3234000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.654    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.0565  |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3235000, episode_reward=1024.20 +/- 16.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3235000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3236000, episode_reward=1024.54 +/- 21.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3236000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 0.718    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.231   |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 924      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3236     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4806     |\n",
      "|    total_timesteps | 3236000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3237000, episode_reward=1027.12 +/- 11.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3237000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3238000, episode_reward=1082.33 +/- 3.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3238000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 0.744    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.0993  |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3239000, episode_reward=1083.45 +/- 14.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3239000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3240000, episode_reward=1068.10 +/- 10.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 0.699    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.304   |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 928      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3240     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4812     |\n",
      "|    total_timesteps | 3240000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3241000, episode_reward=1053.28 +/- 18.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3241000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3242000, episode_reward=1088.90 +/- 15.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3242000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.2     |\n",
      "|    critic_loss     | 0.687    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.44    |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3243000, episode_reward=1082.21 +/- 11.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3243000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3244000, episode_reward=1087.50 +/- 10.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3244000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 935      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3244     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4817     |\n",
      "|    total_timesteps | 3244000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3245000, episode_reward=1052.94 +/- 3.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3245000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.825    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.408   |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3246000, episode_reward=1054.69 +/- 17.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3246000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3247000, episode_reward=1051.24 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3247000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 0.645    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3248000, episode_reward=1051.67 +/- 15.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3248000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 942      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3248     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4823     |\n",
      "|    total_timesteps | 3248000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3249000, episode_reward=1053.72 +/- 11.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3249000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 0.686    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 0.619    |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3250000, episode_reward=1053.86 +/- 12.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3250000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3251000, episode_reward=1048.45 +/- 10.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3251000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 0.605    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3252000, episode_reward=1038.23 +/- 17.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3252000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 944      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3252     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4829     |\n",
      "|    total_timesteps | 3252000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3253000, episode_reward=1040.54 +/- 21.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3253000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.664    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.97    |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3254000, episode_reward=1038.38 +/- 5.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3254000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3255000, episode_reward=1019.69 +/- 4.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3255000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 0.696    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.286   |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3256000, episode_reward=1030.32 +/- 26.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3256000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 950      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3256     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4834     |\n",
      "|    total_timesteps | 3256000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3257000, episode_reward=1046.81 +/- 12.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3257000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.725    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.539    |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3258000, episode_reward=1048.75 +/- 9.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3258000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3259000, episode_reward=1075.34 +/- 15.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3259000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 0.655    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 1.55     |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3260000, episode_reward=1089.08 +/- 14.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3260000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 955      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3260     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4840     |\n",
      "|    total_timesteps | 3260000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3261000, episode_reward=1105.32 +/- 13.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3261000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 0.734    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.783   |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3262000, episode_reward=1123.18 +/- 9.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3262000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3263000, episode_reward=1061.71 +/- 24.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3263000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13       |\n",
      "|    critic_loss     | 0.751    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3264000, episode_reward=1067.55 +/- 12.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3264000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 960      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3264     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4846     |\n",
      "|    total_timesteps | 3264000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3265000, episode_reward=1082.76 +/- 23.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3265000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 0.633    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.353   |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3266000, episode_reward=1074.59 +/- 9.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3266000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3267000, episode_reward=1051.18 +/- 16.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3267000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.1     |\n",
      "|    critic_loss     | 0.719    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.682    |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3268000, episode_reward=1056.08 +/- 19.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3268000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 966      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3268     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4851     |\n",
      "|    total_timesteps | 3268000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3269000, episode_reward=1132.40 +/- 16.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3269000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.716    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270000, episode_reward=1113.73 +/- 19.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3270000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3271000, episode_reward=1065.94 +/- 26.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3271000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.9     |\n",
      "|    critic_loss     | 0.791    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -2.1     |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3272000, episode_reward=1070.80 +/- 18.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3272000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 974      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3272     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4857     |\n",
      "|    total_timesteps | 3272000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3273000, episode_reward=1085.73 +/- 26.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3273000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.1     |\n",
      "|    critic_loss     | 0.643    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3274000, episode_reward=1101.78 +/- 13.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3274000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3275000, episode_reward=1096.01 +/- 18.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3275000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.2     |\n",
      "|    critic_loss     | 0.849    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -0.264   |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3276000, episode_reward=1082.41 +/- 5.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3276000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 981      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3276     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4863     |\n",
      "|    total_timesteps | 3276000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3277000, episode_reward=1085.58 +/- 13.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3277000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 0.888    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3278000, episode_reward=1074.67 +/- 25.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3278000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3279000, episode_reward=1093.44 +/- 13.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3279000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 0.917    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3280000, episode_reward=1085.26 +/- 14.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3280000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 988      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3280     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4868     |\n",
      "|    total_timesteps | 3280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3281000, episode_reward=1112.03 +/- 18.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3281000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.1     |\n",
      "|    critic_loss     | 0.715    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | -1.97    |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3282000, episode_reward=1081.12 +/- 14.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3282000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3283000, episode_reward=1130.54 +/- 21.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3283000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.9     |\n",
      "|    critic_loss     | 0.768    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -0.804   |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3284000, episode_reward=1130.01 +/- 32.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3284000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 993      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3284     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4874     |\n",
      "|    total_timesteps | 3284000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3285000, episode_reward=1110.00 +/- 14.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3285000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 0.796    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3286000, episode_reward=1126.77 +/- 13.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3286000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3287000, episode_reward=1124.11 +/- 14.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3287000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3288000, episode_reward=1146.18 +/- 24.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3288000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.9     |\n",
      "|    critic_loss     | 0.791    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.00171  |\n",
      "|    n_updates       | 16050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 998      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3288     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4879     |\n",
      "|    total_timesteps | 3288000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3289000, episode_reward=1143.39 +/- 16.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3289000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3290000, episode_reward=1126.78 +/- 9.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.8     |\n",
      "|    critic_loss     | 0.805    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 2.07     |\n",
      "|    learning_rate   | 0.00171  |\n",
      "|    n_updates       | 16060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3291000, episode_reward=1119.49 +/- 12.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3291000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3292000, episode_reward=1150.10 +/- 14.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3292000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.8     |\n",
      "|    critic_loss     | 0.744    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.00171  |\n",
      "|    n_updates       | 16070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3292     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4885     |\n",
      "|    total_timesteps | 3292000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3293000, episode_reward=1149.21 +/- 15.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3293000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3294000, episode_reward=1195.55 +/- 14.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3294000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.7     |\n",
      "|    critic_loss     | 0.791    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -2.09    |\n",
      "|    learning_rate   | 0.00171  |\n",
      "|    n_updates       | 16080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3295000, episode_reward=1187.99 +/- 13.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3295000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3296000, episode_reward=1180.08 +/- 7.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3296000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.4     |\n",
      "|    critic_loss     | 0.786    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 0.928    |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16090    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3296     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4891     |\n",
      "|    total_timesteps | 3296000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3297000, episode_reward=1193.84 +/- 21.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3297000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3298000, episode_reward=1177.98 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3298000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.3     |\n",
      "|    critic_loss     | 0.706    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 2.91     |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3299000, episode_reward=1198.80 +/- 24.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3299000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300000, episode_reward=1105.06 +/- 11.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12       |\n",
      "|    critic_loss     | 0.656    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 1.52     |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16110    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3300     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4896     |\n",
      "|    total_timesteps | 3300000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3301000, episode_reward=1123.85 +/- 13.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3301000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3302000, episode_reward=1124.02 +/- 19.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3302000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.1     |\n",
      "|    critic_loss     | 0.522    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 2.8      |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3303000, episode_reward=1120.56 +/- 16.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3303000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3304000, episode_reward=1131.75 +/- 20.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3304000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.1     |\n",
      "|    critic_loss     | 0.454    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.0189   |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16130    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3304     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 4902     |\n",
      "|    total_timesteps | 3304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3305000, episode_reward=1128.41 +/- 23.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3305000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3306000, episode_reward=1161.83 +/- 14.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3306000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12       |\n",
      "|    critic_loss     | 0.501    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.0663  |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3307000, episode_reward=1174.60 +/- 15.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3307000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3308000, episode_reward=1147.13 +/- 10.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3308000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 0.573    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16150    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3308     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4908     |\n",
      "|    total_timesteps | 3308000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3309000, episode_reward=1159.52 +/- 17.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3309000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3310000, episode_reward=1150.49 +/- 13.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.1     |\n",
      "|    critic_loss     | 0.54     |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 1.12     |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3311000, episode_reward=1152.57 +/- 14.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3311000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3312000, episode_reward=1069.40 +/- 16.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3312000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 0.474    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | 0.682    |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16170    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3312     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4913     |\n",
      "|    total_timesteps | 3312000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3313000, episode_reward=1078.01 +/- 15.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3313000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3314000, episode_reward=1135.61 +/- 14.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3314000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.6     |\n",
      "|    critic_loss     | 0.43     |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | 0.706    |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3315000, episode_reward=1140.03 +/- 14.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3315000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3316000, episode_reward=1254.69 +/- 10.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3316000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12       |\n",
      "|    critic_loss     | 0.436    |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | 0.911    |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16190    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3316     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4919     |\n",
      "|    total_timesteps | 3316000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3317000, episode_reward=1273.09 +/- 9.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3317000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3318000, episode_reward=1173.36 +/- 7.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3318000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 0.544    |\n",
      "|    ent_coef        | 0.0158   |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3319000, episode_reward=1177.56 +/- 10.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3319000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3320000, episode_reward=1157.36 +/- 4.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.8     |\n",
      "|    critic_loss     | 0.557    |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | 2.08     |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3320     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4924     |\n",
      "|    total_timesteps | 3320000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3321000, episode_reward=1167.13 +/- 7.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3321000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3322000, episode_reward=1210.70 +/- 6.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3322000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.7     |\n",
      "|    critic_loss     | 0.539    |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | 1.31     |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3323000, episode_reward=1218.16 +/- 12.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3323000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3324000, episode_reward=1138.36 +/- 8.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3324000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.3     |\n",
      "|    critic_loss     | 0.568    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3324     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4930     |\n",
      "|    total_timesteps | 3324000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3325000, episode_reward=1132.01 +/- 11.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3325000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3326000, episode_reward=1184.40 +/- 8.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3326000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.7     |\n",
      "|    critic_loss     | 0.493    |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3327000, episode_reward=1193.78 +/- 9.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3327000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3328000, episode_reward=1197.59 +/- 6.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3328000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3328     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4935     |\n",
      "|    total_timesteps | 3328000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3329000, episode_reward=1264.55 +/- 9.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3329000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.7     |\n",
      "|    critic_loss     | 0.521    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3330000, episode_reward=1265.33 +/- 15.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3330000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3331000, episode_reward=1372.34 +/- 15.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3331000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.7     |\n",
      "|    critic_loss     | 0.602    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 0.295    |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3332000, episode_reward=1373.21 +/- 14.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3332000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3332     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4941     |\n",
      "|    total_timesteps | 3332000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3333000, episode_reward=1282.85 +/- 20.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3333000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.1     |\n",
      "|    critic_loss     | 0.638    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3334000, episode_reward=1271.93 +/- 17.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3334000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3335000, episode_reward=1207.79 +/- 7.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3335000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 0.583    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | 2.25     |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3336000, episode_reward=1214.04 +/- 8.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3336000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3336     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4947     |\n",
      "|    total_timesteps | 3336000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3337000, episode_reward=1126.53 +/- 5.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3337000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.1     |\n",
      "|    critic_loss     | 0.574    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | 1.92     |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3338000, episode_reward=1124.89 +/- 5.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3338000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3339000, episode_reward=1186.58 +/- 4.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3339000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 0.515    |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | 1.86     |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3340000, episode_reward=1184.24 +/- 11.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3340000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3340     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4952     |\n",
      "|    total_timesteps | 3340000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3341000, episode_reward=1160.81 +/- 8.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3341000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.3     |\n",
      "|    critic_loss     | 0.517    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | 0.48     |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3342000, episode_reward=1158.42 +/- 6.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3342000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3343000, episode_reward=1189.39 +/- 15.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3343000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.5     |\n",
      "|    critic_loss     | 0.423    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | -0.381   |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3344000, episode_reward=1187.32 +/- 14.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3344000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3344     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4958     |\n",
      "|    total_timesteps | 3344000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3345000, episode_reward=1203.19 +/- 11.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3345000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 0.493    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | -0.753   |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3346000, episode_reward=1198.83 +/- 4.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3346000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3347000, episode_reward=1146.90 +/- 7.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3347000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 0.531    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | 0.433    |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3348000, episode_reward=1158.85 +/- 4.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3348000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3348     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4963     |\n",
      "|    total_timesteps | 3348000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3349000, episode_reward=1101.85 +/- 6.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3349000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 0.428    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3350000, episode_reward=1101.87 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3350000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3351000, episode_reward=1080.43 +/- 4.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3351000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 0.425    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | 0.352    |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3352000, episode_reward=1077.16 +/- 5.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3352000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3352     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4969     |\n",
      "|    total_timesteps | 3352000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3353000, episode_reward=998.37 +/- 5.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 998      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3353000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 0.404    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3354000, episode_reward=1006.61 +/- 7.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3354000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3355000, episode_reward=1103.29 +/- 12.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3355000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.7     |\n",
      "|    critic_loss     | 0.446    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | 0.371    |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3356000, episode_reward=1090.81 +/- 7.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3356000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3356     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4975     |\n",
      "|    total_timesteps | 3356000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3357000, episode_reward=1027.80 +/- 12.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3357000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.5     |\n",
      "|    critic_loss     | 0.529    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | -0.278   |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3358000, episode_reward=1038.89 +/- 24.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3358000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3359000, episode_reward=1185.77 +/- 8.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3359000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 0.467    |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -1.39    |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=1186.22 +/- 9.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3360000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3360     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4980     |\n",
      "|    total_timesteps | 3360000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3361000, episode_reward=1170.09 +/- 10.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3361000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 0.524    |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -1.8     |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3362000, episode_reward=1172.68 +/- 3.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3362000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3363000, episode_reward=1189.59 +/- 5.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3363000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 0.52     |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | -0.906   |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3364000, episode_reward=1193.26 +/- 8.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3364000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3364     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4986     |\n",
      "|    total_timesteps | 3364000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3365000, episode_reward=1251.93 +/- 13.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3365000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 0.556    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -2.01    |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3366000, episode_reward=1234.16 +/- 10.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3366000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3367000, episode_reward=1323.31 +/- 18.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3367000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.3     |\n",
      "|    critic_loss     | 0.612    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -0.316   |\n",
      "|    learning_rate   | 0.00163  |\n",
      "|    n_updates       | 16440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3368000, episode_reward=1335.09 +/- 14.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3368000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3368     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4991     |\n",
      "|    total_timesteps | 3368000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3369000, episode_reward=1356.73 +/- 23.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3369000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.1     |\n",
      "|    critic_loss     | 0.718    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -0.352   |\n",
      "|    learning_rate   | 0.00163  |\n",
      "|    n_updates       | 16450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3370000, episode_reward=1366.18 +/- 25.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3370000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3371000, episode_reward=1374.14 +/- 7.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3371000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3372000, episode_reward=1270.74 +/- 26.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3372000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 0.723    |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | 0.773    |\n",
      "|    learning_rate   | 0.00163  |\n",
      "|    n_updates       | 16460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3372     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 4997     |\n",
      "|    total_timesteps | 3372000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3373000, episode_reward=1291.32 +/- 15.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3373000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3374000, episode_reward=1278.17 +/- 14.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3374000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 0.696    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.00163  |\n",
      "|    n_updates       | 16470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3375000, episode_reward=1267.51 +/- 19.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3375000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3376000, episode_reward=1197.05 +/- 7.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3376000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.1     |\n",
      "|    critic_loss     | 0.595    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3376     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5003     |\n",
      "|    total_timesteps | 3376000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3377000, episode_reward=1204.33 +/- 27.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3377000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3378000, episode_reward=1179.56 +/- 12.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3378000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 0.517    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | 0.811    |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3379000, episode_reward=1177.78 +/- 12.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3379000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3380000, episode_reward=1180.33 +/- 9.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 0.551    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3380     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5008     |\n",
      "|    total_timesteps | 3380000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3381000, episode_reward=1176.25 +/- 11.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3381000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3382000, episode_reward=1208.27 +/- 4.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3382000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 0.481    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3383000, episode_reward=1208.05 +/- 8.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3383000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3384000, episode_reward=1268.77 +/- 8.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3384000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 0.455    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -1.84    |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16520    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3384     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5014     |\n",
      "|    total_timesteps | 3384000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3385000, episode_reward=1266.96 +/- 6.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3385000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3386000, episode_reward=1243.40 +/- 17.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3386000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.8     |\n",
      "|    critic_loss     | 0.543    |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | -0.732   |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3387000, episode_reward=1254.18 +/- 15.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3387000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3388000, episode_reward=1292.55 +/- 22.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3388000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.1     |\n",
      "|    critic_loss     | 0.594    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 0.0236   |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16540    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3388     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5019     |\n",
      "|    total_timesteps | 3388000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3389000, episode_reward=1300.26 +/- 10.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3389000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3390000, episode_reward=1292.95 +/- 4.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 0.67     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3391000, episode_reward=1293.21 +/- 10.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3391000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3392000, episode_reward=1294.25 +/- 17.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3392000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.1     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | 0.179    |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16560    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3392     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5025     |\n",
      "|    total_timesteps | 3392000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3393000, episode_reward=1310.43 +/- 18.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3393000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3394000, episode_reward=1309.35 +/- 10.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3394000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 0.639    |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | 1.02     |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3395000, episode_reward=1304.69 +/- 8.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3395000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3396000, episode_reward=1377.24 +/- 19.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3396000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.3     |\n",
      "|    critic_loss     | 0.723    |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16580    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3396     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5031     |\n",
      "|    total_timesteps | 3396000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3397000, episode_reward=1377.22 +/- 21.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3397000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3398000, episode_reward=1426.95 +/- 18.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3398000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11       |\n",
      "|    critic_loss     | 0.775    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 1.95     |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3399000, episode_reward=1427.19 +/- 16.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3399000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3400000, episode_reward=1435.13 +/- 34.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10       |\n",
      "|    critic_loss     | 0.699    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16600    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3400     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5036     |\n",
      "|    total_timesteps | 3400000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3401000, episode_reward=1441.78 +/- 50.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3401000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3402000, episode_reward=1483.71 +/- 22.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3402000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 0.853    |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | 0.902    |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3403000, episode_reward=1473.41 +/- 27.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3403000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3404000, episode_reward=1415.04 +/- 8.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3404000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.5      |\n",
      "|    critic_loss     | 0.786    |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | 2        |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16620    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3404     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5042     |\n",
      "|    total_timesteps | 3404000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3405000, episode_reward=1421.59 +/- 10.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3405000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3406000, episode_reward=1396.96 +/- 37.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3406000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.89     |\n",
      "|    critic_loss     | 0.798    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | 1.96     |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3407000, episode_reward=1407.42 +/- 15.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3407000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3408000, episode_reward=1433.94 +/- 41.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3408000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 0.733    |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | 0.92     |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3408     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5047     |\n",
      "|    total_timesteps | 3408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3409000, episode_reward=1416.49 +/- 23.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3409000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3410000, episode_reward=1338.21 +/- 19.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10       |\n",
      "|    critic_loss     | 0.82     |\n",
      "|    ent_coef        | 0.0171   |\n",
      "|    ent_coef_loss   | 2.78     |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3411000, episode_reward=1365.17 +/- 6.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3411000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3412000, episode_reward=1294.37 +/- 19.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3412000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.96     |\n",
      "|    critic_loss     | 0.732    |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | 2.34     |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3412     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5053     |\n",
      "|    total_timesteps | 3412000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3413000, episode_reward=1285.64 +/- 5.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3413000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3414000, episode_reward=1293.76 +/- 13.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3414000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3415000, episode_reward=1287.28 +/- 9.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3415000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 0.765    |\n",
      "|    ent_coef        | 0.0175   |\n",
      "|    ent_coef_loss   | 0.837    |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3416000, episode_reward=1294.13 +/- 23.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3416000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3416     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5058     |\n",
      "|    total_timesteps | 3416000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3417000, episode_reward=1247.86 +/- 8.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3417000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 0.798    |\n",
      "|    ent_coef        | 0.0176   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3418000, episode_reward=1217.57 +/- 21.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3418000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3419000, episode_reward=1389.47 +/- 15.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3419000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 0.775    |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | 1.5      |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3420000, episode_reward=1372.84 +/- 16.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3420000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3420     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5064     |\n",
      "|    total_timesteps | 3420000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3421000, episode_reward=1430.61 +/- 20.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3421000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10       |\n",
      "|    critic_loss     | 0.733    |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | -0.597   |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3422000, episode_reward=1424.48 +/- 15.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3422000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3423000, episode_reward=1439.95 +/- 26.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3423000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.93     |\n",
      "|    critic_loss     | 0.842    |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | 0.366    |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3424000, episode_reward=1456.37 +/- 12.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3424000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3424     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5070     |\n",
      "|    total_timesteps | 3424000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3425000, episode_reward=1524.62 +/- 25.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3425000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.69     |\n",
      "|    critic_loss     | 0.869    |\n",
      "|    ent_coef        | 0.0179   |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16720    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3426000, episode_reward=1449.95 +/- 45.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3426000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3427000, episode_reward=1390.76 +/- 20.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3427000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.74     |\n",
      "|    critic_loss     | 0.944    |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | 2.33     |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3428000, episode_reward=1375.80 +/- 10.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3428000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3428     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5076     |\n",
      "|    total_timesteps | 3428000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3429000, episode_reward=1458.89 +/- 19.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3429000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10       |\n",
      "|    critic_loss     | 0.83     |\n",
      "|    ent_coef        | 0.0182   |\n",
      "|    ent_coef_loss   | 0.824    |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3430000, episode_reward=1476.10 +/- 37.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3430000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3431000, episode_reward=1324.82 +/- 32.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3431000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 0.911    |\n",
      "|    ent_coef        | 0.0183   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3432000, episode_reward=1309.28 +/- 27.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3432000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3432     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5082     |\n",
      "|    total_timesteps | 3432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3433000, episode_reward=1251.91 +/- 17.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3433000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 0.737    |\n",
      "|    ent_coef        | 0.0184   |\n",
      "|    ent_coef_loss   | -0.0835  |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3434000, episode_reward=1248.53 +/- 10.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3434000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3435000, episode_reward=1224.50 +/- 18.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3435000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.3     |\n",
      "|    critic_loss     | 0.7      |\n",
      "|    ent_coef        | 0.0184   |\n",
      "|    ent_coef_loss   | -0.0941  |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3436000, episode_reward=1216.45 +/- 10.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3436000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3436     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5088     |\n",
      "|    total_timesteps | 3436000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3437000, episode_reward=1261.38 +/- 40.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3437000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.3     |\n",
      "|    critic_loss     | 0.78     |\n",
      "|    ent_coef        | 0.0185   |\n",
      "|    ent_coef_loss   | 2.16     |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3438000, episode_reward=1262.81 +/- 19.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3438000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3439000, episode_reward=1355.04 +/- 14.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3439000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 0.885    |\n",
      "|    ent_coef        | 0.0186   |\n",
      "|    ent_coef_loss   | 1.52     |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3440000, episode_reward=1355.22 +/- 20.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3440000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3440     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5093     |\n",
      "|    total_timesteps | 3440000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3441000, episode_reward=1243.37 +/- 26.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3441000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.37     |\n",
      "|    critic_loss     | 0.928    |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | 1.67     |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3442000, episode_reward=1249.91 +/- 24.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3442000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3443000, episode_reward=1146.81 +/- 9.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3443000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 0.935    |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | 1.51     |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3444000, episode_reward=1164.99 +/- 22.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3444000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3444     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5099     |\n",
      "|    total_timesteps | 3444000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3445000, episode_reward=1290.70 +/- 44.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3445000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.6     |\n",
      "|    critic_loss     | 0.758    |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | 0.224    |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3446000, episode_reward=1321.21 +/- 20.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3446000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3447000, episode_reward=1295.19 +/- 15.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3447000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.87     |\n",
      "|    critic_loss     | 0.855    |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3448000, episode_reward=1273.84 +/- 36.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3448000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3448     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5104     |\n",
      "|    total_timesteps | 3448000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3449000, episode_reward=1148.91 +/- 21.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3449000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 0.866    |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | -0.506   |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3450000, episode_reward=1143.05 +/- 21.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3450000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3451000, episode_reward=1195.63 +/- 26.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3451000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 0.709    |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | -0.334   |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3452000, episode_reward=1202.77 +/- 24.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3452000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3452     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5110     |\n",
      "|    total_timesteps | 3452000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3453000, episode_reward=1235.89 +/- 19.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3453000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.7     |\n",
      "|    critic_loss     | 0.781    |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | -0.233   |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3454000, episode_reward=1226.02 +/- 29.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3454000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3455000, episode_reward=1455.68 +/- 14.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3455000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.3     |\n",
      "|    critic_loss     | 0.737    |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | -0.503   |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3456000, episode_reward=1434.33 +/- 15.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3456000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3456     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5116     |\n",
      "|    total_timesteps | 3456000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3457000, episode_reward=1441.21 +/- 17.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3457000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3458000, episode_reward=1340.68 +/- 22.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3458000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 2.41     |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | 1.55     |\n",
      "|    learning_rate   | 0.00154  |\n",
      "|    n_updates       | 16880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3459000, episode_reward=1343.25 +/- 20.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3459000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3460000, episode_reward=1388.12 +/- 13.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.3     |\n",
      "|    critic_loss     | 0.809    |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | -0.093   |\n",
      "|    learning_rate   | 0.00154  |\n",
      "|    n_updates       | 16890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3460     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5121     |\n",
      "|    total_timesteps | 3460000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3461000, episode_reward=1376.50 +/- 6.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3461000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3462000, episode_reward=1425.74 +/- 22.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3462000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.72     |\n",
      "|    critic_loss     | 0.752    |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | 1.47     |\n",
      "|    learning_rate   | 0.00154  |\n",
      "|    n_updates       | 16900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3463000, episode_reward=1424.50 +/- 10.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3463000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3464000, episode_reward=1472.54 +/- 25.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3464000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.3      |\n",
      "|    critic_loss     | 0.875    |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 2.54     |\n",
      "|    learning_rate   | 0.00154  |\n",
      "|    n_updates       | 16910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3464     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5127     |\n",
      "|    total_timesteps | 3464000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3465000, episode_reward=1449.00 +/- 27.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3465000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3466000, episode_reward=1454.43 +/- 17.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3466000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.46     |\n",
      "|    critic_loss     | 0.915    |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | 0.922    |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3467000, episode_reward=1438.19 +/- 13.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3467000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3468000, episode_reward=1445.80 +/- 12.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3468000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.22     |\n",
      "|    critic_loss     | 0.921    |\n",
      "|    ent_coef        | 0.0194   |\n",
      "|    ent_coef_loss   | 0.243    |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16930    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3468     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5132     |\n",
      "|    total_timesteps | 3468000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3469000, episode_reward=1432.29 +/- 17.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3469000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3470000, episode_reward=1504.55 +/- 29.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.71     |\n",
      "|    critic_loss     | 0.942    |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -0.356   |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3471000, episode_reward=1479.81 +/- 40.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3471000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3472000, episode_reward=1458.60 +/- 40.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3472000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.7      |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | 0.0223   |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16950    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3472     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5138     |\n",
      "|    total_timesteps | 3472000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3473000, episode_reward=1438.68 +/- 15.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3473000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3474000, episode_reward=1345.17 +/- 10.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3474000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.28     |\n",
      "|    critic_loss     | 0.944    |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | 0.207    |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3475000, episode_reward=1356.80 +/- 8.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3475000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3476000, episode_reward=1218.92 +/- 12.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3476000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 0.945    |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 16970    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3476     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5143     |\n",
      "|    total_timesteps | 3476000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3477000, episode_reward=1200.64 +/- 16.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3477000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3478000, episode_reward=1346.64 +/- 20.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3478000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.7     |\n",
      "|    critic_loss     | 0.678    |\n",
      "|    ent_coef        | 0.0194   |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 16980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3479000, episode_reward=1063.60 +/- 589.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3479000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480000, episode_reward=1438.17 +/- 13.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.78     |\n",
      "|    critic_loss     | 0.954    |\n",
      "|    ent_coef        | 0.0194   |\n",
      "|    ent_coef_loss   | 0.567    |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 16990    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3480     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5149     |\n",
      "|    total_timesteps | 3480000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3481000, episode_reward=1424.08 +/- 23.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3481000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3482000, episode_reward=1483.93 +/- 19.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3482000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.62     |\n",
      "|    critic_loss     | 0.941    |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 17000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3483000, episode_reward=1481.26 +/- 27.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3483000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3484000, episode_reward=1512.10 +/- 12.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3484000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.97     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -0.337   |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 17010    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3484     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5154     |\n",
      "|    total_timesteps | 3484000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3485000, episode_reward=1491.11 +/- 32.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3485000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3486000, episode_reward=1536.94 +/- 28.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3486000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.08     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -0.206   |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17020    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3487000, episode_reward=1540.45 +/- 24.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3487000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3488000, episode_reward=1552.77 +/- 16.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3488000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.09     |\n",
      "|    critic_loss     | 0.99     |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 0.489    |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17030    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3488     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5160     |\n",
      "|    total_timesteps | 3488000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3489000, episode_reward=1561.55 +/- 35.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3489000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3490000, episode_reward=1488.20 +/- 11.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.68     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | 0.796    |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3491000, episode_reward=1498.40 +/- 9.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3491000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3492000, episode_reward=1418.41 +/- 21.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3492000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.71     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | 0.763    |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3492     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5166     |\n",
      "|    total_timesteps | 3492000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3493000, episode_reward=1399.06 +/- 28.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3493000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3494000, episode_reward=1537.73 +/- 26.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3494000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.6      |\n",
      "|    critic_loss     | 0.979    |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | 0.931    |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3495000, episode_reward=1533.72 +/- 19.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3495000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3496000, episode_reward=1385.49 +/- 16.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3496000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.94     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0194   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3496     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5172     |\n",
      "|    total_timesteps | 3496000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3497000, episode_reward=1103.44 +/- 597.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3497000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3498000, episode_reward=1462.01 +/- 61.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3498000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.21     |\n",
      "|    critic_loss     | 0.929    |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | 0.306    |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3499000, episode_reward=1447.73 +/- 52.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3499000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500000, episode_reward=1467.75 +/- 16.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3500     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5177     |\n",
      "|    total_timesteps | 3500000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3501000, episode_reward=1521.16 +/- 34.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3501000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.89     |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | -0.238   |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3502000, episode_reward=1520.41 +/- 19.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3502000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3503000, episode_reward=1545.90 +/- 29.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3503000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.35     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | 2.66     |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3504000, episode_reward=1565.79 +/- 22.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3504000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3504     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5183     |\n",
      "|    total_timesteps | 3504000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3505000, episode_reward=1471.66 +/- 32.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3505000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.79     |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | 2.21     |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3506000, episode_reward=1458.01 +/- 11.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3506000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3507000, episode_reward=1520.51 +/- 14.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3507000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.4      |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 0.233    |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3508000, episode_reward=1505.44 +/- 19.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3508000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3508     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5188     |\n",
      "|    total_timesteps | 3508000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3509000, episode_reward=1500.97 +/- 10.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3509000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.62     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | -0.418   |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3510000, episode_reward=1496.93 +/- 10.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3510000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3511000, episode_reward=1540.91 +/- 56.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3511000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.75     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | -0.341   |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3512000, episode_reward=1573.97 +/- 15.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3512000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3512     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5194     |\n",
      "|    total_timesteps | 3512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3513000, episode_reward=1545.69 +/- 12.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3513000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.88     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 2.09     |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3514000, episode_reward=1569.29 +/- 18.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3514000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3515000, episode_reward=1556.00 +/- 21.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3515000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.5      |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | 1.88     |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3516000, episode_reward=1571.98 +/- 29.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3516000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3516     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5199     |\n",
      "|    total_timesteps | 3516000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3517000, episode_reward=1569.80 +/- 65.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3517000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.15     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 0.817    |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3518000, episode_reward=1496.62 +/- 92.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3518000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3519000, episode_reward=1545.30 +/- 29.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3519000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.79     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 2.49     |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=1516.80 +/- 52.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3520000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3520     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5205     |\n",
      "|    total_timesteps | 3520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3521000, episode_reward=1586.44 +/- 34.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3521000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.85     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | 2.06     |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17190    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3522000, episode_reward=1600.58 +/- 27.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3522000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3523000, episode_reward=1562.25 +/- 52.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3523000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.15     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3524000, episode_reward=1555.07 +/- 17.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3524000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3524     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5211     |\n",
      "|    total_timesteps | 3524000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3525000, episode_reward=1591.72 +/- 62.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3525000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.91     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | 0.426    |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3526000, episode_reward=1557.48 +/- 46.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3526000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3527000, episode_reward=1639.93 +/- 49.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3527000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.69     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | 0.489    |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17220    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3528000, episode_reward=1647.73 +/- 53.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3528000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3528     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5216     |\n",
      "|    total_timesteps | 3528000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3529000, episode_reward=1566.30 +/- 45.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3529000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.62     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | 0.68     |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3530000, episode_reward=1559.81 +/- 41.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3530000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3531000, episode_reward=1451.34 +/- 143.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3531000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.67     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | 2.5      |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3532000, episode_reward=1508.01 +/- 38.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3532000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3532     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5222     |\n",
      "|    total_timesteps | 3532000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3533000, episode_reward=1145.94 +/- 584.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3533000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.7     |\n",
      "|    critic_loss     | 3.72     |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3534000, episode_reward=1462.84 +/- 35.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3534000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3535000, episode_reward=1384.18 +/- 337.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3535000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3536000, episode_reward=1481.17 +/- 53.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3536000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3536     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5228     |\n",
      "|    total_timesteps | 3536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3537000, episode_reward=1460.94 +/- 409.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3537000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.68     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | -1.58    |\n",
      "|    learning_rate   | 0.00146  |\n",
      "|    n_updates       | 17270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3538000, episode_reward=1643.08 +/- 25.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3538000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3539000, episode_reward=1602.45 +/- 45.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3539000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.76     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 2.66     |\n",
      "|    learning_rate   | 0.00146  |\n",
      "|    n_updates       | 17280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3540000, episode_reward=1602.59 +/- 86.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3540000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3540     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5233     |\n",
      "|    total_timesteps | 3540000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3541000, episode_reward=1564.72 +/- 35.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3541000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.14     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | 2.75     |\n",
      "|    learning_rate   | 0.00146  |\n",
      "|    n_updates       | 17290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3542000, episode_reward=1532.50 +/- 43.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3542000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3543000, episode_reward=1586.85 +/- 25.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3543000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3544000, episode_reward=1559.57 +/- 58.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3544000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.01     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.00146  |\n",
      "|    n_updates       | 17300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3544     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5239     |\n",
      "|    total_timesteps | 3544000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3545000, episode_reward=1564.51 +/- 37.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3545000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3546000, episode_reward=1662.44 +/- 42.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3546000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.34     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0226   |\n",
      "|    ent_coef_loss   | 0.764    |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17310    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3547000, episode_reward=1639.94 +/- 10.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3547000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3548000, episode_reward=1637.33 +/- 43.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3548000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.36     |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3548     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5244     |\n",
      "|    total_timesteps | 3548000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3549000, episode_reward=1642.48 +/- 48.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3549000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3550000, episode_reward=1385.88 +/- 391.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.68     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3551000, episode_reward=1548.44 +/- 33.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3551000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3552000, episode_reward=1564.19 +/- 28.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3552000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.52     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17340    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3552     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5250     |\n",
      "|    total_timesteps | 3552000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3553000, episode_reward=1548.20 +/- 39.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3553000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3554000, episode_reward=1606.13 +/- 19.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3554000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.46     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0234   |\n",
      "|    ent_coef_loss   | 2.96     |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3555000, episode_reward=1597.89 +/- 39.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3555000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3556000, episode_reward=1585.22 +/- 25.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3556000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.57     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 2.47     |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17360    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3556     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5255     |\n",
      "|    total_timesteps | 3556000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3557000, episode_reward=1562.26 +/- 33.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3557000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3558000, episode_reward=1435.83 +/- 13.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3558000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.08     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | 1.02     |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3559000, episode_reward=1453.09 +/- 14.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3559000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3560000, episode_reward=1441.00 +/- 15.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.7      |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 1.65     |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17380    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3560     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 5261     |\n",
      "|    total_timesteps | 3560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3561000, episode_reward=1451.03 +/- 20.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3561000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3562000, episode_reward=1444.45 +/- 16.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3562000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.35     |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | 1.13     |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3563000, episode_reward=1434.49 +/- 9.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3563000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3564000, episode_reward=1527.05 +/- 18.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3564000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.17     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | 0.814    |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17400    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3564     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5276     |\n",
      "|    total_timesteps | 3564000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3565000, episode_reward=1522.17 +/- 11.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3565000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3566000, episode_reward=1491.82 +/- 18.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3566000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.86     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -0.149   |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3567000, episode_reward=1463.00 +/- 12.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3567000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3568000, episode_reward=1589.01 +/- 29.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3568000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.19     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 0.984    |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17420    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3568     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5282     |\n",
      "|    total_timesteps | 3568000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3569000, episode_reward=1554.02 +/- 39.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3569000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570000, episode_reward=1534.05 +/- 29.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.81     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3571000, episode_reward=1523.61 +/- 37.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3571000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3572000, episode_reward=1584.41 +/- 18.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3572000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.28     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -0.951   |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17440    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3572     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5288     |\n",
      "|    total_timesteps | 3572000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3573000, episode_reward=1588.36 +/- 30.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3573000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3574000, episode_reward=1618.86 +/- 20.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3574000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.86     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -0.122   |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3575000, episode_reward=1606.11 +/- 18.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3575000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3576000, episode_reward=1676.50 +/- 18.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3576000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.69     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -0.27    |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17460    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3576     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5294     |\n",
      "|    total_timesteps | 3576000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3577000, episode_reward=1674.10 +/- 30.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3577000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3578000, episode_reward=1686.99 +/- 32.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3578000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.09     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | 0.629    |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17470    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3579000, episode_reward=1702.42 +/- 27.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3579000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3580000, episode_reward=1608.92 +/- 43.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.95     |\n",
      "|    critic_loss     | 2.13     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3580     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5300     |\n",
      "|    total_timesteps | 3580000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3581000, episode_reward=1589.60 +/- 55.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3581000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3582000, episode_reward=1637.56 +/- 35.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3582000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8        |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -0.0618  |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3583000, episode_reward=1617.72 +/- 45.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3583000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3584000, episode_reward=1665.42 +/- 36.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3584000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3584     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5306     |\n",
      "|    total_timesteps | 3584000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3585000, episode_reward=1677.29 +/- 29.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3585000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.53     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 1.85     |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3586000, episode_reward=1688.69 +/- 8.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3586000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3587000, episode_reward=1630.53 +/- 47.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3587000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.54     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | 0.154    |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3588000, episode_reward=1633.80 +/- 60.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3588000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3588     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5312     |\n",
      "|    total_timesteps | 3588000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3589000, episode_reward=1597.51 +/- 43.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3589000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.46     |\n",
      "|    critic_loss     | 2.17     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -0.245   |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3590000, episode_reward=1418.67 +/- 359.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3590000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3591000, episode_reward=1626.77 +/- 23.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3591000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.74     |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3592000, episode_reward=1612.59 +/- 8.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3592000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3592     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5317     |\n",
      "|    total_timesteps | 3592000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3593000, episode_reward=1634.95 +/- 33.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3593000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.4      |\n",
      "|    critic_loss     | 2.01     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -0.673   |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3594000, episode_reward=1627.03 +/- 60.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3594000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3595000, episode_reward=1628.44 +/- 29.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3595000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.54     |\n",
      "|    critic_loss     | 2.17     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 0.388    |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3596000, episode_reward=1592.95 +/- 66.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3596000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3596     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5323     |\n",
      "|    total_timesteps | 3596000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3597000, episode_reward=1607.70 +/- 23.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3597000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.45     |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3598000, episode_reward=1562.08 +/- 41.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3598000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3599000, episode_reward=1597.62 +/- 70.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3599000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.52     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | 2.97     |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3600000, episode_reward=1610.79 +/- 66.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3600000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3600     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5329     |\n",
      "|    total_timesteps | 3600000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3601000, episode_reward=1270.45 +/- 20.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3601000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.34     |\n",
      "|    critic_loss     | 2.25     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 0.784    |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3602000, episode_reward=1294.55 +/- 35.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3602000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3603000, episode_reward=1546.68 +/- 44.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3603000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.6     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -0.218   |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3604000, episode_reward=1566.51 +/- 56.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3604000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3604     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5335     |\n",
      "|    total_timesteps | 3604000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3605000, episode_reward=1609.20 +/- 17.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3605000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.23     |\n",
      "|    critic_loss     | 2.33     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | 0.219    |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3606000, episode_reward=1589.51 +/- 21.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3606000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3607000, episode_reward=1453.41 +/- 34.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3607000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.85     |\n",
      "|    critic_loss     | 2.3      |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3608000, episode_reward=1453.38 +/- 16.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3608000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3608     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5341     |\n",
      "|    total_timesteps | 3608000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3609000, episode_reward=1571.13 +/- 20.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3609000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.31     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | 2.54     |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3610000, episode_reward=1596.70 +/- 14.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3610000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3611000, episode_reward=1580.09 +/- 34.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3611000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.69     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | 0.11     |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3612000, episode_reward=1583.68 +/- 8.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3612000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3612     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5347     |\n",
      "|    total_timesteps | 3612000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3613000, episode_reward=1543.86 +/- 17.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3613000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.09     |\n",
      "|    critic_loss     | 1.86     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3614000, episode_reward=1544.56 +/- 34.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3614000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3615000, episode_reward=1568.97 +/- 31.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3615000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.85     |\n",
      "|    critic_loss     | 2.19     |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3616000, episode_reward=1553.35 +/- 25.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3616000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3616     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5353     |\n",
      "|    total_timesteps | 3616000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3617000, episode_reward=1524.82 +/- 22.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3617000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.58     |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3618000, episode_reward=1495.77 +/- 18.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3618000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3619000, episode_reward=1551.82 +/- 51.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3619000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.46     |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3620000, episode_reward=1553.82 +/- 49.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3620000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3620     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5358     |\n",
      "|    total_timesteps | 3620000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3621000, episode_reward=1499.43 +/- 17.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3621000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.11     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 2.41     |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3622000, episode_reward=1507.74 +/- 14.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3622000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3623000, episode_reward=1526.70 +/- 14.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3623000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.11     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | 1.65     |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3624000, episode_reward=1547.33 +/- 9.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3624000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3624     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5365     |\n",
      "|    total_timesteps | 3624000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3625000, episode_reward=1457.26 +/- 55.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3625000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.36     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3626000, episode_reward=1448.32 +/- 27.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3626000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3627000, episode_reward=1445.23 +/- 39.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3627000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3628000, episode_reward=1500.67 +/- 54.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3628000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.99     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | 0.287    |\n",
      "|    learning_rate   | 0.00137  |\n",
      "|    n_updates       | 17710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3628     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5371     |\n",
      "|    total_timesteps | 3628000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3629000, episode_reward=1480.18 +/- 38.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3629000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3630000, episode_reward=1467.54 +/- 26.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.75     |\n",
      "|    critic_loss     | 2.1      |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 0.694    |\n",
      "|    learning_rate   | 0.00137  |\n",
      "|    n_updates       | 17720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3631000, episode_reward=1466.52 +/- 35.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3631000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3632000, episode_reward=1461.29 +/- 26.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3632000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.78     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | 0.196    |\n",
      "|    learning_rate   | 0.00137  |\n",
      "|    n_updates       | 17730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3632     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5377     |\n",
      "|    total_timesteps | 3632000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3633000, episode_reward=1450.79 +/- 30.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3633000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3634000, episode_reward=1488.96 +/- 28.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3634000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.62     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | 1.74     |\n",
      "|    learning_rate   | 0.00137  |\n",
      "|    n_updates       | 17740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3635000, episode_reward=1514.55 +/- 20.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3635000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3636000, episode_reward=1573.40 +/- 14.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3636000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.3      |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17750    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3636     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5383     |\n",
      "|    total_timesteps | 3636000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3637000, episode_reward=1578.33 +/- 44.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3637000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3638000, episode_reward=1542.48 +/- 61.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3638000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.62     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | 1.71     |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3639000, episode_reward=1623.13 +/- 20.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3639000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3640000, episode_reward=1556.41 +/- 42.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.32     |\n",
      "|    critic_loss     | 2.19     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -0.321   |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17770    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3640     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5389     |\n",
      "|    total_timesteps | 3640000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3641000, episode_reward=1541.06 +/- 31.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3641000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3642000, episode_reward=1502.97 +/- 25.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3642000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.73     |\n",
      "|    critic_loss     | 2.17     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -0.268   |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3643000, episode_reward=1484.38 +/- 14.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3643000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3644000, episode_reward=1656.77 +/- 23.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3644000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.9      |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | 0.319    |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17790    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3644     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5395     |\n",
      "|    total_timesteps | 3644000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3645000, episode_reward=1576.44 +/- 39.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3645000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3646000, episode_reward=1555.85 +/- 45.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3646000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.77     |\n",
      "|    critic_loss     | 2.43     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3647000, episode_reward=1553.01 +/- 28.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3647000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3648000, episode_reward=1561.52 +/- 36.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3648000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.89     |\n",
      "|    critic_loss     | 2.31     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | -2.08    |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17810    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3648     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5401     |\n",
      "|    total_timesteps | 3648000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3649000, episode_reward=1567.24 +/- 51.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3649000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3650000, episode_reward=1546.51 +/- 30.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.02     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 1.97     |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3651000, episode_reward=1543.45 +/- 41.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3651000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3652000, episode_reward=1465.52 +/- 34.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3652000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.84     |\n",
      "|    critic_loss     | 2.57     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | -0.762   |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17830    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3652     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5407     |\n",
      "|    total_timesteps | 3652000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3653000, episode_reward=1488.29 +/- 6.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3653000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3654000, episode_reward=1548.09 +/- 31.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3654000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.33     |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -0.565   |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3655000, episode_reward=1535.21 +/- 45.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3655000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3656000, episode_reward=1556.70 +/- 25.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3656000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10       |\n",
      "|    critic_loss     | 2.52     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 0.576    |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17850    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3656     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5413     |\n",
      "|    total_timesteps | 3656000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3657000, episode_reward=1565.42 +/- 44.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3657000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3658000, episode_reward=1608.40 +/- 39.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3658000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.7      |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -0.623   |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3659000, episode_reward=1564.43 +/- 50.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3659000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660000, episode_reward=1570.51 +/- 38.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.8      |\n",
      "|    critic_loss     | 2.45     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 0.789    |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17870    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3660     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5419     |\n",
      "|    total_timesteps | 3660000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3661000, episode_reward=1604.15 +/- 26.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3661000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3662000, episode_reward=1575.31 +/- 27.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3662000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.89     |\n",
      "|    critic_loss     | 2.27     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 0.385    |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3663000, episode_reward=1582.31 +/- 54.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3663000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3664000, episode_reward=1600.37 +/- 40.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3664000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.83     |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | -0.65    |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3664     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5425     |\n",
      "|    total_timesteps | 3664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3665000, episode_reward=1636.06 +/- 49.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3665000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3666000, episode_reward=1617.99 +/- 44.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3666000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.52     |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3667000, episode_reward=1591.35 +/- 32.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3667000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3668000, episode_reward=1598.52 +/- 9.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3668000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.51     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -0.768   |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3668     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5432     |\n",
      "|    total_timesteps | 3668000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3669000, episode_reward=1635.52 +/- 29.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3669000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3670000, episode_reward=1597.01 +/- 50.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3670000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3671000, episode_reward=1580.38 +/- 15.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3671000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.76     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | -2.12    |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3672000, episode_reward=1598.30 +/- 18.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3672000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3672     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5438     |\n",
      "|    total_timesteps | 3672000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3673000, episode_reward=1616.81 +/- 39.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3673000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.54     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3674000, episode_reward=1644.71 +/- 52.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3674000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3675000, episode_reward=1616.98 +/- 15.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3675000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.31     |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | 0.179    |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3676000, episode_reward=1618.36 +/- 31.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3676000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3676     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5444     |\n",
      "|    total_timesteps | 3676000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3677000, episode_reward=1553.28 +/- 68.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3677000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.77     |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | 0.122    |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3678000, episode_reward=1521.62 +/- 49.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3678000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3679000, episode_reward=1659.45 +/- 25.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3679000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.99     |\n",
      "|    critic_loss     | 2.5      |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | 2        |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=1647.01 +/- 23.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3680000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3680     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5450     |\n",
      "|    total_timesteps | 3680000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3681000, episode_reward=1371.41 +/- 487.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3681000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.07     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | 0.0299   |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3682000, episode_reward=1617.79 +/- 38.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3682000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3683000, episode_reward=1774.83 +/- 46.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3683000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9        |\n",
      "|    critic_loss     | 2.76     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | 0.555    |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17980    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3684000, episode_reward=1706.05 +/- 31.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3684000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3684     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5455     |\n",
      "|    total_timesteps | 3684000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3685000, episode_reward=1611.02 +/- 41.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3685000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.21     |\n",
      "|    critic_loss     | 2.49     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | -0.495   |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3686000, episode_reward=1626.02 +/- 35.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3686000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3687000, episode_reward=1624.20 +/- 47.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3687000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.55     |\n",
      "|    critic_loss     | 2.88     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | 2.63     |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3688000, episode_reward=1663.85 +/- 33.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3688000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3688     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5462     |\n",
      "|    total_timesteps | 3688000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3689000, episode_reward=1563.97 +/- 16.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3689000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.49     |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | 0.727    |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690000, episode_reward=1555.24 +/- 16.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3690000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3691000, episode_reward=1648.20 +/- 34.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3691000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.03     |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -0.0957  |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3692000, episode_reward=1615.14 +/- 55.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3692000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3692     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5468     |\n",
      "|    total_timesteps | 3692000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3693000, episode_reward=1634.43 +/- 53.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3693000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.48     |\n",
      "|    critic_loss     | 2.27     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3694000, episode_reward=1643.19 +/- 27.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3694000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3695000, episode_reward=1683.69 +/- 52.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3695000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9        |\n",
      "|    critic_loss     | 2.5      |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 0.116    |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3696000, episode_reward=1689.91 +/- 51.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3696000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3696     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5474     |\n",
      "|    total_timesteps | 3696000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3697000, episode_reward=1674.39 +/- 36.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3697000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.86     |\n",
      "|    critic_loss     | 2.81     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3698000, episode_reward=1685.28 +/- 28.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3698000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3699000, episode_reward=1611.21 +/- 30.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3699000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.46     |\n",
      "|    critic_loss     | 2.41     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | 0.37     |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3700000, episode_reward=1595.35 +/- 27.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3700000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3700     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5480     |\n",
      "|    total_timesteps | 3700000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3701000, episode_reward=1487.37 +/- 64.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3701000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.26     |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3702000, episode_reward=1530.29 +/- 19.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3702000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3703000, episode_reward=1568.25 +/- 44.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3703000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.47     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | 0.0982   |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3704000, episode_reward=1597.86 +/- 22.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3704000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3704     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 5487     |\n",
      "|    total_timesteps | 3704000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3705000, episode_reward=1567.32 +/- 38.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3705000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.59     |\n",
      "|    critic_loss     | 2.58     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | 0.683    |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3706000, episode_reward=1566.90 +/- 38.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3706000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3707000, episode_reward=1733.00 +/- 36.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3707000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 2.72     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | 0.479    |\n",
      "|    learning_rate   | 0.00129  |\n",
      "|    n_updates       | 18100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3708000, episode_reward=1590.47 +/- 59.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3708000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3708     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5493     |\n",
      "|    total_timesteps | 3708000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3709000, episode_reward=1624.48 +/- 56.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3709000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.05     |\n",
      "|    critic_loss     | 2.71     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | -0.0564  |\n",
      "|    learning_rate   | 0.00129  |\n",
      "|    n_updates       | 18110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3710000, episode_reward=1571.40 +/- 37.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3710000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3711000, episode_reward=1616.95 +/- 28.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3711000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.63     |\n",
      "|    critic_loss     | 3.06     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | 0.755    |\n",
      "|    learning_rate   | 0.00129  |\n",
      "|    n_updates       | 18120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3712000, episode_reward=1560.06 +/- 45.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3712000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3712     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5499     |\n",
      "|    total_timesteps | 3712000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3713000, episode_reward=1567.52 +/- 45.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3713000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3714000, episode_reward=1560.06 +/- 59.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3714000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.25     |\n",
      "|    critic_loss     | 2.61     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -0.912   |\n",
      "|    learning_rate   | 0.00129  |\n",
      "|    n_updates       | 18130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3715000, episode_reward=1540.90 +/- 56.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3715000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3716000, episode_reward=1656.97 +/- 84.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3716000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.18     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -0.592   |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3716     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5505     |\n",
      "|    total_timesteps | 3716000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3717000, episode_reward=1631.83 +/- 20.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3717000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3718000, episode_reward=1569.03 +/- 32.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3718000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.02     |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | -0.467   |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3719000, episode_reward=1586.49 +/- 50.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3719000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3720000, episode_reward=1623.61 +/- 56.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.78     |\n",
      "|    critic_loss     | 2.81     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -0.742   |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3720     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5512     |\n",
      "|    total_timesteps | 3720000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3721000, episode_reward=1659.34 +/- 38.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3721000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3722000, episode_reward=1517.87 +/- 60.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3722000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.45     |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.16    |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3723000, episode_reward=1557.58 +/- 71.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3723000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3724000, episode_reward=1535.14 +/- 41.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3724000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 3.96     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | 0.361    |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18180    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3724     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5518     |\n",
      "|    total_timesteps | 3724000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3725000, episode_reward=1590.97 +/- 65.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3725000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3726000, episode_reward=1462.48 +/- 82.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3726000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 3.41     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | 0.13     |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3727000, episode_reward=1506.18 +/- 24.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3727000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3728000, episode_reward=1539.93 +/- 40.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3728000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.29     |\n",
      "|    critic_loss     | 2.34     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.0451  |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18200    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3728     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5525     |\n",
      "|    total_timesteps | 3728000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3729000, episode_reward=1533.79 +/- 30.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3729000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3730000, episode_reward=1530.08 +/- 66.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.48     |\n",
      "|    critic_loss     | 2.49     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | 0.685    |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3731000, episode_reward=1581.42 +/- 90.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3731000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3732000, episode_reward=1628.93 +/- 113.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3732000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.75     |\n",
      "|    critic_loss     | 2.72     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | 0.259    |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18220    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3732     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5531     |\n",
      "|    total_timesteps | 3732000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3733000, episode_reward=1519.53 +/- 49.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3733000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3734000, episode_reward=1703.81 +/- 96.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3734000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.1     |\n",
      "|    critic_loss     | 4.02     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | 0.584    |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3735000, episode_reward=1795.31 +/- 55.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3735000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3736000, episode_reward=1666.82 +/- 47.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3736000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.66     |\n",
      "|    critic_loss     | 3.29     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18240    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3736     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5537     |\n",
      "|    total_timesteps | 3736000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3737000, episode_reward=1622.93 +/- 60.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3737000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3738000, episode_reward=1663.50 +/- 51.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3738000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.4      |\n",
      "|    critic_loss     | 2.81     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | 1.44     |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3739000, episode_reward=1703.74 +/- 35.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3739000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3740000, episode_reward=1766.12 +/- 37.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.76     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -0.531   |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18260    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3740     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5543     |\n",
      "|    total_timesteps | 3740000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3741000, episode_reward=1754.01 +/- 62.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3741000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3742000, episode_reward=1697.97 +/- 65.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3742000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.29     |\n",
      "|    critic_loss     | 2.66     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | 2.04     |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3743000, episode_reward=1662.64 +/- 69.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3743000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3744000, episode_reward=1658.02 +/- 57.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3744000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.57     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | 1.39     |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18280    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3744     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5548     |\n",
      "|    total_timesteps | 3744000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3745000, episode_reward=1696.26 +/- 44.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3745000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3746000, episode_reward=1805.68 +/- 56.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3746000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.71     |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | 1.31     |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18290    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3747000, episode_reward=1747.70 +/- 73.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3747000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3748000, episode_reward=1749.05 +/- 42.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3748000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.33     |\n",
      "|    critic_loss     | 2.67     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | 1.46     |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3748     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5556     |\n",
      "|    total_timesteps | 3748000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3749000, episode_reward=1724.41 +/- 33.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3749000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750000, episode_reward=1663.63 +/- 61.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.63     |\n",
      "|    critic_loss     | 2.8      |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | 0.12     |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3751000, episode_reward=1640.75 +/- 44.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3751000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3752000, episode_reward=1703.19 +/- 30.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3752000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.22     |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3752     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5561     |\n",
      "|    total_timesteps | 3752000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3753000, episode_reward=1667.12 +/- 21.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3753000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3754000, episode_reward=1641.88 +/- 46.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3754000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.28     |\n",
      "|    critic_loss     | 2.61     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3755000, episode_reward=1617.75 +/- 44.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3755000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3756000, episode_reward=1617.81 +/- 44.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3756000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3756     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5568     |\n",
      "|    total_timesteps | 3756000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3757000, episode_reward=1702.35 +/- 36.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3757000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.51     |\n",
      "|    critic_loss     | 2.76     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | 0.101    |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3758000, episode_reward=1731.80 +/- 81.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3758000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3759000, episode_reward=1728.86 +/- 43.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3759000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.61     |\n",
      "|    critic_loss     | 3.28     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 0.719    |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3760000, episode_reward=1742.55 +/- 31.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3760000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3760     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5575     |\n",
      "|    total_timesteps | 3760000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3761000, episode_reward=1629.74 +/- 80.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3761000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.56     |\n",
      "|    critic_loss     | 2.76     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -0.00361 |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3762000, episode_reward=1607.03 +/- 151.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3762000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3763000, episode_reward=1667.68 +/- 45.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3763000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.22     |\n",
      "|    critic_loss     | 3.24     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3764000, episode_reward=1708.20 +/- 62.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3764000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3764     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5581     |\n",
      "|    total_timesteps | 3764000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3765000, episode_reward=1680.61 +/- 56.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3765000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.06     |\n",
      "|    critic_loss     | 3.11     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.558   |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3766000, episode_reward=1662.66 +/- 53.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3766000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3767000, episode_reward=1718.87 +/- 41.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3767000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.12     |\n",
      "|    critic_loss     | 3.18     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 2.07     |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3768000, episode_reward=1667.41 +/- 58.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3768000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3768     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5588     |\n",
      "|    total_timesteps | 3768000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3769000, episode_reward=1726.66 +/- 34.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3769000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 3.52     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3770000, episode_reward=1663.57 +/- 84.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3770000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3771000, episode_reward=1691.18 +/- 41.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3771000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.3      |\n",
      "|    critic_loss     | 3.35     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.536   |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3772000, episode_reward=1660.22 +/- 76.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3772000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3772     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5594     |\n",
      "|    total_timesteps | 3772000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3773000, episode_reward=1673.32 +/- 74.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3773000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.18     |\n",
      "|    critic_loss     | 2.73     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.12     |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3774000, episode_reward=1678.22 +/- 48.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3774000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3775000, episode_reward=1722.88 +/- 43.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3775000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.13     |\n",
      "|    critic_loss     | 2.99     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | 0.546    |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3776000, episode_reward=1694.59 +/- 34.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3776000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3776     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5600     |\n",
      "|    total_timesteps | 3776000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3777000, episode_reward=1696.14 +/- 80.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3777000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.28     |\n",
      "|    critic_loss     | 2.93     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | 0.296    |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3778000, episode_reward=1696.34 +/- 41.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3778000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3779000, episode_reward=1806.41 +/- 54.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3779000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.79     |\n",
      "|    critic_loss     | 3.1      |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.931    |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18450    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3780000, episode_reward=1708.35 +/- 80.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3780000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3780     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5606     |\n",
      "|    total_timesteps | 3780000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3781000, episode_reward=1733.35 +/- 88.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3781000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.45     |\n",
      "|    critic_loss     | 3.37     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 1.78     |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3782000, episode_reward=1723.72 +/- 42.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3782000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3783000, episode_reward=1759.59 +/- 57.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3783000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.62     |\n",
      "|    critic_loss     | 2.94     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.462    |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3784000, episode_reward=1796.46 +/- 82.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3784000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3784     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5612     |\n",
      "|    total_timesteps | 3784000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3785000, episode_reward=1656.00 +/- 43.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3785000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9        |\n",
      "|    critic_loss     | 3.24     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3786000, episode_reward=1709.76 +/- 68.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3786000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3787000, episode_reward=1831.13 +/- 55.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3787000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.83     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.137    |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18490    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3788000, episode_reward=1772.50 +/- 38.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3788000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3788     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5617     |\n",
      "|    total_timesteps | 3788000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3789000, episode_reward=1788.77 +/- 32.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3789000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.58     |\n",
      "|    critic_loss     | 3.27     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.322    |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3790000, episode_reward=1769.76 +/- 61.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3790000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3791000, episode_reward=1777.51 +/- 48.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3791000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.59     |\n",
      "|    critic_loss     | 2.97     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3792000, episode_reward=1802.78 +/- 58.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3792000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3792     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5623     |\n",
      "|    total_timesteps | 3792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3793000, episode_reward=1864.10 +/- 66.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3793000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.31     |\n",
      "|    critic_loss     | 3.09     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18520    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3794000, episode_reward=1733.01 +/- 49.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3794000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3795000, episode_reward=1816.67 +/- 47.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3795000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.6      |\n",
      "|    critic_loss     | 3.12     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.732   |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3796000, episode_reward=1796.16 +/- 63.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3796000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3796     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5630     |\n",
      "|    total_timesteps | 3796000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3797000, episode_reward=1826.47 +/- 64.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3797000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.81     |\n",
      "|    critic_loss     | 2.71     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | 0.494    |\n",
      "|    learning_rate   | 0.0012   |\n",
      "|    n_updates       | 18540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3798000, episode_reward=1795.18 +/- 37.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3798000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3799000, episode_reward=1813.69 +/- 91.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3799000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3800000, episode_reward=1782.58 +/- 16.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.84     |\n",
      "|    critic_loss     | 3.31     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | 0.544    |\n",
      "|    learning_rate   | 0.0012   |\n",
      "|    n_updates       | 18550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3800     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5635     |\n",
      "|    total_timesteps | 3800000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3801000, episode_reward=1826.42 +/- 46.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3801000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3802000, episode_reward=1756.29 +/- 53.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3802000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.16     |\n",
      "|    critic_loss     | 2.64     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 0.717    |\n",
      "|    learning_rate   | 0.0012   |\n",
      "|    n_updates       | 18560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3803000, episode_reward=1739.85 +/- 48.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3803000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3804000, episode_reward=1736.01 +/- 41.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3804000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.24     |\n",
      "|    critic_loss     | 2.65     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | -0.0551  |\n",
      "|    learning_rate   | 0.0012   |\n",
      "|    n_updates       | 18570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3804     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5641     |\n",
      "|    total_timesteps | 3804000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3805000, episode_reward=1682.71 +/- 34.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3805000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3806000, episode_reward=1787.04 +/- 61.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3806000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.35     |\n",
      "|    critic_loss     | 2.81     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.703    |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3807000, episode_reward=1739.09 +/- 63.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3807000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3808000, episode_reward=1869.80 +/- 30.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3808000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.36     |\n",
      "|    critic_loss     | 2.98     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.000401 |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18590    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3808     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5647     |\n",
      "|    total_timesteps | 3808000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3809000, episode_reward=1824.28 +/- 85.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3809000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810000, episode_reward=1860.05 +/- 40.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3811000, episode_reward=1833.67 +/- 59.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3811000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3812000, episode_reward=1858.89 +/- 41.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3812000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.44     |\n",
      "|    critic_loss     | 2.73     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | 0.528    |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18610    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3812     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5653     |\n",
      "|    total_timesteps | 3812000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3813000, episode_reward=1823.25 +/- 75.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3813000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3814000, episode_reward=1838.67 +/- 45.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3814000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.51     |\n",
      "|    critic_loss     | 3.39     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3815000, episode_reward=1783.14 +/- 39.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3815000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3816000, episode_reward=1839.13 +/- 51.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3816000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.61     |\n",
      "|    critic_loss     | 3.02     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.114    |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18630    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3816     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5659     |\n",
      "|    total_timesteps | 3816000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3817000, episode_reward=1857.02 +/- 79.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3817000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3818000, episode_reward=1850.19 +/- 30.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3818000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.64     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 0.439    |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3819000, episode_reward=1846.30 +/- 38.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3819000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3820000, episode_reward=1835.07 +/- 70.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.64     |\n",
      "|    critic_loss     | 2.74     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18650    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3820     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5665     |\n",
      "|    total_timesteps | 3820000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3821000, episode_reward=1799.42 +/- 59.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3821000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3822000, episode_reward=1840.99 +/- 30.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3822000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.36     |\n",
      "|    critic_loss     | 2.72     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.978    |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3823000, episode_reward=1818.02 +/- 43.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3823000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3824000, episode_reward=1818.04 +/- 45.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3824000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.95     |\n",
      "|    critic_loss     | 2.97     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 1.58     |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18670    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3824     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5671     |\n",
      "|    total_timesteps | 3824000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3825000, episode_reward=1839.97 +/- 44.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3825000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3826000, episode_reward=1849.83 +/- 112.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3826000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.94     |\n",
      "|    critic_loss     | 2.91     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 0.89     |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3827000, episode_reward=1833.10 +/- 53.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3827000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3828000, episode_reward=1818.06 +/- 50.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3828000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.04     |\n",
      "|    critic_loss     | 3.3      |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18690    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3828     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5677     |\n",
      "|    total_timesteps | 3828000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3829000, episode_reward=1811.20 +/- 57.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3829000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3830000, episode_reward=1839.37 +/- 76.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.45     |\n",
      "|    critic_loss     | 3.39     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.905    |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3831000, episode_reward=1841.58 +/- 48.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3831000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3832000, episode_reward=1814.94 +/- 68.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3832000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.62     |\n",
      "|    critic_loss     | 3.58     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 0.178    |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3832     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5684     |\n",
      "|    total_timesteps | 3832000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3833000, episode_reward=1851.23 +/- 80.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3833000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3834000, episode_reward=1768.81 +/- 30.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3834000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.42     |\n",
      "|    critic_loss     | 3.49     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | 0.537    |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3835000, episode_reward=1806.90 +/- 81.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3835000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3836000, episode_reward=1643.54 +/- 105.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3836000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.24     |\n",
      "|    critic_loss     | 3.84     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 2.13     |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3836     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5690     |\n",
      "|    total_timesteps | 3836000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3837000, episode_reward=1602.95 +/- 121.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3837000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3838000, episode_reward=1817.87 +/- 26.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3838000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.7     |\n",
      "|    critic_loss     | 4.04     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.196    |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3839000, episode_reward=1811.27 +/- 91.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3839000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=1801.09 +/- 40.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3840000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3840     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5696     |\n",
      "|    total_timesteps | 3840000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3841000, episode_reward=1759.33 +/- 44.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3841000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.61     |\n",
      "|    critic_loss     | 3.66     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.236    |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3842000, episode_reward=1762.53 +/- 80.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3842000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3843000, episode_reward=1691.83 +/- 48.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3843000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.99     |\n",
      "|    critic_loss     | 3.46     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 0.156    |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3844000, episode_reward=1732.93 +/- 70.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3844000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3844     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5703     |\n",
      "|    total_timesteps | 3844000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3845000, episode_reward=1814.38 +/- 56.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3845000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.15     |\n",
      "|    critic_loss     | 2.97     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3846000, episode_reward=1799.61 +/- 59.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3846000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3847000, episode_reward=1831.40 +/- 31.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3847000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.99     |\n",
      "|    critic_loss     | 3.27     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 0.298    |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3848000, episode_reward=1865.60 +/- 61.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3848000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3848     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5709     |\n",
      "|    total_timesteps | 3848000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3849000, episode_reward=1794.63 +/- 63.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3849000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.82     |\n",
      "|    critic_loss     | 3.19     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.336    |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850000, episode_reward=1813.61 +/- 107.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3850000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3851000, episode_reward=1890.10 +/- 40.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3851000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.42     |\n",
      "|    critic_loss     | 3.8      |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18800    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3852000, episode_reward=1888.80 +/- 40.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3852000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3852     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 5715     |\n",
      "|    total_timesteps | 3852000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3853000, episode_reward=1857.52 +/- 51.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3853000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.65     |\n",
      "|    critic_loss     | 3.32     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | -0.0246  |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3854000, episode_reward=1883.12 +/- 11.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3854000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3855000, episode_reward=1864.31 +/- 50.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3855000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.24     |\n",
      "|    critic_loss     | 3.99     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3856000, episode_reward=1823.91 +/- 83.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3856000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3856     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5721     |\n",
      "|    total_timesteps | 3856000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3857000, episode_reward=1811.18 +/- 38.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3857000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.61     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -0.117   |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3858000, episode_reward=1850.87 +/- 48.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3858000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3859000, episode_reward=1835.38 +/- 53.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3859000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.12     |\n",
      "|    critic_loss     | 3.26     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 0.505    |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3860000, episode_reward=1849.18 +/- 94.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3860000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3860     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5727     |\n",
      "|    total_timesteps | 3860000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3861000, episode_reward=1839.70 +/- 56.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3861000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.05     |\n",
      "|    critic_loss     | 3.23     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | 0.883    |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3862000, episode_reward=1698.75 +/- 281.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3862000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3863000, episode_reward=1838.46 +/- 47.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3863000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.9      |\n",
      "|    critic_loss     | 3.47     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.826   |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3864000, episode_reward=1703.82 +/- 196.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3864000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3864     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5733     |\n",
      "|    total_timesteps | 3864000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3865000, episode_reward=1861.28 +/- 64.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3865000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.9      |\n",
      "|    critic_loss     | 4.22     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.805    |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3866000, episode_reward=1880.21 +/- 48.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3866000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3867000, episode_reward=1834.42 +/- 71.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3867000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.79     |\n",
      "|    critic_loss     | 4.38     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.684    |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3868000, episode_reward=1835.43 +/- 56.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3868000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3868     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5739     |\n",
      "|    total_timesteps | 3868000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3869000, episode_reward=1788.15 +/- 15.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3869000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.52     |\n",
      "|    critic_loss     | 4.25     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870000, episode_reward=1849.91 +/- 43.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3870000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3871000, episode_reward=1495.70 +/- 341.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3871000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.67     |\n",
      "|    critic_loss     | 4.41     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | 0.451    |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3872000, episode_reward=1757.57 +/- 67.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3872000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3872     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5745     |\n",
      "|    total_timesteps | 3872000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3873000, episode_reward=1746.86 +/- 82.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3873000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.8     |\n",
      "|    critic_loss     | 5.45     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 3.39     |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3874000, episode_reward=1778.21 +/- 73.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3874000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3875000, episode_reward=1692.72 +/- 63.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3875000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 4.8      |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 1.73     |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3876000, episode_reward=1744.52 +/- 45.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3876000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3876     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5751     |\n",
      "|    total_timesteps | 3876000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3877000, episode_reward=1698.95 +/- 43.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3877000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.7     |\n",
      "|    critic_loss     | 4.18     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | -0.873   |\n",
      "|    learning_rate   | 0.00112  |\n",
      "|    n_updates       | 18930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3878000, episode_reward=1712.85 +/- 7.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3878000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3879000, episode_reward=1719.81 +/- 104.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3879000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.39     |\n",
      "|    critic_loss     | 3.39     |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | -0.4     |\n",
      "|    learning_rate   | 0.00112  |\n",
      "|    n_updates       | 18940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3880000, episode_reward=1784.38 +/- 90.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3880000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3880     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5757     |\n",
      "|    total_timesteps | 3880000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3881000, episode_reward=1703.44 +/- 50.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3881000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.71     |\n",
      "|    critic_loss     | 4.12     |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.00112  |\n",
      "|    n_updates       | 18950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3882000, episode_reward=1766.39 +/- 90.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3882000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3883000, episode_reward=1443.94 +/- 548.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3883000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3884000, episode_reward=1759.70 +/- 115.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3884000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.3     |\n",
      "|    critic_loss     | 4.66     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.00112  |\n",
      "|    n_updates       | 18960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3884     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5763     |\n",
      "|    total_timesteps | 3884000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3885000, episode_reward=1778.85 +/- 110.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3885000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3886000, episode_reward=1688.75 +/- 45.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3886000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.1     |\n",
      "|    critic_loss     | 4.76     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -0.191   |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 18970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3887000, episode_reward=1705.24 +/- 53.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3887000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3888000, episode_reward=1859.02 +/- 55.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3888000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 6.59     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -2.93    |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 18980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3888     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5769     |\n",
      "|    total_timesteps | 3888000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3889000, episode_reward=1821.89 +/- 33.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3889000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890000, episode_reward=1684.80 +/- 75.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.41     |\n",
      "|    critic_loss     | 4.53     |\n",
      "|    ent_coef        | 0.0391   |\n",
      "|    ent_coef_loss   | 0.289    |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 18990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3891000, episode_reward=1638.69 +/- 72.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3891000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3892000, episode_reward=1655.77 +/- 68.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3892000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 4.6      |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 19000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3892     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5774     |\n",
      "|    total_timesteps | 3892000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3893000, episode_reward=1677.97 +/- 111.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3893000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3894000, episode_reward=1835.25 +/- 90.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3894000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 5.6      |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 19010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3895000, episode_reward=1855.79 +/- 80.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3895000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3896000, episode_reward=1796.84 +/- 90.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3896000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.33     |\n",
      "|    critic_loss     | 5.32     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | 0.883    |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19020    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3896     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5780     |\n",
      "|    total_timesteps | 3896000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3897000, episode_reward=1809.86 +/- 51.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3897000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3898000, episode_reward=1811.56 +/- 36.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3898000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.33     |\n",
      "|    critic_loss     | 4.17     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3899000, episode_reward=1804.87 +/- 70.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3899000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900000, episode_reward=1790.15 +/- 34.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.83     |\n",
      "|    critic_loss     | 4.94     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | 1.76     |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19040    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3900     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5787     |\n",
      "|    total_timesteps | 3900000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3901000, episode_reward=1749.19 +/- 72.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3901000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3902000, episode_reward=1959.05 +/- 61.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3902000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.3     |\n",
      "|    critic_loss     | 5.54     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19050    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3903000, episode_reward=1944.85 +/- 98.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3903000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3904000, episode_reward=1918.26 +/- 36.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3904000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.77     |\n",
      "|    critic_loss     | 4.77     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 0.287    |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19060    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3904     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5793     |\n",
      "|    total_timesteps | 3904000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3905000, episode_reward=1861.73 +/- 79.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3905000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3906000, episode_reward=1928.61 +/- 76.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3906000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.2      |\n",
      "|    critic_loss     | 5.28     |\n",
      "|    ent_coef        | 0.0413   |\n",
      "|    ent_coef_loss   | 2.6      |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3907000, episode_reward=1892.41 +/- 90.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3907000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3908000, episode_reward=1887.65 +/- 55.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3908000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.19     |\n",
      "|    critic_loss     | 4.58     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -0.138   |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19080    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3908     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5800     |\n",
      "|    total_timesteps | 3908000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3909000, episode_reward=1919.62 +/- 68.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3909000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3910000, episode_reward=1939.20 +/- 59.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.77     |\n",
      "|    critic_loss     | 4.3      |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | 0.679    |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3911000, episode_reward=1941.15 +/- 29.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3911000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3912000, episode_reward=1857.88 +/- 91.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3912000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.54     |\n",
      "|    critic_loss     | 5.1      |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19100    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3912     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5807     |\n",
      "|    total_timesteps | 3912000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3913000, episode_reward=1906.60 +/- 41.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3913000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3914000, episode_reward=1858.70 +/- 97.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3914000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.3     |\n",
      "|    critic_loss     | 4.87     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | 0.334    |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3915000, episode_reward=1897.64 +/- 55.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3915000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3916000, episode_reward=1804.55 +/- 17.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3916000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.6     |\n",
      "|    critic_loss     | 5.07     |\n",
      "|    ent_coef        | 0.0425   |\n",
      "|    ent_coef_loss   | 0.562    |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19120    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3916     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5814     |\n",
      "|    total_timesteps | 3916000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3917000, episode_reward=1805.31 +/- 53.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3917000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3918000, episode_reward=1910.79 +/- 63.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3918000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 3.9      |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | -0.505   |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3919000, episode_reward=1900.80 +/- 63.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3919000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3920000, episode_reward=1922.68 +/- 58.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.16     |\n",
      "|    critic_loss     | 4.67     |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | 0.321    |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3920     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5821     |\n",
      "|    total_timesteps | 3920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3921000, episode_reward=1913.55 +/- 54.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3921000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3922000, episode_reward=1871.07 +/- 49.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3922000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.61     |\n",
      "|    critic_loss     | 4.9      |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | 0.043    |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3923000, episode_reward=1885.98 +/- 89.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3923000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3924000, episode_reward=1840.81 +/- 62.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3924000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 5.21     |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | 0.214    |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3924     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5829     |\n",
      "|    total_timesteps | 3924000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3925000, episode_reward=1844.21 +/- 48.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3925000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3926000, episode_reward=1845.27 +/- 46.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3926000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3927000, episode_reward=1920.36 +/- 23.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3927000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.91     |\n",
      "|    critic_loss     | 5.18     |\n",
      "|    ent_coef        | 0.0427   |\n",
      "|    ent_coef_loss   | 0.521    |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3928000, episode_reward=1886.68 +/- 20.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3928000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3928     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5835     |\n",
      "|    total_timesteps | 3928000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3929000, episode_reward=1779.09 +/- 93.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3929000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.76     |\n",
      "|    critic_loss     | 4.67     |\n",
      "|    ent_coef        | 0.0428   |\n",
      "|    ent_coef_loss   | 0.311    |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3930000, episode_reward=1797.99 +/- 58.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3930000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3931000, episode_reward=1751.69 +/- 33.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3931000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.3     |\n",
      "|    critic_loss     | 5.06     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3932000, episode_reward=1781.41 +/- 40.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3932000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3932     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5841     |\n",
      "|    total_timesteps | 3932000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3933000, episode_reward=1798.93 +/- 60.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3933000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.3     |\n",
      "|    critic_loss     | 5.15     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | -0.671   |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3934000, episode_reward=1814.80 +/- 31.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3934000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3935000, episode_reward=1765.23 +/- 138.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3935000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 5.09     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | 0.237    |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3936000, episode_reward=1730.36 +/- 72.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3936000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3936     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5847     |\n",
      "|    total_timesteps | 3936000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3937000, episode_reward=1790.41 +/- 51.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3937000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.1     |\n",
      "|    critic_loss     | 5.24     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | 0.192    |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3938000, episode_reward=1782.08 +/- 74.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3938000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3939000, episode_reward=1732.70 +/- 58.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3939000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11       |\n",
      "|    critic_loss     | 5.51     |\n",
      "|    ent_coef        | 0.0433   |\n",
      "|    ent_coef_loss   | 0.431    |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3940000, episode_reward=1736.18 +/- 58.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3940000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3940     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5853     |\n",
      "|    total_timesteps | 3940000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3941000, episode_reward=1752.39 +/- 90.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3941000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 5.64     |\n",
      "|    ent_coef        | 0.0435   |\n",
      "|    ent_coef_loss   | 1.5      |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3942000, episode_reward=1685.35 +/- 38.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3942000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3943000, episode_reward=1776.58 +/- 59.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3943000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    ent_coef        | 0.0438   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3944000, episode_reward=1762.50 +/- 45.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3944000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3944     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5859     |\n",
      "|    total_timesteps | 3944000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3945000, episode_reward=1906.22 +/- 47.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3945000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.27     |\n",
      "|    critic_loss     | 5        |\n",
      "|    ent_coef        | 0.0442   |\n",
      "|    ent_coef_loss   | 0.0572   |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3946000, episode_reward=1868.17 +/- 92.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3946000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3947000, episode_reward=1838.83 +/- 52.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3947000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.09     |\n",
      "|    critic_loss     | 5.29     |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | -0.64    |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3948000, episode_reward=1870.66 +/- 73.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3948000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3948     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5864     |\n",
      "|    total_timesteps | 3948000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3949000, episode_reward=1889.11 +/- 39.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3949000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 5.54     |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | 0.69     |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3950000, episode_reward=1896.67 +/- 37.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3950000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3951000, episode_reward=1837.26 +/- 54.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3951000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 4.94     |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | -0.481   |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3952000, episode_reward=1875.08 +/- 55.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3952000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3952     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5870     |\n",
      "|    total_timesteps | 3952000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3953000, episode_reward=1831.61 +/- 58.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3953000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.5     |\n",
      "|    critic_loss     | 6.54     |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | 1.74     |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3954000, episode_reward=1813.52 +/- 46.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3954000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3955000, episode_reward=1885.88 +/- 49.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3955000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 5.59     |\n",
      "|    ent_coef        | 0.0447   |\n",
      "|    ent_coef_loss   | 0.934    |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3956000, episode_reward=1816.46 +/- 65.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3956000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3956     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 5877     |\n",
      "|    total_timesteps | 3956000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3957000, episode_reward=1800.99 +/- 57.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3957000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 5.32     |\n",
      "|    ent_coef        | 0.045    |\n",
      "|    ent_coef_loss   | 0.786    |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3958000, episode_reward=1850.46 +/- 61.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3958000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3959000, episode_reward=1832.33 +/- 41.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3959000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 5.43     |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | 0.675    |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960000, episode_reward=1828.16 +/- 65.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3960000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3960     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5884     |\n",
      "|    total_timesteps | 3960000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3961000, episode_reward=1847.45 +/- 49.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3961000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.8     |\n",
      "|    critic_loss     | 4.92     |\n",
      "|    ent_coef        | 0.0456   |\n",
      "|    ent_coef_loss   | -0.761   |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3962000, episode_reward=1851.74 +/- 107.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3962000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3963000, episode_reward=1877.99 +/- 56.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3963000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.6     |\n",
      "|    critic_loss     | 5.66     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | -0.727   |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3964000, episode_reward=1799.60 +/- 45.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3964000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3964     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5891     |\n",
      "|    total_timesteps | 3964000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3965000, episode_reward=1903.68 +/- 52.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3965000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 6.3      |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | 0.815    |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3966000, episode_reward=1866.55 +/- 49.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3966000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3967000, episode_reward=1932.10 +/- 58.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3967000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.55     |\n",
      "|    critic_loss     | 5.62     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | 0.933    |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    n_updates       | 19370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3968000, episode_reward=1896.97 +/- 50.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3968000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3968     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5897     |\n",
      "|    total_timesteps | 3968000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3969000, episode_reward=1935.53 +/- 60.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3969000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3970000, episode_reward=1914.83 +/- 42.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 6.11     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    n_updates       | 19380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3971000, episode_reward=1963.02 +/- 33.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3971000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3972000, episode_reward=1966.29 +/- 35.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3972000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.43     |\n",
      "|    critic_loss     | 4.82     |\n",
      "|    ent_coef        | 0.0461   |\n",
      "|    ent_coef_loss   | 0.0068   |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    n_updates       | 19390    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3972     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5903     |\n",
      "|    total_timesteps | 3972000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3973000, episode_reward=2000.78 +/- 23.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3973000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3974000, episode_reward=1905.95 +/- 69.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3974000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.77     |\n",
      "|    critic_loss     | 5.21     |\n",
      "|    ent_coef        | 0.0462   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    n_updates       | 19400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3975000, episode_reward=1960.17 +/- 22.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3975000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3976000, episode_reward=1895.68 +/- 101.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3976000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.71     |\n",
      "|    critic_loss     | 5.39     |\n",
      "|    ent_coef        | 0.0467   |\n",
      "|    ent_coef_loss   | 1.91     |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3976     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5911     |\n",
      "|    total_timesteps | 3976000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3977000, episode_reward=1908.20 +/- 41.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3977000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3978000, episode_reward=1911.88 +/- 61.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3978000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.7     |\n",
      "|    critic_loss     | 6.3      |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 0.0241   |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3979000, episode_reward=1941.19 +/- 48.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3979000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3980000, episode_reward=1902.30 +/- 28.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 5.02     |\n",
      "|    ent_coef        | 0.0474   |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19430    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3980     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5917     |\n",
      "|    total_timesteps | 3980000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3981000, episode_reward=1920.53 +/- 37.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3981000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3982000, episode_reward=1941.28 +/- 56.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3982000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.6     |\n",
      "|    critic_loss     | 4.9      |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | -0.839   |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3983000, episode_reward=1896.45 +/- 23.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3983000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3984000, episode_reward=1869.16 +/- 61.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3984000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.9     |\n",
      "|    critic_loss     | 4.79     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19450    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3984     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5924     |\n",
      "|    total_timesteps | 3984000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3985000, episode_reward=1874.96 +/- 46.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3985000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3986000, episode_reward=2001.31 +/- 78.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3986000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 4.93     |\n",
      "|    ent_coef        | 0.0464   |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19460    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3987000, episode_reward=1977.42 +/- 65.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3987000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3988000, episode_reward=1840.31 +/- 63.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3988000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.56     |\n",
      "|    critic_loss     | 5.7      |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | -0.776   |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19470    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3988     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5930     |\n",
      "|    total_timesteps | 3988000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3989000, episode_reward=1885.97 +/- 31.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3989000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990000, episode_reward=1943.20 +/- 65.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 5.77     |\n",
      "|    ent_coef        | 0.0457   |\n",
      "|    ent_coef_loss   | 0.184    |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3991000, episode_reward=1931.04 +/- 53.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3991000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3992000, episode_reward=1958.26 +/- 51.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3992000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.2     |\n",
      "|    critic_loss     | 6.5      |\n",
      "|    ent_coef        | 0.0457   |\n",
      "|    ent_coef_loss   | 1.49     |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19490    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3992     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5936     |\n",
      "|    total_timesteps | 3992000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3993000, episode_reward=1948.33 +/- 77.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3993000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3994000, episode_reward=1890.46 +/- 74.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3994000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.6      |\n",
      "|    critic_loss     | 6.36     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3995000, episode_reward=1881.00 +/- 40.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3995000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3996000, episode_reward=1853.10 +/- 65.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3996000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 6.49     |\n",
      "|    ent_coef        | 0.0464   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19510    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3996     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5943     |\n",
      "|    total_timesteps | 3996000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3997000, episode_reward=1835.69 +/- 104.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3997000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3998000, episode_reward=1821.04 +/- 60.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3998000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.5     |\n",
      "|    critic_loss     | 6.11     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | 0.895    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3999000, episode_reward=1812.90 +/- 79.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3999000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000000, episode_reward=1914.05 +/- 33.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 6.94     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 0.319    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19530    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4000     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5949     |\n",
      "|    total_timesteps | 4000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4001000, episode_reward=1880.34 +/- 62.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4001000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4002000, episode_reward=1820.47 +/- 15.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4002000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.75     |\n",
      "|    critic_loss     | 6.18     |\n",
      "|    ent_coef        | 0.0473   |\n",
      "|    ent_coef_loss   | -0.959   |\n",
      "|    learning_rate   | 0.000998 |\n",
      "|    n_updates       | 19540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4003000, episode_reward=1825.81 +/- 48.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4003000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4004000, episode_reward=1861.40 +/- 70.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4004000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 6.25     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | -0.386   |\n",
      "|    learning_rate   | 0.000996 |\n",
      "|    n_updates       | 19550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4004     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5955     |\n",
      "|    total_timesteps | 4004000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4005000, episode_reward=1825.61 +/- 46.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4005000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4006000, episode_reward=1851.23 +/- 22.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4006000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 6.25     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -0.924   |\n",
      "|    learning_rate   | 0.000994 |\n",
      "|    n_updates       | 19560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4007000, episode_reward=1842.14 +/- 45.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4007000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4008000, episode_reward=1881.16 +/- 47.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4008000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.8     |\n",
      "|    critic_loss     | 6.26     |\n",
      "|    ent_coef        | 0.0467   |\n",
      "|    ent_coef_loss   | -0.935   |\n",
      "|    learning_rate   | 0.000992 |\n",
      "|    n_updates       | 19570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4008     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5961     |\n",
      "|    total_timesteps | 4008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4009000, episode_reward=1894.93 +/- 66.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4009000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4010000, episode_reward=1836.34 +/- 43.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 6.17     |\n",
      "|    ent_coef        | 0.0464   |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.00099  |\n",
      "|    n_updates       | 19580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4011000, episode_reward=1874.73 +/- 81.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4011000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4012000, episode_reward=1904.56 +/- 52.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4012000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4012     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5967     |\n",
      "|    total_timesteps | 4012000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4013000, episode_reward=1847.23 +/- 30.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4013000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.1     |\n",
      "|    critic_loss     | 6.1      |\n",
      "|    ent_coef        | 0.0463   |\n",
      "|    ent_coef_loss   | 0.0572   |\n",
      "|    learning_rate   | 0.000988 |\n",
      "|    n_updates       | 19590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4014000, episode_reward=1820.91 +/- 80.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4014000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4015000, episode_reward=1843.63 +/- 58.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4015000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 6.21     |\n",
      "|    ent_coef        | 0.0462   |\n",
      "|    ent_coef_loss   | 0.155    |\n",
      "|    learning_rate   | 0.000986 |\n",
      "|    n_updates       | 19600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4016000, episode_reward=1871.82 +/- 39.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4016000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4016     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5972     |\n",
      "|    total_timesteps | 4016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4017000, episode_reward=1776.84 +/- 38.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4017000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11       |\n",
      "|    critic_loss     | 5.96     |\n",
      "|    ent_coef        | 0.0462   |\n",
      "|    ent_coef_loss   | -0.262   |\n",
      "|    learning_rate   | 0.000984 |\n",
      "|    n_updates       | 19610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4018000, episode_reward=1784.09 +/- 85.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4018000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4019000, episode_reward=1837.24 +/- 23.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4019000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 6.08     |\n",
      "|    ent_coef        | 0.0461   |\n",
      "|    ent_coef_loss   | -0.756   |\n",
      "|    learning_rate   | 0.000982 |\n",
      "|    n_updates       | 19620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4020000, episode_reward=1819.14 +/- 25.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4020000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4020     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5978     |\n",
      "|    total_timesteps | 4020000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4021000, episode_reward=1773.84 +/- 55.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4021000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11       |\n",
      "|    critic_loss     | 5.61     |\n",
      "|    ent_coef        | 0.0459   |\n",
      "|    ent_coef_loss   | -0.599   |\n",
      "|    learning_rate   | 0.00098  |\n",
      "|    n_updates       | 19630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4022000, episode_reward=1803.10 +/- 113.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4022000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4023000, episode_reward=1820.68 +/- 33.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4023000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 6.01     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.652    |\n",
      "|    learning_rate   | 0.000978 |\n",
      "|    n_updates       | 19640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4024000, episode_reward=1823.23 +/- 50.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4024000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4024     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5984     |\n",
      "|    total_timesteps | 4024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4025000, episode_reward=1883.47 +/- 19.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4025000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.3     |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.000976 |\n",
      "|    n_updates       | 19650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4026000, episode_reward=1886.65 +/- 48.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4026000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4027000, episode_reward=1852.41 +/- 44.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4027000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.7     |\n",
      "|    critic_loss     | 5.44     |\n",
      "|    ent_coef        | 0.0456   |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.000974 |\n",
      "|    n_updates       | 19660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4028000, episode_reward=1830.37 +/- 48.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4028000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4028     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5990     |\n",
      "|    total_timesteps | 4028000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4029000, episode_reward=1849.94 +/- 46.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4029000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.6     |\n",
      "|    critic_loss     | 6.09     |\n",
      "|    ent_coef        | 0.0452   |\n",
      "|    ent_coef_loss   | -0.312   |\n",
      "|    learning_rate   | 0.000972 |\n",
      "|    n_updates       | 19670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4030000, episode_reward=1796.94 +/- 63.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4030000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4031000, episode_reward=1781.34 +/- 37.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4031000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.3     |\n",
      "|    critic_loss     | 6.42     |\n",
      "|    ent_coef        | 0.045    |\n",
      "|    ent_coef_loss   | -1.5     |\n",
      "|    learning_rate   | 0.00097  |\n",
      "|    n_updates       | 19680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4032000, episode_reward=1762.05 +/- 34.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4032000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4032     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 5996     |\n",
      "|    total_timesteps | 4032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4033000, episode_reward=1805.08 +/- 57.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4033000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.4     |\n",
      "|    critic_loss     | 5.49     |\n",
      "|    ent_coef        | 0.0446   |\n",
      "|    ent_coef_loss   | -0.894   |\n",
      "|    learning_rate   | 0.000967 |\n",
      "|    n_updates       | 19690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4034000, episode_reward=1859.04 +/- 42.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4034000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4035000, episode_reward=1823.58 +/- 43.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4035000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 6.03     |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.000965 |\n",
      "|    n_updates       | 19700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4036000, episode_reward=1840.92 +/- 54.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4036000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4036     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6002     |\n",
      "|    total_timesteps | 4036000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4037000, episode_reward=1816.06 +/- 46.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4037000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.1     |\n",
      "|    critic_loss     | 5.82     |\n",
      "|    ent_coef        | 0.0438   |\n",
      "|    ent_coef_loss   | -0.789   |\n",
      "|    learning_rate   | 0.000963 |\n",
      "|    n_updates       | 19710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4038000, episode_reward=1784.90 +/- 47.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4038000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4039000, episode_reward=1845.69 +/- 17.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4039000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.6     |\n",
      "|    critic_loss     | 5.82     |\n",
      "|    ent_coef        | 0.0435   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.000961 |\n",
      "|    n_updates       | 19720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4040000, episode_reward=1822.92 +/- 71.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4040000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4040     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6008     |\n",
      "|    total_timesteps | 4040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4041000, episode_reward=1787.57 +/- 12.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4041000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.9     |\n",
      "|    critic_loss     | 5.46     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | -0.93    |\n",
      "|    learning_rate   | 0.000959 |\n",
      "|    n_updates       | 19730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4042000, episode_reward=1824.04 +/- 35.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4042000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4043000, episode_reward=1833.71 +/- 53.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4043000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 5.78     |\n",
      "|    ent_coef        | 0.0428   |\n",
      "|    ent_coef_loss   | -0.617   |\n",
      "|    learning_rate   | 0.000957 |\n",
      "|    n_updates       | 19740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4044000, episode_reward=1819.05 +/- 34.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4044000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4044     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6015     |\n",
      "|    total_timesteps | 4044000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4045000, episode_reward=1789.78 +/- 62.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4045000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.8     |\n",
      "|    critic_loss     | 5.88     |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | -0.317   |\n",
      "|    learning_rate   | 0.000955 |\n",
      "|    n_updates       | 19750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4046000, episode_reward=1832.38 +/- 53.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4046000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4047000, episode_reward=1775.00 +/- 18.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4047000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.1     |\n",
      "|    critic_loss     | 6.07     |\n",
      "|    ent_coef        | 0.0425   |\n",
      "|    ent_coef_loss   | -0.423   |\n",
      "|    learning_rate   | 0.000953 |\n",
      "|    n_updates       | 19760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4048000, episode_reward=1802.12 +/- 23.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4048000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4048     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6021     |\n",
      "|    total_timesteps | 4048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4049000, episode_reward=1871.35 +/- 39.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4049000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.9     |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -0.16    |\n",
      "|    learning_rate   | 0.000951 |\n",
      "|    n_updates       | 19770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050000, episode_reward=1855.06 +/- 54.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4050000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4051000, episode_reward=1898.20 +/- 48.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4051000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.8     |\n",
      "|    critic_loss     | 6.01     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | 0.164    |\n",
      "|    learning_rate   | 0.000949 |\n",
      "|    n_updates       | 19780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4052000, episode_reward=1928.12 +/- 36.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4052000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4052     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6027     |\n",
      "|    total_timesteps | 4052000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4053000, episode_reward=1853.19 +/- 30.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4053000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.8     |\n",
      "|    critic_loss     | 6        |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.000947 |\n",
      "|    n_updates       | 19790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4054000, episode_reward=1835.09 +/- 55.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4054000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4055000, episode_reward=1840.13 +/- 23.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4055000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4056000, episode_reward=1857.45 +/- 39.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4056000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12       |\n",
      "|    critic_loss     | 5.6      |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | -0.499   |\n",
      "|    learning_rate   | 0.000945 |\n",
      "|    n_updates       | 19800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4056     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6033     |\n",
      "|    total_timesteps | 4056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4057000, episode_reward=1806.17 +/- 49.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4057000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4058000, episode_reward=1887.48 +/- 53.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4058000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 5.8      |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -0.00671 |\n",
      "|    learning_rate   | 0.000943 |\n",
      "|    n_updates       | 19810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4059000, episode_reward=1759.95 +/- 54.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4059000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4060000, episode_reward=1834.78 +/- 68.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.4     |\n",
      "|    critic_loss     | 5.96     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 0.409    |\n",
      "|    learning_rate   | 0.000941 |\n",
      "|    n_updates       | 19820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4060     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6039     |\n",
      "|    total_timesteps | 4060000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4061000, episode_reward=1848.56 +/- 47.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4061000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4062000, episode_reward=1869.19 +/- 44.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4062000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.5     |\n",
      "|    critic_loss     | 6.06     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 0.506    |\n",
      "|    learning_rate   | 0.000939 |\n",
      "|    n_updates       | 19830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4063000, episode_reward=1830.75 +/- 78.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4063000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4064000, episode_reward=1826.88 +/- 57.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4064000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.7     |\n",
      "|    critic_loss     | 5.68     |\n",
      "|    ent_coef        | 0.0419   |\n",
      "|    ent_coef_loss   | -0.409   |\n",
      "|    learning_rate   | 0.000937 |\n",
      "|    n_updates       | 19840    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4064     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6045     |\n",
      "|    total_timesteps | 4064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4065000, episode_reward=1910.69 +/- 39.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4065000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4066000, episode_reward=1880.12 +/- 71.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4066000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.2     |\n",
      "|    critic_loss     | 6.08     |\n",
      "|    ent_coef        | 0.0419   |\n",
      "|    ent_coef_loss   | -0.409   |\n",
      "|    learning_rate   | 0.000935 |\n",
      "|    n_updates       | 19850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4067000, episode_reward=1842.46 +/- 74.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4067000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4068000, episode_reward=1851.24 +/- 32.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4068000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 5.95     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 0.101    |\n",
      "|    learning_rate   | 0.000933 |\n",
      "|    n_updates       | 19860    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4068     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6052     |\n",
      "|    total_timesteps | 4068000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4069000, episode_reward=1825.97 +/- 41.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4069000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4070000, episode_reward=1874.77 +/- 50.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.9     |\n",
      "|    critic_loss     | 6.35     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 0.00114  |\n",
      "|    learning_rate   | 0.000931 |\n",
      "|    n_updates       | 19870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4071000, episode_reward=1912.03 +/- 33.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4071000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4072000, episode_reward=1805.32 +/- 71.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4072000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 7.55     |\n",
      "|    ent_coef        | 0.0419   |\n",
      "|    ent_coef_loss   | 0.898    |\n",
      "|    learning_rate   | 0.000929 |\n",
      "|    n_updates       | 19880    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4072     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6059     |\n",
      "|    total_timesteps | 4072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4073000, episode_reward=1823.31 +/- 50.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4073000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4074000, episode_reward=1831.81 +/- 11.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4074000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 7.1      |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | 0.701    |\n",
      "|    learning_rate   | 0.000927 |\n",
      "|    n_updates       | 19890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4075000, episode_reward=1858.27 +/- 82.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4075000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4076000, episode_reward=1865.70 +/- 46.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4076000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13       |\n",
      "|    critic_loss     | 6.58     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | 0.125    |\n",
      "|    learning_rate   | 0.000924 |\n",
      "|    n_updates       | 19900    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4076     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6065     |\n",
      "|    total_timesteps | 4076000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4077000, episode_reward=1876.51 +/- 43.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4077000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4078000, episode_reward=1818.38 +/- 32.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4078000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 6.1      |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -0.528   |\n",
      "|    learning_rate   | 0.000922 |\n",
      "|    n_updates       | 19910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4079000, episode_reward=1838.86 +/- 44.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4079000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4080000, episode_reward=1781.93 +/- 51.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.1     |\n",
      "|    critic_loss     | 6.15     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -0.401   |\n",
      "|    learning_rate   | 0.00092  |\n",
      "|    n_updates       | 19920    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4080     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6071     |\n",
      "|    total_timesteps | 4080000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4081000, episode_reward=1802.84 +/- 49.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4081000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4082000, episode_reward=1748.93 +/- 102.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4082000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13       |\n",
      "|    critic_loss     | 6.23     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | -0.0765  |\n",
      "|    learning_rate   | 0.000918 |\n",
      "|    n_updates       | 19930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4083000, episode_reward=1818.15 +/- 61.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4083000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4084000, episode_reward=1806.49 +/- 34.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4084000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 7.44     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | 0.736    |\n",
      "|    learning_rate   | 0.000916 |\n",
      "|    n_updates       | 19940    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4084     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6077     |\n",
      "|    total_timesteps | 4084000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4085000, episode_reward=1841.66 +/- 49.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4085000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4086000, episode_reward=1750.13 +/- 32.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4086000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 6.39     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -0.271   |\n",
      "|    learning_rate   | 0.000914 |\n",
      "|    n_updates       | 19950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4087000, episode_reward=1763.72 +/- 73.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4087000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4088000, episode_reward=1783.84 +/- 53.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4088000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 6.97     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | 0.499    |\n",
      "|    learning_rate   | 0.000912 |\n",
      "|    n_updates       | 19960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4088     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6083     |\n",
      "|    total_timesteps | 4088000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4089000, episode_reward=1766.85 +/- 35.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4089000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4090000, episode_reward=1820.14 +/- 44.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.8     |\n",
      "|    critic_loss     | 6.61     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | 0.276    |\n",
      "|    learning_rate   | 0.00091  |\n",
      "|    n_updates       | 19970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4091000, episode_reward=1819.57 +/- 43.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4091000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4092000, episode_reward=1874.65 +/- 10.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4092000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.2     |\n",
      "|    critic_loss     | 6.24     |\n",
      "|    ent_coef        | 0.0424   |\n",
      "|    ent_coef_loss   | -0.3     |\n",
      "|    learning_rate   | 0.000908 |\n",
      "|    n_updates       | 19980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4092     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6089     |\n",
      "|    total_timesteps | 4092000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4093000, episode_reward=1862.60 +/- 26.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4093000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4094000, episode_reward=1835.39 +/- 87.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4094000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 6.56     |\n",
      "|    ent_coef        | 0.0424   |\n",
      "|    ent_coef_loss   | 0.124    |\n",
      "|    learning_rate   | 0.000906 |\n",
      "|    n_updates       | 19990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4095000, episode_reward=1784.51 +/- 49.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4095000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4096000, episode_reward=1809.80 +/- 29.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4096000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4096     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6095     |\n",
      "|    total_timesteps | 4096000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4097000, episode_reward=1844.25 +/- 19.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4097000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 7.17     |\n",
      "|    ent_coef        | 0.0424   |\n",
      "|    ent_coef_loss   | 0.0915   |\n",
      "|    learning_rate   | 0.000904 |\n",
      "|    n_updates       | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4098000, episode_reward=1818.02 +/- 34.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4098000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4099000, episode_reward=1833.53 +/- 49.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4099000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 5.92     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.000902 |\n",
      "|    n_updates       | 20010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4100000, episode_reward=1876.14 +/- 37.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4100000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4100     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6102     |\n",
      "|    total_timesteps | 4100000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4101000, episode_reward=1798.89 +/- 60.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4101000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 6.77     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -0.259   |\n",
      "|    learning_rate   | 0.0009   |\n",
      "|    n_updates       | 20020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4102000, episode_reward=1767.22 +/- 38.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4102000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4103000, episode_reward=1787.53 +/- 63.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4103000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 6.75     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | -0.37    |\n",
      "|    learning_rate   | 0.000898 |\n",
      "|    n_updates       | 20030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4104000, episode_reward=1788.98 +/- 42.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4104000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4104     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6109     |\n",
      "|    total_timesteps | 4104000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4105000, episode_reward=1788.65 +/- 12.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4105000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 7.6      |\n",
      "|    ent_coef        | 0.0419   |\n",
      "|    ent_coef_loss   | -0.323   |\n",
      "|    learning_rate   | 0.000896 |\n",
      "|    n_updates       | 20040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4106000, episode_reward=1747.32 +/- 40.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4106000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4107000, episode_reward=1778.69 +/- 51.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4107000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 6.01     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -1.42    |\n",
      "|    learning_rate   | 0.000894 |\n",
      "|    n_updates       | 20050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4108000, episode_reward=1774.91 +/- 68.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4108000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4108     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6115     |\n",
      "|    total_timesteps | 4108000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4109000, episode_reward=1813.07 +/- 32.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4109000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 6.06     |\n",
      "|    ent_coef        | 0.0415   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.000892 |\n",
      "|    n_updates       | 20060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110000, episode_reward=1830.09 +/- 72.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4110000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4111000, episode_reward=1798.76 +/- 71.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4111000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.9     |\n",
      "|    critic_loss     | 6.39     |\n",
      "|    ent_coef        | 0.0411   |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.00089  |\n",
      "|    n_updates       | 20070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4112000, episode_reward=1841.08 +/- 40.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4112000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4112     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6121     |\n",
      "|    total_timesteps | 4112000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4113000, episode_reward=1813.25 +/- 27.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4113000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 7.16     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | 0.0889   |\n",
      "|    learning_rate   | 0.000888 |\n",
      "|    n_updates       | 20080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4114000, episode_reward=1838.33 +/- 53.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4114000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4115000, episode_reward=1850.75 +/- 66.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4115000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 6.76     |\n",
      "|    ent_coef        | 0.0407   |\n",
      "|    ent_coef_loss   | 0.783    |\n",
      "|    learning_rate   | 0.000886 |\n",
      "|    n_updates       | 20090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4116000, episode_reward=1835.30 +/- 20.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4116000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4116     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6127     |\n",
      "|    total_timesteps | 4116000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4117000, episode_reward=1797.75 +/- 52.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4117000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 6.52     |\n",
      "|    ent_coef        | 0.0407   |\n",
      "|    ent_coef_loss   | 0.141    |\n",
      "|    learning_rate   | 0.000884 |\n",
      "|    n_updates       | 20100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4118000, episode_reward=1824.66 +/- 68.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4118000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4119000, episode_reward=1824.72 +/- 65.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4119000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.4     |\n",
      "|    critic_loss     | 6.48     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | 0.164    |\n",
      "|    learning_rate   | 0.000881 |\n",
      "|    n_updates       | 20110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4120000, episode_reward=1821.71 +/- 76.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4120000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4120     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6133     |\n",
      "|    total_timesteps | 4120000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4121000, episode_reward=1790.75 +/- 62.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4121000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 6.78     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.000879 |\n",
      "|    n_updates       | 20120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4122000, episode_reward=1795.86 +/- 20.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4122000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4123000, episode_reward=1843.60 +/- 39.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4123000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 6.15     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | -0.826   |\n",
      "|    learning_rate   | 0.000877 |\n",
      "|    n_updates       | 20130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4124000, episode_reward=1789.68 +/- 74.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4124000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4124     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6139     |\n",
      "|    total_timesteps | 4124000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4125000, episode_reward=1795.99 +/- 41.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4125000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 7.22     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 0.344    |\n",
      "|    learning_rate   | 0.000875 |\n",
      "|    n_updates       | 20140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4126000, episode_reward=1809.13 +/- 55.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4126000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4127000, episode_reward=1859.48 +/- 31.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4127000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.9     |\n",
      "|    critic_loss     | 6.16     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 0.0823   |\n",
      "|    learning_rate   | 0.000873 |\n",
      "|    n_updates       | 20150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4128000, episode_reward=1821.81 +/- 37.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4128000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4128     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6145     |\n",
      "|    total_timesteps | 4128000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4129000, episode_reward=1831.12 +/- 15.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4129000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 6.36     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | -0.666   |\n",
      "|    learning_rate   | 0.000871 |\n",
      "|    n_updates       | 20160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4130000, episode_reward=1850.49 +/- 38.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4130000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4131000, episode_reward=1871.53 +/- 28.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4131000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.9     |\n",
      "|    critic_loss     | 6.35     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | -0.61    |\n",
      "|    learning_rate   | 0.000869 |\n",
      "|    n_updates       | 20170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4132000, episode_reward=1859.98 +/- 24.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4132000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4132     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6153     |\n",
      "|    total_timesteps | 4132000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4133000, episode_reward=1811.26 +/- 43.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4133000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 6.6      |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | -0.235   |\n",
      "|    learning_rate   | 0.000867 |\n",
      "|    n_updates       | 20180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4134000, episode_reward=1793.79 +/- 41.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4134000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4135000, episode_reward=1858.89 +/- 58.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4135000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 6.4      |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -0.574   |\n",
      "|    learning_rate   | 0.000865 |\n",
      "|    n_updates       | 20190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4136000, episode_reward=1881.42 +/- 47.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4136000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4136     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6158     |\n",
      "|    total_timesteps | 4136000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4137000, episode_reward=1770.94 +/- 12.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4137000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.8     |\n",
      "|    critic_loss     | 6.67     |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | -0.687   |\n",
      "|    learning_rate   | 0.000863 |\n",
      "|    n_updates       | 20200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4138000, episode_reward=1784.23 +/- 50.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4138000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4139000, episode_reward=1776.82 +/- 30.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4139000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4140000, episode_reward=1840.52 +/- 34.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.000861 |\n",
      "|    n_updates       | 20210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4140     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6164     |\n",
      "|    total_timesteps | 4140000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4141000, episode_reward=1827.99 +/- 22.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4141000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4142000, episode_reward=1810.53 +/- 25.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4142000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.8     |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.000859 |\n",
      "|    n_updates       | 20220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4143000, episode_reward=1831.69 +/- 58.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4143000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4144000, episode_reward=1829.60 +/- 21.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4144000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13       |\n",
      "|    critic_loss     | 6.55     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.536   |\n",
      "|    learning_rate   | 0.000857 |\n",
      "|    n_updates       | 20230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4144     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6169     |\n",
      "|    total_timesteps | 4144000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4145000, episode_reward=1805.37 +/- 61.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4145000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4146000, episode_reward=1807.48 +/- 44.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4146000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 6.18     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.000855 |\n",
      "|    n_updates       | 20240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4147000, episode_reward=1813.61 +/- 25.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4147000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4148000, episode_reward=1831.21 +/- 27.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4148000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 7.4      |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | -0.857   |\n",
      "|    learning_rate   | 0.000853 |\n",
      "|    n_updates       | 20250    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4148     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6175     |\n",
      "|    total_timesteps | 4148000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4149000, episode_reward=1849.61 +/- 27.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4149000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4150000, episode_reward=1837.62 +/- 31.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.5     |\n",
      "|    critic_loss     | 7.23     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | -0.432   |\n",
      "|    learning_rate   | 0.000851 |\n",
      "|    n_updates       | 20260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4151000, episode_reward=1832.94 +/- 26.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4151000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4152000, episode_reward=1807.99 +/- 26.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4152000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.9     |\n",
      "|    critic_loss     | 6.54     |\n",
      "|    ent_coef        | 0.0388   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.000849 |\n",
      "|    n_updates       | 20270    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4152     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6181     |\n",
      "|    total_timesteps | 4152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4153000, episode_reward=1808.60 +/- 12.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4153000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4154000, episode_reward=1809.78 +/- 35.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4154000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 5.95     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | -0.661   |\n",
      "|    learning_rate   | 0.000847 |\n",
      "|    n_updates       | 20280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4155000, episode_reward=1781.99 +/- 37.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4155000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4156000, episode_reward=1813.53 +/- 17.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4156000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 5.98     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.000845 |\n",
      "|    n_updates       | 20290    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4156     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6186     |\n",
      "|    total_timesteps | 4156000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4157000, episode_reward=1802.03 +/- 45.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4157000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4158000, episode_reward=1860.41 +/- 44.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4158000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 6.56     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | -0.848   |\n",
      "|    learning_rate   | 0.000843 |\n",
      "|    n_updates       | 20300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4159000, episode_reward=1819.16 +/- 28.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4159000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4160000, episode_reward=1817.83 +/- 30.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 6.1      |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -0.168   |\n",
      "|    learning_rate   | 0.000841 |\n",
      "|    n_updates       | 20310    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4160     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6192     |\n",
      "|    total_timesteps | 4160000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4161000, episode_reward=1841.00 +/- 29.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4161000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4162000, episode_reward=1891.90 +/- 47.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4162000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16       |\n",
      "|    critic_loss     | 6.52     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.0628   |\n",
      "|    learning_rate   | 0.000838 |\n",
      "|    n_updates       | 20320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4163000, episode_reward=1860.50 +/- 23.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4163000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4164000, episode_reward=1779.09 +/- 44.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4164000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 7.16     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.252    |\n",
      "|    learning_rate   | 0.000836 |\n",
      "|    n_updates       | 20330    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4164     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6198     |\n",
      "|    total_timesteps | 4164000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4165000, episode_reward=1816.47 +/- 33.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4165000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4166000, episode_reward=1825.57 +/- 40.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4166000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 6.53     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | -0.551   |\n",
      "|    learning_rate   | 0.000834 |\n",
      "|    n_updates       | 20340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4167000, episode_reward=1825.94 +/- 14.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4167000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4168000, episode_reward=1824.55 +/- 22.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4168000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 6.56     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | -0.685   |\n",
      "|    learning_rate   | 0.000832 |\n",
      "|    n_updates       | 20350    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4168     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6204     |\n",
      "|    total_timesteps | 4168000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4169000, episode_reward=1860.05 +/- 26.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4169000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4170000, episode_reward=1798.04 +/- 22.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 6.73     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | -0.247   |\n",
      "|    learning_rate   | 0.00083  |\n",
      "|    n_updates       | 20360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4171000, episode_reward=1814.61 +/- 37.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4171000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4172000, episode_reward=1836.82 +/- 10.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4172000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 6.8      |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -0.47    |\n",
      "|    learning_rate   | 0.000828 |\n",
      "|    n_updates       | 20370    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4172     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6209     |\n",
      "|    total_timesteps | 4172000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4173000, episode_reward=1827.05 +/- 33.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4173000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4174000, episode_reward=1794.34 +/- 35.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4174000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.9     |\n",
      "|    critic_loss     | 6.55     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 0.325    |\n",
      "|    learning_rate   | 0.000826 |\n",
      "|    n_updates       | 20380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4175000, episode_reward=1784.25 +/- 44.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4175000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4176000, episode_reward=1800.52 +/- 39.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4176000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 5.57     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.31     |\n",
      "|    learning_rate   | 0.000824 |\n",
      "|    n_updates       | 20390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4176     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6215     |\n",
      "|    total_timesteps | 4176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4177000, episode_reward=1838.69 +/- 23.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4177000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4178000, episode_reward=1840.87 +/- 17.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4178000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 6.45     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.302    |\n",
      "|    learning_rate   | 0.000822 |\n",
      "|    n_updates       | 20400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4179000, episode_reward=1815.81 +/- 42.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4179000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4180000, episode_reward=1839.25 +/- 19.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 6.93     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | 0.136    |\n",
      "|    learning_rate   | 0.00082  |\n",
      "|    n_updates       | 20410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4180     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6221     |\n",
      "|    total_timesteps | 4180000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4181000, episode_reward=1787.44 +/- 68.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4181000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4182000, episode_reward=1759.95 +/- 38.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4182000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4183000, episode_reward=1806.38 +/- 32.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4183000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.8     |\n",
      "|    critic_loss     | 6.88     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.981    |\n",
      "|    learning_rate   | 0.000818 |\n",
      "|    n_updates       | 20420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4184000, episode_reward=1779.90 +/- 44.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4184000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4184     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6226     |\n",
      "|    total_timesteps | 4184000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4185000, episode_reward=1817.15 +/- 55.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4185000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.7     |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | -0.00844 |\n",
      "|    learning_rate   | 0.000816 |\n",
      "|    n_updates       | 20430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4186000, episode_reward=1848.14 +/- 10.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4186000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4187000, episode_reward=1824.90 +/- 11.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4187000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.3     |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.927    |\n",
      "|    learning_rate   | 0.000814 |\n",
      "|    n_updates       | 20440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4188000, episode_reward=1807.05 +/- 31.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4188000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4188     |\n",
      "|    fps             | 671      |\n",
      "|    time_elapsed    | 6232     |\n",
      "|    total_timesteps | 4188000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4189000, episode_reward=1859.42 +/- 28.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4189000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 7.32     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.000812 |\n",
      "|    n_updates       | 20450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4190000, episode_reward=1821.60 +/- 43.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4190000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4191000, episode_reward=1816.79 +/- 25.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4191000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 6.52     |\n",
      "|    ent_coef        | 0.0388   |\n",
      "|    ent_coef_loss   | 0.828    |\n",
      "|    learning_rate   | 0.00081  |\n",
      "|    n_updates       | 20460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4192000, episode_reward=1839.58 +/- 23.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4192000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4192     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6237     |\n",
      "|    total_timesteps | 4192000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4193000, episode_reward=1840.89 +/- 26.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4193000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.2     |\n",
      "|    critic_loss     | 6.37     |\n",
      "|    ent_coef        | 0.0391   |\n",
      "|    ent_coef_loss   | 0.973    |\n",
      "|    learning_rate   | 0.000808 |\n",
      "|    n_updates       | 20470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4194000, episode_reward=1801.76 +/- 34.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4194000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4195000, episode_reward=1785.74 +/- 61.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4195000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.5     |\n",
      "|    critic_loss     | 6.28     |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.953    |\n",
      "|    learning_rate   | 0.000806 |\n",
      "|    n_updates       | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4196000, episode_reward=1852.12 +/- 87.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4196000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4196     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6243     |\n",
      "|    total_timesteps | 4196000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4197000, episode_reward=1849.87 +/- 47.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4197000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 7.09     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | 1.39     |\n",
      "|    learning_rate   | 0.000804 |\n",
      "|    n_updates       | 20490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4198000, episode_reward=1816.69 +/- 43.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4198000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4199000, episode_reward=1842.29 +/- 17.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4199000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 5.97     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | -0.108   |\n",
      "|    learning_rate   | 0.000802 |\n",
      "|    n_updates       | 20500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200000, episode_reward=1857.28 +/- 52.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4200000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4200     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6249     |\n",
      "|    total_timesteps | 4200000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4201000, episode_reward=1841.32 +/- 35.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4201000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.8     |\n",
      "|    critic_loss     | 7.01     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.0626   |\n",
      "|    learning_rate   | 0.0008   |\n",
      "|    n_updates       | 20510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4202000, episode_reward=1844.64 +/- 46.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4202000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4203000, episode_reward=1849.51 +/- 18.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4203000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 6.01     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.233   |\n",
      "|    learning_rate   | 0.000798 |\n",
      "|    n_updates       | 20520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4204000, episode_reward=1838.75 +/- 28.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4204000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4204     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6254     |\n",
      "|    total_timesteps | 4204000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4205000, episode_reward=1925.08 +/- 51.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4205000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.8     |\n",
      "|    critic_loss     | 6.21     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.26    |\n",
      "|    learning_rate   | 0.000795 |\n",
      "|    n_updates       | 20530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4206000, episode_reward=1891.99 +/- 20.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4206000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4207000, episode_reward=1864.25 +/- 32.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4207000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.3     |\n",
      "|    critic_loss     | 6.79     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.000793 |\n",
      "|    n_updates       | 20540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4208000, episode_reward=1842.07 +/- 49.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4208000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4208     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6260     |\n",
      "|    total_timesteps | 4208000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4209000, episode_reward=1780.91 +/- 39.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4209000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14       |\n",
      "|    critic_loss     | 6.66     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | -0.341   |\n",
      "|    learning_rate   | 0.000791 |\n",
      "|    n_updates       | 20550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4210000, episode_reward=1779.88 +/- 35.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4210000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4211000, episode_reward=1778.84 +/- 15.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4211000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 5.95     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.289    |\n",
      "|    learning_rate   | 0.000789 |\n",
      "|    n_updates       | 20560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4212000, episode_reward=1781.22 +/- 45.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4212000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4212     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6265     |\n",
      "|    total_timesteps | 4212000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4213000, episode_reward=1770.09 +/- 17.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4213000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | -0.831   |\n",
      "|    learning_rate   | 0.000787 |\n",
      "|    n_updates       | 20570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4214000, episode_reward=1760.63 +/- 31.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4214000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4215000, episode_reward=1787.13 +/- 34.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4215000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15       |\n",
      "|    critic_loss     | 5.99     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | -0.639   |\n",
      "|    learning_rate   | 0.000785 |\n",
      "|    n_updates       | 20580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4216000, episode_reward=1804.71 +/- 47.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4216000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4216     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6271     |\n",
      "|    total_timesteps | 4216000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4217000, episode_reward=1798.31 +/- 32.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4217000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 6.57     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | 0.165    |\n",
      "|    learning_rate   | 0.000783 |\n",
      "|    n_updates       | 20590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4218000, episode_reward=1786.68 +/- 23.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4218000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4219000, episode_reward=1782.55 +/- 38.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4219000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.4     |\n",
      "|    critic_loss     | 6.71     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | 0.414    |\n",
      "|    learning_rate   | 0.000781 |\n",
      "|    n_updates       | 20600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4220000, episode_reward=1777.11 +/- 34.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4220000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4220     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6276     |\n",
      "|    total_timesteps | 4220000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4221000, episode_reward=1766.57 +/- 32.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4221000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 5.63     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.317   |\n",
      "|    learning_rate   | 0.000779 |\n",
      "|    n_updates       | 20610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4222000, episode_reward=1777.99 +/- 49.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4222000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4223000, episode_reward=1861.57 +/- 73.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4223000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.2     |\n",
      "|    critic_loss     | 6.11     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.276   |\n",
      "|    learning_rate   | 0.000777 |\n",
      "|    n_updates       | 20620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4224000, episode_reward=1831.61 +/- 62.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4224000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4224     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6282     |\n",
      "|    total_timesteps | 4224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4225000, episode_reward=1881.15 +/- 57.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4225000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4226000, episode_reward=1781.45 +/- 47.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4226000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.1     |\n",
      "|    critic_loss     | 6.97     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | 0.817    |\n",
      "|    learning_rate   | 0.000775 |\n",
      "|    n_updates       | 20630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4227000, episode_reward=1812.35 +/- 87.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4227000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4228000, episode_reward=1829.96 +/- 55.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4228000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.1     |\n",
      "|    critic_loss     | 7.23     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.000773 |\n",
      "|    n_updates       | 20640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4228     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6288     |\n",
      "|    total_timesteps | 4228000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4229000, episode_reward=1789.86 +/- 49.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4229000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4230000, episode_reward=1815.39 +/- 27.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.9     |\n",
      "|    critic_loss     | 7.06     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.147    |\n",
      "|    learning_rate   | 0.000771 |\n",
      "|    n_updates       | 20650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4231000, episode_reward=1776.61 +/- 19.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4231000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4232000, episode_reward=1767.06 +/- 44.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4232000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.5     |\n",
      "|    critic_loss     | 6.91     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.000769 |\n",
      "|    n_updates       | 20660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4232     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6293     |\n",
      "|    total_timesteps | 4232000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4233000, episode_reward=1806.92 +/- 27.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4233000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4234000, episode_reward=1783.20 +/- 26.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4234000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 6.78     |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | 0.44     |\n",
      "|    learning_rate   | 0.000767 |\n",
      "|    n_updates       | 20670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4235000, episode_reward=1775.59 +/- 39.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4235000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4236000, episode_reward=1822.27 +/- 35.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4236000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.8     |\n",
      "|    critic_loss     | 7.04     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.609    |\n",
      "|    learning_rate   | 0.000765 |\n",
      "|    n_updates       | 20680    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4236     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6299     |\n",
      "|    total_timesteps | 4236000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4237000, episode_reward=1789.30 +/- 22.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4237000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4238000, episode_reward=1848.72 +/- 54.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4238000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 6.47     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | 0.0409   |\n",
      "|    learning_rate   | 0.000763 |\n",
      "|    n_updates       | 20690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4239000, episode_reward=1858.51 +/- 84.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4239000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4240000, episode_reward=1856.18 +/- 57.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 7.02     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | 0.309    |\n",
      "|    learning_rate   | 0.000761 |\n",
      "|    n_updates       | 20700    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4240     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6305     |\n",
      "|    total_timesteps | 4240000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4241000, episode_reward=1847.24 +/- 66.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4241000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4242000, episode_reward=1866.83 +/- 23.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4242000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.3     |\n",
      "|    critic_loss     | 6.26     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | -0.158   |\n",
      "|    learning_rate   | 0.000759 |\n",
      "|    n_updates       | 20710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4243000, episode_reward=1876.06 +/- 62.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4243000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4244000, episode_reward=1857.13 +/- 61.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4244000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.6     |\n",
      "|    critic_loss     | 6.68     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | 0.519    |\n",
      "|    learning_rate   | 0.000757 |\n",
      "|    n_updates       | 20720    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4244     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6310     |\n",
      "|    total_timesteps | 4244000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4245000, episode_reward=1805.63 +/- 16.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4245000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4246000, episode_reward=1818.23 +/- 60.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4246000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.2     |\n",
      "|    critic_loss     | 6.91     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.000754 |\n",
      "|    n_updates       | 20730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4247000, episode_reward=1824.15 +/- 52.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4247000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4248000, episode_reward=1841.10 +/- 44.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4248000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.6     |\n",
      "|    critic_loss     | 6.35     |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | 0.226    |\n",
      "|    learning_rate   | 0.000752 |\n",
      "|    n_updates       | 20740    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4248     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6316     |\n",
      "|    total_timesteps | 4248000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4249000, episode_reward=1833.53 +/- 53.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4249000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4250000, episode_reward=1807.46 +/- 26.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 6.64     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | -0.0556  |\n",
      "|    learning_rate   | 0.00075  |\n",
      "|    n_updates       | 20750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4251000, episode_reward=1809.46 +/- 54.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4251000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4252000, episode_reward=1785.14 +/- 47.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4252000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16       |\n",
      "|    critic_loss     | 6.35     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | 0.279    |\n",
      "|    learning_rate   | 0.000748 |\n",
      "|    n_updates       | 20760    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4252     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6321     |\n",
      "|    total_timesteps | 4252000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4253000, episode_reward=1822.94 +/- 29.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4253000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4254000, episode_reward=1813.85 +/- 46.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4254000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.5     |\n",
      "|    critic_loss     | 6.82     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.000746 |\n",
      "|    n_updates       | 20770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4255000, episode_reward=1805.01 +/- 42.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4255000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4256000, episode_reward=1888.56 +/- 46.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4256000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.3     |\n",
      "|    critic_loss     | 5.9      |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | -1.32    |\n",
      "|    learning_rate   | 0.000744 |\n",
      "|    n_updates       | 20780    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4256     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6327     |\n",
      "|    total_timesteps | 4256000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4257000, episode_reward=1883.07 +/- 24.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4257000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4258000, episode_reward=1864.67 +/- 44.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4258000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 5.77     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | -2.12    |\n",
      "|    learning_rate   | 0.000742 |\n",
      "|    n_updates       | 20790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4259000, episode_reward=1841.20 +/- 12.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4259000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4260000, episode_reward=1873.01 +/- 34.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 5.96     |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | -1.95    |\n",
      "|    learning_rate   | 0.00074  |\n",
      "|    n_updates       | 20800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4260     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6332     |\n",
      "|    total_timesteps | 4260000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4261000, episode_reward=1865.58 +/- 70.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4261000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4262000, episode_reward=1805.14 +/- 71.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4262000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16       |\n",
      "|    critic_loss     | 6.36     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.294    |\n",
      "|    learning_rate   | 0.000738 |\n",
      "|    n_updates       | 20810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4263000, episode_reward=1830.69 +/- 41.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4263000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4264000, episode_reward=1784.24 +/- 49.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4264000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 6.67     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | 0.0314   |\n",
      "|    learning_rate   | 0.000736 |\n",
      "|    n_updates       | 20820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4264     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6338     |\n",
      "|    total_timesteps | 4264000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4265000, episode_reward=1784.75 +/- 45.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4265000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4266000, episode_reward=1844.73 +/- 68.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4266000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 6.55     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.812   |\n",
      "|    learning_rate   | 0.000734 |\n",
      "|    n_updates       | 20830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4267000, episode_reward=1864.12 +/- 54.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4267000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4268000, episode_reward=1844.48 +/- 58.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4268000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4268     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6343     |\n",
      "|    total_timesteps | 4268000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4269000, episode_reward=1828.40 +/- 32.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4269000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 6.69     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | 0.663    |\n",
      "|    learning_rate   | 0.000732 |\n",
      "|    n_updates       | 20840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4270000, episode_reward=1777.88 +/- 25.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4270000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4271000, episode_reward=1826.76 +/- 39.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4271000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 7.16     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | 0.803    |\n",
      "|    learning_rate   | 0.00073  |\n",
      "|    n_updates       | 20850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4272000, episode_reward=1843.57 +/- 30.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4272000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4272     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6349     |\n",
      "|    total_timesteps | 4272000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4273000, episode_reward=1808.77 +/- 43.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4273000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 6.9      |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.988    |\n",
      "|    learning_rate   | 0.000728 |\n",
      "|    n_updates       | 20860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4274000, episode_reward=1767.11 +/- 57.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4274000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4275000, episode_reward=1822.28 +/- 59.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4275000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 6.76     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | 1.31     |\n",
      "|    learning_rate   | 0.000726 |\n",
      "|    n_updates       | 20870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4276000, episode_reward=1830.11 +/- 54.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4276000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4276     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6354     |\n",
      "|    total_timesteps | 4276000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4277000, episode_reward=1802.17 +/- 62.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4277000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.6     |\n",
      "|    critic_loss     | 6.76     |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | 0.263    |\n",
      "|    learning_rate   | 0.000724 |\n",
      "|    n_updates       | 20880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4278000, episode_reward=1798.01 +/- 48.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4278000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4279000, episode_reward=1764.79 +/- 45.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4279000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 6.81     |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | -0.138   |\n",
      "|    learning_rate   | 0.000722 |\n",
      "|    n_updates       | 20890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4280000, episode_reward=1763.18 +/- 30.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4280000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4280     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6360     |\n",
      "|    total_timesteps | 4280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4281000, episode_reward=1741.00 +/- 41.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4281000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 7.25     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.0663   |\n",
      "|    learning_rate   | 0.00072  |\n",
      "|    n_updates       | 20900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4282000, episode_reward=1730.64 +/- 33.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4282000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4283000, episode_reward=1847.08 +/- 39.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4283000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 6.28     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -0.661   |\n",
      "|    learning_rate   | 0.000718 |\n",
      "|    n_updates       | 20910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4284000, episode_reward=1802.68 +/- 15.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4284000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4284     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6366     |\n",
      "|    total_timesteps | 4284000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4285000, episode_reward=1837.58 +/- 30.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4285000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 6.5      |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | -0.493   |\n",
      "|    learning_rate   | 0.000716 |\n",
      "|    n_updates       | 20920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4286000, episode_reward=1831.65 +/- 61.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4286000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4287000, episode_reward=1851.11 +/- 14.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4287000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 6.99     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | -0.481   |\n",
      "|    learning_rate   | 0.000714 |\n",
      "|    n_updates       | 20930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4288000, episode_reward=1849.55 +/- 43.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4288000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4288     |\n",
      "|    fps             | 672      |\n",
      "|    time_elapsed    | 6371     |\n",
      "|    total_timesteps | 4288000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4289000, episode_reward=1782.63 +/- 52.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4289000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.7     |\n",
      "|    critic_loss     | 6.88     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | -0.16    |\n",
      "|    learning_rate   | 0.000711 |\n",
      "|    n_updates       | 20940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4290000, episode_reward=1800.58 +/- 42.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4290000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4291000, episode_reward=1886.89 +/- 23.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4291000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 6.36     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.753   |\n",
      "|    learning_rate   | 0.000709 |\n",
      "|    n_updates       | 20950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4292000, episode_reward=1811.14 +/- 39.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4292000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4292     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6377     |\n",
      "|    total_timesteps | 4292000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4293000, episode_reward=1881.71 +/- 55.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4293000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.3     |\n",
      "|    critic_loss     | 6.75     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.000707 |\n",
      "|    n_updates       | 20960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4294000, episode_reward=1869.26 +/- 53.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4294000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4295000, episode_reward=1773.76 +/- 48.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4295000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | -0.999   |\n",
      "|    learning_rate   | 0.000705 |\n",
      "|    n_updates       | 20970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4296000, episode_reward=1803.82 +/- 32.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4296000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4296     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6382     |\n",
      "|    total_timesteps | 4296000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4297000, episode_reward=1869.65 +/- 64.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4297000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19       |\n",
      "|    critic_loss     | 6.26     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -0.914   |\n",
      "|    learning_rate   | 0.000703 |\n",
      "|    n_updates       | 20980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4298000, episode_reward=1863.26 +/- 37.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4298000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4299000, episode_reward=1812.76 +/- 34.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4299000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.8     |\n",
      "|    critic_loss     | 6.27     |\n",
      "|    ent_coef        | 0.0391   |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.000701 |\n",
      "|    n_updates       | 20990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4300000, episode_reward=1802.57 +/- 40.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4300000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4300     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6388     |\n",
      "|    total_timesteps | 4300000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4301000, episode_reward=1810.46 +/- 92.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4301000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 6.53     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | -0.677   |\n",
      "|    learning_rate   | 0.000699 |\n",
      "|    n_updates       | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4302000, episode_reward=1861.92 +/- 47.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4302000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4303000, episode_reward=1891.04 +/- 67.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4303000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 6.74     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.163   |\n",
      "|    learning_rate   | 0.000697 |\n",
      "|    n_updates       | 21010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4304000, episode_reward=1876.97 +/- 12.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4304000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4304     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6393     |\n",
      "|    total_timesteps | 4304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4305000, episode_reward=1840.18 +/- 42.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4305000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 6.03     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | -0.809   |\n",
      "|    learning_rate   | 0.000695 |\n",
      "|    n_updates       | 21020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4306000, episode_reward=1829.53 +/- 35.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4306000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4307000, episode_reward=1938.05 +/- 38.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4307000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 6.12     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -0.742   |\n",
      "|    learning_rate   | 0.000693 |\n",
      "|    n_updates       | 21030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4308000, episode_reward=1932.68 +/- 55.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4308000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4308     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6399     |\n",
      "|    total_timesteps | 4308000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4309000, episode_reward=1819.83 +/- 38.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4309000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.8     |\n",
      "|    critic_loss     | 6.58     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | -0.465   |\n",
      "|    learning_rate   | 0.000691 |\n",
      "|    n_updates       | 21040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4310000, episode_reward=1808.23 +/- 59.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4310000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4311000, episode_reward=1832.68 +/- 40.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4311000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4312000, episode_reward=1783.01 +/- 43.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4312000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 6.11     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | -0.0617  |\n",
      "|    learning_rate   | 0.000689 |\n",
      "|    n_updates       | 21050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4312     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6405     |\n",
      "|    total_timesteps | 4312000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4313000, episode_reward=1850.12 +/- 44.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4313000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4314000, episode_reward=1833.60 +/- 43.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4314000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 6.65     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.177    |\n",
      "|    learning_rate   | 0.000687 |\n",
      "|    n_updates       | 21060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4315000, episode_reward=1787.06 +/- 29.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4315000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4316000, episode_reward=1815.75 +/- 55.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4316000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 6.52     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.479    |\n",
      "|    learning_rate   | 0.000685 |\n",
      "|    n_updates       | 21070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4316     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6410     |\n",
      "|    total_timesteps | 4316000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4317000, episode_reward=1827.82 +/- 84.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4317000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4318000, episode_reward=1792.70 +/- 27.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4318000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.9     |\n",
      "|    critic_loss     | 6.9      |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.387    |\n",
      "|    learning_rate   | 0.000683 |\n",
      "|    n_updates       | 21080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4319000, episode_reward=1833.32 +/- 86.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4319000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320000, episode_reward=1788.61 +/- 58.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 6.34     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | -0.332   |\n",
      "|    learning_rate   | 0.000681 |\n",
      "|    n_updates       | 21090    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4320     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6416     |\n",
      "|    total_timesteps | 4320000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4321000, episode_reward=1811.38 +/- 21.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4321000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4322000, episode_reward=1820.66 +/- 22.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4322000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 6.11     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | -0.178   |\n",
      "|    learning_rate   | 0.000679 |\n",
      "|    n_updates       | 21100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4323000, episode_reward=1843.55 +/- 50.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4323000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4324000, episode_reward=1875.14 +/- 34.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4324000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.4     |\n",
      "|    critic_loss     | 7        |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | 0.673    |\n",
      "|    learning_rate   | 0.000677 |\n",
      "|    n_updates       | 21110    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4324     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6422     |\n",
      "|    total_timesteps | 4324000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4325000, episode_reward=1869.12 +/- 17.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4325000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4326000, episode_reward=1868.22 +/- 48.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4326000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 6.2      |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.442    |\n",
      "|    learning_rate   | 0.000675 |\n",
      "|    n_updates       | 21120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4327000, episode_reward=1874.97 +/- 43.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4327000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4328000, episode_reward=1872.56 +/- 62.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4328000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 6        |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 0.0959   |\n",
      "|    learning_rate   | 0.000673 |\n",
      "|    n_updates       | 21130    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4328     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6428     |\n",
      "|    total_timesteps | 4328000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4329000, episode_reward=1918.15 +/- 16.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4329000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4330000, episode_reward=1868.83 +/- 28.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 6.54     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 0.291    |\n",
      "|    learning_rate   | 0.000671 |\n",
      "|    n_updates       | 21140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4331000, episode_reward=1820.90 +/- 70.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4331000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4332000, episode_reward=1794.19 +/- 48.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4332000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 6.36     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | -0.44    |\n",
      "|    learning_rate   | 0.000668 |\n",
      "|    n_updates       | 21150    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4332     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6434     |\n",
      "|    total_timesteps | 4332000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4333000, episode_reward=1834.63 +/- 45.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4333000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4334000, episode_reward=1828.99 +/- 69.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4334000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 5.66     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -0.248   |\n",
      "|    learning_rate   | 0.000666 |\n",
      "|    n_updates       | 21160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4335000, episode_reward=1861.33 +/- 47.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4335000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4336000, episode_reward=1870.48 +/- 45.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4336000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 6.41     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -0.524   |\n",
      "|    learning_rate   | 0.000664 |\n",
      "|    n_updates       | 21170    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4336     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6439     |\n",
      "|    total_timesteps | 4336000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4337000, episode_reward=1888.65 +/- 67.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4337000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4338000, episode_reward=1841.44 +/- 48.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4338000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 6.35     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.000662 |\n",
      "|    n_updates       | 21180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4339000, episode_reward=1821.99 +/- 73.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4339000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4340000, episode_reward=1843.41 +/- 100.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 6.29     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | -0.681   |\n",
      "|    learning_rate   | 0.00066  |\n",
      "|    n_updates       | 21190    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4340     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6445     |\n",
      "|    total_timesteps | 4340000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4341000, episode_reward=1889.12 +/- 84.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4341000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4342000, episode_reward=1904.64 +/- 60.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4342000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 7.03     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.0705   |\n",
      "|    learning_rate   | 0.000658 |\n",
      "|    n_updates       | 21200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4343000, episode_reward=1900.73 +/- 41.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4343000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4344000, episode_reward=1912.25 +/- 55.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4344000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.1     |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | 0.0213   |\n",
      "|    learning_rate   | 0.000656 |\n",
      "|    n_updates       | 21210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4344     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6451     |\n",
      "|    total_timesteps | 4344000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4345000, episode_reward=1892.33 +/- 52.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4345000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4346000, episode_reward=1880.29 +/- 92.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4346000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 6.6      |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | -0.253   |\n",
      "|    learning_rate   | 0.000654 |\n",
      "|    n_updates       | 21220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4347000, episode_reward=1859.09 +/- 21.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4347000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4348000, episode_reward=1840.79 +/- 26.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4348000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 7.08     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | -0.21    |\n",
      "|    learning_rate   | 0.000652 |\n",
      "|    n_updates       | 21230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4348     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6458     |\n",
      "|    total_timesteps | 4348000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4349000, episode_reward=1891.00 +/- 45.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4349000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4350000, episode_reward=1915.22 +/- 26.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 6.65     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -0.799   |\n",
      "|    learning_rate   | 0.00065  |\n",
      "|    n_updates       | 21240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4351000, episode_reward=1895.58 +/- 13.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4351000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4352000, episode_reward=1884.93 +/- 37.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4352000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4352     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6463     |\n",
      "|    total_timesteps | 4352000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4353000, episode_reward=1940.80 +/- 51.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4353000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.4     |\n",
      "|    critic_loss     | 6.14     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | -0.588   |\n",
      "|    learning_rate   | 0.000648 |\n",
      "|    n_updates       | 21250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4354000, episode_reward=1962.06 +/- 28.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4354000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4355000, episode_reward=1878.18 +/- 47.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4355000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.5     |\n",
      "|    critic_loss     | 6.12     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | 0.0217   |\n",
      "|    learning_rate   | 0.000646 |\n",
      "|    n_updates       | 21260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4356000, episode_reward=1919.20 +/- 39.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4356000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4356     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6469     |\n",
      "|    total_timesteps | 4356000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4357000, episode_reward=1973.12 +/- 45.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4357000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 6.86     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | 0.207    |\n",
      "|    learning_rate   | 0.000644 |\n",
      "|    n_updates       | 21270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4358000, episode_reward=1976.68 +/- 37.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4358000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4359000, episode_reward=1962.31 +/- 38.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4359000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17       |\n",
      "|    critic_loss     | 6.34     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | -0.611   |\n",
      "|    learning_rate   | 0.000642 |\n",
      "|    n_updates       | 21280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4360000, episode_reward=1974.01 +/- 29.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4360000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4360     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6474     |\n",
      "|    total_timesteps | 4360000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4361000, episode_reward=1947.96 +/- 32.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4361000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.5     |\n",
      "|    critic_loss     | 6.41     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | -1.31    |\n",
      "|    learning_rate   | 0.00064  |\n",
      "|    n_updates       | 21290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4362000, episode_reward=1974.25 +/- 25.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4362000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4363000, episode_reward=1971.33 +/- 29.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4363000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 6.35     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.56    |\n",
      "|    learning_rate   | 0.000638 |\n",
      "|    n_updates       | 21300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4364000, episode_reward=1977.40 +/- 80.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4364000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4364     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6480     |\n",
      "|    total_timesteps | 4364000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4365000, episode_reward=1866.14 +/- 64.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4365000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.2     |\n",
      "|    critic_loss     | 7.3      |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.000636 |\n",
      "|    n_updates       | 21310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4366000, episode_reward=1919.13 +/- 19.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4366000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4367000, episode_reward=1922.62 +/- 55.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4367000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.7     |\n",
      "|    critic_loss     | 6.12     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 0.181    |\n",
      "|    learning_rate   | 0.000634 |\n",
      "|    n_updates       | 21320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4368000, episode_reward=1967.65 +/- 66.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4368000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4368     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6486     |\n",
      "|    total_timesteps | 4368000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4369000, episode_reward=1935.96 +/- 23.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4369000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 6.25     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | 0.281    |\n",
      "|    learning_rate   | 0.000632 |\n",
      "|    n_updates       | 21330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4370000, episode_reward=1955.73 +/- 32.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4370000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4371000, episode_reward=1879.58 +/- 35.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4371000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 5.93     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | -0.149   |\n",
      "|    learning_rate   | 0.00063  |\n",
      "|    n_updates       | 21340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4372000, episode_reward=1896.34 +/- 61.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4372000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4372     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6491     |\n",
      "|    total_timesteps | 4372000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4373000, episode_reward=1886.05 +/- 52.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4373000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.7     |\n",
      "|    critic_loss     | 5.6      |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | -0.143   |\n",
      "|    learning_rate   | 0.000628 |\n",
      "|    n_updates       | 21350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4374000, episode_reward=1910.80 +/- 43.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4374000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4375000, episode_reward=1809.10 +/- 23.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4375000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 6.31     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | 0.0375   |\n",
      "|    learning_rate   | 0.000625 |\n",
      "|    n_updates       | 21360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4376000, episode_reward=1850.63 +/- 64.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4376000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4376     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6497     |\n",
      "|    total_timesteps | 4376000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4377000, episode_reward=1905.90 +/- 34.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4377000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 5.99     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.551   |\n",
      "|    learning_rate   | 0.000623 |\n",
      "|    n_updates       | 21370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4378000, episode_reward=1919.04 +/- 49.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4378000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4379000, episode_reward=1860.87 +/- 53.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4379000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.7     |\n",
      "|    critic_loss     | 6.03     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.596   |\n",
      "|    learning_rate   | 0.000621 |\n",
      "|    n_updates       | 21380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380000, episode_reward=1853.37 +/- 39.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4380000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4380     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6502     |\n",
      "|    total_timesteps | 4380000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4381000, episode_reward=1940.93 +/- 30.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4381000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.2     |\n",
      "|    critic_loss     | 6.38     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -0.073   |\n",
      "|    learning_rate   | 0.000619 |\n",
      "|    n_updates       | 21390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4382000, episode_reward=1880.03 +/- 75.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4382000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4383000, episode_reward=1882.00 +/- 56.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4383000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 6.5      |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -0.102   |\n",
      "|    learning_rate   | 0.000617 |\n",
      "|    n_updates       | 21400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4384000, episode_reward=1918.48 +/- 48.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4384000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4384     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6508     |\n",
      "|    total_timesteps | 4384000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4385000, episode_reward=1852.33 +/- 52.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4385000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 5.92     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.0404   |\n",
      "|    learning_rate   | 0.000615 |\n",
      "|    n_updates       | 21410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4386000, episode_reward=1851.81 +/- 37.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4386000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4387000, episode_reward=1880.60 +/- 33.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4387000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.9     |\n",
      "|    critic_loss     | 6.21     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.0224  |\n",
      "|    learning_rate   | 0.000613 |\n",
      "|    n_updates       | 21420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4388000, episode_reward=1848.11 +/- 70.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4388000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4388     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6514     |\n",
      "|    total_timesteps | 4388000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4389000, episode_reward=1894.59 +/- 50.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4389000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.2     |\n",
      "|    critic_loss     | 5.84     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.606   |\n",
      "|    learning_rate   | 0.000611 |\n",
      "|    n_updates       | 21430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4390000, episode_reward=1917.32 +/- 32.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4390000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4391000, episode_reward=1893.60 +/- 84.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4391000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 6.32     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.343    |\n",
      "|    learning_rate   | 0.000609 |\n",
      "|    n_updates       | 21440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4392000, episode_reward=1909.59 +/- 61.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4392000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4392     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6519     |\n",
      "|    total_timesteps | 4392000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4393000, episode_reward=1833.72 +/- 63.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4393000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 6.16     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.107    |\n",
      "|    learning_rate   | 0.000607 |\n",
      "|    n_updates       | 21450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4394000, episode_reward=1804.33 +/- 93.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4394000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4395000, episode_reward=1911.51 +/- 33.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4395000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4396000, episode_reward=1796.12 +/- 79.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4396000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 6.6      |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.684    |\n",
      "|    learning_rate   | 0.000605 |\n",
      "|    n_updates       | 21460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4396     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6525     |\n",
      "|    total_timesteps | 4396000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4397000, episode_reward=1796.99 +/- 41.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4397000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4398000, episode_reward=1865.58 +/- 46.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4398000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.3     |\n",
      "|    critic_loss     | 6.37     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -0.259   |\n",
      "|    learning_rate   | 0.000603 |\n",
      "|    n_updates       | 21470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4399000, episode_reward=1882.84 +/- 52.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4399000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4400000, episode_reward=1838.96 +/- 87.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.9     |\n",
      "|    critic_loss     | 6.5      |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -0.308   |\n",
      "|    learning_rate   | 0.000601 |\n",
      "|    n_updates       | 21480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4400     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6530     |\n",
      "|    total_timesteps | 4400000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4401000, episode_reward=1860.86 +/- 31.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4401000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4402000, episode_reward=1854.85 +/- 39.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4402000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 6.62     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.0342  |\n",
      "|    learning_rate   | 0.000599 |\n",
      "|    n_updates       | 21490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4403000, episode_reward=1878.39 +/- 44.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4403000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4404000, episode_reward=1881.72 +/- 60.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4404000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.9     |\n",
      "|    critic_loss     | 6.32     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.64    |\n",
      "|    learning_rate   | 0.000597 |\n",
      "|    n_updates       | 21500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4404     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6536     |\n",
      "|    total_timesteps | 4404000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4405000, episode_reward=1906.40 +/- 79.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4405000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4406000, episode_reward=1957.91 +/- 83.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4406000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 6.46     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | -0.677   |\n",
      "|    learning_rate   | 0.000595 |\n",
      "|    n_updates       | 21510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4407000, episode_reward=1957.99 +/- 38.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4407000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4408000, episode_reward=1952.81 +/- 62.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4408000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.3     |\n",
      "|    critic_loss     | 7.59     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | 0.832    |\n",
      "|    learning_rate   | 0.000593 |\n",
      "|    n_updates       | 21520    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4408     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6541     |\n",
      "|    total_timesteps | 4408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4409000, episode_reward=1945.16 +/- 51.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4409000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410000, episode_reward=1908.09 +/- 34.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 6.88     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.483    |\n",
      "|    learning_rate   | 0.000591 |\n",
      "|    n_updates       | 21530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4411000, episode_reward=1939.99 +/- 33.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4411000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4412000, episode_reward=1897.46 +/- 28.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4412000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19       |\n",
      "|    critic_loss     | 6.09     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.208   |\n",
      "|    learning_rate   | 0.000589 |\n",
      "|    n_updates       | 21540    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4412     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6547     |\n",
      "|    total_timesteps | 4412000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4413000, episode_reward=1888.09 +/- 64.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4413000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4414000, episode_reward=1892.43 +/- 15.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4414000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.3     |\n",
      "|    critic_loss     | 6.5      |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.257   |\n",
      "|    learning_rate   | 0.000587 |\n",
      "|    n_updates       | 21550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4415000, episode_reward=1899.45 +/- 34.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4415000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4416000, episode_reward=1939.17 +/- 43.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4416000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.5     |\n",
      "|    critic_loss     | 6.09     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | -0.231   |\n",
      "|    learning_rate   | 0.000585 |\n",
      "|    n_updates       | 21560    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4416     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6552     |\n",
      "|    total_timesteps | 4416000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4417000, episode_reward=1880.86 +/- 41.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4417000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4418000, episode_reward=1968.44 +/- 59.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4418000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.5     |\n",
      "|    critic_loss     | 6.73     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | -0.381   |\n",
      "|    learning_rate   | 0.000582 |\n",
      "|    n_updates       | 21570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4419000, episode_reward=1936.03 +/- 38.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4419000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4420000, episode_reward=1949.19 +/- 39.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 6.91     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | 0.441    |\n",
      "|    learning_rate   | 0.00058  |\n",
      "|    n_updates       | 21580    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4420     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6558     |\n",
      "|    total_timesteps | 4420000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4421000, episode_reward=1911.77 +/- 37.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4421000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4422000, episode_reward=1899.71 +/- 30.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4422000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 6.63     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.253    |\n",
      "|    learning_rate   | 0.000578 |\n",
      "|    n_updates       | 21590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4423000, episode_reward=1937.67 +/- 29.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4423000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4424000, episode_reward=1955.39 +/- 51.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4424000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 6.43     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | -0.355   |\n",
      "|    learning_rate   | 0.000576 |\n",
      "|    n_updates       | 21600    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4424     |\n",
      "|    fps             | 673      |\n",
      "|    time_elapsed    | 6563     |\n",
      "|    total_timesteps | 4424000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4425000, episode_reward=1956.94 +/- 29.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4425000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4426000, episode_reward=1971.50 +/- 74.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4426000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 7.36     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.659    |\n",
      "|    learning_rate   | 0.000574 |\n",
      "|    n_updates       | 21610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4427000, episode_reward=1975.21 +/- 69.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4427000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4428000, episode_reward=1942.35 +/- 57.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4428000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 7.19     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.366    |\n",
      "|    learning_rate   | 0.000572 |\n",
      "|    n_updates       | 21620    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4428     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6569     |\n",
      "|    total_timesteps | 4428000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4429000, episode_reward=1913.32 +/- 50.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4429000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4430000, episode_reward=1881.02 +/- 67.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.9     |\n",
      "|    critic_loss     | 6.99     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.588   |\n",
      "|    learning_rate   | 0.00057  |\n",
      "|    n_updates       | 21630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4431000, episode_reward=1955.50 +/- 51.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4431000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4432000, episode_reward=1879.10 +/- 74.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4432000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.2     |\n",
      "|    critic_loss     | 6.64     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.000568 |\n",
      "|    n_updates       | 21640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4432     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6575     |\n",
      "|    total_timesteps | 4432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4433000, episode_reward=1936.66 +/- 56.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4433000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4434000, episode_reward=1929.09 +/- 24.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4434000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.1     |\n",
      "|    critic_loss     | 6.7      |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | -0.922   |\n",
      "|    learning_rate   | 0.000566 |\n",
      "|    n_updates       | 21650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4435000, episode_reward=1969.63 +/- 62.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4435000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4436000, episode_reward=1958.67 +/- 40.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4436000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.5     |\n",
      "|    critic_loss     | 6.87     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 0.133    |\n",
      "|    learning_rate   | 0.000564 |\n",
      "|    n_updates       | 21660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4436     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6580     |\n",
      "|    total_timesteps | 4436000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4437000, episode_reward=1974.45 +/- 96.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4437000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4438000, episode_reward=1924.88 +/- 70.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4438000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4439000, episode_reward=1883.49 +/- 36.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4439000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.1     |\n",
      "|    critic_loss     | 6.99     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 0.104    |\n",
      "|    learning_rate   | 0.000562 |\n",
      "|    n_updates       | 21670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4440000, episode_reward=1989.42 +/- 17.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4440000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4440     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6586     |\n",
      "|    total_timesteps | 4440000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4441000, episode_reward=1983.39 +/- 62.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4441000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.7     |\n",
      "|    critic_loss     | 7.37     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 0.304    |\n",
      "|    learning_rate   | 0.00056  |\n",
      "|    n_updates       | 21680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4442000, episode_reward=1988.11 +/- 38.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4442000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4443000, episode_reward=1921.18 +/- 85.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4443000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.8     |\n",
      "|    critic_loss     | 6.36     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 0.073    |\n",
      "|    learning_rate   | 0.000558 |\n",
      "|    n_updates       | 21690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4444000, episode_reward=1921.91 +/- 53.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4444000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4444     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6591     |\n",
      "|    total_timesteps | 4444000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4445000, episode_reward=1897.11 +/- 53.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4445000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20       |\n",
      "|    critic_loss     | 6.91     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -0.264   |\n",
      "|    learning_rate   | 0.000556 |\n",
      "|    n_updates       | 21700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4446000, episode_reward=1917.21 +/- 49.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4446000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4447000, episode_reward=1878.11 +/- 95.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4447000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 8.3      |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 0.772    |\n",
      "|    learning_rate   | 0.000554 |\n",
      "|    n_updates       | 21710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4448000, episode_reward=1876.33 +/- 64.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4448000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4448     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6597     |\n",
      "|    total_timesteps | 4448000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4449000, episode_reward=1901.74 +/- 81.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4449000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.1     |\n",
      "|    critic_loss     | 7.52     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | -0.535   |\n",
      "|    learning_rate   | 0.000552 |\n",
      "|    n_updates       | 21720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4450000, episode_reward=1884.10 +/- 60.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4450000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4451000, episode_reward=1919.70 +/- 58.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4451000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.9     |\n",
      "|    critic_loss     | 6.88     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.00055  |\n",
      "|    n_updates       | 21730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4452000, episode_reward=1928.97 +/- 77.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4452000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4452     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6602     |\n",
      "|    total_timesteps | 4452000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4453000, episode_reward=1961.78 +/- 67.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4453000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.2     |\n",
      "|    critic_loss     | 6.91     |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | -0.797   |\n",
      "|    learning_rate   | 0.000548 |\n",
      "|    n_updates       | 21740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4454000, episode_reward=1976.32 +/- 51.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4454000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4455000, episode_reward=1929.97 +/- 44.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4455000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.7     |\n",
      "|    critic_loss     | 6.03     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -1.89    |\n",
      "|    learning_rate   | 0.000546 |\n",
      "|    n_updates       | 21750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4456000, episode_reward=1927.38 +/- 48.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4456000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4456     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6608     |\n",
      "|    total_timesteps | 4456000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4457000, episode_reward=1795.17 +/- 50.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4457000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.9     |\n",
      "|    critic_loss     | 7.02     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.583    |\n",
      "|    learning_rate   | 0.000544 |\n",
      "|    n_updates       | 21760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4458000, episode_reward=1812.06 +/- 18.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4458000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4459000, episode_reward=1853.55 +/- 80.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4459000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.9     |\n",
      "|    critic_loss     | 7.17     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.817    |\n",
      "|    learning_rate   | 0.000542 |\n",
      "|    n_updates       | 21770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4460000, episode_reward=1846.80 +/- 21.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4460000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4460     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6613     |\n",
      "|    total_timesteps | 4460000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4461000, episode_reward=1870.27 +/- 68.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4461000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.1     |\n",
      "|    critic_loss     | 6.96     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.616   |\n",
      "|    learning_rate   | 0.000539 |\n",
      "|    n_updates       | 21780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4462000, episode_reward=1928.54 +/- 52.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4462000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4463000, episode_reward=1944.32 +/- 59.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4463000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.4     |\n",
      "|    critic_loss     | 7.18     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.136   |\n",
      "|    learning_rate   | 0.000537 |\n",
      "|    n_updates       | 21790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4464000, episode_reward=1920.72 +/- 53.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4464000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4464     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6619     |\n",
      "|    total_timesteps | 4464000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4465000, episode_reward=1874.22 +/- 72.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4465000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.8     |\n",
      "|    critic_loss     | 7.76     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.227    |\n",
      "|    learning_rate   | 0.000535 |\n",
      "|    n_updates       | 21800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4466000, episode_reward=1921.35 +/- 54.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4466000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4467000, episode_reward=1929.25 +/- 57.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4467000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.5     |\n",
      "|    critic_loss     | 7.41     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.8      |\n",
      "|    learning_rate   | 0.000533 |\n",
      "|    n_updates       | 21810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4468000, episode_reward=1934.01 +/- 67.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4468000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4468     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6624     |\n",
      "|    total_timesteps | 4468000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4469000, episode_reward=1969.78 +/- 19.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4469000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19       |\n",
      "|    critic_loss     | 7.99     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 1.61     |\n",
      "|    learning_rate   | 0.000531 |\n",
      "|    n_updates       | 21820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470000, episode_reward=1948.34 +/- 32.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4470000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4471000, episode_reward=2039.41 +/- 32.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4471000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.8     |\n",
      "|    critic_loss     | 7.12     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | 0.226    |\n",
      "|    learning_rate   | 0.000529 |\n",
      "|    n_updates       | 21830    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4472000, episode_reward=2048.08 +/- 55.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4472000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4472     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6630     |\n",
      "|    total_timesteps | 4472000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4473000, episode_reward=1991.63 +/- 36.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4473000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.2     |\n",
      "|    critic_loss     | 7.15     |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | 0.43     |\n",
      "|    learning_rate   | 0.000527 |\n",
      "|    n_updates       | 21840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4474000, episode_reward=2008.04 +/- 56.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4474000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4475000, episode_reward=2017.56 +/- 82.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4475000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.6     |\n",
      "|    critic_loss     | 7.11     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 0.85     |\n",
      "|    learning_rate   | 0.000525 |\n",
      "|    n_updates       | 21850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4476000, episode_reward=1958.99 +/- 50.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4476000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4476     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6635     |\n",
      "|    total_timesteps | 4476000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4477000, episode_reward=2041.78 +/- 72.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4477000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.2     |\n",
      "|    critic_loss     | 7.65     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 0.58     |\n",
      "|    learning_rate   | 0.000523 |\n",
      "|    n_updates       | 21860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4478000, episode_reward=2024.42 +/- 29.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4478000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4479000, episode_reward=1887.35 +/- 83.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4479000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.4     |\n",
      "|    critic_loss     | 7.94     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.814    |\n",
      "|    learning_rate   | 0.000521 |\n",
      "|    n_updates       | 21870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4480000, episode_reward=2009.76 +/- 38.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4480000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4480     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6641     |\n",
      "|    total_timesteps | 4480000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4481000, episode_reward=1987.19 +/- 36.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4481000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4482000, episode_reward=2013.00 +/- 128.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4482000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.8     |\n",
      "|    critic_loss     | 8.22     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.221    |\n",
      "|    learning_rate   | 0.000519 |\n",
      "|    n_updates       | 21880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4483000, episode_reward=2001.37 +/- 85.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4483000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4484000, episode_reward=2033.21 +/- 51.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4484000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.6     |\n",
      "|    critic_loss     | 8.09     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.255    |\n",
      "|    learning_rate   | 0.000517 |\n",
      "|    n_updates       | 21890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4484     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6646     |\n",
      "|    total_timesteps | 4484000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4485000, episode_reward=2020.40 +/- 75.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4485000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4486000, episode_reward=1889.00 +/- 75.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4486000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.3     |\n",
      "|    critic_loss     | 7.79     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 1.53     |\n",
      "|    learning_rate   | 0.000515 |\n",
      "|    n_updates       | 21900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4487000, episode_reward=1898.57 +/- 39.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4487000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4488000, episode_reward=2016.15 +/- 31.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4488000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.8     |\n",
      "|    critic_loss     | 7.79     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | 0.231    |\n",
      "|    learning_rate   | 0.000513 |\n",
      "|    n_updates       | 21910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4488     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6652     |\n",
      "|    total_timesteps | 4488000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4489000, episode_reward=2035.87 +/- 54.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4489000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4490000, episode_reward=1899.18 +/- 71.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.2     |\n",
      "|    critic_loss     | 7.95     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | 0.538    |\n",
      "|    learning_rate   | 0.000511 |\n",
      "|    n_updates       | 21920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4491000, episode_reward=1913.38 +/- 58.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4491000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4492000, episode_reward=1928.37 +/- 37.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4492000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.3     |\n",
      "|    critic_loss     | 7.82     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | -0.43    |\n",
      "|    learning_rate   | 0.000509 |\n",
      "|    n_updates       | 21930    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4492     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6657     |\n",
      "|    total_timesteps | 4492000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4493000, episode_reward=1964.98 +/- 50.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4493000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4494000, episode_reward=1895.69 +/- 89.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4494000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18.8     |\n",
      "|    critic_loss     | 7.22     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | 0.503    |\n",
      "|    learning_rate   | 0.000507 |\n",
      "|    n_updates       | 21940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4495000, episode_reward=1878.15 +/- 60.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4495000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4496000, episode_reward=1870.31 +/- 39.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4496000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.2     |\n",
      "|    critic_loss     | 7.98     |\n",
      "|    ent_coef        | 0.0377   |\n",
      "|    ent_coef_loss   | 0.574    |\n",
      "|    learning_rate   | 0.000505 |\n",
      "|    n_updates       | 21950    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4496     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6663     |\n",
      "|    total_timesteps | 4496000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4497000, episode_reward=1914.05 +/- 37.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4497000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4498000, episode_reward=1985.53 +/- 57.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4498000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.9     |\n",
      "|    critic_loss     | 7.56     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 0.152    |\n",
      "|    learning_rate   | 0.000503 |\n",
      "|    n_updates       | 21960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4499000, episode_reward=1970.27 +/- 56.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4499000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500000, episode_reward=1858.82 +/- 33.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.7     |\n",
      "|    critic_loss     | 7.87     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 0.618    |\n",
      "|    learning_rate   | 0.000501 |\n",
      "|    n_updates       | 21970    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4500     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6669     |\n",
      "|    total_timesteps | 4500000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4501000, episode_reward=1875.30 +/- 41.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4501000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4502000, episode_reward=1961.07 +/- 73.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4502000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.2     |\n",
      "|    critic_loss     | 6.96     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -0.229   |\n",
      "|    learning_rate   | 0.000498 |\n",
      "|    n_updates       | 21980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4503000, episode_reward=1961.77 +/- 20.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4503000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4504000, episode_reward=1869.53 +/- 52.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4504000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21       |\n",
      "|    critic_loss     | 7.89     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.463    |\n",
      "|    learning_rate   | 0.000496 |\n",
      "|    n_updates       | 21990    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4504     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6674     |\n",
      "|    total_timesteps | 4504000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4505000, episode_reward=1919.85 +/- 46.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4505000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4506000, episode_reward=1939.88 +/- 50.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4506000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.6     |\n",
      "|    critic_loss     | 7.76     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | -0.392   |\n",
      "|    learning_rate   | 0.000494 |\n",
      "|    n_updates       | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4507000, episode_reward=1968.41 +/- 42.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4507000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4508000, episode_reward=2006.06 +/- 48.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4508000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21       |\n",
      "|    critic_loss     | 8.06     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -0.0778  |\n",
      "|    learning_rate   | 0.000492 |\n",
      "|    n_updates       | 22010    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4508     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6680     |\n",
      "|    total_timesteps | 4508000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4509000, episode_reward=1933.48 +/- 41.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4509000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4510000, episode_reward=1942.31 +/- 81.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.4     |\n",
      "|    critic_loss     | 7.57     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.0385   |\n",
      "|    learning_rate   | 0.00049  |\n",
      "|    n_updates       | 22020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4511000, episode_reward=2023.07 +/- 26.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4511000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4512000, episode_reward=1971.97 +/- 37.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4512000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.1     |\n",
      "|    critic_loss     | 7.39     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.596    |\n",
      "|    learning_rate   | 0.000488 |\n",
      "|    n_updates       | 22030    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4512     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6685     |\n",
      "|    total_timesteps | 4512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4513000, episode_reward=1950.79 +/- 75.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4513000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4514000, episode_reward=1966.09 +/- 60.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4514000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.9     |\n",
      "|    critic_loss     | 7.8      |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | 0.875    |\n",
      "|    learning_rate   | 0.000486 |\n",
      "|    n_updates       | 22040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4515000, episode_reward=1949.75 +/- 48.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4515000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4516000, episode_reward=1960.99 +/- 70.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4516000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.7     |\n",
      "|    critic_loss     | 8.06     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.231    |\n",
      "|    learning_rate   | 0.000484 |\n",
      "|    n_updates       | 22050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4516     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6691     |\n",
      "|    total_timesteps | 4516000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4517000, episode_reward=1979.24 +/- 66.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4517000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4518000, episode_reward=1940.57 +/- 62.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4518000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.7     |\n",
      "|    critic_loss     | 7.37     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | -0.0838  |\n",
      "|    learning_rate   | 0.000482 |\n",
      "|    n_updates       | 22060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4519000, episode_reward=1999.09 +/- 83.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4519000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4520000, episode_reward=1969.87 +/- 24.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.9     |\n",
      "|    critic_loss     | 7.68     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.454    |\n",
      "|    learning_rate   | 0.00048  |\n",
      "|    n_updates       | 22070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4520     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6696     |\n",
      "|    total_timesteps | 4520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4521000, episode_reward=1987.20 +/- 55.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4521000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4522000, episode_reward=1997.34 +/- 47.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4522000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.3     |\n",
      "|    critic_loss     | 7.71     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.308    |\n",
      "|    learning_rate   | 0.000478 |\n",
      "|    n_updates       | 22080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4523000, episode_reward=2055.38 +/- 54.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4523000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4524000, episode_reward=2023.98 +/- 74.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4524000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4524     |\n",
      "|    fps             | 674      |\n",
      "|    time_elapsed    | 6702     |\n",
      "|    total_timesteps | 4524000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4525000, episode_reward=2032.50 +/- 65.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4525000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.1     |\n",
      "|    critic_loss     | 7.45     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 0.336    |\n",
      "|    learning_rate   | 0.000476 |\n",
      "|    n_updates       | 22090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4526000, episode_reward=2011.50 +/- 21.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4526000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4527000, episode_reward=1912.52 +/- 51.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4527000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.8     |\n",
      "|    critic_loss     | 8.53     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 0.0487   |\n",
      "|    learning_rate   | 0.000474 |\n",
      "|    n_updates       | 22100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4528000, episode_reward=1973.66 +/- 48.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4528000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4528     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6707     |\n",
      "|    total_timesteps | 4528000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4529000, episode_reward=1939.73 +/- 45.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4529000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.7     |\n",
      "|    critic_loss     | 8.08     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | 0.0702   |\n",
      "|    learning_rate   | 0.000472 |\n",
      "|    n_updates       | 22110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4530000, episode_reward=1921.38 +/- 43.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4530000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4531000, episode_reward=1943.87 +/- 14.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4531000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.5     |\n",
      "|    critic_loss     | 7.91     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | 0.317    |\n",
      "|    learning_rate   | 0.00047  |\n",
      "|    n_updates       | 22120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4532000, episode_reward=1944.00 +/- 30.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4532000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4532     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6713     |\n",
      "|    total_timesteps | 4532000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4533000, episode_reward=1878.82 +/- 72.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4533000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.1     |\n",
      "|    critic_loss     | 7.58     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | -0.801   |\n",
      "|    learning_rate   | 0.000468 |\n",
      "|    n_updates       | 22130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4534000, episode_reward=1919.26 +/- 13.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4534000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4535000, episode_reward=1968.22 +/- 29.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4535000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22       |\n",
      "|    critic_loss     | 7.84     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | -0.282   |\n",
      "|    learning_rate   | 0.000466 |\n",
      "|    n_updates       | 22140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4536000, episode_reward=1948.55 +/- 65.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4536000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4536     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6718     |\n",
      "|    total_timesteps | 4536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4537000, episode_reward=1958.04 +/- 33.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4537000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.5     |\n",
      "|    critic_loss     | 7.65     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -0.864   |\n",
      "|    learning_rate   | 0.000464 |\n",
      "|    n_updates       | 22150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4538000, episode_reward=1996.43 +/- 33.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4538000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4539000, episode_reward=2029.07 +/- 98.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4539000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.1     |\n",
      "|    critic_loss     | 7.48     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | -0.503   |\n",
      "|    learning_rate   | 0.000462 |\n",
      "|    n_updates       | 22160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4540000, episode_reward=2070.89 +/- 27.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4540000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4540     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6724     |\n",
      "|    total_timesteps | 4540000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4541000, episode_reward=2042.55 +/- 32.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4541000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.6     |\n",
      "|    critic_loss     | 6.97     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | -1.63    |\n",
      "|    learning_rate   | 0.00046  |\n",
      "|    n_updates       | 22170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4542000, episode_reward=2038.40 +/- 22.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4542000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4543000, episode_reward=1910.59 +/- 51.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4543000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.3     |\n",
      "|    critic_loss     | 7.6      |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -0.779   |\n",
      "|    learning_rate   | 0.000458 |\n",
      "|    n_updates       | 22180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4544000, episode_reward=2015.67 +/- 53.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4544000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4544     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6730     |\n",
      "|    total_timesteps | 4544000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4545000, episode_reward=2071.40 +/- 37.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4545000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.2     |\n",
      "|    critic_loss     | 6.74     |\n",
      "|    ent_coef        | 0.0377   |\n",
      "|    ent_coef_loss   | -0.987   |\n",
      "|    learning_rate   | 0.000455 |\n",
      "|    n_updates       | 22190    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4546000, episode_reward=2039.46 +/- 89.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4546000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4547000, episode_reward=1966.81 +/- 22.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4547000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.4     |\n",
      "|    critic_loss     | 7.52     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | -0.571   |\n",
      "|    learning_rate   | 0.000453 |\n",
      "|    n_updates       | 22200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4548000, episode_reward=2039.68 +/- 11.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4548000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4548     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6735     |\n",
      "|    total_timesteps | 4548000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4549000, episode_reward=1947.54 +/- 85.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4549000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22       |\n",
      "|    critic_loss     | 7.43     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | 0.107    |\n",
      "|    learning_rate   | 0.000451 |\n",
      "|    n_updates       | 22210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4550000, episode_reward=1928.99 +/- 24.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4550000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4551000, episode_reward=1892.95 +/- 58.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4551000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.8     |\n",
      "|    critic_loss     | 8.21     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.147   |\n",
      "|    learning_rate   | 0.000449 |\n",
      "|    n_updates       | 22220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4552000, episode_reward=1964.47 +/- 80.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4552000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4552     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6741     |\n",
      "|    total_timesteps | 4552000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4553000, episode_reward=1972.89 +/- 38.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4553000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.8     |\n",
      "|    critic_loss     | 7.01     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.98    |\n",
      "|    learning_rate   | 0.000447 |\n",
      "|    n_updates       | 22230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4554000, episode_reward=1946.16 +/- 41.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4554000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4555000, episode_reward=1938.82 +/- 45.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4555000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24       |\n",
      "|    critic_loss     | 7.05     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.899   |\n",
      "|    learning_rate   | 0.000445 |\n",
      "|    n_updates       | 22240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4556000, episode_reward=1951.37 +/- 74.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4556000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4556     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6746     |\n",
      "|    total_timesteps | 4556000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4557000, episode_reward=1960.40 +/- 24.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4557000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.4     |\n",
      "|    critic_loss     | 8.07     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.52     |\n",
      "|    learning_rate   | 0.000443 |\n",
      "|    n_updates       | 22250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4558000, episode_reward=1953.02 +/- 66.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4558000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4559000, episode_reward=1902.64 +/- 43.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4559000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.6     |\n",
      "|    critic_loss     | 7.73     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.872    |\n",
      "|    learning_rate   | 0.000441 |\n",
      "|    n_updates       | 22260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4560000, episode_reward=1942.69 +/- 75.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4560000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4560     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6752     |\n",
      "|    total_timesteps | 4560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4561000, episode_reward=1931.27 +/- 19.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4561000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.9     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 0.527    |\n",
      "|    learning_rate   | 0.000439 |\n",
      "|    n_updates       | 22270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4562000, episode_reward=1984.89 +/- 86.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4562000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4563000, episode_reward=2041.62 +/- 29.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4563000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.5     |\n",
      "|    critic_loss     | 6.88     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.132   |\n",
      "|    learning_rate   | 0.000437 |\n",
      "|    n_updates       | 22280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4564000, episode_reward=1985.97 +/- 48.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4564000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4564     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6758     |\n",
      "|    total_timesteps | 4564000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4565000, episode_reward=2083.28 +/- 32.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4565000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23       |\n",
      "|    critic_loss     | 7.68     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | 0.0177   |\n",
      "|    learning_rate   | 0.000435 |\n",
      "|    n_updates       | 22290    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4566000, episode_reward=2088.32 +/- 64.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4566000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4567000, episode_reward=2063.45 +/- 25.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4567000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4568000, episode_reward=1990.73 +/- 76.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4568000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.2     |\n",
      "|    critic_loss     | 7.21     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.266   |\n",
      "|    learning_rate   | 0.000433 |\n",
      "|    n_updates       | 22300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4568     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6763     |\n",
      "|    total_timesteps | 4568000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4569000, episode_reward=2053.61 +/- 35.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4569000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4570000, episode_reward=1987.69 +/- 45.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.3     |\n",
      "|    critic_loss     | 7.8      |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.047   |\n",
      "|    learning_rate   | 0.000431 |\n",
      "|    n_updates       | 22310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4571000, episode_reward=2028.52 +/- 38.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4571000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4572000, episode_reward=1851.87 +/- 52.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4572000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.2     |\n",
      "|    critic_loss     | 6.53     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.000429 |\n",
      "|    n_updates       | 22320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4572     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6769     |\n",
      "|    total_timesteps | 4572000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4573000, episode_reward=1985.53 +/- 41.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4573000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4574000, episode_reward=1969.08 +/- 17.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4574000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.1     |\n",
      "|    critic_loss     | 7.33     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.545    |\n",
      "|    learning_rate   | 0.000427 |\n",
      "|    n_updates       | 22330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4575000, episode_reward=1966.15 +/- 58.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4575000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4576000, episode_reward=1962.59 +/- 59.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4576000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.1     |\n",
      "|    critic_loss     | 8.25     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.377    |\n",
      "|    learning_rate   | 0.000425 |\n",
      "|    n_updates       | 22340    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4576     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6774     |\n",
      "|    total_timesteps | 4576000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4577000, episode_reward=1918.28 +/- 86.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4577000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4578000, episode_reward=1914.95 +/- 89.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4578000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25       |\n",
      "|    critic_loss     | 6.98     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -0.436   |\n",
      "|    learning_rate   | 0.000423 |\n",
      "|    n_updates       | 22350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4579000, episode_reward=1959.34 +/- 27.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4579000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4580000, episode_reward=1873.78 +/- 85.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.8     |\n",
      "|    critic_loss     | 7.28     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -0.578   |\n",
      "|    learning_rate   | 0.000421 |\n",
      "|    n_updates       | 22360    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4580     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6780     |\n",
      "|    total_timesteps | 4580000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4581000, episode_reward=1940.11 +/- 104.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4581000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4582000, episode_reward=1866.94 +/- 81.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4582000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.1     |\n",
      "|    critic_loss     | 7.85     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.164    |\n",
      "|    learning_rate   | 0.000419 |\n",
      "|    n_updates       | 22370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4583000, episode_reward=1848.19 +/- 48.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4583000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4584000, episode_reward=1927.67 +/- 57.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4584000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.5     |\n",
      "|    critic_loss     | 6.98     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.292   |\n",
      "|    learning_rate   | 0.000417 |\n",
      "|    n_updates       | 22380    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4584     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6785     |\n",
      "|    total_timesteps | 4584000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4585000, episode_reward=1980.28 +/- 71.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4585000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4586000, episode_reward=1894.35 +/- 78.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4586000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.6     |\n",
      "|    critic_loss     | 6.51     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.485   |\n",
      "|    learning_rate   | 0.000415 |\n",
      "|    n_updates       | 22390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4587000, episode_reward=1953.45 +/- 44.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4587000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4588000, episode_reward=2021.52 +/- 73.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4588000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.8     |\n",
      "|    critic_loss     | 7.6      |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.959    |\n",
      "|    learning_rate   | 0.000412 |\n",
      "|    n_updates       | 22400    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4588     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6791     |\n",
      "|    total_timesteps | 4588000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4589000, episode_reward=1973.96 +/- 72.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4589000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4590000, episode_reward=2012.13 +/- 43.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.9     |\n",
      "|    critic_loss     | 7.59     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.518    |\n",
      "|    learning_rate   | 0.00041  |\n",
      "|    n_updates       | 22410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4591000, episode_reward=2032.45 +/- 40.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4591000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4592000, episode_reward=1935.13 +/- 24.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4592000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.3     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.445    |\n",
      "|    learning_rate   | 0.000408 |\n",
      "|    n_updates       | 22420    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4592     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6796     |\n",
      "|    total_timesteps | 4592000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4593000, episode_reward=1966.37 +/- 73.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4593000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4594000, episode_reward=2006.47 +/- 41.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4594000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.5     |\n",
      "|    critic_loss     | 8.65     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 0.689    |\n",
      "|    learning_rate   | 0.000406 |\n",
      "|    n_updates       | 22430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4595000, episode_reward=1958.16 +/- 73.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4595000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4596000, episode_reward=1968.51 +/- 54.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4596000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24       |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.375   |\n",
      "|    learning_rate   | 0.000404 |\n",
      "|    n_updates       | 22440    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4596     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6802     |\n",
      "|    total_timesteps | 4596000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4597000, episode_reward=1920.66 +/- 42.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4597000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4598000, episode_reward=1785.67 +/- 48.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4598000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.2     |\n",
      "|    critic_loss     | 7.67     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.132   |\n",
      "|    learning_rate   | 0.000402 |\n",
      "|    n_updates       | 22450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4599000, episode_reward=1920.82 +/- 86.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4599000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4600000, episode_reward=1910.45 +/- 70.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.3     |\n",
      "|    critic_loss     | 7.35     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.181   |\n",
      "|    learning_rate   | 0.0004   |\n",
      "|    n_updates       | 22460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4600     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6811     |\n",
      "|    total_timesteps | 4600000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4601000, episode_reward=1954.55 +/- 22.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4601000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4602000, episode_reward=1928.67 +/- 28.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4602000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.4     |\n",
      "|    critic_loss     | 7.37     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.532   |\n",
      "|    learning_rate   | 0.000398 |\n",
      "|    n_updates       | 22470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4603000, episode_reward=1826.57 +/- 43.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4603000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4604000, episode_reward=1785.77 +/- 73.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4604000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.6     |\n",
      "|    critic_loss     | 8.18     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 0.18     |\n",
      "|    learning_rate   | 0.000396 |\n",
      "|    n_updates       | 22480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4604     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6818     |\n",
      "|    total_timesteps | 4604000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4605000, episode_reward=1841.40 +/- 60.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4605000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4606000, episode_reward=1830.14 +/- 27.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4606000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.4     |\n",
      "|    critic_loss     | 7.39     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 0.383    |\n",
      "|    learning_rate   | 0.000394 |\n",
      "|    n_updates       | 22490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4607000, episode_reward=1888.12 +/- 105.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4607000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4608000, episode_reward=1857.44 +/- 87.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4608000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4608     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6823     |\n",
      "|    total_timesteps | 4608000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4609000, episode_reward=1973.79 +/- 98.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4609000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.1     |\n",
      "|    critic_loss     | 7.35     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.888   |\n",
      "|    learning_rate   | 0.000392 |\n",
      "|    n_updates       | 22500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4610000, episode_reward=1943.55 +/- 61.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4610000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4611000, episode_reward=1915.90 +/- 35.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4611000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.7     |\n",
      "|    critic_loss     | 7.39     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.0861   |\n",
      "|    learning_rate   | 0.00039  |\n",
      "|    n_updates       | 22510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4612000, episode_reward=1943.33 +/- 105.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4612000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4612     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6829     |\n",
      "|    total_timesteps | 4612000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4613000, episode_reward=1871.16 +/- 57.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4613000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.7     |\n",
      "|    critic_loss     | 8.02     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.352    |\n",
      "|    learning_rate   | 0.000388 |\n",
      "|    n_updates       | 22520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4614000, episode_reward=1918.13 +/- 47.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4614000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4615000, episode_reward=1882.46 +/- 27.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4615000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.2     |\n",
      "|    critic_loss     | 7.09     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -0.44    |\n",
      "|    learning_rate   | 0.000386 |\n",
      "|    n_updates       | 22530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4616000, episode_reward=1896.90 +/- 55.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4616000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4616     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6834     |\n",
      "|    total_timesteps | 4616000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4617000, episode_reward=1948.55 +/- 146.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4617000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28       |\n",
      "|    critic_loss     | 6.26     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.000384 |\n",
      "|    n_updates       | 22540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4618000, episode_reward=2004.33 +/- 79.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4618000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4619000, episode_reward=1983.74 +/- 59.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4619000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.7     |\n",
      "|    critic_loss     | 8.1      |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.398    |\n",
      "|    learning_rate   | 0.000382 |\n",
      "|    n_updates       | 22550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620000, episode_reward=2021.22 +/- 75.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4620000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4620     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6840     |\n",
      "|    total_timesteps | 4620000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4621000, episode_reward=1987.33 +/- 92.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4621000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.5     |\n",
      "|    critic_loss     | 9.26     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.457    |\n",
      "|    learning_rate   | 0.00038  |\n",
      "|    n_updates       | 22560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4622000, episode_reward=2008.09 +/- 40.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4622000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4623000, episode_reward=1887.19 +/- 107.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4623000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.4     |\n",
      "|    critic_loss     | 7.31     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | -0.904   |\n",
      "|    learning_rate   | 0.000378 |\n",
      "|    n_updates       | 22570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4624000, episode_reward=1907.00 +/- 57.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4624000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4624     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6845     |\n",
      "|    total_timesteps | 4624000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4625000, episode_reward=1951.96 +/- 67.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4625000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.2     |\n",
      "|    critic_loss     | 7.05     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.000376 |\n",
      "|    n_updates       | 22580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4626000, episode_reward=1856.69 +/- 155.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4626000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4627000, episode_reward=1951.29 +/- 45.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4627000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.6     |\n",
      "|    critic_loss     | 7.18     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.000374 |\n",
      "|    n_updates       | 22590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4628000, episode_reward=1990.69 +/- 47.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4628000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4628     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6851     |\n",
      "|    total_timesteps | 4628000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4629000, episode_reward=2016.09 +/- 55.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4629000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.4     |\n",
      "|    critic_loss     | 7.88     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -0.102   |\n",
      "|    learning_rate   | 0.000372 |\n",
      "|    n_updates       | 22600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4630000, episode_reward=1852.75 +/- 105.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4630000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4631000, episode_reward=1917.97 +/- 69.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4631000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.1     |\n",
      "|    critic_loss     | 8.29     |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | -0.376   |\n",
      "|    learning_rate   | 0.000369 |\n",
      "|    n_updates       | 22610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4632000, episode_reward=1864.31 +/- 160.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4632000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4632     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6856     |\n",
      "|    total_timesteps | 4632000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4633000, episode_reward=1837.55 +/- 224.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4633000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.4     |\n",
      "|    critic_loss     | 6.95     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | -0.743   |\n",
      "|    learning_rate   | 0.000367 |\n",
      "|    n_updates       | 22620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4634000, episode_reward=1601.63 +/- 578.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4634000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4635000, episode_reward=1698.58 +/- 190.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4635000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.2     |\n",
      "|    critic_loss     | 4.54     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -4.82    |\n",
      "|    learning_rate   | 0.000365 |\n",
      "|    n_updates       | 22630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4636000, episode_reward=1793.58 +/- 245.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4636000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4636     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6862     |\n",
      "|    total_timesteps | 4636000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4637000, episode_reward=1883.84 +/- 103.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4637000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.9     |\n",
      "|    critic_loss     | 3.66     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -5.51    |\n",
      "|    learning_rate   | 0.000363 |\n",
      "|    n_updates       | 22640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4638000, episode_reward=1925.80 +/- 109.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4638000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4639000, episode_reward=1401.97 +/- 643.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4639000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.1     |\n",
      "|    critic_loss     | 7.54     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.996   |\n",
      "|    learning_rate   | 0.000361 |\n",
      "|    n_updates       | 22650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4640000, episode_reward=1783.48 +/- 311.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4640000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4640     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6868     |\n",
      "|    total_timesteps | 4640000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4641000, episode_reward=1208.78 +/- 659.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4641000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.9     |\n",
      "|    critic_loss     | 6.3      |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -1.38    |\n",
      "|    learning_rate   | 0.000359 |\n",
      "|    n_updates       | 22660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4642000, episode_reward=1226.65 +/- 602.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4642000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4643000, episode_reward=921.01 +/- 531.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 921      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4643000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 38.5     |\n",
      "|    critic_loss     | 3.59     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -4.97    |\n",
      "|    learning_rate   | 0.000357 |\n",
      "|    n_updates       | 22670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4644000, episode_reward=981.59 +/- 677.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 982      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4644000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4644     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6873     |\n",
      "|    total_timesteps | 4644000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4645000, episode_reward=1609.56 +/- 439.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4645000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42       |\n",
      "|    critic_loss     | 2.92     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -6.65    |\n",
      "|    learning_rate   | 0.000355 |\n",
      "|    n_updates       | 22680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4646000, episode_reward=1379.86 +/- 675.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4646000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4647000, episode_reward=1135.02 +/- 651.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4647000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 33.1     |\n",
      "|    critic_loss     | 5.06     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -2.65    |\n",
      "|    learning_rate   | 0.000353 |\n",
      "|    n_updates       | 22690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4648000, episode_reward=278.08 +/- 472.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 278      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4648000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4648     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6879     |\n",
      "|    total_timesteps | 4648000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4649000, episode_reward=652.50 +/- 624.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 653      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4649000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.8     |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -7.32    |\n",
      "|    learning_rate   | 0.000351 |\n",
      "|    n_updates       | 22700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4650000, episode_reward=1111.67 +/- 885.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4650000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4651000, episode_reward=1333.40 +/- 734.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4651000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4652000, episode_reward=905.22 +/- 797.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 905      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4652000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.7     |\n",
      "|    critic_loss     | 3.44     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -3.35    |\n",
      "|    learning_rate   | 0.000349 |\n",
      "|    n_updates       | 22710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4652     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6885     |\n",
      "|    total_timesteps | 4652000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4653000, episode_reward=872.20 +/- 775.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 872      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4653000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4654000, episode_reward=262.58 +/- 495.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 263      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4654000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.5     |\n",
      "|    critic_loss     | 2.9      |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | -4.75    |\n",
      "|    learning_rate   | 0.000347 |\n",
      "|    n_updates       | 22720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4655000, episode_reward=432.50 +/- 613.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 432      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4655000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4656000, episode_reward=506.54 +/- 698.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 507      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4656000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.9     |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -6.11    |\n",
      "|    learning_rate   | 0.000345 |\n",
      "|    n_updates       | 22730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4656     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6890     |\n",
      "|    total_timesteps | 4656000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4657000, episode_reward=35.53 +/- 83.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 35.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4657000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4658000, episode_reward=736.39 +/- 692.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 736      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4658000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.3     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -5.78    |\n",
      "|    learning_rate   | 0.000343 |\n",
      "|    n_updates       | 22740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4659000, episode_reward=-54.16 +/- 4.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -54.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4659000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4660000, episode_reward=657.42 +/- 871.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 657      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.3     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | -6.29    |\n",
      "|    learning_rate   | 0.000341 |\n",
      "|    n_updates       | 22750    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4660     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6898     |\n",
      "|    total_timesteps | 4660000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4661000, episode_reward=383.91 +/- 740.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 384      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4661000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4662000, episode_reward=234.21 +/- 559.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 234      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4662000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 34.2     |\n",
      "|    critic_loss     | 4.54     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -2.17    |\n",
      "|    learning_rate   | 0.000339 |\n",
      "|    n_updates       | 22760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4663000, episode_reward=315.23 +/- 719.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 315      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4663000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4664000, episode_reward=723.60 +/- 876.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 724      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4664000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45       |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -7.23    |\n",
      "|    learning_rate   | 0.000337 |\n",
      "|    n_updates       | 22770    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4664     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6905     |\n",
      "|    total_timesteps | 4664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4665000, episode_reward=1166.40 +/- 851.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4665000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4666000, episode_reward=-14.26 +/- 54.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -14.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4666000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.9     |\n",
      "|    critic_loss     | 3.85     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | -2.13    |\n",
      "|    learning_rate   | 0.000335 |\n",
      "|    n_updates       | 22780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4667000, episode_reward=-40.90 +/- 4.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4667000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4668000, episode_reward=29.40 +/- 132.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 29.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4668000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.5     |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | -5.03    |\n",
      "|    learning_rate   | 0.000333 |\n",
      "|    n_updates       | 22790    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4668     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6912     |\n",
      "|    total_timesteps | 4668000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4669000, episode_reward=224.66 +/- 263.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 225      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4669000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4670000, episode_reward=-50.62 +/- 2.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.4     |\n",
      "|    critic_loss     | 2.7      |\n",
      "|    ent_coef        | 0.0307   |\n",
      "|    ent_coef_loss   | -4.26    |\n",
      "|    learning_rate   | 0.000331 |\n",
      "|    n_updates       | 22800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4671000, episode_reward=-50.19 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4671000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4672000, episode_reward=713.16 +/- 867.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 713      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4672000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.3     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -6.72    |\n",
      "|    learning_rate   | 0.000329 |\n",
      "|    n_updates       | 22810    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4672     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6918     |\n",
      "|    total_timesteps | 4672000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4673000, episode_reward=370.69 +/- 708.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 371      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4673000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4674000, episode_reward=364.42 +/- 838.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 364      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4674000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35.1     |\n",
      "|    critic_loss     | 4.4      |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | -1.63    |\n",
      "|    learning_rate   | 0.000326 |\n",
      "|    n_updates       | 22820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4675000, episode_reward=330.85 +/- 781.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 331      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4675000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4676000, episode_reward=146.68 +/- 324.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 147      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4676000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.9     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | -7.51    |\n",
      "|    learning_rate   | 0.000324 |\n",
      "|    n_updates       | 22830    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4676     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6924     |\n",
      "|    total_timesteps | 4676000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4677000, episode_reward=793.93 +/- 924.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 794      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4677000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4678000, episode_reward=153.48 +/- 414.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 153      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4678000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.3     |\n",
      "|    critic_loss     | 4.1      |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | -1.6     |\n",
      "|    learning_rate   | 0.000322 |\n",
      "|    n_updates       | 22840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4679000, episode_reward=-52.64 +/- 2.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4679000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680000, episode_reward=-41.88 +/- 20.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.7     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -5.62    |\n",
      "|    learning_rate   | 0.00032  |\n",
      "|    n_updates       | 22850    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4680     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6930     |\n",
      "|    total_timesteps | 4680000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4681000, episode_reward=301.71 +/- 702.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4681000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4682000, episode_reward=-64.22 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -64.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4682000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 35       |\n",
      "|    critic_loss     | 4.28     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | -0.903   |\n",
      "|    learning_rate   | 0.000318 |\n",
      "|    n_updates       | 22860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4683000, episode_reward=337.31 +/- 799.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 337      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4683000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4684000, episode_reward=111.91 +/- 368.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 112      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4684000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.2     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -5.23    |\n",
      "|    learning_rate   | 0.000316 |\n",
      "|    n_updates       | 22870    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4684     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6937     |\n",
      "|    total_timesteps | 4684000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4685000, episode_reward=427.09 +/- 729.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4685000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4686000, episode_reward=-2.48 +/- 137.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.48    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4686000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.5     |\n",
      "|    critic_loss     | 0.563    |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -7.17    |\n",
      "|    learning_rate   | 0.000314 |\n",
      "|    n_updates       | 22880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4687000, episode_reward=-70.45 +/- 2.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4687000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4688000, episode_reward=153.80 +/- 389.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 154      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4688000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.6     |\n",
      "|    critic_loss     | 0.563    |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -6.96    |\n",
      "|    learning_rate   | 0.000312 |\n",
      "|    n_updates       | 22890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4688     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6944     |\n",
      "|    total_timesteps | 4688000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4689000, episode_reward=596.15 +/- 825.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 596      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4689000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4690000, episode_reward=-70.30 +/- 35.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.5     |\n",
      "|    critic_loss     | 0.681    |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | -5.68    |\n",
      "|    learning_rate   | 0.00031  |\n",
      "|    n_updates       | 22900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4691000, episode_reward=164.17 +/- 459.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 164      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4691000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4692000, episode_reward=334.67 +/- 753.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 335      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4692000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.4     |\n",
      "|    critic_loss     | 0.584    |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -5.81    |\n",
      "|    learning_rate   | 0.000308 |\n",
      "|    n_updates       | 22910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4692     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6950     |\n",
      "|    total_timesteps | 4692000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4693000, episode_reward=88.13 +/- 178.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 88.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4693000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4694000, episode_reward=63.54 +/- 290.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 63.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4694000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4695000, episode_reward=-79.98 +/- 6.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4695000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.9     |\n",
      "|    critic_loss     | 2.06     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -3.16    |\n",
      "|    learning_rate   | 0.000306 |\n",
      "|    n_updates       | 22920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4696000, episode_reward=224.67 +/- 617.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 225      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4696000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4696     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6956     |\n",
      "|    total_timesteps | 4696000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4697000, episode_reward=-80.55 +/- 3.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4697000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.6     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -5       |\n",
      "|    learning_rate   | 0.000304 |\n",
      "|    n_updates       | 22930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4698000, episode_reward=713.21 +/- 761.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 713      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4698000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4699000, episode_reward=-78.85 +/- 3.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -78.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4699000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.4     |\n",
      "|    critic_loss     | 0.576    |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | -6.01    |\n",
      "|    learning_rate   | 0.000302 |\n",
      "|    n_updates       | 22940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4700000, episode_reward=-70.44 +/- 19.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4700000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 960      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4700     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6962     |\n",
      "|    total_timesteps | 4700000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4701000, episode_reward=-86.07 +/- 3.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4701000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.3     |\n",
      "|    critic_loss     | 0.611    |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | -5.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4702000, episode_reward=330.27 +/- 798.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 330      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4702000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4703000, episode_reward=-52.78 +/- 28.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4703000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.3     |\n",
      "|    critic_loss     | 0.527    |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | -5.12    |\n",
      "|    learning_rate   | 0.000298 |\n",
      "|    n_updates       | 22960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4704000, episode_reward=-62.53 +/- 29.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -62.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4704000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 889      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4704     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6967     |\n",
      "|    total_timesteps | 4704000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4705000, episode_reward=-15.99 +/- 62.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4705000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.4     |\n",
      "|    critic_loss     | 0.499    |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | -4.88    |\n",
      "|    learning_rate   | 0.000296 |\n",
      "|    n_updates       | 22970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4706000, episode_reward=-18.68 +/- 57.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4706000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4707000, episode_reward=-57.84 +/- 33.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -57.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4707000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.3     |\n",
      "|    critic_loss     | 0.496    |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | -4.6     |\n",
      "|    learning_rate   | 0.000294 |\n",
      "|    n_updates       | 22980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4708000, episode_reward=351.78 +/- 828.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 352      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4708000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 819      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4708     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6973     |\n",
      "|    total_timesteps | 4708000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4709000, episode_reward=-49.50 +/- 39.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4709000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.1     |\n",
      "|    critic_loss     | 0.577    |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | -3.91    |\n",
      "|    learning_rate   | 0.000292 |\n",
      "|    n_updates       | 22990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710000, episode_reward=-46.95 +/- 36.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -47      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4710000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4711000, episode_reward=316.96 +/- 777.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 317      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4711000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.2     |\n",
      "|    critic_loss     | 0.48     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | -4.12    |\n",
      "|    learning_rate   | 0.00029  |\n",
      "|    n_updates       | 23000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4712000, episode_reward=329.47 +/- 852.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 329      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4712000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 746      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4712     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6979     |\n",
      "|    total_timesteps | 4712000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4713000, episode_reward=118.94 +/- 405.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 119      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4713000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 0.522    |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -3.59    |\n",
      "|    learning_rate   | 0.000288 |\n",
      "|    n_updates       | 23010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4714000, episode_reward=-94.51 +/- 8.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4714000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4715000, episode_reward=349.63 +/- 769.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 350      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4715000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.5     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | 0.104    |\n",
      "|    learning_rate   | 0.000286 |\n",
      "|    n_updates       | 23020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4716000, episode_reward=345.45 +/- 781.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 345      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4716000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 701      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4716     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6984     |\n",
      "|    total_timesteps | 4716000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4717000, episode_reward=23.04 +/- 250.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 23       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4717000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.3     |\n",
      "|    critic_loss     | 0.653    |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | -3.88    |\n",
      "|    learning_rate   | 0.000283 |\n",
      "|    n_updates       | 23030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4718000, episode_reward=160.45 +/- 521.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4718000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4719000, episode_reward=-98.89 +/- 3.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4719000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.6     |\n",
      "|    critic_loss     | 0.448    |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | -4.23    |\n",
      "|    learning_rate   | 0.000281 |\n",
      "|    n_updates       | 23040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4720000, episode_reward=-23.24 +/- 160.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4720000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 624      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4720     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6990     |\n",
      "|    total_timesteps | 4720000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4721000, episode_reward=-96.78 +/- 3.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -96.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4721000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.7     |\n",
      "|    critic_loss     | 0.415    |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | -3.73    |\n",
      "|    learning_rate   | 0.000279 |\n",
      "|    n_updates       | 23050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4722000, episode_reward=-95.03 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4722000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4723000, episode_reward=138.88 +/- 480.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 139      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4723000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 0.472    |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | -3.12    |\n",
      "|    learning_rate   | 0.000277 |\n",
      "|    n_updates       | 23060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4724000, episode_reward=-106.22 +/- 3.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4724000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 557      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4724     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 6996     |\n",
      "|    total_timesteps | 4724000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4725000, episode_reward=225.95 +/- 632.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 226      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4725000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.6     |\n",
      "|    critic_loss     | 0.46     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | -3.37    |\n",
      "|    learning_rate   | 0.000275 |\n",
      "|    n_updates       | 23070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4726000, episode_reward=-5.36 +/- 168.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.36    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4726000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4727000, episode_reward=-85.26 +/- 5.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4727000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.6     |\n",
      "|    critic_loss     | 0.365    |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | -3.53    |\n",
      "|    learning_rate   | 0.000273 |\n",
      "|    n_updates       | 23080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4728000, episode_reward=-85.66 +/- 3.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4728000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 490      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4728     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7001     |\n",
      "|    total_timesteps | 4728000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4729000, episode_reward=-90.80 +/- 5.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4729000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.4     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.000271 |\n",
      "|    n_updates       | 23090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4730000, episode_reward=-89.42 +/- 3.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -89.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4730000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4731000, episode_reward=-78.46 +/- 20.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -78.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4731000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.1     |\n",
      "|    critic_loss     | 0.597    |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | -3.06    |\n",
      "|    learning_rate   | 0.000269 |\n",
      "|    n_updates       | 23100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4732000, episode_reward=67.37 +/- 303.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 67.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4732000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 431      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4732     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7007     |\n",
      "|    total_timesteps | 4732000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4733000, episode_reward=320.86 +/- 837.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4733000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.9     |\n",
      "|    critic_loss     | 0.556    |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | -2.82    |\n",
      "|    learning_rate   | 0.000267 |\n",
      "|    n_updates       | 23110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4734000, episode_reward=-94.68 +/- 4.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -94.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4734000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4735000, episode_reward=-85.28 +/- 3.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4735000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.2     |\n",
      "|    critic_loss     | 0.445    |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -3.26    |\n",
      "|    learning_rate   | 0.000265 |\n",
      "|    n_updates       | 23120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4736000, episode_reward=-18.78 +/- 124.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4736000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 391      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4736     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7012     |\n",
      "|    total_timesteps | 4736000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4737000, episode_reward=-82.42 +/- 4.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -82.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4737000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4738000, episode_reward=-127.74 +/- 2.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4738000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 0.546    |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -2.58    |\n",
      "|    learning_rate   | 0.000263 |\n",
      "|    n_updates       | 23130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4739000, episode_reward=-16.01 +/- 129.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4739000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4740000, episode_reward=-103.99 +/- 4.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 0.482    |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.000261 |\n",
      "|    n_updates       | 23140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 334      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4740     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7018     |\n",
      "|    total_timesteps | 4740000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4741000, episode_reward=-101.74 +/- 2.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4741000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4742000, episode_reward=181.67 +/- 563.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 182      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4742000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.4     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | -1.9     |\n",
      "|    learning_rate   | 0.000259 |\n",
      "|    n_updates       | 23150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4743000, episode_reward=-100.26 +/- 2.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4743000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4744000, episode_reward=-70.21 +/- 13.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4744000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 0.62     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | -2.04    |\n",
      "|    learning_rate   | 0.000257 |\n",
      "|    n_updates       | 23160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 304      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4744     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7024     |\n",
      "|    total_timesteps | 4744000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4745000, episode_reward=-94.17 +/- 1.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -94.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4745000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4746000, episode_reward=106.89 +/- 280.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4746000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.9     |\n",
      "|    critic_loss     | 0.621    |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | -2.14    |\n",
      "|    learning_rate   | 0.000255 |\n",
      "|    n_updates       | 23170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4747000, episode_reward=-13.38 +/- 14.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -13.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4747000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4748000, episode_reward=-88.14 +/- 4.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -88.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4748000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 0.62     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | -2.5     |\n",
      "|    learning_rate   | 0.000253 |\n",
      "|    n_updates       | 23180    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 268      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4748     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7029     |\n",
      "|    total_timesteps | 4748000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4749000, episode_reward=-16.36 +/- 142.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4749000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4750000, episode_reward=22.01 +/- 119.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 22       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.9     |\n",
      "|    critic_loss     | 0.557    |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | -2.29    |\n",
      "|    learning_rate   | 0.000251 |\n",
      "|    n_updates       | 23190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4751000, episode_reward=-39.93 +/- 6.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -39.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4751000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4752000, episode_reward=-101.12 +/- 14.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4752000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 0.579    |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | -2.32    |\n",
      "|    learning_rate   | 0.000249 |\n",
      "|    n_updates       | 23200    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 240      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4752     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7035     |\n",
      "|    total_timesteps | 4752000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4753000, episode_reward=-101.28 +/- 13.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4753000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4754000, episode_reward=64.71 +/- 367.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 64.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4754000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.3     |\n",
      "|    critic_loss     | 0.489    |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | -2.57    |\n",
      "|    learning_rate   | 0.000247 |\n",
      "|    n_updates       | 23210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4755000, episode_reward=304.59 +/- 836.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 305      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4755000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4756000, episode_reward=288.57 +/- 845.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 289      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4756000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 0.467    |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 0.000245 |\n",
      "|    n_updates       | 23220    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 230      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4756     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7041     |\n",
      "|    total_timesteps | 4756000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4757000, episode_reward=-132.02 +/- 4.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4757000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4758000, episode_reward=-83.85 +/- 2.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -83.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4758000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.9     |\n",
      "|    critic_loss     | 0.61     |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | -2.03    |\n",
      "|    learning_rate   | 0.000242 |\n",
      "|    n_updates       | 23230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4759000, episode_reward=57.42 +/- 273.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 57.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4759000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4760000, episode_reward=252.82 +/- 754.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 253      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 0.563    |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | -2.13    |\n",
      "|    learning_rate   | 0.00024  |\n",
      "|    n_updates       | 23240    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 216      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4760     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7046     |\n",
      "|    total_timesteps | 4760000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4761000, episode_reward=706.13 +/- 1023.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 706      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4761000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4762000, episode_reward=-140.36 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4762000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 0.583    |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.000238 |\n",
      "|    n_updates       | 23250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4763000, episode_reward=69.30 +/- 386.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 69.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4763000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4764000, episode_reward=141.76 +/- 261.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 142      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4764000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.9     |\n",
      "|    critic_loss     | 0.53     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | -2.08    |\n",
      "|    learning_rate   | 0.000236 |\n",
      "|    n_updates       | 23260    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 187      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4764     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7052     |\n",
      "|    total_timesteps | 4764000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4765000, episode_reward=29.79 +/- 32.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 29.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4765000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4766000, episode_reward=23.78 +/- 30.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4766000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.2     |\n",
      "|    critic_loss     | 0.653    |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.000234 |\n",
      "|    n_updates       | 23270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4767000, episode_reward=27.14 +/- 13.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 27.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4767000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4768000, episode_reward=415.49 +/- 760.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 415      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4768000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.6     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | -0.598   |\n",
      "|    learning_rate   | 0.000232 |\n",
      "|    n_updates       | 23280    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 172      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4768     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7058     |\n",
      "|    total_timesteps | 4768000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4769000, episode_reward=30.42 +/- 4.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 30.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4769000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4770000, episode_reward=93.85 +/- 34.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 93.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.3     |\n",
      "|    critic_loss     | 0.787    |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | -2.06    |\n",
      "|    learning_rate   | 0.00023  |\n",
      "|    n_updates       | 23290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4771000, episode_reward=160.38 +/- 168.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4771000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4772000, episode_reward=59.65 +/- 4.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 59.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4772000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.4     |\n",
      "|    critic_loss     | 0.739    |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.000228 |\n",
      "|    n_updates       | 23300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4772     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7063     |\n",
      "|    total_timesteps | 4772000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4773000, episode_reward=511.54 +/- 572.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 512      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4773000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4774000, episode_reward=400.02 +/- 748.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 400      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4774000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 0.673    |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | -2.45    |\n",
      "|    learning_rate   | 0.000226 |\n",
      "|    n_updates       | 23310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4775000, episode_reward=432.70 +/- 809.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 433      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4775000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4776000, episode_reward=262.28 +/- 332.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 262      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4776000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 0.63     |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | -2.37    |\n",
      "|    learning_rate   | 0.000224 |\n",
      "|    n_updates       | 23320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 130      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4776     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7069     |\n",
      "|    total_timesteps | 4776000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4777000, episode_reward=244.12 +/- 280.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 244      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4777000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4778000, episode_reward=379.32 +/- 493.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 379      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4778000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.9     |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | 0.426    |\n",
      "|    learning_rate   | 0.000222 |\n",
      "|    n_updates       | 23330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4779000, episode_reward=219.13 +/- 230.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 219      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4779000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4780000, episode_reward=755.87 +/- 547.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 756      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4780000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 139      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4780     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7075     |\n",
      "|    total_timesteps | 4780000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4781000, episode_reward=85.63 +/- 32.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 85.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4781000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.9     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | 0.16     |\n",
      "|    learning_rate   | 0.00022  |\n",
      "|    n_updates       | 23340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4782000, episode_reward=91.19 +/- 11.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 91.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4782000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4783000, episode_reward=73.43 +/- 8.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 73.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4783000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.1     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | -0.905   |\n",
      "|    learning_rate   | 0.000218 |\n",
      "|    n_updates       | 23350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4784000, episode_reward=145.89 +/- 115.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 146      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4784000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 124      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4784     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7080     |\n",
      "|    total_timesteps | 4784000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4785000, episode_reward=74.26 +/- 4.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 74.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4785000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.6     |\n",
      "|    critic_loss     | 0.928    |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.000216 |\n",
      "|    n_updates       | 23360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4786000, episode_reward=206.59 +/- 270.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 207      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4786000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4787000, episode_reward=83.27 +/- 5.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 83.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4787000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.7     |\n",
      "|    critic_loss     | 0.666    |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | -2.31    |\n",
      "|    learning_rate   | 0.000214 |\n",
      "|    n_updates       | 23370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4788000, episode_reward=82.27 +/- 9.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 82.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4788000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 131      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4788     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7086     |\n",
      "|    total_timesteps | 4788000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4789000, episode_reward=374.07 +/- 470.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4789000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.1     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.000212 |\n",
      "|    n_updates       | 23380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4790000, episode_reward=122.01 +/- 101.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 122      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4790000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4791000, episode_reward=328.06 +/- 99.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 328      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4791000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 0.725    |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | -2.22    |\n",
      "|    learning_rate   | 0.00021  |\n",
      "|    n_updates       | 23390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4792000, episode_reward=282.94 +/- 10.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 283      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4792000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 136      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4792     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7092     |\n",
      "|    total_timesteps | 4792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4793000, episode_reward=366.22 +/- 185.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 366      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4793000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.1     |\n",
      "|    critic_loss     | 0.855    |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.000208 |\n",
      "|    n_updates       | 23400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4794000, episode_reward=285.66 +/- 9.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 286      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4794000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4795000, episode_reward=416.37 +/- 187.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 416      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4795000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.1     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -0.178   |\n",
      "|    learning_rate   | 0.000206 |\n",
      "|    n_updates       | 23410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4796000, episode_reward=331.33 +/- 100.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 331      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4796000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 139      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4796     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7097     |\n",
      "|    total_timesteps | 4796000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4797000, episode_reward=241.56 +/- 12.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 242      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4797000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.2     |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -0.2     |\n",
      "|    learning_rate   | 0.000204 |\n",
      "|    n_updates       | 23420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4798000, episode_reward=238.90 +/- 24.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 239      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4798000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4799000, episode_reward=312.15 +/- 29.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 312      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4799000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 0.731    |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.000202 |\n",
      "|    n_updates       | 23430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800000, episode_reward=418.44 +/- 242.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 418      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4800000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4800     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7103     |\n",
      "|    total_timesteps | 4800000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4801000, episode_reward=260.12 +/- 16.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 260      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4801000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.5     |\n",
      "|    critic_loss     | 0.81     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -1.67    |\n",
      "|    learning_rate   | 0.000199 |\n",
      "|    n_updates       | 23440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4802000, episode_reward=256.59 +/- 17.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 257      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4802000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4803000, episode_reward=303.52 +/- 9.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 304      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4803000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.7     |\n",
      "|    critic_loss     | 0.723    |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.000197 |\n",
      "|    n_updates       | 23450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4804000, episode_reward=582.97 +/- 354.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 583      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4804000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4804     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7109     |\n",
      "|    total_timesteps | 4804000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4805000, episode_reward=678.39 +/- 518.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 678      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4805000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.3     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | -0.643   |\n",
      "|    learning_rate   | 0.000195 |\n",
      "|    n_updates       | 23460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4806000, episode_reward=302.57 +/- 3.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 303      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4806000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4807000, episode_reward=292.38 +/- 12.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4807000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.2     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | -0.0745  |\n",
      "|    learning_rate   | 0.000193 |\n",
      "|    n_updates       | 23470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4808000, episode_reward=356.08 +/- 84.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 356      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4808000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4808     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7114     |\n",
      "|    total_timesteps | 4808000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4809000, episode_reward=292.40 +/- 48.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4809000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.3     |\n",
      "|    critic_loss     | 0.944    |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | -1.5     |\n",
      "|    learning_rate   | 0.000191 |\n",
      "|    n_updates       | 23480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4810000, episode_reward=284.12 +/- 45.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 284      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4810000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4811000, episode_reward=603.13 +/- 552.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 603      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4811000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.5     |\n",
      "|    critic_loss     | 0.836    |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.000189 |\n",
      "|    n_updates       | 23490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4812000, episode_reward=404.82 +/- 175.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 405      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4812000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 169      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4812     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7120     |\n",
      "|    total_timesteps | 4812000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4813000, episode_reward=308.99 +/- 95.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4813000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.7     |\n",
      "|    critic_loss     | 0.825    |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.000187 |\n",
      "|    n_updates       | 23500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4814000, episode_reward=304.82 +/- 68.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 305      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4814000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4815000, episode_reward=601.91 +/- 399.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 602      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4815000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 0.745    |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.000185 |\n",
      "|    n_updates       | 23510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4816000, episode_reward=448.69 +/- 205.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 449      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4816000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4816     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7126     |\n",
      "|    total_timesteps | 4816000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4817000, episode_reward=300.70 +/- 51.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 301      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4817000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.2     |\n",
      "|    critic_loss     | 0.907    |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -0.865   |\n",
      "|    learning_rate   | 0.000183 |\n",
      "|    n_updates       | 23520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4818000, episode_reward=275.31 +/- 4.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 275      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4818000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4819000, episode_reward=295.61 +/- 25.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 296      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4819000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.7     |\n",
      "|    critic_loss     | 0.798    |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.000181 |\n",
      "|    n_updates       | 23530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4820000, episode_reward=281.34 +/- 5.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 281      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4820000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 164      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4820     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7131     |\n",
      "|    total_timesteps | 4820000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4821000, episode_reward=306.50 +/- 91.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 306      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4821000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.4     |\n",
      "|    critic_loss     | 0.895    |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.000179 |\n",
      "|    n_updates       | 23540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4822000, episode_reward=362.87 +/- 187.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 363      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4822000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4823000, episode_reward=323.08 +/- 66.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 323      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4823000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4824000, episode_reward=272.32 +/- 3.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 272      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4824000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.7     |\n",
      "|    critic_loss     | 0.862    |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.000177 |\n",
      "|    n_updates       | 23550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 171      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4824     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7137     |\n",
      "|    total_timesteps | 4824000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4825000, episode_reward=420.37 +/- 300.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 420      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4825000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4826000, episode_reward=342.29 +/- 90.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 342      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4826000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.5     |\n",
      "|    critic_loss     | 0.892    |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.000175 |\n",
      "|    n_updates       | 23560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4827000, episode_reward=293.99 +/- 6.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 294      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4827000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4828000, episode_reward=405.09 +/- 149.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 405      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4828000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44       |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -0.0071  |\n",
      "|    learning_rate   | 0.000173 |\n",
      "|    n_updates       | 23570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 184      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4828     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7142     |\n",
      "|    total_timesteps | 4828000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4829000, episode_reward=323.08 +/- 48.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 323      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4829000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4830000, episode_reward=504.35 +/- 381.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 504      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.7     |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.000171 |\n",
      "|    n_updates       | 23580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4831000, episode_reward=269.10 +/- 3.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 269      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4831000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4832000, episode_reward=412.27 +/- 196.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 412      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4832000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.5     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.000169 |\n",
      "|    n_updates       | 23590    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 196      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4832     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7148     |\n",
      "|    total_timesteps | 4832000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4833000, episode_reward=507.65 +/- 279.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 508      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4833000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4834000, episode_reward=257.42 +/- 22.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 257      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4834000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.8     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.000167 |\n",
      "|    n_updates       | 23600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4835000, episode_reward=265.28 +/- 39.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 265      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4835000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4836000, episode_reward=396.28 +/- 153.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 396      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4836000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44       |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -0.233   |\n",
      "|    learning_rate   | 0.000165 |\n",
      "|    n_updates       | 23610    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 210      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4836     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7154     |\n",
      "|    total_timesteps | 4836000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4837000, episode_reward=271.72 +/- 52.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 272      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4837000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4838000, episode_reward=359.93 +/- 105.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 360      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4838000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.6     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -0.376   |\n",
      "|    learning_rate   | 0.000163 |\n",
      "|    n_updates       | 23620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4839000, episode_reward=279.29 +/- 6.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 279      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4839000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4840000, episode_reward=292.03 +/- 6.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.7     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -0.812   |\n",
      "|    learning_rate   | 0.000161 |\n",
      "|    n_updates       | 23630    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 225      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4840     |\n",
      "|    fps             | 675      |\n",
      "|    time_elapsed    | 7159     |\n",
      "|    total_timesteps | 4840000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4841000, episode_reward=425.83 +/- 173.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 426      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4841000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4842000, episode_reward=292.02 +/- 7.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4842000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.000159 |\n",
      "|    n_updates       | 23640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4843000, episode_reward=285.85 +/- 14.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 286      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4843000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4844000, episode_reward=352.68 +/- 121.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 353      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4844000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.9     |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 0.472    |\n",
      "|    learning_rate   | 0.000156 |\n",
      "|    n_updates       | 23650    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 246      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4844     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7165     |\n",
      "|    total_timesteps | 4844000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4845000, episode_reward=290.98 +/- 5.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 291      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4845000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4846000, episode_reward=287.78 +/- 4.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 288      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4846000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.8     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -0.692   |\n",
      "|    learning_rate   | 0.000154 |\n",
      "|    n_updates       | 23660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4847000, episode_reward=531.85 +/- 357.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 532      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4847000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4848000, episode_reward=279.05 +/- 22.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 279      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4848000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.9     |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 0.347    |\n",
      "|    learning_rate   | 0.000152 |\n",
      "|    n_updates       | 23670    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 262      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4848     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7171     |\n",
      "|    total_timesteps | 4848000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4849000, episode_reward=370.82 +/- 213.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 371      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4849000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4850000, episode_reward=271.21 +/- 15.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 271      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.1     |\n",
      "|    critic_loss     | 0.895    |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -1.7     |\n",
      "|    learning_rate   | 0.00015  |\n",
      "|    n_updates       | 23680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4851000, episode_reward=333.37 +/- 136.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4851000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4852000, episode_reward=460.94 +/- 369.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 461      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4852000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.000148 |\n",
      "|    n_updates       | 23690    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 268      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4852     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7176     |\n",
      "|    total_timesteps | 4852000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4853000, episode_reward=291.17 +/- 28.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 291      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4853000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4854000, episode_reward=328.94 +/- 85.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 329      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4854000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.000146 |\n",
      "|    n_updates       | 23700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4855000, episode_reward=302.81 +/- 34.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 303      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4855000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4856000, episode_reward=297.38 +/- 5.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 297      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4856000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.000144 |\n",
      "|    n_updates       | 23710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 276      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4856     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7182     |\n",
      "|    total_timesteps | 4856000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4857000, episode_reward=504.63 +/- 395.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 505      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4857000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4858000, episode_reward=387.00 +/- 142.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 387      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4858000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -2.15    |\n",
      "|    learning_rate   | 0.000142 |\n",
      "|    n_updates       | 23720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4859000, episode_reward=546.19 +/- 382.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 546      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4859000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4860000, episode_reward=493.08 +/- 354.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 493      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.5     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.00014  |\n",
      "|    n_updates       | 23730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 284      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4860     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7188     |\n",
      "|    total_timesteps | 4860000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4861000, episode_reward=317.47 +/- 2.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 317      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4861000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4862000, episode_reward=537.14 +/- 473.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 537      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4862000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.4     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.000138 |\n",
      "|    n_updates       | 23740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4863000, episode_reward=587.26 +/- 447.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 587      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4863000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4864000, episode_reward=321.36 +/- 33.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4864000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 294      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4864     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7193     |\n",
      "|    total_timesteps | 4864000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4865000, episode_reward=528.01 +/- 400.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 528      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4865000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.9     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -1.65    |\n",
      "|    learning_rate   | 0.000136 |\n",
      "|    n_updates       | 23750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4866000, episode_reward=359.14 +/- 54.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 359      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4866000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4867000, episode_reward=624.32 +/- 273.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 624      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4867000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.000134 |\n",
      "|    n_updates       | 23760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4868000, episode_reward=358.93 +/- 85.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 359      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4868000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 292      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4868     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7199     |\n",
      "|    total_timesteps | 4868000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4869000, episode_reward=330.54 +/- 35.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 331      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4869000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46       |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.000132 |\n",
      "|    n_updates       | 23770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4870000, episode_reward=312.64 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 313      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4870000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4871000, episode_reward=427.06 +/- 187.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4871000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.9     |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -0.232   |\n",
      "|    learning_rate   | 0.00013  |\n",
      "|    n_updates       | 23780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4872000, episode_reward=361.65 +/- 50.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 362      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4872000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 305      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4872     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7204     |\n",
      "|    total_timesteps | 4872000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4873000, episode_reward=337.53 +/- 1.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 338      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4873000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.000128 |\n",
      "|    n_updates       | 23790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4874000, episode_reward=347.54 +/- 20.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 348      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4874000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4875000, episode_reward=400.10 +/- 147.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 400      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4875000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.000126 |\n",
      "|    n_updates       | 23800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4876000, episode_reward=325.57 +/- 3.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 326      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4876000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 312      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4876     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7210     |\n",
      "|    total_timesteps | 4876000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4877000, episode_reward=326.24 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 326      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4877000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.6     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -1.4     |\n",
      "|    learning_rate   | 0.000124 |\n",
      "|    n_updates       | 23810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4878000, episode_reward=427.12 +/- 172.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4878000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4879000, episode_reward=317.58 +/- 8.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 318      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4879000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.4     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.000122 |\n",
      "|    n_updates       | 23820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4880000, episode_reward=314.37 +/- 3.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 314      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4880000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 298      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4880     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7216     |\n",
      "|    total_timesteps | 4880000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4881000, episode_reward=408.47 +/- 165.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 408      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4881000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.3     |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.00012  |\n",
      "|    n_updates       | 23830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4882000, episode_reward=396.73 +/- 141.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 397      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4882000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4883000, episode_reward=340.06 +/- 45.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 340      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4883000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.000118 |\n",
      "|    n_updates       | 23840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4884000, episode_reward=402.10 +/- 173.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 402      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4884000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 295      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4884     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7221     |\n",
      "|    total_timesteps | 4884000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4885000, episode_reward=607.40 +/- 619.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 607      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4885000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.8     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -1.9     |\n",
      "|    learning_rate   | 0.000116 |\n",
      "|    n_updates       | 23850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4886000, episode_reward=302.48 +/- 2.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4886000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4887000, episode_reward=330.70 +/- 35.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 331      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4887000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.1     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -1.73    |\n",
      "|    learning_rate   | 0.000113 |\n",
      "|    n_updates       | 23860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4888000, episode_reward=414.98 +/- 130.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 415      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4888000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 304      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4888     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7227     |\n",
      "|    total_timesteps | 4888000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4889000, episode_reward=349.84 +/- 2.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 350      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4889000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.4     |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -1.09    |\n",
      "|    learning_rate   | 0.000111 |\n",
      "|    n_updates       | 23870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4890000, episode_reward=588.21 +/- 290.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 588      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4890000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4891000, episode_reward=484.17 +/- 188.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 484      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4891000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.2     |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -0.555   |\n",
      "|    learning_rate   | 0.000109 |\n",
      "|    n_updates       | 23880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4892000, episode_reward=628.05 +/- 395.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 628      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4892000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 316      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4892     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7233     |\n",
      "|    total_timesteps | 4892000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4893000, episode_reward=322.71 +/- 4.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 323      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4893000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46       |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.000107 |\n",
      "|    n_updates       | 23890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4894000, episode_reward=337.37 +/- 28.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 337      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4894000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4895000, episode_reward=386.44 +/- 145.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 386      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4895000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.9     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -2.09    |\n",
      "|    learning_rate   | 0.000105 |\n",
      "|    n_updates       | 23900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4896000, episode_reward=754.82 +/- 596.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 755      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4896000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 324      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4896     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7238     |\n",
      "|    total_timesteps | 4896000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4897000, episode_reward=451.44 +/- 252.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4897000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.7     |\n",
      "|    critic_loss     | 3.56     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | 0.95     |\n",
      "|    learning_rate   | 0.000103 |\n",
      "|    n_updates       | 23910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4898000, episode_reward=346.16 +/- 46.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 346      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4898000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4899000, episode_reward=563.16 +/- 333.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 563      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4899000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -2.18    |\n",
      "|    learning_rate   | 0.000101 |\n",
      "|    n_updates       | 23920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4900000, episode_reward=560.79 +/- 472.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 561      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4900000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 330      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4900     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7244     |\n",
      "|    total_timesteps | 4900000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4901000, episode_reward=384.89 +/- 145.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 385      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4901000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47       |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -2.38    |\n",
      "|    learning_rate   | 9.91e-05 |\n",
      "|    n_updates       | 23930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4902000, episode_reward=467.00 +/- 249.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 467      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4902000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4903000, episode_reward=348.36 +/- 64.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 348      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4903000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.4     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -2.03    |\n",
      "|    learning_rate   | 9.71e-05 |\n",
      "|    n_updates       | 23940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4904000, episode_reward=327.81 +/- 38.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 328      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4904000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 336      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4904     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7250     |\n",
      "|    total_timesteps | 4904000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4905000, episode_reward=724.27 +/- 607.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 724      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4905000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.5     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -2.63    |\n",
      "|    learning_rate   | 9.5e-05  |\n",
      "|    n_updates       | 23950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4906000, episode_reward=471.64 +/- 248.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 472      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4906000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4907000, episode_reward=369.36 +/- 92.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 369      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4907000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4908000, episode_reward=376.69 +/- 85.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4908000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.2     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -1.95    |\n",
      "|    learning_rate   | 9.3e-05  |\n",
      "|    n_updates       | 23960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 337      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4908     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7255     |\n",
      "|    total_timesteps | 4908000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4909000, episode_reward=370.11 +/- 76.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 370      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4909000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4910000, episode_reward=485.00 +/- 109.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 485      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.8     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -1.95    |\n",
      "|    learning_rate   | 9.09e-05 |\n",
      "|    n_updates       | 23970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4911000, episode_reward=529.67 +/- 395.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 530      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4911000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4912000, episode_reward=528.45 +/- 132.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 528      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4912000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.9     |\n",
      "|    critic_loss     | 2.42     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -0.676   |\n",
      "|    learning_rate   | 8.89e-05 |\n",
      "|    n_updates       | 23980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 366      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4912     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7262     |\n",
      "|    total_timesteps | 4912000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4913000, episode_reward=646.60 +/- 643.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 647      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4913000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4914000, episode_reward=458.08 +/- 276.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 458      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4914000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.2     |\n",
      "|    critic_loss     | 4.1      |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | 1.85     |\n",
      "|    learning_rate   | 8.68e-05 |\n",
      "|    n_updates       | 23990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4915000, episode_reward=320.37 +/- 2.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 320      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4915000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4916000, episode_reward=641.94 +/- 588.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 642      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4916000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.6     |\n",
      "|    critic_loss     | 3.69     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | 0.673    |\n",
      "|    learning_rate   | 8.48e-05 |\n",
      "|    n_updates       | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 390      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4916     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7268     |\n",
      "|    total_timesteps | 4916000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4917000, episode_reward=443.58 +/- 159.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 444      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4917000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4918000, episode_reward=627.87 +/- 398.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 628      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4918000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.6     |\n",
      "|    critic_loss     | 2.46     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -0.676   |\n",
      "|    learning_rate   | 8.28e-05 |\n",
      "|    n_updates       | 24010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4919000, episode_reward=357.20 +/- 75.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 357      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4919000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4920000, episode_reward=636.75 +/- 645.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 637      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.7     |\n",
      "|    critic_loss     | 3.01     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -0.32    |\n",
      "|    learning_rate   | 8.07e-05 |\n",
      "|    n_updates       | 24020    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 408      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4920     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7274     |\n",
      "|    total_timesteps | 4920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4921000, episode_reward=339.76 +/- 56.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 340      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4921000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4922000, episode_reward=588.64 +/- 366.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 589      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4922000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.2     |\n",
      "|    critic_loss     | 2.14     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 7.87e-05 |\n",
      "|    n_updates       | 24030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4923000, episode_reward=330.70 +/- 45.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 331      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4923000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4924000, episode_reward=468.10 +/- 307.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 468      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4924000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.2     |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -0.723   |\n",
      "|    learning_rate   | 7.66e-05 |\n",
      "|    n_updates       | 24040    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 425      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4924     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7280     |\n",
      "|    total_timesteps | 4924000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4925000, episode_reward=417.17 +/- 143.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 417      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4925000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4926000, episode_reward=682.36 +/- 406.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 682      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4926000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.2     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -3.17    |\n",
      "|    learning_rate   | 7.46e-05 |\n",
      "|    n_updates       | 24050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4927000, episode_reward=454.42 +/- 195.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 454      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4927000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4928000, episode_reward=299.76 +/- 3.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 300      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4928000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.8     |\n",
      "|    critic_loss     | 2.59     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -0.873   |\n",
      "|    learning_rate   | 7.25e-05 |\n",
      "|    n_updates       | 24060    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 431      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4928     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7285     |\n",
      "|    total_timesteps | 4928000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4929000, episode_reward=367.56 +/- 125.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 368      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4929000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4930000, episode_reward=792.54 +/- 708.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 793      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.3     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -3.37    |\n",
      "|    learning_rate   | 7.05e-05 |\n",
      "|    n_updates       | 24070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4931000, episode_reward=868.52 +/- 604.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 869      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4931000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4932000, episode_reward=891.55 +/- 614.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 892      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4932000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.6     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -2.88    |\n",
      "|    learning_rate   | 6.84e-05 |\n",
      "|    n_updates       | 24080    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 424      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4932     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7291     |\n",
      "|    total_timesteps | 4932000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4933000, episode_reward=328.52 +/- 38.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 329      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4933000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4934000, episode_reward=407.02 +/- 132.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 407      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4934000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.2     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -2.97    |\n",
      "|    learning_rate   | 6.64e-05 |\n",
      "|    n_updates       | 24090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4935000, episode_reward=778.02 +/- 622.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 778      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4935000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4936000, episode_reward=310.43 +/- 1.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 310      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4936000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48       |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -2.62    |\n",
      "|    learning_rate   | 6.43e-05 |\n",
      "|    n_updates       | 24100    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 421      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4936     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7297     |\n",
      "|    total_timesteps | 4936000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4937000, episode_reward=746.28 +/- 577.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 746      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4937000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4938000, episode_reward=668.98 +/- 625.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 669      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4938000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48       |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -2.84    |\n",
      "|    learning_rate   | 6.23e-05 |\n",
      "|    n_updates       | 24110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4939000, episode_reward=612.09 +/- 412.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 612      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4939000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4940000, episode_reward=833.20 +/- 503.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 833      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.9     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -2.02    |\n",
      "|    learning_rate   | 6.02e-05 |\n",
      "|    n_updates       | 24120    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 421      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4940     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7302     |\n",
      "|    total_timesteps | 4940000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4941000, episode_reward=756.81 +/- 594.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 757      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4941000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4942000, episode_reward=452.19 +/- 198.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 452      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4942000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.4     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -2.74    |\n",
      "|    learning_rate   | 5.82e-05 |\n",
      "|    n_updates       | 24130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4943000, episode_reward=375.98 +/- 92.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 376      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4943000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4944000, episode_reward=720.64 +/- 607.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 721      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4944000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 37.6     |\n",
      "|    critic_loss     | 6.32     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 2.37     |\n",
      "|    learning_rate   | 5.61e-05 |\n",
      "|    n_updates       | 24140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 433      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4944     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7308     |\n",
      "|    total_timesteps | 4944000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4945000, episode_reward=312.56 +/- 3.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 313      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4945000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4946000, episode_reward=433.59 +/- 201.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 434      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4946000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 46.2     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 5.41e-05 |\n",
      "|    n_updates       | 24150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4947000, episode_reward=620.81 +/- 411.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 621      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4947000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4948000, episode_reward=915.29 +/- 622.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 915      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4948000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.5     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -2.98    |\n",
      "|    learning_rate   | 5.2e-05  |\n",
      "|    n_updates       | 24160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 429      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4948     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7313     |\n",
      "|    total_timesteps | 4948000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4949000, episode_reward=428.97 +/- 178.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 429      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4949000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4950000, episode_reward=328.17 +/- 37.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 328      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4950000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4951000, episode_reward=498.27 +/- 248.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 498      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4951000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.5     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -2.93    |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 24170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4952000, episode_reward=314.51 +/- 12.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 315      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4952000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 451      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4952     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7319     |\n",
      "|    total_timesteps | 4952000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4953000, episode_reward=500.74 +/- 270.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 501      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4953000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 37.4     |\n",
      "|    critic_loss     | 5.35     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 2.53     |\n",
      "|    learning_rate   | 4.79e-05 |\n",
      "|    n_updates       | 24180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4954000, episode_reward=897.29 +/- 722.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 897      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4954000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4955000, episode_reward=568.35 +/- 400.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 568      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4955000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.1     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 4.59e-05 |\n",
      "|    n_updates       | 24190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4956000, episode_reward=754.96 +/- 618.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 755      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4956000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 460      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4956     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7325     |\n",
      "|    total_timesteps | 4956000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4957000, episode_reward=659.27 +/- 617.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 659      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4957000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.6     |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 4.38e-05 |\n",
      "|    n_updates       | 24200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4958000, episode_reward=586.05 +/- 238.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 586      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4958000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4959000, episode_reward=701.73 +/- 229.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 702      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4959000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.9     |\n",
      "|    critic_loss     | 3.81     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 0.48     |\n",
      "|    learning_rate   | 4.18e-05 |\n",
      "|    n_updates       | 24210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4960000, episode_reward=681.91 +/- 606.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 682      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4960000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 476      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4960     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7330     |\n",
      "|    total_timesteps | 4960000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4961000, episode_reward=324.90 +/- 56.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 325      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4961000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.3     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -2.36    |\n",
      "|    learning_rate   | 3.97e-05 |\n",
      "|    n_updates       | 24220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4962000, episode_reward=622.47 +/- 653.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 622      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4962000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4963000, episode_reward=228.67 +/- 127.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4963000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49       |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -3.36    |\n",
      "|    learning_rate   | 3.77e-05 |\n",
      "|    n_updates       | 24230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4964000, episode_reward=545.39 +/- 212.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 545      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4964000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 475      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4964     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7336     |\n",
      "|    total_timesteps | 4964000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4965000, episode_reward=413.29 +/- 186.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 413      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4965000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.1     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -2.79    |\n",
      "|    learning_rate   | 3.56e-05 |\n",
      "|    n_updates       | 24240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4966000, episode_reward=610.62 +/- 402.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 611      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4966000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4967000, episode_reward=1087.70 +/- 626.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4967000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.4     |\n",
      "|    critic_loss     | 2.65     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 3.36e-05 |\n",
      "|    n_updates       | 24250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4968000, episode_reward=350.15 +/- 102.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 350      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4968000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 482      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4968     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7342     |\n",
      "|    total_timesteps | 4968000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4969000, episode_reward=808.07 +/- 632.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 808      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4969000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.4     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 3.16e-05 |\n",
      "|    n_updates       | 24260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4970000, episode_reward=351.58 +/- 96.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 352      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4970000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4971000, episode_reward=597.56 +/- 528.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 598      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4971000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.3     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -3.21    |\n",
      "|    learning_rate   | 2.95e-05 |\n",
      "|    n_updates       | 24270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4972000, episode_reward=571.70 +/- 539.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 572      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4972000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 478      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4972     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7347     |\n",
      "|    total_timesteps | 4972000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4973000, episode_reward=373.99 +/- 136.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4973000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.7     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -2.47    |\n",
      "|    learning_rate   | 2.75e-05 |\n",
      "|    n_updates       | 24280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4974000, episode_reward=367.26 +/- 122.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 367      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4974000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4975000, episode_reward=612.68 +/- 624.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 613      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4975000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 45.3     |\n",
      "|    critic_loss     | 2.56     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 2.54e-05 |\n",
      "|    n_updates       | 24290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4976000, episode_reward=452.73 +/- 269.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 453      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4976000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 488      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4976     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7353     |\n",
      "|    total_timesteps | 4976000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4977000, episode_reward=965.84 +/- 809.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 966      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4977000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.3     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -2.88    |\n",
      "|    learning_rate   | 2.34e-05 |\n",
      "|    n_updates       | 24300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4978000, episode_reward=612.55 +/- 239.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 613      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4978000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4979000, episode_reward=1219.66 +/- 536.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4979000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.7     |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -0.2     |\n",
      "|    learning_rate   | 2.13e-05 |\n",
      "|    n_updates       | 24310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4980000, episode_reward=845.89 +/- 659.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 846      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4980000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 496      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4980     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7359     |\n",
      "|    total_timesteps | 4980000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4981000, episode_reward=422.50 +/- 100.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 423      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4981000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 43.1     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -0.31    |\n",
      "|    learning_rate   | 1.93e-05 |\n",
      "|    n_updates       | 24320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4982000, episode_reward=631.14 +/- 559.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 631      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4982000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4983000, episode_reward=764.11 +/- 586.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 764      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4983000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48       |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -2.51    |\n",
      "|    learning_rate   | 1.72e-05 |\n",
      "|    n_updates       | 24330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4984000, episode_reward=1016.65 +/- 742.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4984000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 509      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4984     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7364     |\n",
      "|    total_timesteps | 4984000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4985000, episode_reward=485.55 +/- 235.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 486      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4985000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.3     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -2.77    |\n",
      "|    learning_rate   | 1.52e-05 |\n",
      "|    n_updates       | 24340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4986000, episode_reward=508.57 +/- 356.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 509      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4986000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4987000, episode_reward=1024.50 +/- 723.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4987000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.5     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -2.83    |\n",
      "|    learning_rate   | 1.31e-05 |\n",
      "|    n_updates       | 24350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4988000, episode_reward=474.17 +/- 163.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 474      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4988000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 506      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4988     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7370     |\n",
      "|    total_timesteps | 4988000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4989000, episode_reward=665.46 +/- 341.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 665      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4989000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.8     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -2.72    |\n",
      "|    learning_rate   | 1.11e-05 |\n",
      "|    n_updates       | 24360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4990000, episode_reward=848.13 +/- 583.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 848      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4990000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4991000, episode_reward=928.00 +/- 620.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 928      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4991000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 49.5     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -2.93    |\n",
      "|    learning_rate   | 9.02e-06 |\n",
      "|    n_updates       | 24370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4992000, episode_reward=902.79 +/- 739.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 903      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4992000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 502      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4992     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7376     |\n",
      "|    total_timesteps | 4992000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4993000, episode_reward=664.87 +/- 457.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 665      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4993000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4994000, episode_reward=1105.97 +/- 745.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4994000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.9     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -2.17    |\n",
      "|    learning_rate   | 6.98e-06 |\n",
      "|    n_updates       | 24380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4995000, episode_reward=626.06 +/- 651.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 626      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4995000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4996000, episode_reward=339.04 +/- 70.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 339      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4996000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 40.7     |\n",
      "|    critic_loss     | 4.5      |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | 0.808    |\n",
      "|    learning_rate   | 4.93e-06 |\n",
      "|    n_updates       | 24390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 504      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4996     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7381     |\n",
      "|    total_timesteps | 4996000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4997000, episode_reward=698.29 +/- 523.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 698      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4997000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4998000, episode_reward=486.87 +/- 269.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 487      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4998000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 48.9     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -2.75    |\n",
      "|    learning_rate   | 2.88e-06 |\n",
      "|    n_updates       | 24400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4999000, episode_reward=1224.68 +/- 593.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4999000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000000, episode_reward=579.20 +/- 315.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 579      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.4     |\n",
      "|    critic_loss     | 4.51     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | 0.74     |\n",
      "|    learning_rate   | 8.32e-07 |\n",
      "|    n_updates       | 24410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 524      |\n",
      "| time/              |          |\n",
      "|    episodes        | 5000     |\n",
      "|    fps             | 676      |\n",
      "|    time_elapsed    | 7387     |\n",
      "|    total_timesteps | 5000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5001000, episode_reward=498.29 +/- 388.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 498      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5001000  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 504x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_id = 'HalfCheetah-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "env = VecNormalize(env, norm_reward=False)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = VecNormalize(eval_env, norm_reward=False)\n",
    "\n",
    "pmd_model = PMD(\"MlpPolicy\",\n",
    "                  env,\n",
    "                  train_freq=2048,\n",
    "                  verbose=1,\n",
    "                  gamma=1.0, # average reward\n",
    "                  learning_rate=linear_schedule(5e-3))\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='logs/{}-apmd-on/'.format(env_id),\n",
    "                             log_path='logs/{}-apmd-on/'.format(env_id), eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "pmd_model.learn(total_timesteps=5e6, callback=eval_callback)\n",
    "\n",
    "results = np.array(eval_callback.evaluations_results)\n",
    "mdpo_mean_reward = np.mean(results, axis=1)\n",
    "mdpo_std_reward = np.std(results, axis=1)\n",
    "np.save(\"{}-mdpo-mean-redundent.npy\".format(env_id), mdpo_mean_reward)\n",
    "np.save(\"{}-mdpo-std-redundent.npy\".format(env_id), mdpo_std_reward)\n",
    "\n",
    "plot_costs([mdpo_mean_reward], names=['MDPO'], smoothing_window=50, n=1, fig_name=env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=1000, episode_reward=-1.54 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.54    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1.49 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.49    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1.27 +/- 0.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.27    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.27    |\n",
      "|    critic_loss     | 3.21     |\n",
      "|    ent_coef        | 0.978    |\n",
      "|    ent_coef_loss   | -0.225   |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 10       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1.40 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -265     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 659      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-1.14 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.14    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.86    |\n",
      "|    critic_loss     | 0.928    |\n",
      "|    ent_coef        | 0.93     |\n",
      "|    ent_coef_loss   | -0.733   |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 20       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-1.15 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.15    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-5.38 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.38    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.68    |\n",
      "|    critic_loss     | 0.492    |\n",
      "|    ent_coef        | 0.885    |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 30       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-5.32 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.32    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -242     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 651      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-3.59 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.59    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.62    |\n",
      "|    critic_loss     | 0.576    |\n",
      "|    ent_coef        | 0.841    |\n",
      "|    ent_coef_loss   | -1.73    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 40       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-3.30 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-2.05 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.05    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.76    |\n",
      "|    critic_loss     | 0.463    |\n",
      "|    ent_coef        | 0.8      |\n",
      "|    ent_coef_loss   | -2.24    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 50       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-1.71 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.71    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -252     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 598      |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-3.28 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.28    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.75    |\n",
      "|    critic_loss     | 0.328    |\n",
      "|    ent_coef        | 0.761    |\n",
      "|    ent_coef_loss   | -2.74    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 60       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-2.78 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.78    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-8.29 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.29    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.66    |\n",
      "|    critic_loss     | 0.29     |\n",
      "|    ent_coef        | 0.724    |\n",
      "|    ent_coef_loss   | -3.24    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 70       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-8.42 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.42    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -263     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 566      |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-9.64 +/- 0.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.64    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.6     |\n",
      "|    critic_loss     | 0.334    |\n",
      "|    ent_coef        | 0.689    |\n",
      "|    ent_coef_loss   | -3.71    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 80       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-10.47 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-7.25 +/- 0.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.25    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.65    |\n",
      "|    critic_loss     | 0.302    |\n",
      "|    ent_coef        | 0.656    |\n",
      "|    ent_coef_loss   | -4.24    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 90       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-7.23 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.23    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -265     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 558      |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-7.37 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.37    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.86    |\n",
      "|    critic_loss     | 0.288    |\n",
      "|    ent_coef        | 0.624    |\n",
      "|    ent_coef_loss   | -4.75    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 100      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-8.55 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.55    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-14.25 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -14.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.81    |\n",
      "|    critic_loss     | 0.228    |\n",
      "|    ent_coef        | 0.593    |\n",
      "|    ent_coef_loss   | -5.22    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 110      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-13.95 +/- 0.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -14      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 558      |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-12.96 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.52    |\n",
      "|    critic_loss     | 0.297    |\n",
      "|    ent_coef        | 0.565    |\n",
      "|    ent_coef_loss   | -5.68    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 120      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-12.39 +/- 0.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-21.93 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.39    |\n",
      "|    critic_loss     | 0.207    |\n",
      "|    ent_coef        | 0.537    |\n",
      "|    ent_coef_loss   | -6.18    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 130      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-21.95 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -255     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 555      |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-15.45 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -15.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.36    |\n",
      "|    critic_loss     | 0.35     |\n",
      "|    ent_coef        | 0.511    |\n",
      "|    ent_coef_loss   | -6.64    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 140      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-15.91 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -15.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-12.46 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.37    |\n",
      "|    critic_loss     | 0.235    |\n",
      "|    ent_coef        | 0.487    |\n",
      "|    ent_coef_loss   | -7.17    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 150      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-13.08 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -13.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -256     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 566      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-24.65 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.44    |\n",
      "|    critic_loss     | 0.255    |\n",
      "|    ent_coef        | 0.463    |\n",
      "|    ent_coef_loss   | -7.61    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 160      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-24.94 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-28.17 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -28.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.34    |\n",
      "|    critic_loss     | 0.225    |\n",
      "|    ent_coef        | 0.441    |\n",
      "|    ent_coef_loss   | -8.08    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 170      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-28.09 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -28.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 575      |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-30.85 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -30.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.27    |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.42     |\n",
      "|    ent_coef_loss   | -8.57    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 180      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-30.83 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -30.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-31.95 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.29    |\n",
      "|    critic_loss     | 0.2      |\n",
      "|    ent_coef        | 0.4      |\n",
      "|    ent_coef_loss   | -8.98    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 190      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-33.16 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -33.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -247     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 580      |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-40.13 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.18    |\n",
      "|    critic_loss     | 0.258    |\n",
      "|    ent_coef        | 0.381    |\n",
      "|    ent_coef_loss   | -9.43    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 200      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-39.49 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -39.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-40.31 +/- 0.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-41.92 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.04    |\n",
      "|    critic_loss     | 0.233    |\n",
      "|    ent_coef        | 0.363    |\n",
      "|    ent_coef_loss   | -9.82    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 210      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -239     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 587      |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-42.81 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-46.22 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -46.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.13    |\n",
      "|    critic_loss     | 0.249    |\n",
      "|    ent_coef        | 0.346    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 220      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-46.16 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -46.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-65.27 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -65.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.94    |\n",
      "|    critic_loss     | 0.228    |\n",
      "|    ent_coef        | 0.329    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 230      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -236     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 589      |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-66.28 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -66.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-71.39 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -71.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.91    |\n",
      "|    critic_loss     | 0.21     |\n",
      "|    ent_coef        | 0.314    |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 240      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-71.52 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -71.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-56.10 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.48    |\n",
      "|    critic_loss     | 0.244    |\n",
      "|    ent_coef        | 0.299    |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 250      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -228     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 593      |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-56.58 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-48.75 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.85    |\n",
      "|    critic_loss     | 0.289    |\n",
      "|    ent_coef        | 0.285    |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 260      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-48.76 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-87.13 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.84    |\n",
      "|    critic_loss     | 0.246    |\n",
      "|    ent_coef        | 0.272    |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 270      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 596      |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-87.74 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-64.32 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -64.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.78    |\n",
      "|    critic_loss     | 0.251    |\n",
      "|    ent_coef        | 0.259    |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 280      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-65.16 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -65.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-35.00 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -35      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.67    |\n",
      "|    critic_loss     | 0.199    |\n",
      "|    ent_coef        | 0.247    |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 290      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -222     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 599      |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-35.55 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -35.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-29.15 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -29.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.57    |\n",
      "|    critic_loss     | 0.192    |\n",
      "|    ent_coef        | 0.236    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-29.54 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -29.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-40.07 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.66    |\n",
      "|    critic_loss     | 0.199    |\n",
      "|    ent_coef        | 0.225    |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 310      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 605      |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-40.19 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-64.16 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -64.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.57    |\n",
      "|    critic_loss     | 0.222    |\n",
      "|    ent_coef        | 0.214    |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 320      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-64.14 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -64.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-47.43 +/- 32.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -47.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.06    |\n",
      "|    critic_loss     | 0.221    |\n",
      "|    ent_coef        | 0.205    |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 330      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -219     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 609      |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-73.83 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -73.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-69.14 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.65    |\n",
      "|    critic_loss     | 0.208    |\n",
      "|    ent_coef        | 0.195    |\n",
      "|    ent_coef_loss   | -14.9    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 340      |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "env_id = 'HalfCheetah-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "env = VecNormalize(env, norm_reward=False)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = VecNormalize(eval_env, norm_reward=False)\n",
    "\n",
    "pmd_model = PMD(\"MlpPolicy\",\n",
    "                  env,\n",
    "                  train_freq=2048,\n",
    "                  verbose=1,\n",
    "                  gamma=1.0, # average reward\n",
    "                  learning_rate=linear_schedule(5e-3))\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='logs/{}-apmd-ar/'.format(env_id),\n",
    "                             log_path='logs/{}-apmd-ar/'.format(env_id), eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "pmd_model.learn(total_timesteps=5e6, callback=eval_callback)\n",
    "\n",
    "results = np.array(eval_callback.evaluations_results)\n",
    "mdpo_mean_reward = np.mean(results, axis=1)\n",
    "mdpo_std_reward = np.std(results, axis=1)\n",
    "np.save(\"{}-mdpo-mean-redundent.npy\".format(env_id), mdpo_mean_reward)\n",
    "np.save(\"{}-mdpo-std-redundent.npy\".format(env_id), mdpo_std_reward)\n",
    "\n",
    "plot_costs([mdpo_mean_reward], names=['MDPO'], smoothing_window=50, n=1, fig_name=env_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d2e891c9d48cbc5657a17ab4ab08b2c1d2ec0060cc3e51694592beb2a6aa825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}