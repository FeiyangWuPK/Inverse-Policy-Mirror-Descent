{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# On-policy Entropy Regularized RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import mujoco_py\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from apmd_on.apmd import PMD\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EvaluateCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0, eval_env=None, model=None):\n",
    "        super(EvaluateCallback, self).__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.eval_env = eval_env\n",
    "        self.model = model\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        self.iter = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=5, deterministic=False)\n",
    "        print(f\"Iter {self.iter:d} mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        self.means.append(mean_reward)\n",
    "        self.stds.append(std_reward)\n",
    "        self.iter += 1\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Learning rate schedule \n",
    "from typing import Callable\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "def plot_costs(rewards, names, smoothing_window=10, n=3, fig_name=\"acrobot.png\", stds=None):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    colors = ['tomato', 'royalblue', 'mediumpurple']\n",
    "    for i in range(n):\n",
    "        extend = np.concatenate([np.ones(smoothing_window)*rewards[i][0], rewards[i]])\n",
    "        rewards_smoothed = pd.Series(extend).rolling(smoothing_window, min_periods=smoothing_window).mean().to_numpy()\n",
    "        rewards_smoothed = rewards_smoothed[smoothing_window-1:]\n",
    "        rewards_smoothed = rewards_smoothed[:5000]\n",
    "        x = np.linspace(1, 5000, num=5000)\n",
    "        if stds is None:\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3)\n",
    "        else:\n",
    "            lower = rewards_smoothed - stds[i][ :5000]\n",
    "            upper = rewards_smoothed + stds[i][ :5000]\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3, c=colors[i])\n",
    "            plt.fill_between(x, y1=lower, y2=upper, interpolate=True, c=colors[i], alpha=0.5)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episodic Costs\")\n",
    "    plt.title(fig_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "    # plt.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=1000, episode_reward=-1.66 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.66    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1.30 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-0.96 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.964   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.42    |\n",
      "|    critic_loss     | 3.54     |\n",
      "|    ent_coef        | 0.978    |\n",
      "|    ent_coef_loss   | -0.225   |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 10       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-1.24 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.24    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 680      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.78 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.781   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.83    |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.93     |\n",
      "|    ent_coef_loss   | -0.73    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 20       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-1.03 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-2.71 +/- 0.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.71    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.48    |\n",
      "|    critic_loss     | 0.644    |\n",
      "|    ent_coef        | 0.885    |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 30       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-2.88 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.88    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -281     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-0.78 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.776   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.26    |\n",
      "|    critic_loss     | 0.446    |\n",
      "|    ent_coef        | 0.841    |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 40       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-0.66 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.658   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-8.23 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.23    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.35    |\n",
      "|    critic_loss     | 0.312    |\n",
      "|    ent_coef        | 0.8      |\n",
      "|    ent_coef_loss   | -2.25    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 50       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-7.15 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.15    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -299     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-6.16 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -6.16    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.88    |\n",
      "|    critic_loss     | 0.37     |\n",
      "|    ent_coef        | 0.761    |\n",
      "|    ent_coef_loss   | -2.72    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 60       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-6.92 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -6.92    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-7.68 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.68    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.75    |\n",
      "|    critic_loss     | 0.313    |\n",
      "|    ent_coef        | 0.724    |\n",
      "|    ent_coef_loss   | -3.23    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 70       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-8.05 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.05    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -301     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-10.80 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.63    |\n",
      "|    critic_loss     | 0.283    |\n",
      "|    ent_coef        | 0.689    |\n",
      "|    ent_coef_loss   | -3.73    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 80       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-10.65 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-9.89 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.89    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.39    |\n",
      "|    critic_loss     | 0.32     |\n",
      "|    ent_coef        | 0.656    |\n",
      "|    ent_coef_loss   | -4.21    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 90       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-9.63 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.63    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -291     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-12.02 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.3      |\n",
      "|    ent_coef        | 0.624    |\n",
      "|    ent_coef_loss   | -4.72    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 100      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-11.56 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-11.35 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.594    |\n",
      "|    ent_coef_loss   | -5.22    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 110      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-11.19 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -283     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 700      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-16.44 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.02    |\n",
      "|    critic_loss     | 0.247    |\n",
      "|    ent_coef        | 0.565    |\n",
      "|    ent_coef_loss   | -5.71    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 120      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-16.70 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-21.73 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.85    |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.538    |\n",
      "|    ent_coef_loss   | -6.19    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 130      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-21.55 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -276     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 702      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-23.66 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.81    |\n",
      "|    critic_loss     | 0.257    |\n",
      "|    ent_coef        | 0.512    |\n",
      "|    ent_coef_loss   | -6.66    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 140      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-22.70 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-24.41 +/- 0.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 0.194    |\n",
      "|    ent_coef        | 0.487    |\n",
      "|    ent_coef_loss   | -7.16    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 150      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-23.64 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -262     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-22.18 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.43    |\n",
      "|    critic_loss     | 0.173    |\n",
      "|    ent_coef        | 0.463    |\n",
      "|    ent_coef_loss   | -7.64    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 160      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-21.94 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-30.10 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -30.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.267    |\n",
      "|    ent_coef        | 0.441    |\n",
      "|    ent_coef_loss   | -8.13    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 170      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-29.93 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -29.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -262     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-26.01 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2     |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.42     |\n",
      "|    ent_coef_loss   | -8.6     |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 180      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-26.22 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-40.25 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.928   |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.4      |\n",
      "|    ent_coef_loss   | -9.03    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 190      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-39.56 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -39.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -258     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-32.52 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.959   |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.381    |\n",
      "|    ent_coef_loss   | -9.49    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 200      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-32.90 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-34.86 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -34.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-22.19 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.855   |\n",
      "|    critic_loss     | 0.215    |\n",
      "|    ent_coef        | 0.362    |\n",
      "|    ent_coef_loss   | -9.89    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 210      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -257     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-22.18 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-49.62 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.67    |\n",
      "|    critic_loss     | 0.209    |\n",
      "|    ent_coef        | 0.345    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 220      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-50.24 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-56.22 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.692   |\n",
      "|    critic_loss     | 0.235    |\n",
      "|    ent_coef        | 0.329    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 230      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -249     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-56.68 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-51.47 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -51.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.311   |\n",
      "|    critic_loss     | 0.162    |\n",
      "|    ent_coef        | 0.313    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 240      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-52.19 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-55.87 +/- 31.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -55.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.179   |\n",
      "|    critic_loss     | 0.249    |\n",
      "|    ent_coef        | 0.298    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 250      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -250     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 703      |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-56.60 +/- 30.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-51.14 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -51.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.457   |\n",
      "|    critic_loss     | 0.212    |\n",
      "|    ent_coef        | 0.284    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 260      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-52.54 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-53.65 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.19    |\n",
      "|    critic_loss     | 0.229    |\n",
      "|    ent_coef        | 0.271    |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 270      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -245     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-53.64 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-80.59 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.144   |\n",
      "|    critic_loss     | 0.189    |\n",
      "|    ent_coef        | 0.258    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 280      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-81.61 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-80.81 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0511   |\n",
      "|    critic_loss     | 0.165    |\n",
      "|    ent_coef        | 0.246    |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 290      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-82.09 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -82.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-58.82 +/- 36.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -58.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0355   |\n",
      "|    critic_loss     | 0.154    |\n",
      "|    ent_coef        | 0.235    |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-41.62 +/- 44.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-61.77 +/- 16.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -61.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.00949  |\n",
      "|    critic_loss     | 0.219    |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 310      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -235     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 703      |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-70.89 +/- 1.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-52.36 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.282    |\n",
      "|    critic_loss     | 0.179    |\n",
      "|    ent_coef        | 0.213    |\n",
      "|    ent_coef_loss   | -14.5    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 320      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-52.56 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-69.10 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.314    |\n",
      "|    critic_loss     | 0.146    |\n",
      "|    ent_coef        | 0.203    |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 330      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 700      |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-69.53 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-24.43 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.441    |\n",
      "|    critic_loss     | 0.177    |\n",
      "|    ent_coef        | 0.194    |\n",
      "|    ent_coef_loss   | -15.4    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 340      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-24.55 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-36.74 +/- 33.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.419    |\n",
      "|    critic_loss     | 0.218    |\n",
      "|    ent_coef        | 0.185    |\n",
      "|    ent_coef_loss   | -15.6    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 350      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 699      |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-36.74 +/- 32.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-47.01 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -47      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.571    |\n",
      "|    critic_loss     | 0.166    |\n",
      "|    ent_coef        | 0.176    |\n",
      "|    ent_coef_loss   | -16      |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 360      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-48.25 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-56.18 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.59     |\n",
      "|    critic_loss     | 0.167    |\n",
      "|    ent_coef        | 0.168    |\n",
      "|    ent_coef_loss   | -16.7    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 370      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 76000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-56.97 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -57      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-26.17 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.593    |\n",
      "|    critic_loss     | 0.24     |\n",
      "|    ent_coef        | 0.161    |\n",
      "|    ent_coef_loss   | -16.8    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 380      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-27.45 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -27.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-90.15 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.512    |\n",
      "|    critic_loss     | 0.259    |\n",
      "|    ent_coef        | 0.153    |\n",
      "|    ent_coef_loss   | -17.5    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 390      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -218     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 699      |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-90.84 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-63.03 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.758    |\n",
      "|    critic_loss     | 0.257    |\n",
      "|    ent_coef        | 0.146    |\n",
      "|    ent_coef_loss   | -17.4    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 400      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-62.85 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -62.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-72.40 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -72.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.881    |\n",
      "|    critic_loss     | 0.22     |\n",
      "|    ent_coef        | 0.14     |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 410      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 700      |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-72.54 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -72.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-72.79 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -72.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-22.11 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.857    |\n",
      "|    critic_loss     | 0.3      |\n",
      "|    ent_coef        | 0.133    |\n",
      "|    ent_coef_loss   | -18.1    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 420      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-24.34 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -213     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 702      |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-69.62 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.731    |\n",
      "|    critic_loss     | 0.223    |\n",
      "|    ent_coef        | 0.127    |\n",
      "|    ent_coef_loss   | -18.1    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 430      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-68.50 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -68.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-78.30 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -78.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.825    |\n",
      "|    critic_loss     | 0.254    |\n",
      "|    ent_coef        | 0.122    |\n",
      "|    ent_coef_loss   | -20      |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 440      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-79.30 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -79.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 703      |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-49.60 +/- 39.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1        |\n",
      "|    critic_loss     | 0.203    |\n",
      "|    ent_coef        | 0.116    |\n",
      "|    ent_coef_loss   | -19.6    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 450      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-81.43 +/- 32.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-45.82 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.12     |\n",
      "|    critic_loss     | 0.146    |\n",
      "|    ent_coef        | 0.111    |\n",
      "|    ent_coef_loss   | -19.6    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 460      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-46.87 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -46.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 703      |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-41.53 +/- 7.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.04     |\n",
      "|    critic_loss     | 0.198    |\n",
      "|    ent_coef        | 0.106    |\n",
      "|    ent_coef_loss   | -21      |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 470      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-45.69 +/- 8.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-57.89 +/- 19.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -57.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.13     |\n",
      "|    critic_loss     | 0.144    |\n",
      "|    ent_coef        | 0.101    |\n",
      "|    ent_coef_loss   | -19.5    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 480      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-50.21 +/- 19.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -205     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 704      |\n",
      "|    time_elapsed    | 141      |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-86.44 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.16     |\n",
      "|    critic_loss     | 0.116    |\n",
      "|    ent_coef        | 0.0966   |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 490      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-76.05 +/- 25.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -76.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-77.33 +/- 6.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.16     |\n",
      "|    critic_loss     | 0.176    |\n",
      "|    ent_coef        | 0.0922   |\n",
      "|    ent_coef_loss   | -21.1    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 500      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-77.87 +/- 6.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -202     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-54.21 +/- 2.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -54.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.28     |\n",
      "|    critic_loss     | 0.165    |\n",
      "|    ent_coef        | 0.0881   |\n",
      "|    ent_coef_loss   | -21.9    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 510      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-54.90 +/- 2.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -54.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-203.79 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -204     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.04     |\n",
      "|    critic_loss     | 0.252    |\n",
      "|    ent_coef        | 0.0841   |\n",
      "|    ent_coef_loss   | -22.1    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 520      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-205.71 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -195     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 153      |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-100.07 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.44     |\n",
      "|    critic_loss     | 0.192    |\n",
      "|    ent_coef        | 0.0804   |\n",
      "|    ent_coef_loss   | -21.2    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 530      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-99.72 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-104.07 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 111000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.59     |\n",
      "|    critic_loss     | 0.157    |\n",
      "|    ent_coef        | 0.0769   |\n",
      "|    ent_coef_loss   | -21.6    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 540      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-105.49 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 705      |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-103.33 +/- 1.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.33     |\n",
      "|    critic_loss     | 0.193    |\n",
      "|    ent_coef        | 0.0736   |\n",
      "|    ent_coef_loss   | -23.5    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 550      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-104.15 +/- 2.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-101.37 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.09     |\n",
      "|    critic_loss     | 0.163    |\n",
      "|    ent_coef        | 0.0703   |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 560      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-99.87 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -178     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 706      |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-115.06 +/- 24.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.12     |\n",
      "|    critic_loss     | 0.296    |\n",
      "|    ent_coef        | 0.0672   |\n",
      "|    ent_coef_loss   | -24.6    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 570      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-118.18 +/- 23.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-84.52 +/- 36.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 119000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.63     |\n",
      "|    critic_loss     | 0.143    |\n",
      "|    ent_coef        | 0.0641   |\n",
      "|    ent_coef_loss   | -22.7    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 580      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-71.22 +/- 9.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -71.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -173     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 707      |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-70.51 +/- 2.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.32     |\n",
      "|    critic_loss     | 0.152    |\n",
      "|    ent_coef        | 0.0613   |\n",
      "|    ent_coef_loss   | -24.6    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 590      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-92.57 +/- 47.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=367.10 +/- 96.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 367      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 123000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.12     |\n",
      "|    critic_loss     | 0.184    |\n",
      "|    ent_coef        | 0.0586   |\n",
      "|    ent_coef_loss   | -24.8    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 600      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=124000, episode_reward=419.37 +/- 17.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 419      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -164     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 701      |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-265.65 +/- 11.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -266     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 125000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.603    |\n",
      "|    critic_loss     | 0.273    |\n",
      "|    ent_coef        | 0.0561   |\n",
      "|    ent_coef_loss   | -22      |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 610      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-267.75 +/- 11.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -268     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-52.40 +/- 2.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 127000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.43     |\n",
      "|    critic_loss     | 0.207    |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 620      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-50.48 +/- 4.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -158     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-49.23 +/- 3.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-98.45 +/- 2.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.17     |\n",
      "|    critic_loss     | 0.307    |\n",
      "|    ent_coef        | 0.0515   |\n",
      "|    ent_coef_loss   | -26.9    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 630      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-99.25 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-128.63 +/- 4.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.1      |\n",
      "|    critic_loss     | 0.312    |\n",
      "|    ent_coef        | 0.0493   |\n",
      "|    ent_coef_loss   | -23.2    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 640      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-131.83 +/- 4.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=49.48 +/- 8.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 49.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.728    |\n",
      "|    critic_loss     | 0.32     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | -21.1    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 650      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=47.70 +/- 10.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 47.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-3.88 +/- 207.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.88    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.734    |\n",
      "|    critic_loss     | 0.445    |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | -22.4    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 660      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 136000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-80.14 +/- 182.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=11.46 +/- 190.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.891    |\n",
      "|    critic_loss     | 0.481    |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -20.6    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 670      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-33.19 +/- 144.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -33.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-95.68 +/- 15.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.987    |\n",
      "|    critic_loss     | 0.262    |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -22.8    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 680      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-98.12 +/- 13.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=60.84 +/- 152.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 60.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.03     |\n",
      "|    critic_loss     | 0.364    |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -21.1    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 690      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-49.47 +/- 18.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-224.35 +/- 36.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.22     |\n",
      "|    critic_loss     | 0.999    |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | -21.6    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 700      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -135     |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-216.65 +/- 30.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -217     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-69.62 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.27     |\n",
      "|    critic_loss     | 0.459    |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | -19.2    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 710      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-69.06 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-63.21 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 148000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.55     |\n",
      "|    critic_loss     | 0.27     |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | -24.2    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 720      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -135     |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 148000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-63.92 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-132.86 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.52     |\n",
      "|    critic_loss     | 0.367    |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -26      |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 730      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-133.00 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-357.71 +/- 5.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -358     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 152000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.78     |\n",
      "|    critic_loss     | 0.392    |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -26.1    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 740      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -133     |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 699      |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 152000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-352.54 +/- 5.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-40.45 +/- 2.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 154000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.429    |\n",
      "|    critic_loss     | 0.943    |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 750      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-40.59 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-190.32 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 156000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.82     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 760      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -138     |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 698      |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 156000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-189.50 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -189     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-145.93 +/- 24.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 158000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.31     |\n",
      "|    critic_loss     | 0.558    |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | -25      |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 770      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=-147.26 +/- 26.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-33.52 +/- 4.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -33.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.12     |\n",
      "|    critic_loss     | 0.645    |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -27.3    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 780      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -140     |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 697      |\n",
      "|    time_elapsed    | 229      |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=-31.97 +/- 3.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-308.14 +/- 3.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 162000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.79     |\n",
      "|    critic_loss     | 0.376    |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -20.2    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 790      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-304.29 +/- 3.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -304     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-169.02 +/- 4.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -169     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 164000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 0.296    |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | -19.7    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 800      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -142     |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 164000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-204.75 +/- 41.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -205     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-762.57 +/- 40.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -763     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 166000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.98     |\n",
      "|    critic_loss     | 0.78     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 810      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-780.82 +/- 41.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -781     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=370.00 +/- 3.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 370      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 168000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.97     |\n",
      "|    critic_loss     | 0.665    |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | -18.6    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 820      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -142     |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 695      |\n",
      "|    time_elapsed    | 241      |\n",
      "|    total_timesteps | 168000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=361.29 +/- 9.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 361      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-147.15 +/- 158.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.65     |\n",
      "|    critic_loss     | 0.537    |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -21.9    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 830      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-258.39 +/- 116.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-135.25 +/- 167.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -145     |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=-297.93 +/- 92.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -298     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 0.434    |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -29.5    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 840      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-373.06 +/- 81.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-48.07 +/- 2.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.738    |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -7.07    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 850      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-45.03 +/- 2.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 253      |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-143.31 +/- 2.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.85     |\n",
      "|    critic_loss     | 0.406    |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 860      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-142.46 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-52.86 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 179000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 0.257    |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -29      |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 870      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-53.35 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -146     |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-560.24 +/- 2.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9      |\n",
      "|    critic_loss     | 0.298    |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -18.9    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 880      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-559.85 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-106.65 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.3      |\n",
      "|    critic_loss     | 0.358    |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | -5.31    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 890      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-107.60 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -155     |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-96.19 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -96.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 185000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.82     |\n",
      "|    critic_loss     | 0.389    |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 900      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-98.03 +/- 2.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-190.71 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.58     |\n",
      "|    critic_loss     | 0.33     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 910      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-189.83 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -160     |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 694      |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-743.00 +/- 19.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -743     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 189000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.04     |\n",
      "|    critic_loss     | 0.515    |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | -21      |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 920      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-727.04 +/- 13.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -727     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-116.25 +/- 16.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.84     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -17.8    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 930      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-104.59 +/- 17.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -169     |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=-150.66 +/- 49.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 193000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.67     |\n",
      "|    critic_loss     | 0.511    |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | -27.6    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 940      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-183.43 +/- 18.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-139.04 +/- 20.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0726   |\n",
      "|    critic_loss     | 0.904    |\n",
      "|    ent_coef        | 0.0183   |\n",
      "|    ent_coef_loss   | -23.2    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 950      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-157.26 +/- 33.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 283      |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-283.06 +/- 51.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -283     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 197000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.47     |\n",
      "|    critic_loss     | 0.3      |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | -8.83    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 960      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-273.07 +/- 35.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -273     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-84.24 +/- 325.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 199000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.09     |\n",
      "|    critic_loss     | 0.219    |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | -14.9    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 970      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=223.15 +/- 358.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 223      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -183     |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 289      |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=-246.86 +/- 16.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -247     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 201000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.15     |\n",
      "|    critic_loss     | 0.392    |\n",
      "|    ent_coef        | 0.0169   |\n",
      "|    ent_coef_loss   | -18.2    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 980      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=-241.16 +/- 11.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=-253.70 +/- 16.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -254     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 203000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.25     |\n",
      "|    critic_loss     | 0.188    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 990      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-252.79 +/- 9.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -253     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -189     |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 295      |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-379.86 +/- 16.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -380     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 205000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.25     |\n",
      "|    critic_loss     | 0.107    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 1000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=-335.36 +/- 96.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=596.63 +/- 115.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 597      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 207000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.71     |\n",
      "|    critic_loss     | 0.0742   |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1010     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=208000, episode_reward=160.71 +/- 339.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 161      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=387.56 +/- 51.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 388      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 209000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.59     |\n",
      "|    critic_loss     | 0.336    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=403.98 +/- 48.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 404      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=-718.34 +/- 5.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -718     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 211000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.74     |\n",
      "|    critic_loss     | 0.482    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-719.37 +/- 9.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -719     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -196     |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 307      |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=-277.91 +/- 14.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -278     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 213000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.81     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 43.4     |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=-272.37 +/- 15.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -272     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-270.11 +/- 9.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -270     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 215000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-381.00 +/- 11.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.34     |\n",
      "|    critic_loss     | 0.449    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -20      |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 313      |\n",
      "|    total_timesteps | 216000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=-382.90 +/- 10.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 217000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=-250.60 +/- 7.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -251     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.05     |\n",
      "|    critic_loss     | 0.247    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -19.6    |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=-240.57 +/- 3.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 219000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-248.43 +/- 4.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.42     |\n",
      "|    critic_loss     | 0.171    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -24.8    |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -219     |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 318      |\n",
      "|    total_timesteps | 220000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=-248.45 +/- 10.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 221000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-289.11 +/- 6.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -289     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 222000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.49     |\n",
      "|    critic_loss     | 0.172    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -25      |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=-275.91 +/- 17.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -276     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 223000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-278.85 +/- 28.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -279     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.3      |\n",
      "|    critic_loss     | 0.597    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -19      |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1090     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -232     |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 324      |\n",
      "|    total_timesteps | 224000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-159.73 +/- 203.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 225000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=-144.36 +/- 106.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 226000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.05     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -16.9    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=6.27 +/- 150.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 6.27     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 227000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-42.79 +/- 2.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 228000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.35     |\n",
      "|    critic_loss     | 0.361    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | -26.2    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1110     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 329      |\n",
      "|    total_timesteps | 228000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=-40.71 +/- 3.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 229000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-576.66 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -577     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.44     |\n",
      "|    critic_loss     | 0.586    |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=-576.27 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -576     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 231000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-234.28 +/- 2.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 232000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.51     |\n",
      "|    critic_loss     | 0.0944   |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -9.15    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1130     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -253     |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 335      |\n",
      "|    total_timesteps | 232000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=-233.16 +/- 2.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 233000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-405.67 +/- 4.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -406     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 234000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.08     |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-409.80 +/- 11.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -410     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-288.89 +/- 3.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -289     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 236000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.09     |\n",
      "|    critic_loss     | 0.0867   |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | -20.3    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1150     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -268     |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 340      |\n",
      "|    total_timesteps | 236000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=-286.98 +/- 2.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -287     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=-224.77 +/- 33.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 238000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.91     |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | -26.6    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=-250.76 +/- 2.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -251     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-211.43 +/- 22.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.76     |\n",
      "|    critic_loss     | 0.297    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -27.8    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1170     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -276     |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 346      |\n",
      "|    total_timesteps | 240000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=-228.67 +/- 4.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=-241.45 +/- 24.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 242000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.58     |\n",
      "|    critic_loss     | 0.154    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -28.7    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=-251.10 +/- 26.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -251     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-259.95 +/- 48.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -260     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 244000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.04     |\n",
      "|    critic_loss     | 0.377    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -24.4    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1190     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -280     |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 244000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-279.71 +/- 8.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -280     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-341.56 +/- 2.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -342     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 246000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.09     |\n",
      "|    critic_loss     | 0.291    |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=-350.09 +/- 8.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-377.46 +/- 76.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 248000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.24     |\n",
      "|    critic_loss     | 0.181    |\n",
      "|    ent_coef        | 0.00982  |\n",
      "|    ent_coef_loss   | -8.02    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1210     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -292     |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 357      |\n",
      "|    total_timesteps | 248000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=-366.05 +/- 75.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -366     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-525.47 +/- 3.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -525     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.41     |\n",
      "|    critic_loss     | 0.246    |\n",
      "|    ent_coef        | 0.00963  |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=-520.57 +/- 5.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -521     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=6.95 +/- 5.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 6.95     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 252000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.54     |\n",
      "|    critic_loss     | 0.124    |\n",
      "|    ent_coef        | 0.00946  |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1230     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -302     |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 364      |\n",
      "|    total_timesteps | 252000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=7.58 +/- 8.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 7.58     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=-314.27 +/- 9.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -314     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 254000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.56     |\n",
      "|    critic_loss     | 0.187    |\n",
      "|    ent_coef        | 0.00923  |\n",
      "|    ent_coef_loss   | -27.9    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-318.52 +/- 13.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -319     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-306.96 +/- 2.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -307     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -301     |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 370      |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=-243.51 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 257000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.12     |\n",
      "|    critic_loss     | 0.0556   |\n",
      "|    ent_coef        | 0.00896  |\n",
      "|    ent_coef_loss   | -7.08    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-242.44 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 258000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=-201.70 +/- 2.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -202     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 259000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.79     |\n",
      "|    critic_loss     | 0.071    |\n",
      "|    ent_coef        | 0.00878  |\n",
      "|    ent_coef_loss   | -24      |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-200.10 +/- 3.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -305     |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 377      |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=-460.34 +/- 4.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -460     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.09     |\n",
      "|    critic_loss     | 0.0571   |\n",
      "|    ent_coef        | 0.00854  |\n",
      "|    ent_coef_loss   | -23.8    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=-461.26 +/- 8.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -461     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 262000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=-178.67 +/- 3.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -179     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 263000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.3      |\n",
      "|    critic_loss     | 0.0557   |\n",
      "|    ent_coef        | 0.0083   |\n",
      "|    ent_coef_loss   | -16.1    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-176.37 +/- 2.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -176     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -314     |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 383      |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-238.06 +/- 10.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 265000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.95     |\n",
      "|    critic_loss     | 0.0501   |\n",
      "|    ent_coef        | 0.00807  |\n",
      "|    ent_coef_loss   | -22.9    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=-229.27 +/- 7.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 266000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=-191.10 +/- 11.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 267000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.14     |\n",
      "|    critic_loss     | 0.0441   |\n",
      "|    ent_coef        | 0.00784  |\n",
      "|    ent_coef_loss   | -23.4    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-183.38 +/- 2.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -320     |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 388      |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=-239.52 +/- 9.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -240     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 269000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.86     |\n",
      "|    critic_loss     | 0.0431   |\n",
      "|    ent_coef        | 0.0076   |\n",
      "|    ent_coef_loss   | -23.2    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-249.17 +/- 17.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -249     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=-84.70 +/- 106.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 271000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.89     |\n",
      "|    critic_loss     | 0.0363   |\n",
      "|    ent_coef        | 0.00736  |\n",
      "|    ent_coef_loss   | -26.5    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-62.13 +/- 91.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -62.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -320     |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 394      |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=39.38 +/- 78.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 39.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.48     |\n",
      "|    critic_loss     | 0.0665   |\n",
      "|    ent_coef        | 0.00712  |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=65.58 +/- 38.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 65.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 274000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-120.20 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 275000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.88     |\n",
      "|    critic_loss     | 0.135    |\n",
      "|    ent_coef        | 0.00691  |\n",
      "|    ent_coef_loss   | -7.5     |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-34.86 +/- 114.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -34.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -314     |\n",
      "| time/              |          |\n",
      "|    episodes        | 276      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 400      |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=52.75 +/- 102.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 52.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 277000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.26     |\n",
      "|    critic_loss     | 0.167    |\n",
      "|    ent_coef        | 0.00677  |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=55.08 +/- 103.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 55.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=32.53 +/- 5.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 32.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 279000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 0.135    |\n",
      "|    ent_coef        | 0.00664  |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=34.73 +/- 6.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 34.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -305     |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 406      |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=47.84 +/- 7.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 47.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 281000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.4      |\n",
      "|    critic_loss     | 0.0679   |\n",
      "|    ent_coef        | 0.00651  |\n",
      "|    ent_coef_loss   | -14      |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=62.22 +/- 12.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 62.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=22.27 +/- 4.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 22.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 283000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.37     |\n",
      "|    critic_loss     | 0.0832   |\n",
      "|    ent_coef        | 0.00639  |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=35.19 +/- 11.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 35.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -290     |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-53.08 +/- 4.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 285000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.61     |\n",
      "|    critic_loss     | 0.0741   |\n",
      "|    ent_coef        | 0.00624  |\n",
      "|    ent_coef_loss   | -20.9    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=-48.16 +/- 7.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=-17.22 +/- 118.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -17.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 287000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.56     |\n",
      "|    critic_loss     | 0.0682   |\n",
      "|    ent_coef        | 0.00609  |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=73.53 +/- 33.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 73.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -280     |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 417      |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=-414.87 +/- 2.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -415     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 289000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.34     |\n",
      "|    critic_loss     | 0.0964   |\n",
      "|    ent_coef        | 0.00597  |\n",
      "|    ent_coef_loss   | -6.67    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-179.55 +/- 292.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -180     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=-346.86 +/- 13.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 291000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.73     |\n",
      "|    critic_loss     | 0.0901   |\n",
      "|    ent_coef        | 0.0059   |\n",
      "|    ent_coef_loss   | -6.79    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-352.75 +/- 3.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -270     |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 423      |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=-334.68 +/- 3.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 293000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.28     |\n",
      "|    critic_loss     | 0.0368   |\n",
      "|    ent_coef        | 0.00583  |\n",
      "|    ent_coef_loss   | -6.23    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=-329.75 +/- 3.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=171.48 +/- 23.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 171      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 295000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.08     |\n",
      "|    critic_loss     | 0.0377   |\n",
      "|    ent_coef        | 0.00576  |\n",
      "|    ent_coef_loss   | -22.1    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=175.09 +/- 19.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 175      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -264     |\n",
      "| time/              |          |\n",
      "|    episodes        | 296      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 428      |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=393.00 +/- 6.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 393      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 297000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.07     |\n",
      "|    critic_loss     | 0.117    |\n",
      "|    ent_coef        | 0.00564  |\n",
      "|    ent_coef_loss   | -3.66    |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=374.19 +/- 20.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=392.55 +/- 3.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 393      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 299000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=327.96 +/- 19.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 328      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.48     |\n",
      "|    critic_loss     | 0.126    |\n",
      "|    ent_coef        | 0.00558  |\n",
      "|    ent_coef_loss   | 0.44     |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1460     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 434      |\n",
      "|    total_timesteps | 300000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=301000, episode_reward=340.27 +/- 10.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 340      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 301000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=241.71 +/- 11.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 242      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 302000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.7      |\n",
      "|    critic_loss     | 0.104    |\n",
      "|    ent_coef        | 0.00555  |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=245.97 +/- 18.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 246      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 303000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=66.86 +/- 7.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 66.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.89     |\n",
      "|    critic_loss     | 0.0887   |\n",
      "|    ent_coef        | 0.0055   |\n",
      "|    ent_coef_loss   | -2.23    |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    episodes        | 304      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 440      |\n",
      "|    total_timesteps | 304000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=39.99 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 40       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 305000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=168.03 +/- 8.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 168      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.17     |\n",
      "|    critic_loss     | 0.0789   |\n",
      "|    ent_coef        | 0.00546  |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=307000, episode_reward=185.90 +/- 16.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 186      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 307000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=334.30 +/- 29.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 334      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 308000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.06     |\n",
      "|    critic_loss     | 0.116    |\n",
      "|    ent_coef        | 0.00539  |\n",
      "|    ent_coef_loss   | -4.21    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -204     |\n",
      "| time/              |          |\n",
      "|    episodes        | 308      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 445      |\n",
      "|    total_timesteps | 308000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=321.41 +/- 39.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 309000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=511.64 +/- 28.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 512      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.61     |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.00535  |\n",
      "|    ent_coef_loss   | 2.39     |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=311000, episode_reward=491.91 +/- 21.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 492      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 311000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=320.76 +/- 32.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 312000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.64     |\n",
      "|    critic_loss     | 0.204    |\n",
      "|    ent_coef        | 0.00532  |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1520     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -191     |\n",
      "| time/              |          |\n",
      "|    episodes        | 312      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 451      |\n",
      "|    total_timesteps | 312000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=313000, episode_reward=328.50 +/- 50.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 329      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 313000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=136.73 +/- 28.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 137      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 314000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.14     |\n",
      "|    critic_loss     | 0.187    |\n",
      "|    ent_coef        | 0.00523  |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=45.95 +/- 43.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 45.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 315000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=110.31 +/- 14.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 316000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.11     |\n",
      "|    critic_loss     | 0.237    |\n",
      "|    ent_coef        | 0.00514  |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1540     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -171     |\n",
      "| time/              |          |\n",
      "|    episodes        | 316      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 456      |\n",
      "|    total_timesteps | 316000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=317000, episode_reward=119.80 +/- 18.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 120      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 317000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=196.61 +/- 11.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 197      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 318000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.5      |\n",
      "|    critic_loss     | 0.162    |\n",
      "|    ent_coef        | 0.00505  |\n",
      "|    ent_coef_loss   | -17.3    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=319000, episode_reward=212.27 +/- 9.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 212      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 319000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=124.30 +/- 25.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 124      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.01     |\n",
      "|    critic_loss     | 0.168    |\n",
      "|    ent_coef        | 0.00494  |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1560     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 462      |\n",
      "|    total_timesteps | 320000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=321000, episode_reward=96.71 +/- 33.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=329.32 +/- 34.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 329      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 322000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.91     |\n",
      "|    critic_loss     | 0.274    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -6.19    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323000, episode_reward=331.80 +/- 23.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 332      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-61.13 +/- 47.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -61.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 324000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.55     |\n",
      "|    critic_loss     | 0.239    |\n",
      "|    ent_coef        | 0.00479  |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1580     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    episodes        | 324      |\n",
      "|    fps             | 693      |\n",
      "|    time_elapsed    | 467      |\n",
      "|    total_timesteps | 324000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-9.05 +/- 29.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.05    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=124.75 +/- 53.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 125      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 326000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.46     |\n",
      "|    critic_loss     | 0.151    |\n",
      "|    ent_coef        | 0.00476  |\n",
      "|    ent_coef_loss   | -19.3    |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=68.43 +/- 44.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 68.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=272.99 +/- 16.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 273      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 328000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.22     |\n",
      "|    critic_loss     | 0.113    |\n",
      "|    ent_coef        | 0.00468  |\n",
      "|    ent_coef_loss   | -4.31    |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1600     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -119     |\n",
      "| time/              |          |\n",
      "|    episodes        | 328      |\n",
      "|    fps             | 692      |\n",
      "|    time_elapsed    | 473      |\n",
      "|    total_timesteps | 328000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=329000, episode_reward=286.81 +/- 28.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 287      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=250.63 +/- 21.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 251      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.54     |\n",
      "|    critic_loss     | 0.14     |\n",
      "|    ent_coef        | 0.00464  |\n",
      "|    ent_coef_loss   | 4.96     |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331000, episode_reward=236.36 +/- 26.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=308.98 +/- 28.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 332000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.87     |\n",
      "|    critic_loss     | 0.127    |\n",
      "|    ent_coef        | 0.00464  |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1620     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -94.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 332      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 480      |\n",
      "|    total_timesteps | 332000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=298.92 +/- 16.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 299      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=288.77 +/- 14.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 289      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 334000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.6      |\n",
      "|    critic_loss     | 0.145    |\n",
      "|    ent_coef        | 0.00466  |\n",
      "|    ent_coef_loss   | 10.8     |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=308.85 +/- 21.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=366.85 +/- 33.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 367      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 336000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.89     |\n",
      "|    critic_loss     | 0.121    |\n",
      "|    ent_coef        | 0.00469  |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1640     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -68.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 336      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 486      |\n",
      "|    total_timesteps | 336000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=337000, episode_reward=332.59 +/- 20.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 337000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=368.29 +/- 22.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 368      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 338000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.8      |\n",
      "|    critic_loss     | 0.124    |\n",
      "|    ent_coef        | 0.00467  |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339000, episode_reward=386.79 +/- 19.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 387      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=470.66 +/- 36.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 471      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.42     |\n",
      "|    critic_loss     | 0.183    |\n",
      "|    ent_coef        | 0.00467  |\n",
      "|    ent_coef_loss   | 4.73     |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1660     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -46.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 492      |\n",
      "|    total_timesteps | 340000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=341000, episode_reward=469.55 +/- 46.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=490.43 +/- 32.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 490      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=343000, episode_reward=370.61 +/- 31.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 371      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 343000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.17     |\n",
      "|    critic_loss     | 0.239    |\n",
      "|    ent_coef        | 0.00469  |\n",
      "|    ent_coef_loss   | 2.25     |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=389.07 +/- 20.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 389      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -22.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 344      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 497      |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=279.61 +/- 23.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 345000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.68     |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.00469  |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=308.83 +/- 7.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 346000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=347000, episode_reward=148.19 +/- 27.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 148      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.59     |\n",
      "|    critic_loss     | 0.184    |\n",
      "|    ent_coef        | 0.00465  |\n",
      "|    ent_coef_loss   | -0.0869  |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=155.16 +/- 58.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 155      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77     |\n",
      "| time/              |          |\n",
      "|    episodes        | 348      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 503      |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=349000, episode_reward=191.73 +/- 25.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 192      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 349000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.76     |\n",
      "|    critic_loss     | 0.389    |\n",
      "|    ent_coef        | 0.00465  |\n",
      "|    ent_coef_loss   | 19.4     |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=76.37 +/- 241.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 76.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=351000, episode_reward=221.69 +/- 254.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 222      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 351000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3        |\n",
      "|    critic_loss     | 0.231    |\n",
      "|    ent_coef        | 0.0047   |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=280.55 +/- 51.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 281      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 352      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 510      |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=353000, episode_reward=137.22 +/- 370.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 137      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 353000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.21     |\n",
      "|    critic_loss     | 0.283    |\n",
      "|    ent_coef        | 0.00467  |\n",
      "|    ent_coef_loss   | -8.08    |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=-313.16 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -313     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 354000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=64.90 +/- 278.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 64.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 355000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.05     |\n",
      "|    critic_loss     | 0.242    |\n",
      "|    ent_coef        | 0.00463  |\n",
      "|    ent_coef_loss   | 13       |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=261.37 +/- 43.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 261      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 45.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 356      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 515      |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=-143.23 +/- 80.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 357000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.23     |\n",
      "|    critic_loss     | 0.331    |\n",
      "|    ent_coef        | 0.00468  |\n",
      "|    ent_coef_loss   | 17.9     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=43.54 +/- 173.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 43.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 358000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=359000, episode_reward=-75.27 +/- 81.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -75.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.78     |\n",
      "|    critic_loss     | 0.192    |\n",
      "|    ent_coef        | 0.00477  |\n",
      "|    ent_coef_loss   | 12.3     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-183.89 +/- 181.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -184     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 521      |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=361000, episode_reward=282.45 +/- 31.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 282      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 361000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.7      |\n",
      "|    critic_loss     | 0.123    |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=50.75 +/- 244.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 50.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 362000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=26.71 +/- 219.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 26.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 363000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.42     |\n",
      "|    critic_loss     | 0.189    |\n",
      "|    ent_coef        | 0.00491  |\n",
      "|    ent_coef_loss   | 6.41     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=26.11 +/- 220.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 26.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 82.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 364      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 527      |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=350.75 +/- 19.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 351      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 365000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.44     |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.00495  |\n",
      "|    ent_coef_loss   | 1.92     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=379.09 +/- 2.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 379      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=367000, episode_reward=297.24 +/- 10.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 297      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 367000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.35     |\n",
      "|    critic_loss     | 0.152    |\n",
      "|    ent_coef        | 0.00498  |\n",
      "|    ent_coef_loss   | -0.779   |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=304.15 +/- 17.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 304      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    episodes        | 368      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 532      |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=21.85 +/- 393.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 21.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 369000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.25     |\n",
      "|    critic_loss     | 0.125    |\n",
      "|    ent_coef        | 0.00498  |\n",
      "|    ent_coef_loss   | -9.01    |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=31.66 +/- 400.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=371000, episode_reward=333.05 +/- 16.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 371000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.24     |\n",
      "|    critic_loss     | 0.23     |\n",
      "|    ent_coef        | 0.00494  |\n",
      "|    ent_coef_loss   | -2.47    |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=156.03 +/- 294.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 156      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 117      |\n",
      "| time/              |          |\n",
      "|    episodes        | 372      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 538      |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=373000, episode_reward=382.43 +/- 16.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 382      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 373000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.22     |\n",
      "|    critic_loss     | 0.148    |\n",
      "|    ent_coef        | 0.00494  |\n",
      "|    ent_coef_loss   | 12.3     |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=377.23 +/- 15.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=478.03 +/- 21.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 478      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 375000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.23     |\n",
      "|    critic_loss     | 0.174    |\n",
      "|    ent_coef        | 0.00496  |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=453.02 +/- 41.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 453      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 131      |\n",
      "| time/              |          |\n",
      "|    episodes        | 376      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 543      |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=377000, episode_reward=501.36 +/- 19.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 501      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 377000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9      |\n",
      "|    critic_loss     | 0.209    |\n",
      "|    ent_coef        | 0.00492  |\n",
      "|    ent_coef_loss   | -4.54    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=498.25 +/- 46.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 498      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=379000, episode_reward=618.09 +/- 25.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 618      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 379000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.93     |\n",
      "|    critic_loss     | 0.313    |\n",
      "|    ent_coef        | 0.0049   |\n",
      "|    ent_coef_loss   | 5.41     |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1850     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=380000, episode_reward=642.52 +/- 38.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 643      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 138      |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 691      |\n",
      "|    time_elapsed    | 549      |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=699.96 +/- 27.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 700      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 381000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.37     |\n",
      "|    critic_loss     | 0.242    |\n",
      "|    ent_coef        | 0.0049   |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1860     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=382000, episode_reward=705.81 +/- 26.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 706      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=383000, episode_reward=558.31 +/- 43.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 558      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 383000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.79     |\n",
      "|    critic_loss     | 0.198    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -5.24    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=569.30 +/- 33.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 569      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    episodes        | 384      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 556      |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=594.57 +/- 53.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 595      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 385000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=-60.69 +/- 335.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -60.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 386000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9      |\n",
      "|    critic_loss     | 0.266    |\n",
      "|    ent_coef        | 0.0048   |\n",
      "|    ent_coef_loss   | -7.05    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=119.21 +/- 429.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 119      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 387000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=103.67 +/- 433.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 388000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.92     |\n",
      "|    critic_loss     | 0.279    |\n",
      "|    ent_coef        | 0.00476  |\n",
      "|    ent_coef_loss   | 3.33     |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1890     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 178      |\n",
      "| time/              |          |\n",
      "|    episodes        | 388      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 562      |\n",
      "|    total_timesteps | 388000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=389000, episode_reward=282.41 +/- 437.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 282      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 389000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=399.25 +/- 44.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.72     |\n",
      "|    critic_loss     | 0.27     |\n",
      "|    ent_coef        | 0.00475  |\n",
      "|    ent_coef_loss   | -3.22    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391000, episode_reward=312.18 +/- 283.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 312      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 391000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=451.48 +/- 358.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 392000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.94     |\n",
      "|    critic_loss     | 0.281    |\n",
      "|    ent_coef        | 0.00473  |\n",
      "|    ent_coef_loss   | -8.19    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 204      |\n",
      "| time/              |          |\n",
      "|    episodes        | 392      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 568      |\n",
      "|    total_timesteps | 392000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=607.31 +/- 61.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 607      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 393000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=478.08 +/- 44.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 478      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 394000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.83     |\n",
      "|    critic_loss     | 0.247    |\n",
      "|    ent_coef        | 0.00469  |\n",
      "|    ent_coef_loss   | 4.34     |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=307.97 +/- 291.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 308      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 395000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=452.66 +/- 355.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 453      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.72     |\n",
      "|    critic_loss     | 0.319    |\n",
      "|    ent_coef        | 0.00471  |\n",
      "|    ent_coef_loss   | 15.5     |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1930     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 229      |\n",
      "| time/              |          |\n",
      "|    episodes        | 396      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 573      |\n",
      "|    total_timesteps | 396000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397000, episode_reward=450.24 +/- 356.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 450      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 397000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=289.30 +/- 24.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 289      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 398000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 0.207    |\n",
      "|    ent_coef        | 0.00476  |\n",
      "|    ent_coef_loss   | -9.79    |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=93.65 +/- 278.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 93.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 399000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=617.67 +/- 447.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 618      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.73     |\n",
      "|    critic_loss     | 0.291    |\n",
      "|    ent_coef        | 0.00475  |\n",
      "|    ent_coef_loss   | -5.33    |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1950     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 235      |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 580      |\n",
      "|    total_timesteps | 400000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401000, episode_reward=398.69 +/- 550.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 401000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=254.63 +/- 450.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 255      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 402000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.22     |\n",
      "|    critic_loss     | 0.314    |\n",
      "|    ent_coef        | 0.00473  |\n",
      "|    ent_coef_loss   | 8.53     |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403000, episode_reward=632.32 +/- 13.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 632      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 403000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=596.89 +/- 24.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 597      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 404000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.83     |\n",
      "|    critic_loss     | 0.274    |\n",
      "|    ent_coef        | 0.00475  |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1970     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 254      |\n",
      "| time/              |          |\n",
      "|    episodes        | 404      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 586      |\n",
      "|    total_timesteps | 404000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=623.38 +/- 27.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 623      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=608.87 +/- 24.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 609      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 406000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.7      |\n",
      "|    critic_loss     | 0.282    |\n",
      "|    ent_coef        | 0.00478  |\n",
      "|    ent_coef_loss   | 11.8     |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 1980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=407000, episode_reward=596.49 +/- 35.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 596      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=585.60 +/- 40.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 586      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 408000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.21     |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.00483  |\n",
      "|    ent_coef_loss   | -0.16    |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 1990     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 269      |\n",
      "| time/              |          |\n",
      "|    episodes        | 408      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 592      |\n",
      "|    total_timesteps | 408000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409000, episode_reward=573.50 +/- 37.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 574      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=554.52 +/- 44.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 555      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.99     |\n",
      "|    critic_loss     | 0.204    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -3.23    |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=513.48 +/- 52.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 513      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=560.35 +/- 33.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 560      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 412000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.14     |\n",
      "|    critic_loss     | 0.318    |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | 7.63     |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 2010     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 280      |\n",
      "| time/              |          |\n",
      "|    episodes        | 412      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 597      |\n",
      "|    total_timesteps | 412000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=413000, episode_reward=593.61 +/- 17.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 594      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=650.04 +/- 20.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 650      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 414000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.04     |\n",
      "|    critic_loss     | 0.33     |\n",
      "|    ent_coef        | 0.0049   |\n",
      "|    ent_coef_loss   | 7.34     |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 2020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=692.44 +/- 21.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 692      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=119.35 +/- 480.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 119      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 416000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.86     |\n",
      "|    critic_loss     | 0.278    |\n",
      "|    ent_coef        | 0.00494  |\n",
      "|    ent_coef_loss   | -3.01    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2030     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 300      |\n",
      "| time/              |          |\n",
      "|    episodes        | 416      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 603      |\n",
      "|    total_timesteps | 416000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=-73.57 +/- 396.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -73.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=75.84 +/- 445.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 75.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 418000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.8      |\n",
      "|    critic_loss     | 0.336    |\n",
      "|    ent_coef        | 0.00495  |\n",
      "|    ent_coef_loss   | 0.556    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=419000, episode_reward=577.16 +/- 43.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 577      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=337.54 +/- 267.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 338      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.87     |\n",
      "|    critic_loss     | 0.293    |\n",
      "|    ent_coef        | 0.00495  |\n",
      "|    ent_coef_loss   | -8.46    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 319      |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 608      |\n",
      "|    total_timesteps | 420000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421000, episode_reward=193.42 +/- 315.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=392.80 +/- 325.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 393      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 422000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.59     |\n",
      "|    critic_loss     | 0.287    |\n",
      "|    ent_coef        | 0.0049   |\n",
      "|    ent_coef_loss   | -6.15    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=237.75 +/- 402.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 238      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=567.17 +/- 30.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 567      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 424000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 0.245    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -2.9     |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 329      |\n",
      "| time/              |          |\n",
      "|    episodes        | 424      |\n",
      "|    fps             | 690      |\n",
      "|    time_elapsed    | 614      |\n",
      "|    total_timesteps | 424000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=563.63 +/- 15.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 564      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=706.07 +/- 31.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 706      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 426000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.16     |\n",
      "|    critic_loss     | 0.308    |\n",
      "|    ent_coef        | 0.00482  |\n",
      "|    ent_coef_loss   | 6.21     |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2080     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=427000, episode_reward=726.01 +/- 29.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 726      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=428000, episode_reward=708.48 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 708      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 350      |\n",
      "| time/              |          |\n",
      "|    episodes        | 428      |\n",
      "|    fps             | 689      |\n",
      "|    time_elapsed    | 620      |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=809.97 +/- 20.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 810      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 429000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.43     |\n",
      "|    critic_loss     | 0.282    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 1.73     |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2090     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=430000, episode_reward=808.78 +/- 21.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 809      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=431000, episode_reward=655.87 +/- 23.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 656      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 431000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.42     |\n",
      "|    critic_loss     | 0.229    |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | -4.61    |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=638.17 +/- 36.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 638      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 366      |\n",
      "| time/              |          |\n",
      "|    episodes        | 432      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 627      |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=433000, episode_reward=559.21 +/- 19.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 559      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 0.282    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 0.203    |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=565.74 +/- 33.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 566      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 434000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=576.31 +/- 417.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 576      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 435000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.77     |\n",
      "|    critic_loss     | 0.325    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 3.84     |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=161.30 +/- 514.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 161      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 381      |\n",
      "| time/              |          |\n",
      "|    episodes        | 436      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 632      |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=437000, episode_reward=665.02 +/- 20.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 665      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 437000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.44     |\n",
      "|    critic_loss     | 0.284    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -4.22    |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=688.43 +/- 27.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 688      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 438000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=439000, episode_reward=-94.06 +/- 408.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -94.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 439000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.67     |\n",
      "|    critic_loss     | 0.323    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=133.53 +/- 502.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 134      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 397      |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 639      |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=441000, episode_reward=297.64 +/- 40.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 298      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 441000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.38     |\n",
      "|    critic_loss     | 0.303    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 6.63     |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=293.28 +/- 8.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 293      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 442000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443000, episode_reward=750.09 +/- 28.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 750      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 443000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.79     |\n",
      "|    critic_loss     | 0.282    |\n",
      "|    ent_coef        | 0.00487  |\n",
      "|    ent_coef_loss   | -6.22    |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=769.93 +/- 33.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 770      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 402      |\n",
      "| time/              |          |\n",
      "|    episodes        | 444      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 645      |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=505.31 +/- 39.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 505      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.66     |\n",
      "|    critic_loss     | 0.292    |\n",
      "|    ent_coef        | 0.00485  |\n",
      "|    ent_coef_loss   | -2.48    |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=460.51 +/- 38.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 461      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 446000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=634.25 +/- 17.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 634      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 447000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.41     |\n",
      "|    critic_loss     | 0.323    |\n",
      "|    ent_coef        | 0.00484  |\n",
      "|    ent_coef_loss   | 4.33     |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=641.01 +/- 22.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 641      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 416      |\n",
      "| time/              |          |\n",
      "|    episodes        | 448      |\n",
      "|    fps             | 688      |\n",
      "|    time_elapsed    | 651      |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=449000, episode_reward=-386.32 +/- 1.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -386     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 449000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.72     |\n",
      "|    critic_loss     | 0.272    |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | 9.04     |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-387.41 +/- 2.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -387     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=451000, episode_reward=481.53 +/- 27.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 482      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 451000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 0.295    |\n",
      "|    ent_coef        | 0.00492  |\n",
      "|    ent_coef_loss   | 11.8     |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=501.11 +/- 31.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 501      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 433      |\n",
      "| time/              |          |\n",
      "|    episodes        | 452      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 657      |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=453000, episode_reward=481.39 +/- 29.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 481      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 453000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.13     |\n",
      "|    critic_loss     | 0.253    |\n",
      "|    ent_coef        | 0.00501  |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=469.97 +/- 34.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=280.27 +/- 32.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 455000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.24     |\n",
      "|    critic_loss     | 0.262    |\n",
      "|    ent_coef        | 0.00503  |\n",
      "|    ent_coef_loss   | -1.76    |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=266.02 +/- 19.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 266      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 438      |\n",
      "| time/              |          |\n",
      "|    episodes        | 456      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 663      |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=457000, episode_reward=576.54 +/- 33.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 577      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 457000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.95     |\n",
      "|    critic_loss     | 0.249    |\n",
      "|    ent_coef        | 0.00503  |\n",
      "|    ent_coef_loss   | -4.99    |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=565.15 +/- 22.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 565      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=464.63 +/- 40.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 459000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.86     |\n",
      "|    critic_loss     | 0.292    |\n",
      "|    ent_coef        | 0.005    |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=442.35 +/- 35.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 442      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 456      |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 687      |\n",
      "|    time_elapsed    | 669      |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=461000, episode_reward=662.65 +/- 17.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 663      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 461000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.11     |\n",
      "|    critic_loss     | 0.321    |\n",
      "|    ent_coef        | 0.00498  |\n",
      "|    ent_coef_loss   | 8.35     |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=662.51 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 663      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463000, episode_reward=569.60 +/- 20.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 570      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 463000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.84     |\n",
      "|    critic_loss     | 0.276    |\n",
      "|    ent_coef        | 0.00501  |\n",
      "|    ent_coef_loss   | -3.6     |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2260     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "env_id = 'HalfCheetah-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "env = VecNormalize(env, norm_reward=False)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = VecNormalize(eval_env, norm_reward=False)\n",
    "\n",
    "pmd_model = PMD(\"MlpPolicy\",\n",
    "                  env,\n",
    "                  train_freq=2048,\n",
    "                  verbose=1,\n",
    "                  gamma=1.0, # average reward\n",
    "                  learning_rate=linear_schedule(5e-3))\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='logs/{}-apmd-on/'.format(env_id),\n",
    "                             log_path='logs/{}-apmd-on/'.format(env_id), eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "pmd_model.learn(total_timesteps=5e6, callback=eval_callback)\n",
    "\n",
    "results = np.array(eval_callback.evaluations_results)\n",
    "mdpo_mean_reward = np.mean(results, axis=1)\n",
    "mdpo_std_reward = np.std(results, axis=1)\n",
    "np.save(\"{}-mdpo-mean-redundent.npy\".format(env_id), mdpo_mean_reward)\n",
    "np.save(\"{}-mdpo-std-redundent.npy\".format(env_id), mdpo_std_reward)\n",
    "\n",
    "plot_costs([mdpo_mean_reward], names=['MDPO'], smoothing_window=50, n=1, fig_name=env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d2e891c9d48cbc5657a17ab4ab08b2c1d2ec0060cc3e51694592beb2a6aa825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}