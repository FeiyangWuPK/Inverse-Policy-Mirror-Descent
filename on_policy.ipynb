{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-policy Entropy Regularized RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import mujoco_py\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from apmd_on.apmd import SAC\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0, eval_env=None, model=None):\n",
    "        super(EvaluateCallback, self).__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.eval_env = eval_env\n",
    "        self.model = model\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        self.iter = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=5, deterministic=False)\n",
    "        print(f\"Iter {self.iter:d} mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        self.means.append(mean_reward)\n",
    "        self.stds.append(std_reward)\n",
    "        self.iter += 1\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Learning rate schedule \n",
    "from typing import Callable\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "def plot_costs(rewards, names, smoothing_window=10, n=3, fig_name=\"acrobot.png\", stds=None):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    colors = ['tomato', 'royalblue', 'mediumpurple']\n",
    "    for i in range(n):\n",
    "        extend = np.concatenate([np.ones(smoothing_window)*rewards[i][0], rewards[i]])\n",
    "        rewards_smoothed = pd.Series(extend).rolling(smoothing_window, min_periods=smoothing_window).mean().to_numpy()\n",
    "        rewards_smoothed = rewards_smoothed[smoothing_window-1:]\n",
    "        rewards_smoothed = rewards_smoothed[:5000]\n",
    "        x = np.linspace(1, 5000, num=5000)\n",
    "        if stds is None:\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3)\n",
    "        else:\n",
    "            lower = rewards_smoothed - stds[i][ :5000]\n",
    "            upper = rewards_smoothed + stds[i][ :5000]\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3, c=colors[i])\n",
    "            plt.fill_between(x, y1=lower, y2=upper, interpolate=True, c=colors[i], alpha=0.5)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episodic Costs\")\n",
    "    plt.title(fig_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "    # plt.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=1000, episode_reward=-2.25 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.25    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-3.20 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.60 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.598   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.28    |\n",
      "|    critic_loss     | 3.26     |\n",
      "|    ent_coef        | 0.978    |\n",
      "|    ent_coef_loss   | -0.225   |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 10       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-0.70 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.698   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 560      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.74 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.744   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.66    |\n",
      "|    critic_loss     | 0.806    |\n",
      "|    ent_coef        | 0.93     |\n",
      "|    ent_coef_loss   | -0.732   |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 20       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-1.05 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.05    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.98 +/- 0.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.979   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.39    |\n",
      "|    critic_loss     | 0.495    |\n",
      "|    ent_coef        | 0.885    |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 30       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.91 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.908   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -270     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 595      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-0.87 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.22    |\n",
      "|    critic_loss     | 0.303    |\n",
      "|    ent_coef        | 0.841    |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 40       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.52 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.519   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-0.60 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.596   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.18    |\n",
      "|    critic_loss     | 0.241    |\n",
      "|    ent_coef        | 0.8      |\n",
      "|    ent_coef_loss   | -2.25    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 50       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-0.42 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.416   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 588      |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-0.11 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.114   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.02    |\n",
      "|    critic_loss     | 0.181    |\n",
      "|    ent_coef        | 0.761    |\n",
      "|    ent_coef_loss   | -2.75    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 60       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=-0.41 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.411   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-0.23 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.228   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.89    |\n",
      "|    critic_loss     | 0.154    |\n",
      "|    ent_coef        | 0.724    |\n",
      "|    ent_coef_loss   | -3.26    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 70       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-0.32 +/- 0.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.316   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 593      |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-0.21 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.211   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.72    |\n",
      "|    critic_loss     | 0.145    |\n",
      "|    ent_coef        | 0.689    |\n",
      "|    ent_coef_loss   | -3.77    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 80       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-0.43 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.433   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-0.49 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.489   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.54    |\n",
      "|    critic_loss     | 0.11     |\n",
      "|    ent_coef        | 0.655    |\n",
      "|    ent_coef_loss   | -4.26    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 90       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-1.04 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.04    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 602      |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-0.45 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.452   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.45    |\n",
      "|    critic_loss     | 0.0952   |\n",
      "|    ent_coef        | 0.623    |\n",
      "|    ent_coef_loss   | -4.77    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 100      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-0.51 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.508   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-0.47 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.47    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.29    |\n",
      "|    critic_loss     | 0.0892   |\n",
      "|    ent_coef        | 0.593    |\n",
      "|    ent_coef_loss   | -5.26    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 110      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-0.05 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.0512  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -279     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 608      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-0.36 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.36    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.19    |\n",
      "|    critic_loss     | 0.0707   |\n",
      "|    ent_coef        | 0.564    |\n",
      "|    ent_coef_loss   | -5.77    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 120      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-0.70 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.703   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-0.24 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.24    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.07    |\n",
      "|    critic_loss     | 0.069    |\n",
      "|    ent_coef        | 0.537    |\n",
      "|    ent_coef_loss   | -6.28    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 130      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-0.23 +/- 0.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.23    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -274     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 611      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-0.53 +/- 0.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.527   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.94    |\n",
      "|    critic_loss     | 0.0583   |\n",
      "|    ent_coef        | 0.511    |\n",
      "|    ent_coef_loss   | -6.77    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 140      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-0.25 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.246   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-0.46 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.458   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.85    |\n",
      "|    critic_loss     | 0.0523   |\n",
      "|    ent_coef        | 0.486    |\n",
      "|    ent_coef_loss   | -7.28    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 150      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-0.36 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.362   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -272     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 615      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=0.20 +/- 0.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 0.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.7     |\n",
      "|    critic_loss     | 0.0561   |\n",
      "|    ent_coef        | 0.462    |\n",
      "|    ent_coef_loss   | -7.78    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 160      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=-0.38 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.378   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-0.38 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.383   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.59    |\n",
      "|    critic_loss     | 0.0462   |\n",
      "|    ent_coef        | 0.44     |\n",
      "|    ent_coef_loss   | -8.27    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 170      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-0.03 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.0298  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 616      |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-0.88 +/- 0.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.884   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.48    |\n",
      "|    critic_loss     | 0.0397   |\n",
      "|    ent_coef        | 0.419    |\n",
      "|    ent_coef_loss   | -8.77    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 180      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-0.80 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.803   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-0.43 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.429   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.4     |\n",
      "|    critic_loss     | 0.0362   |\n",
      "|    ent_coef        | 0.398    |\n",
      "|    ent_coef_loss   | -9.27    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 190      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-0.25 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.25    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -285     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 618      |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-1.06 +/- 0.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.06    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.3     |\n",
      "|    critic_loss     | 0.0367   |\n",
      "|    ent_coef        | 0.379    |\n",
      "|    ent_coef_loss   | -9.78    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 200      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-1.17 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.17    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-0.71 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.709   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-1.28 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.28    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.28    |\n",
      "|    critic_loss     | 0.0338   |\n",
      "|    ent_coef        | 0.361    |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 210      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -288     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 620      |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-1.41 +/- 0.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.41    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-0.54 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.537   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.26    |\n",
      "|    critic_loss     | 0.0422   |\n",
      "|    ent_coef        | 0.343    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 220      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-0.74 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.739   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=0.10 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 0.0972   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.02    |\n",
      "|    critic_loss     | 0.0293   |\n",
      "|    ent_coef        | 0.327    |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 230      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-0.21 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.206   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-0.65 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.65    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.02    |\n",
      "|    critic_loss     | 0.0315   |\n",
      "|    ent_coef        | 0.311    |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 240      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-0.69 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.687   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-1.01 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.01    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.94    |\n",
      "|    critic_loss     | 0.0262   |\n",
      "|    ent_coef        | 0.296    |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 250      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -287     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-0.63 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.635   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-0.65 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.652   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.92    |\n",
      "|    critic_loss     | 0.0257   |\n",
      "|    ent_coef        | 0.282    |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 260      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-1.34 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.34    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-0.76 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.761   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.85    |\n",
      "|    critic_loss     | 0.0245   |\n",
      "|    ent_coef        | 0.268    |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 270      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -284     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-0.35 +/- 0.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.35    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-0.41 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.411   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.75    |\n",
      "|    critic_loss     | 0.0204   |\n",
      "|    ent_coef        | 0.255    |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 280      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-0.99 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.993   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-1.62 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.62    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.79    |\n",
      "|    critic_loss     | 0.0253   |\n",
      "|    ent_coef        | 0.243    |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 290      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -285     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-1.24 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.24    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-0.71 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.712   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.63    |\n",
      "|    critic_loss     | 0.0239   |\n",
      "|    ent_coef        | 0.231    |\n",
      "|    ent_coef_loss   | -14.7    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-0.53 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.534   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-0.78 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.783   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.61    |\n",
      "|    critic_loss     | 0.0217   |\n",
      "|    ent_coef        | 0.22     |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 310      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -283     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 625      |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-0.57 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.566   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-0.63 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.625   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.52    |\n",
      "|    critic_loss     | 0.0179   |\n",
      "|    ent_coef        | 0.21     |\n",
      "|    ent_coef_loss   | -15.7    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 320      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-0.63 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.628   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-1.13 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.13    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.5     |\n",
      "|    critic_loss     | 0.019    |\n",
      "|    ent_coef        | 0.2      |\n",
      "|    ent_coef_loss   | -16.1    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 330      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -283     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-1.13 +/- 0.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.13    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-1.84 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.84    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.43    |\n",
      "|    critic_loss     | 0.0194   |\n",
      "|    ent_coef        | 0.19     |\n",
      "|    ent_coef_loss   | -16.6    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 340      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-1.63 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.63    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-1.23 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.23    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.38    |\n",
      "|    critic_loss     | 0.0188   |\n",
      "|    ent_coef        | 0.181    |\n",
      "|    ent_coef_loss   | -17      |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 350      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -284     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 627      |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-1.82 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.82    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-1.53 +/- 0.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.53    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.39    |\n",
      "|    critic_loss     | 0.0176   |\n",
      "|    ent_coef        | 0.172    |\n",
      "|    ent_coef_loss   | -17.6    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 360      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-2.06 +/- 0.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.06    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-1.44 +/- 0.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.44    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.34    |\n",
      "|    critic_loss     | 0.0178   |\n",
      "|    ent_coef        | 0.164    |\n",
      "|    ent_coef_loss   | -18      |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 370      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -281     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 121      |\n",
      "|    total_timesteps | 76000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-1.60 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-2.21 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.21    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.27    |\n",
      "|    critic_loss     | 0.0159   |\n",
      "|    ent_coef        | 0.156    |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 380      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-1.98 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.98    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-2.59 +/- 0.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.59    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.27    |\n",
      "|    critic_loss     | 0.0163   |\n",
      "|    ent_coef        | 0.149    |\n",
      "|    ent_coef_loss   | -18.9    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 390      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -279     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 627      |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-3.00 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-2.56 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.56    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.23    |\n",
      "|    critic_loss     | 0.0175   |\n",
      "|    ent_coef        | 0.142    |\n",
      "|    ent_coef_loss   | -19.4    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 400      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-2.16 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.16    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-2.96 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.96    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.16    |\n",
      "|    critic_loss     | 0.0169   |\n",
      "|    ent_coef        | 0.135    |\n",
      "|    ent_coef_loss   | -19.8    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 410      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -276     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 627      |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-2.30 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-2.25 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.25    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-1.39 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.39    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.15    |\n",
      "|    critic_loss     | 0.0146   |\n",
      "|    ent_coef        | 0.129    |\n",
      "|    ent_coef_loss   | -20.4    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 420      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-2.36 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.36    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -276     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 628      |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-2.16 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.16    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.11    |\n",
      "|    critic_loss     | 0.0165   |\n",
      "|    ent_coef        | 0.122    |\n",
      "|    ent_coef_loss   | -20.8    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 430      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-3.16 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.16    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-2.08 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.08    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.11    |\n",
      "|    critic_loss     | 0.018    |\n",
      "|    ent_coef        | 0.117    |\n",
      "|    ent_coef_loss   | -21.2    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 440      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-2.54 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.54    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -274     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 628      |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-2.96 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.96    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.07    |\n",
      "|    critic_loss     | 0.0158   |\n",
      "|    ent_coef        | 0.111    |\n",
      "|    ent_coef_loss   | -21.8    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 450      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-2.76 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.76    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-2.90 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.05    |\n",
      "|    critic_loss     | 0.0161   |\n",
      "|    ent_coef        | 0.106    |\n",
      "|    ent_coef_loss   | -22.3    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 460      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-2.68 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.68    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -272     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-1.22 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.22    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.03    |\n",
      "|    critic_loss     | 0.0163   |\n",
      "|    ent_coef        | 0.101    |\n",
      "|    ent_coef_loss   | -22.7    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 470      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-1.72 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.72    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-1.47 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.47    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.02    |\n",
      "|    critic_loss     | 0.0167   |\n",
      "|    ent_coef        | 0.0961   |\n",
      "|    ent_coef_loss   | -23.1    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 480      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-1.59 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.59    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -272     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-1.79 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.79    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.98    |\n",
      "|    critic_loss     | 0.0168   |\n",
      "|    ent_coef        | 0.0916   |\n",
      "|    ent_coef_loss   | -23.4    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 490      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-2.26 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.26    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-2.22 +/- 0.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.22    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.95    |\n",
      "|    critic_loss     | 0.0161   |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -23.9    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 500      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-3.15 +/- 0.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.15    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -272     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 165      |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-3.18 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.18    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.92    |\n",
      "|    critic_loss     | 0.017    |\n",
      "|    ent_coef        | 0.0832   |\n",
      "|    ent_coef_loss   | -24.3    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 510      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-2.80 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -2.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-3.95 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.95    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.91    |\n",
      "|    critic_loss     | 0.0165   |\n",
      "|    ent_coef        | 0.0793   |\n",
      "|    ent_coef_loss   | -24.8    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 520      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-4.20 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-5.74 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.74    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.88    |\n",
      "|    critic_loss     | 0.0146   |\n",
      "|    ent_coef        | 0.0756   |\n",
      "|    ent_coef_loss   | -25.3    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 530      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-5.57 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.57    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-5.54 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.54    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 111000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.87    |\n",
      "|    critic_loss     | 0.0165   |\n",
      "|    ent_coef        | 0.0721   |\n",
      "|    ent_coef_loss   | -25.5    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 540      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-5.80 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -274     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 177      |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-7.19 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.19    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.86    |\n",
      "|    critic_loss     | 0.0186   |\n",
      "|    ent_coef        | 0.0687   |\n",
      "|    ent_coef_loss   | -25.9    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 550      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-7.33 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.33    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-5.30 +/- 0.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.83    |\n",
      "|    critic_loss     | 0.0162   |\n",
      "|    ent_coef        | 0.0655   |\n",
      "|    ent_coef_loss   | -26.4    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 560      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-5.21 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.21    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -275     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-5.13 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.13    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.83    |\n",
      "|    critic_loss     | 0.0196   |\n",
      "|    ent_coef        | 0.0625   |\n",
      "|    ent_coef_loss   | -26.6    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 570      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-4.92 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.92    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-3.90 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 119000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.81    |\n",
      "|    critic_loss     | 0.018    |\n",
      "|    ent_coef        | 0.0596   |\n",
      "|    ent_coef_loss   | -27.2    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 580      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-3.19 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.19    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -271     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 190      |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-5.09 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.09    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.79    |\n",
      "|    critic_loss     | 0.0154   |\n",
      "|    ent_coef        | 0.0569   |\n",
      "|    ent_coef_loss   | -27.6    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 590      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-4.89 +/- 0.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.89    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-9.41 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.41    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 123000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.78    |\n",
      "|    critic_loss     | 0.0162   |\n",
      "|    ent_coef        | 0.0542   |\n",
      "|    ent_coef_loss   | -27.9    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 600      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-8.44 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.44    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -268     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-4.53 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.53    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 125000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.75    |\n",
      "|    critic_loss     | 0.0163   |\n",
      "|    ent_coef        | 0.0517   |\n",
      "|    ent_coef_loss   | -28.2    |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 610      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-4.97 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.97    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-8.21 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.21    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 127000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.9     |\n",
      "|    critic_loss     | 0.0375   |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | -27.7    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 620      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-8.99 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.99    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -268     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-8.66 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.66    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-8.75 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.75    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.71    |\n",
      "|    critic_loss     | 0.022    |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | -28.7    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 630      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-9.52 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -9.52    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-3.76 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.76    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.7     |\n",
      "|    critic_loss     | 0.0198   |\n",
      "|    ent_coef        | 0.045    |\n",
      "|    ent_coef_loss   | -28.6    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 640      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -266     |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-4.42 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.42    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-4.21 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.21    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 134000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.71    |\n",
      "|    critic_loss     | 0.0214   |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | -29.1    |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 650      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-4.93 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.93    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-5.03 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 136000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.69    |\n",
      "|    critic_loss     | 0.0171   |\n",
      "|    ent_coef        | 0.0411   |\n",
      "|    ent_coef_loss   | -30      |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 660      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -262     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 215      |\n",
      "|    total_timesteps | 136000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-4.92 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.92    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-6.21 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -6.21    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.73    |\n",
      "|    critic_loss     | 0.0264   |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | -28.6    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 670      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-6.06 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -6.06    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-7.61 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.61    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.68    |\n",
      "|    critic_loss     | 0.0218   |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | -30      |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 680      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -255     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 221      |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-7.00 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-7.44 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.44    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.69    |\n",
      "|    critic_loss     | 0.0248   |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | -28.9    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 690      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-7.07 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.07    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-4.52 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.52    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.66    |\n",
      "|    critic_loss     | 0.0206   |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -29.8    |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 700      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -250     |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-4.22 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.22    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-4.13 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.13    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.64    |\n",
      "|    critic_loss     | 0.0189   |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | -31.5    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 710      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-3.64 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.64    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-4.47 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.47    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 148000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.62    |\n",
      "|    critic_loss     | 0.0169   |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -31.2    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 720      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -247     |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 234      |\n",
      "|    total_timesteps | 148000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-3.44 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -3.44    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-75.82 +/- 5.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -75.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.62    |\n",
      "|    critic_loss     | 0.0215   |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | -30.6    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 730      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-63.72 +/- 25.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-27.73 +/- 0.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -27.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 152000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.6     |\n",
      "|    critic_loss     | 0.0209   |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | -31.2    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 740      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -243     |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 152000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-28.07 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -28.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-14.60 +/- 1.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -14.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 154000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.67    |\n",
      "|    critic_loss     | 0.0217   |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | -32.8    |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 750      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-15.20 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -15.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-18.66 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 156000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.64    |\n",
      "|    critic_loss     | 0.0197   |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | -31.1    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 760      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -243     |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 246      |\n",
      "|    total_timesteps | 156000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-18.98 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -19      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-22.32 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 158000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.62    |\n",
      "|    critic_loss     | 0.0186   |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -32      |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 770      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=-22.24 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-12.95 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.6     |\n",
      "|    critic_loss     | 0.0209   |\n",
      "|    ent_coef        | 0.0241   |\n",
      "|    ent_coef_loss   | -31      |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 780      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -240     |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=-12.31 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-8.14 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.14    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 162000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.54    |\n",
      "|    critic_loss     | 0.0191   |\n",
      "|    ent_coef        | 0.0231   |\n",
      "|    ent_coef_loss   | -32.5    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 790      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-8.05 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.05    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-6.56 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -6.56    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 164000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.51    |\n",
      "|    critic_loss     | 0.0189   |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | -31.5    |\n",
      "|    learning_rate   | 0.00484  |\n",
      "|    n_updates       | 800      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -239     |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 164000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-7.43 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.43    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-23.74 +/- 5.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 166000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.52    |\n",
      "|    critic_loss     | 0.0179   |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | -32.1    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 810      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-27.66 +/- 6.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -27.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-212.20 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -212     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 168000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.47    |\n",
      "|    critic_loss     | 0.0155   |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -33.7    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 820      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -236     |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 265      |\n",
      "|    total_timesteps | 168000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-204.99 +/- 5.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -205     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-1.09 +/- 11.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.09    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.5     |\n",
      "|    critic_loss     | 0.0167   |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -33.1    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 830      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-16.18 +/- 14.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-10.16 +/- 14.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 272      |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=13.83 +/- 64.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 13.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 173000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.48    |\n",
      "|    critic_loss     | 0.0162   |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | -33.1    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 840      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=174000, episode_reward=58.43 +/- 46.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 58.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=-168.33 +/- 118.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -168     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.52    |\n",
      "|    critic_loss     | 0.0157   |\n",
      "|    ent_coef        | 0.0179   |\n",
      "|    ent_coef_loss   | -32.8    |\n",
      "|    learning_rate   | 0.00483  |\n",
      "|    n_updates       | 850      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-96.35 +/- 72.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -96.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -228     |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 628      |\n",
      "|    time_elapsed    | 279      |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-162.14 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 177000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.64    |\n",
      "|    critic_loss     | 0.0174   |\n",
      "|    ent_coef        | 0.0172   |\n",
      "|    ent_coef_loss   | -34.1    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 860      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-163.43 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=186.49 +/- 65.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 186      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 179000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.43    |\n",
      "|    critic_loss     | 0.016    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | -32.7    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 870      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=141.54 +/- 14.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 142      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-102.15 +/- 10.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.44    |\n",
      "|    critic_loss     | 0.0157   |\n",
      "|    ent_coef        | 0.0158   |\n",
      "|    ent_coef_loss   | -32.1    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 880      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-105.75 +/- 7.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-400.91 +/- 39.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -401     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 183000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.56    |\n",
      "|    critic_loss     | 0.0122   |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | -36.5    |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 890      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-109.61 +/- 213.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 625      |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=186.00 +/- 47.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 186      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 185000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.39    |\n",
      "|    critic_loss     | 0.0122   |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -36      |\n",
      "|    learning_rate   | 0.00482  |\n",
      "|    n_updates       | 900      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=193.52 +/- 38.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 194      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=187000, episode_reward=-131.88 +/- 225.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.42    |\n",
      "|    critic_loss     | 0.0129   |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -33.2    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 910      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-114.97 +/- 236.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 300      |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=476.19 +/- 41.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 476      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 189000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.44    |\n",
      "|    critic_loss     | 0.0162   |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -30.4    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 920      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=465.16 +/- 28.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-857.30 +/- 6.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -857     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 191000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.41    |\n",
      "|    critic_loss     | 0.0141   |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -33.8    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 930      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-645.68 +/- 406.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -646     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -219     |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 307      |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=94.30 +/- 30.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 94.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 193000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.41    |\n",
      "|    critic_loss     | 0.0174   |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | -30.3    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 940      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=80.65 +/- 53.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 80.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-195.33 +/- 21.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -195     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.42    |\n",
      "|    critic_loss     | 0.0171   |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | -31.7    |\n",
      "|    learning_rate   | 0.00481  |\n",
      "|    n_updates       | 950      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-168.38 +/- 56.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -168     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 314      |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-848.75 +/- 148.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -849     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 197000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.37    |\n",
      "|    critic_loss     | 0.0137   |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | -35.2    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 960      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-502.36 +/- 282.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -502     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-223.67 +/- 83.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 199000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.53    |\n",
      "|    critic_loss     | 0.0146   |\n",
      "|    ent_coef        | 0.0111   |\n",
      "|    ent_coef_loss   | -35.7    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 970      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-241.41 +/- 67.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 321      |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=-121.98 +/- 24.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 201000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.53    |\n",
      "|    critic_loss     | 0.014    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -33.1    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 980      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=-110.89 +/- 29.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=-46.73 +/- 44.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -46.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 203000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.53    |\n",
      "|    critic_loss     | 0.0162   |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | -29      |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 990      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-91.06 +/- 46.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -91.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 327      |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=85.48 +/- 224.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 85.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 205000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.37    |\n",
      "|    critic_loss     | 0.0153   |\n",
      "|    ent_coef        | 0.00986  |\n",
      "|    ent_coef_loss   | -32.7    |\n",
      "|    learning_rate   | 0.0048   |\n",
      "|    n_updates       | 1000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=175.28 +/- 51.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 175      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=37.83 +/- 74.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 37.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 207000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.34    |\n",
      "|    critic_loss     | 0.0114   |\n",
      "|    ent_coef        | 0.0095   |\n",
      "|    ent_coef_loss   | -34.3    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=18.70 +/- 114.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 18.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 333      |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=19.05 +/- 94.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 19       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 209000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.32    |\n",
      "|    critic_loss     | 0.0105   |\n",
      "|    ent_coef        | 0.00914  |\n",
      "|    ent_coef_loss   | -37.1    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=19.03 +/- 130.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 19       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=-73.30 +/- 300.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -73.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 211000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.29    |\n",
      "|    critic_loss     | 0.00955  |\n",
      "|    ent_coef        | 0.00878  |\n",
      "|    ent_coef_loss   | -38.6    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=150.17 +/- 340.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 150      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 339      |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=101.48 +/- 67.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 101      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 213000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.33    |\n",
      "|    critic_loss     | 0.00913  |\n",
      "|    ent_coef        | 0.00842  |\n",
      "|    ent_coef_loss   | -39.1    |\n",
      "|    learning_rate   | 0.00479  |\n",
      "|    n_updates       | 1040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=129.03 +/- 80.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 129      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=143.03 +/- 26.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 143      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 215000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=0.36 +/- 16.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 0.363    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 216000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.31    |\n",
      "|    critic_loss     | 0.0108   |\n",
      "|    ent_coef        | 0.00807  |\n",
      "|    ent_coef_loss   | -35.5    |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -207     |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 346      |\n",
      "|    total_timesteps | 216000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=7.14 +/- 29.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 7.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 217000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=-0.57 +/- 19.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.568   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.49    |\n",
      "|    critic_loss     | 0.0163   |\n",
      "|    ent_coef        | 0.00776  |\n",
      "|    ent_coef_loss   | -30.7    |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=-34.92 +/- 65.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -34.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 219000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-160.36 +/- 28.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.32    |\n",
      "|    critic_loss     | 0.0138   |\n",
      "|    ent_coef        | 0.00748  |\n",
      "|    ent_coef_loss   | -29.1    |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -206     |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 220000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=-133.85 +/- 31.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 221000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-23.55 +/- 75.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 222000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.31    |\n",
      "|    critic_loss     | 0.0156   |\n",
      "|    ent_coef        | 0.00725  |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=-12.80 +/- 60.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 223000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=5.46 +/- 14.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.46     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.26    |\n",
      "|    critic_loss     | 0.0103   |\n",
      "|    ent_coef        | 0.00703  |\n",
      "|    ent_coef_loss   | -39.9    |\n",
      "|    learning_rate   | 0.00478  |\n",
      "|    n_updates       | 1090     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -208     |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 359      |\n",
      "|    total_timesteps | 224000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=11.16 +/- 3.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 11.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 225000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=-131.88 +/- 57.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 226000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.24    |\n",
      "|    critic_loss     | 0.00892  |\n",
      "|    ent_coef        | 0.00677  |\n",
      "|    ent_coef_loss   | -41.9    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=-122.83 +/- 22.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 227000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-68.10 +/- 130.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -68.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 228000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.25    |\n",
      "|    critic_loss     | 0.00927  |\n",
      "|    ent_coef        | 0.00649  |\n",
      "|    ent_coef_loss   | -37.6    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1110     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 366      |\n",
      "|    total_timesteps | 228000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=-90.92 +/- 70.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 229000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-159.74 +/- 59.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.31    |\n",
      "|    critic_loss     | 0.014    |\n",
      "|    ent_coef        | 0.00623  |\n",
      "|    ent_coef_loss   | -31.2    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=-90.42 +/- 26.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 231000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-555.67 +/- 317.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -556     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 232000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.28    |\n",
      "|    critic_loss     | 0.0143   |\n",
      "|    ent_coef        | 0.00602  |\n",
      "|    ent_coef_loss   | -27.9    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1130     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 373      |\n",
      "|    total_timesteps | 232000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=-762.00 +/- 287.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -762     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 233000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-97.39 +/- 105.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 234000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.29    |\n",
      "|    critic_loss     | 0.0117   |\n",
      "|    ent_coef        | 0.00582  |\n",
      "|    ent_coef_loss   | -33.9    |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 1140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-125.79 +/- 195.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-298.13 +/- 240.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -298     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 236000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.25    |\n",
      "|    critic_loss     | 0.0101   |\n",
      "|    ent_coef        | 0.00563  |\n",
      "|    ent_coef_loss   | -33.3    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1150     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -206     |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 379      |\n",
      "|    total_timesteps | 236000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=25.37 +/- 39.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 25.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=107.27 +/- 61.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 238000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.28    |\n",
      "|    critic_loss     | 0.00964  |\n",
      "|    ent_coef        | 0.00543  |\n",
      "|    ent_coef_loss   | -33.7    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=91.17 +/- 30.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 91.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-238.71 +/- 157.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -239     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.43    |\n",
      "|    critic_loss     | 0.0128   |\n",
      "|    ent_coef        | 0.00524  |\n",
      "|    ent_coef_loss   | -35.9    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1170     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -206     |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 620      |\n",
      "|    time_elapsed    | 386      |\n",
      "|    total_timesteps | 240000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=-325.01 +/- 14.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -325     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=-432.54 +/- 9.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -433     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 242000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.24    |\n",
      "|    critic_loss     | 0.00856  |\n",
      "|    ent_coef        | 0.00506  |\n",
      "|    ent_coef_loss   | -31.3    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=-417.38 +/- 57.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -417     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-82.06 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -82.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 244000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.26    |\n",
      "|    critic_loss     | 0.0105   |\n",
      "|    ent_coef        | 0.00489  |\n",
      "|    ent_coef_loss   | -30.6    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 1190     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -202     |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 392      |\n",
      "|    total_timesteps | 244000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-80.66 +/- 3.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-65.90 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -65.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 246000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.26    |\n",
      "|    critic_loss     | 0.00911  |\n",
      "|    ent_coef        | 0.00473  |\n",
      "|    ent_coef_loss   | -35.7    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=-65.29 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -65.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-66.45 +/- 190.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -66.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 248000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.32    |\n",
      "|    critic_loss     | 0.0102   |\n",
      "|    ent_coef        | 0.00457  |\n",
      "|    ent_coef_loss   | -36.1    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1210     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -196     |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 398      |\n",
      "|    total_timesteps | 248000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=8.61 +/- 76.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 8.61     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=80.52 +/- 34.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 80.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.23    |\n",
      "|    critic_loss     | 0.00912  |\n",
      "|    ent_coef        | 0.0044   |\n",
      "|    ent_coef_loss   | -32.1    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=43.04 +/- 75.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 43       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=85.37 +/- 46.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 85.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 252000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.23    |\n",
      "|    critic_loss     | 0.00976  |\n",
      "|    ent_coef        | 0.00426  |\n",
      "|    ent_coef_loss   | -27.1    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1230     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -195     |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 405      |\n",
      "|    total_timesteps | 252000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=-125.55 +/- 153.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=-501.86 +/- 115.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -502     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 254000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.22    |\n",
      "|    critic_loss     | 0.00985  |\n",
      "|    ent_coef        | 0.00413  |\n",
      "|    ent_coef_loss   | -28.5    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 1240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-514.07 +/- 243.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -514     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-559.57 +/- 124.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -193     |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 411      |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=-64.37 +/- 133.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -64.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 257000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.63    |\n",
      "|    critic_loss     | 0.0379   |\n",
      "|    ent_coef        | 0.00401  |\n",
      "|    ent_coef_loss   | -32.7    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-8.09 +/- 56.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.09    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 258000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=-71.99 +/- 116.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -72      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 259000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.18    |\n",
      "|    critic_loss     | 0.0187   |\n",
      "|    ent_coef        | 0.00388  |\n",
      "|    ent_coef_loss   | -37.3    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-56.42 +/- 137.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -56.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -189     |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 417      |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=69.17 +/- 170.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 69.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.18    |\n",
      "|    critic_loss     | 0.0141   |\n",
      "|    ent_coef        | 0.00375  |\n",
      "|    ent_coef_loss   | -29.5    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=143.00 +/- 37.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 143      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 262000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=112.02 +/- 37.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 112      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 263000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.13    |\n",
      "|    critic_loss     | 0.00946  |\n",
      "|    ent_coef        | 0.00363  |\n",
      "|    ent_coef_loss   | -29.1    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=118.36 +/- 122.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 118      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -191     |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 423      |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=166.01 +/- 72.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 166      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 265000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.13    |\n",
      "|    critic_loss     | 0.00973  |\n",
      "|    ent_coef        | 0.00352  |\n",
      "|    ent_coef_loss   | -29.6    |\n",
      "|    learning_rate   | 0.00474  |\n",
      "|    n_updates       | 1290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=193.07 +/- 68.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 266000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=375.47 +/- 91.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 375      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 267000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.17    |\n",
      "|    critic_loss     | 0.0135   |\n",
      "|    ent_coef        | 0.00342  |\n",
      "|    ent_coef_loss   | -31.8    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=398.73 +/- 66.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -191     |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 430      |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=302.66 +/- 268.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 303      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 269000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.15    |\n",
      "|    critic_loss     | 0.00939  |\n",
      "|    ent_coef        | 0.00331  |\n",
      "|    ent_coef_loss   | -30.5    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=287.65 +/- 139.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 288      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=91.15 +/- 145.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 91.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 271000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.12    |\n",
      "|    critic_loss     | 0.00728  |\n",
      "|    ent_coef        | 0.00321  |\n",
      "|    ent_coef_loss   | -26.9    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=190.28 +/- 20.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 190      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -191     |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=41.47 +/- 90.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 41.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.19    |\n",
      "|    critic_loss     | 0.0085   |\n",
      "|    ent_coef        | 0.00312  |\n",
      "|    ent_coef_loss   | -25.3    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=102.91 +/- 57.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 103      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 274000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-37.30 +/- 85.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -37.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 275000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.15    |\n",
      "|    critic_loss     | 0.00694  |\n",
      "|    ent_coef        | 0.00304  |\n",
      "|    ent_coef_loss   | -25.1    |\n",
      "|    learning_rate   | 0.00473  |\n",
      "|    n_updates       | 1340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-70.17 +/- 106.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -189     |\n",
      "| time/              |          |\n",
      "|    episodes        | 276      |\n",
      "|    fps             | 620      |\n",
      "|    time_elapsed    | 444      |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=110.53 +/- 26.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 111      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 277000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.25    |\n",
      "|    critic_loss     | 0.0119   |\n",
      "|    ent_coef        | 0.00296  |\n",
      "|    ent_coef_loss   | -25      |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=108.32 +/- 146.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=68.25 +/- 34.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 68.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 279000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.23    |\n",
      "|    critic_loss     | 0.00806  |\n",
      "|    ent_coef        | 0.00289  |\n",
      "|    ent_coef_loss   | -24.4    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=39.04 +/- 50.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 39       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 619      |\n",
      "|    time_elapsed    | 451      |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=9.06 +/- 111.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 9.06     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 281000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.12    |\n",
      "|    critic_loss     | 0.00692  |\n",
      "|    ent_coef        | 0.00282  |\n",
      "|    ent_coef_loss   | -34.9    |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=-21.48 +/- 74.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=-122.39 +/- 33.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 283000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.13    |\n",
      "|    critic_loss     | 0.00599  |\n",
      "|    ent_coef        | 0.00273  |\n",
      "|    ent_coef_loss   | -27      |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-73.33 +/- 77.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -73.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 618      |\n",
      "|    time_elapsed    | 459      |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-190.30 +/- 205.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 285000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.14    |\n",
      "|    critic_loss     | 0.00838  |\n",
      "|    ent_coef        | 0.00265  |\n",
      "|    ent_coef_loss   | -29      |\n",
      "|    learning_rate   | 0.00472  |\n",
      "|    n_updates       | 1390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=-50.15 +/- 59.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=-260.08 +/- 129.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -260     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 287000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.11    |\n",
      "|    critic_loss     | 0.00676  |\n",
      "|    ent_coef        | 0.00258  |\n",
      "|    ent_coef_loss   | -29.3    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-98.27 +/- 101.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 618      |\n",
      "|    time_elapsed    | 465      |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=-81.08 +/- 50.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 289000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.11    |\n",
      "|    critic_loss     | 0.00765  |\n",
      "|    ent_coef        | 0.00251  |\n",
      "|    ent_coef_loss   | -26.3    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-133.56 +/- 64.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=-35.01 +/- 61.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -35      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 291000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.12    |\n",
      "|    critic_loss     | 0.00663  |\n",
      "|    ent_coef        | 0.00244  |\n",
      "|    ent_coef_loss   | -23.3    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-63.69 +/- 36.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -185     |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 618      |\n",
      "|    time_elapsed    | 471      |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=41.20 +/- 40.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 41.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 293000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.12    |\n",
      "|    critic_loss     | 0.00632  |\n",
      "|    ent_coef        | 0.00238  |\n",
      "|    ent_coef_loss   | -24.4    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=40.88 +/- 32.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 40.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=95.75 +/- 44.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 295000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.12    |\n",
      "|    critic_loss     | 0.00651  |\n",
      "|    ent_coef        | 0.00232  |\n",
      "|    ent_coef_loss   | -30.8    |\n",
      "|    learning_rate   | 0.00471  |\n",
      "|    n_updates       | 1440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=43.65 +/- 36.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 43.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -183     |\n",
      "| time/              |          |\n",
      "|    episodes        | 296      |\n",
      "|    fps             | 619      |\n",
      "|    time_elapsed    | 478      |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=71.59 +/- 38.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 71.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 297000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.59    |\n",
      "|    critic_loss     | 0.0142   |\n",
      "|    ent_coef        | 0.00225  |\n",
      "|    ent_coef_loss   | -31.8    |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=189.55 +/- 102.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 190      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=96.55 +/- 69.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 299000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=124.74 +/- 174.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 125      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.09    |\n",
      "|    critic_loss     | 0.00818  |\n",
      "|    ent_coef        | 0.00219  |\n",
      "|    ent_coef_loss   | -28.2    |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1460     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -172     |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 619      |\n",
      "|    time_elapsed    | 484      |\n",
      "|    total_timesteps | 300000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=301000, episode_reward=164.14 +/- 101.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 164      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 301000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=-33.38 +/- 89.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -33.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 302000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.09    |\n",
      "|    critic_loss     | 0.00734  |\n",
      "|    ent_coef        | 0.00212  |\n",
      "|    ent_coef_loss   | -30.8    |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=-50.04 +/- 134.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -50      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 303000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-18.42 +/- 396.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.06    |\n",
      "|    critic_loss     | 0.00612  |\n",
      "|    ent_coef        | 0.00206  |\n",
      "|    ent_coef_loss   | -28      |\n",
      "|    learning_rate   | 0.0047   |\n",
      "|    n_updates       | 1480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -169     |\n",
      "| time/              |          |\n",
      "|    episodes        | 304      |\n",
      "|    fps             | 619      |\n",
      "|    time_elapsed    | 490      |\n",
      "|    total_timesteps | 304000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=78.57 +/- 144.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 78.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 305000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=82.79 +/- 287.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 82.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.08    |\n",
      "|    critic_loss     | 0.006    |\n",
      "|    ent_coef        | 0.002    |\n",
      "|    ent_coef_loss   | -32.7    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=307000, episode_reward=197.73 +/- 206.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 198      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 307000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-117.68 +/- 325.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 308000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.04    |\n",
      "|    critic_loss     | 0.00598  |\n",
      "|    ent_coef        | 0.00194  |\n",
      "|    ent_coef_loss   | -36.9    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -170     |\n",
      "| time/              |          |\n",
      "|    episodes        | 308      |\n",
      "|    fps             | 620      |\n",
      "|    time_elapsed    | 496      |\n",
      "|    total_timesteps | 308000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=-170.66 +/- 280.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -171     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 309000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-806.22 +/- 223.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -806     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.05    |\n",
      "|    critic_loss     | 0.00561  |\n",
      "|    ent_coef        | 0.00188  |\n",
      "|    ent_coef_loss   | -36.2    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=311000, episode_reward=-608.93 +/- 372.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -609     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 311000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-447.02 +/- 578.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -447     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 312000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.03    |\n",
      "|    critic_loss     | 0.00525  |\n",
      "|    ent_coef        | 0.00181  |\n",
      "|    ent_coef_loss   | -38      |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1520     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -169     |\n",
      "| time/              |          |\n",
      "|    episodes        | 312      |\n",
      "|    fps             | 620      |\n",
      "|    time_elapsed    | 502      |\n",
      "|    total_timesteps | 312000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=313000, episode_reward=-107.80 +/- 242.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 313000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=-152.00 +/- 380.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 314000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.04    |\n",
      "|    critic_loss     | 0.00605  |\n",
      "|    ent_coef        | 0.00175  |\n",
      "|    ent_coef_loss   | -36.5    |\n",
      "|    learning_rate   | 0.00469  |\n",
      "|    n_updates       | 1530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-127.38 +/- 251.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 315000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-87.29 +/- 241.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 316000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.05    |\n",
      "|    critic_loss     | 0.00525  |\n",
      "|    ent_coef        | 0.00168  |\n",
      "|    ent_coef_loss   | -35.1    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1540     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -166     |\n",
      "| time/              |          |\n",
      "|    episodes        | 316      |\n",
      "|    fps             | 620      |\n",
      "|    time_elapsed    | 509      |\n",
      "|    total_timesteps | 316000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=317000, episode_reward=53.70 +/- 207.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 53.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 317000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=-14.40 +/- 135.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -14.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 318000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.03    |\n",
      "|    critic_loss     | 0.00531  |\n",
      "|    ent_coef        | 0.00163  |\n",
      "|    ent_coef_loss   | -33.8    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=319000, episode_reward=-37.72 +/- 220.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -37.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 319000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-94.80 +/- 31.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -94.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.05    |\n",
      "|    critic_loss     | 0.00515  |\n",
      "|    ent_coef        | 0.00157  |\n",
      "|    ent_coef_loss   | -26.8    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1560     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -163     |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 620      |\n",
      "|    time_elapsed    | 515      |\n",
      "|    total_timesteps | 320000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=321000, episode_reward=-339.64 +/- 290.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=-373.96 +/- 540.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -374     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 322000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.07    |\n",
      "|    critic_loss     | 0.0057   |\n",
      "|    ent_coef        | 0.00153  |\n",
      "|    ent_coef_loss   | -19.7    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323000, episode_reward=-292.58 +/- 175.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -293     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-471.64 +/- 573.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -472     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 324000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.05    |\n",
      "|    critic_loss     | 0.00516  |\n",
      "|    ent_coef        | 0.0015   |\n",
      "|    ent_coef_loss   | -20.6    |\n",
      "|    learning_rate   | 0.00468  |\n",
      "|    n_updates       | 1580     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -155     |\n",
      "| time/              |          |\n",
      "|    episodes        | 324      |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 521      |\n",
      "|    total_timesteps | 324000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-519.01 +/- 538.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -519     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=-427.73 +/- 625.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -428     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 326000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.06    |\n",
      "|    critic_loss     | 0.00511  |\n",
      "|    ent_coef        | 0.00146  |\n",
      "|    ent_coef_loss   | -31.2    |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=-656.22 +/- 714.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -656     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-84.97 +/- 302.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 328000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.03    |\n",
      "|    critic_loss     | 0.0045   |\n",
      "|    ent_coef        | 0.00142  |\n",
      "|    ent_coef_loss   | -33.5    |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1600     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -150     |\n",
      "| time/              |          |\n",
      "|    episodes        | 328      |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 527      |\n",
      "|    total_timesteps | 328000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=329000, episode_reward=186.88 +/- 90.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 187      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-268.09 +/- 14.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -268     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.5     |\n",
      "|    critic_loss     | 0.0135   |\n",
      "|    ent_coef        | 0.00138  |\n",
      "|    ent_coef_loss   | -34.4    |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331000, episode_reward=-256.22 +/- 64.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -256     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-193.55 +/- 253.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 332000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.05    |\n",
      "|    critic_loss     | 0.00706  |\n",
      "|    ent_coef        | 0.00134  |\n",
      "|    ent_coef_loss   | -22.7    |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1620     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    episodes        | 332      |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 534      |\n",
      "|    total_timesteps | 332000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=-159.07 +/- 218.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=-7.74 +/- 182.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.74    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 334000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.08    |\n",
      "|    critic_loss     | 0.00483  |\n",
      "|    ent_coef        | 0.00131  |\n",
      "|    ent_coef_loss   | -20.7    |\n",
      "|    learning_rate   | 0.00467  |\n",
      "|    n_updates       | 1630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=22.56 +/- 156.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 22.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-1098.68 +/- 31.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 336000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.05    |\n",
      "|    critic_loss     | 0.00574  |\n",
      "|    ent_coef        | 0.00128  |\n",
      "|    ent_coef_loss   | -22.4    |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1640     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -141     |\n",
      "| time/              |          |\n",
      "|    episodes        | 336      |\n",
      "|    fps             | 621      |\n",
      "|    time_elapsed    | 540      |\n",
      "|    total_timesteps | 336000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=337000, episode_reward=-1123.43 +/- 16.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -1.12e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 337000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=-511.13 +/- 428.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -511     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 338000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.03    |\n",
      "|    critic_loss     | 0.00738  |\n",
      "|    ent_coef        | 0.00125  |\n",
      "|    ent_coef_loss   | -20.8    |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339000, episode_reward=-377.22 +/- 432.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-118.83 +/- 12.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.04    |\n",
      "|    critic_loss     | 0.00699  |\n",
      "|    ent_coef        | 0.00122  |\n",
      "|    ent_coef_loss   | -23.8    |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1660     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 546      |\n",
      "|    total_timesteps | 340000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=341000, episode_reward=-136.17 +/- 31.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=-119.48 +/- 11.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=343000, episode_reward=-185.80 +/- 28.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -186     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 343000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.98    |\n",
      "|    critic_loss     | 0.00482  |\n",
      "|    ent_coef        | 0.0012   |\n",
      "|    ent_coef_loss   | -8.17    |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-166.73 +/- 21.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -167     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -150     |\n",
      "| time/              |          |\n",
      "|    episodes        | 344      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 552      |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-360.48 +/- 7.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -360     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 345000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3       |\n",
      "|    critic_loss     | 0.00503  |\n",
      "|    ent_coef        | 0.00118  |\n",
      "|    ent_coef_loss   | -18.2    |\n",
      "|    learning_rate   | 0.00466  |\n",
      "|    n_updates       | 1680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=-324.32 +/- 68.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -324     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 346000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=347000, episode_reward=-293.73 +/- 115.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -294     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.99    |\n",
      "|    critic_loss     | 0.00515  |\n",
      "|    ent_coef        | 0.00116  |\n",
      "|    ent_coef_loss   | -21.3    |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-268.56 +/- 164.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -269     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -153     |\n",
      "| time/              |          |\n",
      "|    episodes        | 348      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 559      |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=349000, episode_reward=-470.39 +/- 236.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -470     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 349000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.99    |\n",
      "|    critic_loss     | 0.00591  |\n",
      "|    ent_coef        | 0.00114  |\n",
      "|    ent_coef_loss   | -16.8    |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-388.53 +/- 174.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -389     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=351000, episode_reward=-987.24 +/- 38.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -987     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 351000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.05    |\n",
      "|    critic_loss     | 0.00729  |\n",
      "|    ent_coef        | 0.00112  |\n",
      "|    ent_coef_loss   | -8.21    |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-1003.93 +/- 39.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1e+03   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -162     |\n",
      "| time/              |          |\n",
      "|    episodes        | 352      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 565      |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=353000, episode_reward=-659.11 +/- 45.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -659     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 353000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.04    |\n",
      "|    critic_loss     | 0.00724  |\n",
      "|    ent_coef        | 0.00111  |\n",
      "|    ent_coef_loss   | -6.96    |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=-630.82 +/- 28.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -631     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 354000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=-822.66 +/- 9.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -823     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 355000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.17    |\n",
      "|    critic_loss     | 0.00763  |\n",
      "|    ent_coef        | 0.0011   |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.00465  |\n",
      "|    n_updates       | 1730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-818.60 +/- 13.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -819     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -165     |\n",
      "| time/              |          |\n",
      "|    episodes        | 356      |\n",
      "|    fps             | 622      |\n",
      "|    time_elapsed    | 571      |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=-1094.06 +/- 58.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 357000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.05     |\n",
      "|    critic_loss     | 0.00535   |\n",
      "|    ent_coef        | 0.00108   |\n",
      "|    ent_coef_loss   | -17.7     |\n",
      "|    learning_rate   | 0.00464   |\n",
      "|    n_updates       | 1740      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=-1061.37 +/- 69.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -1.06e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 358000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=359000, episode_reward=238.31 +/- 69.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 238      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.08    |\n",
      "|    critic_loss     | 0.00501  |\n",
      "|    ent_coef        | 0.00107  |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=240.45 +/- 122.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 240      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -168     |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 577      |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=361000, episode_reward=158.32 +/- 46.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 158      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 361000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.98    |\n",
      "|    critic_loss     | 0.00671  |\n",
      "|    ent_coef        | 0.00105  |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=201.66 +/- 38.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 202      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 362000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=-11.30 +/- 39.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 363000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.98    |\n",
      "|    critic_loss     | 0.00491  |\n",
      "|    ent_coef        | 0.00104  |\n",
      "|    ent_coef_loss   | 1.84     |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=12.37 +/- 47.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 12.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -155     |\n",
      "| time/              |          |\n",
      "|    episodes        | 364      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 584      |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=256.39 +/- 46.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 365000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.7     |\n",
      "|    critic_loss     | 0.0279   |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | 2.2      |\n",
      "|    learning_rate   | 0.00464  |\n",
      "|    n_updates       | 1780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=258.55 +/- 81.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 259      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=367000, episode_reward=197.43 +/- 55.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 197      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 367000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.21    |\n",
      "|    critic_loss     | 0.0129   |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | 2.88     |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=217.54 +/- 36.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 218      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -146     |\n",
      "| time/              |          |\n",
      "|    episodes        | 368      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 590      |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=157.31 +/- 76.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 157      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 369000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.93    |\n",
      "|    critic_loss     | 0.00798  |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | 2.64     |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=187.78 +/- 48.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 188      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=371000, episode_reward=370.40 +/- 48.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 370      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 371000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.96    |\n",
      "|    critic_loss     | 0.00554  |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | -7.19    |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=187.78 +/- 290.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 188      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -139     |\n",
      "| time/              |          |\n",
      "|    episodes        | 372      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 596      |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=373000, episode_reward=291.94 +/- 62.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 373000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.99    |\n",
      "|    critic_loss     | 0.00544  |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | -5.35    |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=296.11 +/- 63.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 296      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=321.83 +/- 47.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 322      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 375000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.99    |\n",
      "|    critic_loss     | 0.00605  |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | -4.79    |\n",
      "|    learning_rate   | 0.00463  |\n",
      "|    n_updates       | 1830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=312.65 +/- 51.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 313      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    episodes        | 376      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 602      |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=377000, episode_reward=167.57 +/- 204.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 168      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 377000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.97    |\n",
      "|    critic_loss     | 0.00458  |\n",
      "|    ent_coef        | 0.00102  |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=128.89 +/- 179.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 129      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=379000, episode_reward=-98.36 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 379000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.08    |\n",
      "|    critic_loss     | 0.00582  |\n",
      "|    ent_coef        | 0.00101  |\n",
      "|    ent_coef_loss   | -26.5    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-99.10 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 609      |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=-88.57 +/- 10.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -88.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 381000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.95    |\n",
      "|    critic_loss     | 0.00417  |\n",
      "|    ent_coef        | 0.000985 |\n",
      "|    ent_coef_loss   | -30.1    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=-97.43 +/- 9.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383000, episode_reward=-131.85 +/- 56.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 383000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.92    |\n",
      "|    critic_loss     | 0.00329  |\n",
      "|    ent_coef        | 0.000958 |\n",
      "|    ent_coef_loss   | -25.1    |\n",
      "|    learning_rate   | 0.00462  |\n",
      "|    n_updates       | 1870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-87.31 +/- 26.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    episodes        | 384      |\n",
      "|    fps             | 623      |\n",
      "|    time_elapsed    | 615      |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-125.76 +/- 61.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 385000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=-431.79 +/- 19.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -432     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 386000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.02    |\n",
      "|    critic_loss     | 0.00493  |\n",
      "|    ent_coef        | 0.000933 |\n",
      "|    ent_coef_loss   | -26.3    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=-421.58 +/- 39.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -422     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 387000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-1095.73 +/- 177.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 388000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.91    |\n",
      "|    critic_loss     | 0.00436  |\n",
      "|    ent_coef        | 0.000908 |\n",
      "|    ent_coef_loss   | -23      |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1890     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -136     |\n",
      "| time/              |          |\n",
      "|    episodes        | 388      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 621      |\n",
      "|    total_timesteps | 388000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=389000, episode_reward=-1069.03 +/- 398.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -1.07e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 389000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-311.47 +/- 53.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -311     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.02    |\n",
      "|    critic_loss     | 0.0048   |\n",
      "|    ent_coef        | 0.000885 |\n",
      "|    ent_coef_loss   | -27.8    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391000, episode_reward=-321.44 +/- 27.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -321     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 391000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-205.95 +/- 1.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 392000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.84    |\n",
      "|    critic_loss     | 0.00321  |\n",
      "|    ent_coef        | 0.000862 |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -148     |\n",
      "| time/              |          |\n",
      "|    episodes        | 392      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 628      |\n",
      "|    total_timesteps | 392000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=-207.63 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -208     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 393000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=-235.58 +/- 3.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 394000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.87    |\n",
      "|    critic_loss     | 0.00368  |\n",
      "|    ent_coef        | 0.000842 |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.00461  |\n",
      "|    n_updates       | 1920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=-235.52 +/- 2.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 395000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=304.18 +/- 233.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 304      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.95    |\n",
      "|    critic_loss     | 0.00405  |\n",
      "|    ent_coef        | 0.000826 |\n",
      "|    ent_coef_loss   | -7.87    |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1930     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -145     |\n",
      "| time/              |          |\n",
      "|    episodes        | 396      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 634      |\n",
      "|    total_timesteps | 396000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397000, episode_reward=423.85 +/- 52.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 424      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 397000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=179.81 +/- 70.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 180      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 398000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.97    |\n",
      "|    critic_loss     | 0.00465  |\n",
      "|    ent_coef        | 0.000816 |\n",
      "|    ent_coef_loss   | 4.74     |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=203.18 +/- 69.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 203      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 399000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-157.23 +/- 2.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.99    |\n",
      "|    critic_loss     | 0.00612  |\n",
      "|    ent_coef        | 0.000816 |\n",
      "|    ent_coef_loss   | 20.1     |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1950     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -139     |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 640      |\n",
      "|    total_timesteps | 400000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401000, episode_reward=-156.89 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 401000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=-111.63 +/- 3.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 402000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.96    |\n",
      "|    critic_loss     | 0.00505  |\n",
      "|    ent_coef        | 0.000827 |\n",
      "|    ent_coef_loss   | 16.7     |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403000, episode_reward=-114.83 +/- 3.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 403000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=-94.52 +/- 2.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 404000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.95    |\n",
      "|    critic_loss     | 0.00332  |\n",
      "|    ent_coef        | 0.000836 |\n",
      "|    ent_coef_loss   | -18.9    |\n",
      "|    learning_rate   | 0.0046   |\n",
      "|    n_updates       | 1970     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -131     |\n",
      "| time/              |          |\n",
      "|    episodes        | 404      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 647      |\n",
      "|    total_timesteps | 404000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=-96.22 +/- 2.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -96.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=-115.74 +/- 2.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 406000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.89    |\n",
      "|    critic_loss     | 0.00373  |\n",
      "|    ent_coef        | 0.00083  |\n",
      "|    ent_coef_loss   | -16.9    |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 1980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=407000, episode_reward=-113.13 +/- 4.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=-122.32 +/- 2.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 408000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.91    |\n",
      "|    critic_loss     | 0.00343  |\n",
      "|    ent_coef        | 0.000818 |\n",
      "|    ent_coef_loss   | -16.7    |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 1990     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -131     |\n",
      "| time/              |          |\n",
      "|    episodes        | 408      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 653      |\n",
      "|    total_timesteps | 408000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409000, episode_reward=-121.67 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-108.74 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.93    |\n",
      "|    critic_loss     | 0.00267  |\n",
      "|    ent_coef        | 0.000805 |\n",
      "|    ent_coef_loss   | -20.6    |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=-109.64 +/- 2.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=-379.49 +/- 28.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -379     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 412000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.9     |\n",
      "|    critic_loss     | 0.00254  |\n",
      "|    ent_coef        | 0.00079  |\n",
      "|    ent_coef_loss   | -14.6    |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 2010     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -121     |\n",
      "| time/              |          |\n",
      "|    episodes        | 412      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 659      |\n",
      "|    total_timesteps | 412000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=413000, episode_reward=-378.73 +/- 40.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -379     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=-437.40 +/- 13.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -437     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 414000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.93    |\n",
      "|    critic_loss     | 0.00355  |\n",
      "|    ent_coef        | 0.000775 |\n",
      "|    ent_coef_loss   | -26.9    |\n",
      "|    learning_rate   | 0.00459  |\n",
      "|    n_updates       | 2020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=-413.83 +/- 50.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -414     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=-250.23 +/- 35.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -250     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 416000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.86    |\n",
      "|    critic_loss     | 0.0031   |\n",
      "|    ent_coef        | 0.000756 |\n",
      "|    ent_coef_loss   | -29.4    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2030     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -128     |\n",
      "| time/              |          |\n",
      "|    episodes        | 416      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 665      |\n",
      "|    total_timesteps | 416000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=-225.43 +/- 42.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=36.40 +/- 49.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 418000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.85    |\n",
      "|    critic_loss     | 0.00288  |\n",
      "|    ent_coef        | 0.000734 |\n",
      "|    ent_coef_loss   | -30.3    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=419000, episode_reward=76.32 +/- 20.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 76.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=219.51 +/- 132.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 220      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.84    |\n",
      "|    critic_loss     | 0.00247  |\n",
      "|    ent_coef        | 0.000713 |\n",
      "|    ent_coef_loss   | -17.6    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -133     |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 624      |\n",
      "|    time_elapsed    | 672      |\n",
      "|    total_timesteps | 420000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421000, episode_reward=199.45 +/- 125.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 199      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=-137.91 +/- 116.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 422000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.82    |\n",
      "|    critic_loss     | 0.00229  |\n",
      "|    ent_coef        | 0.000695 |\n",
      "|    ent_coef_loss   | -29.3    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=-47.53 +/- 155.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -47.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=-123.81 +/- 47.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 424000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.79    |\n",
      "|    critic_loss     | 0.0017   |\n",
      "|    ent_coef        | 0.000676 |\n",
      "|    ent_coef_loss   | -22.9    |\n",
      "|    learning_rate   | 0.00458  |\n",
      "|    n_updates       | 2070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -142     |\n",
      "| time/              |          |\n",
      "|    episodes        | 424      |\n",
      "|    fps             | 625      |\n",
      "|    time_elapsed    | 678      |\n",
      "|    total_timesteps | 424000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=-105.12 +/- 45.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=-224.28 +/- 41.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 426000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.8     |\n",
      "|    critic_loss     | 0.00119  |\n",
      "|    ent_coef        | 0.000658 |\n",
      "|    ent_coef_loss   | -28.5    |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=427000, episode_reward=-192.46 +/- 46.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=-164.60 +/- 35.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -165     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    episodes        | 428      |\n",
      "|    fps             | 625      |\n",
      "|    time_elapsed    | 684      |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=-125.37 +/- 21.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 429000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.8     |\n",
      "|    critic_loss     | 0.00156  |\n",
      "|    ent_coef        | 0.000641 |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-124.92 +/- 10.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=431000, episode_reward=-269.42 +/- 40.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -269     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 431000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.32    |\n",
      "|    critic_loss     | 0.00742  |\n",
      "|    ent_coef        | 0.000634 |\n",
      "|    ent_coef_loss   | 25.1     |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=-197.81 +/- 40.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -198     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -152     |\n",
      "| time/              |          |\n",
      "|    episodes        | 432      |\n",
      "|    fps             | 625      |\n",
      "|    time_elapsed    | 690      |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=433000, episode_reward=-280.27 +/- 212.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -280     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.8     |\n",
      "|    critic_loss     | 0.00217  |\n",
      "|    ent_coef        | 0.000636 |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=-418.62 +/- 47.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -419     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 434000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=-303.82 +/- 48.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -304     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 435000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.86    |\n",
      "|    critic_loss     | 0.00205  |\n",
      "|    ent_coef        | 0.000632 |\n",
      "|    ent_coef_loss   | -23.4    |\n",
      "|    learning_rate   | 0.00457  |\n",
      "|    n_updates       | 2120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=-370.96 +/- 36.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -150     |\n",
      "| time/              |          |\n",
      "|    episodes        | 436      |\n",
      "|    fps             | 625      |\n",
      "|    time_elapsed    | 696      |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=437000, episode_reward=-121.44 +/- 87.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 437000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.83    |\n",
      "|    critic_loss     | 0.00192  |\n",
      "|    ent_coef        | 0.000619 |\n",
      "|    ent_coef_loss   | -32.5    |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=-180.36 +/- 56.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -180     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 438000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=439000, episode_reward=-5.44 +/- 132.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.44    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 439000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.3     |\n",
      "|    critic_loss     | 0.00736  |\n",
      "|    ent_coef        | 0.000603 |\n",
      "|    ent_coef_loss   | -2.01    |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-24.48 +/- 175.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -24.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 625      |\n",
      "|    time_elapsed    | 703      |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=441000, episode_reward=261.99 +/- 147.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 262      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 441000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.96    |\n",
      "|    critic_loss     | 0.00725  |\n",
      "|    ent_coef        | 0.000598 |\n",
      "|    ent_coef_loss   | 21.6     |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=172.90 +/- 265.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 173      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 442000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443000, episode_reward=-217.79 +/- 52.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 443000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.23    |\n",
      "|    critic_loss     | 0.00381  |\n",
      "|    ent_coef        | 0.000607 |\n",
      "|    ent_coef_loss   | 46.7     |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=-268.35 +/- 92.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -268     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    episodes        | 444      |\n",
      "|    fps             | 625      |\n",
      "|    time_elapsed    | 709      |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=212.73 +/- 177.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 213      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.81    |\n",
      "|    critic_loss     | 0.00221  |\n",
      "|    ent_coef        | 0.000624 |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.00456  |\n",
      "|    n_updates       | 2170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=287.76 +/- 131.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 288      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 446000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=-376.36 +/- 43.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 447000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.87    |\n",
      "|    critic_loss     | 0.00253  |\n",
      "|    ent_coef        | 0.000629 |\n",
      "|    ent_coef_loss   | 6.34     |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=-402.96 +/- 89.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -127     |\n",
      "| time/              |          |\n",
      "|    episodes        | 448      |\n",
      "|    fps             | 625      |\n",
      "|    time_elapsed    | 715      |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=449000, episode_reward=179.56 +/- 96.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 180      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 449000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.83    |\n",
      "|    critic_loss     | 0.00217  |\n",
      "|    ent_coef        | 0.000632 |\n",
      "|    ent_coef_loss   | 2.73     |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=141.18 +/- 263.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 141      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=451000, episode_reward=-625.52 +/- 67.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -626     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 451000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.88    |\n",
      "|    critic_loss     | 0.00297  |\n",
      "|    ent_coef        | 0.000634 |\n",
      "|    ent_coef_loss   | -3.55    |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=-601.18 +/- 155.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -601     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -112     |\n",
      "| time/              |          |\n",
      "|    episodes        | 452      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 721      |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=453000, episode_reward=-180.03 +/- 288.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -180     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 453000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.88    |\n",
      "|    critic_loss     | 0.00308  |\n",
      "|    ent_coef        | 0.000633 |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=-556.71 +/- 41.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -557     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=-16.88 +/- 43.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 455000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.86    |\n",
      "|    critic_loss     | 0.0032   |\n",
      "|    ent_coef        | 0.000626 |\n",
      "|    ent_coef_loss   | -35.8    |\n",
      "|    learning_rate   | 0.00455  |\n",
      "|    n_updates       | 2220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=15.46 +/- 104.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 15.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 456      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 728      |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=457000, episode_reward=224.02 +/- 46.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 224      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 457000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.86    |\n",
      "|    critic_loss     | 0.00262  |\n",
      "|    ent_coef        | 0.000609 |\n",
      "|    ent_coef_loss   | -20.3    |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=260.71 +/- 44.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 261      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=346.27 +/- 74.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 346      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 459000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.86    |\n",
      "|    critic_loss     | 0.00167  |\n",
      "|    ent_coef        | 0.000597 |\n",
      "|    ent_coef_loss   | 7.5      |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=429.31 +/- 59.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 429      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -96.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 734      |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=461000, episode_reward=13.81 +/- 100.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 13.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 461000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.92    |\n",
      "|    critic_loss     | 0.00236  |\n",
      "|    ent_coef        | 0.000593 |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=-104.50 +/- 53.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463000, episode_reward=-19.26 +/- 190.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -19.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 463000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.83    |\n",
      "|    critic_loss     | 0.0026   |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | 25.6     |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=124.91 +/- 267.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 125      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 464000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 464      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 740      |\n",
      "|    total_timesteps | 464000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-101.46 +/- 189.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 465000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.84    |\n",
      "|    critic_loss     | 0.0021   |\n",
      "|    ent_coef        | 0.000596 |\n",
      "|    ent_coef_loss   | 4.4      |\n",
      "|    learning_rate   | 0.00454  |\n",
      "|    n_updates       | 2270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=-63.27 +/- 161.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 466000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=467000, episode_reward=298.68 +/- 68.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 299      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 467000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.83    |\n",
      "|    critic_loss     | 0.00239  |\n",
      "|    ent_coef        | 0.000599 |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.00453  |\n",
      "|    n_updates       | 2280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=265.95 +/- 62.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 266      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 468      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 746      |\n",
      "|    total_timesteps | 468000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=469000, episode_reward=-86.59 +/- 83.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -86.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 469000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.85    |\n",
      "|    critic_loss     | 0.00237  |\n",
      "|    ent_coef        | 0.000595 |\n",
      "|    ent_coef_loss   | -8.24    |\n",
      "|    learning_rate   | 0.00453  |\n",
      "|    n_updates       | 2290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-134.70 +/- 27.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471000, episode_reward=-105.47 +/- 56.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 471000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=633.44 +/- 134.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 633      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 472000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.87    |\n",
      "|    critic_loss     | 0.00216  |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | 8.7      |\n",
      "|    learning_rate   | 0.00453  |\n",
      "|    n_updates       | 2300     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -96.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 472      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 753      |\n",
      "|    total_timesteps | 472000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=473000, episode_reward=545.00 +/- 118.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 545      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 473000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=-312.02 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -312     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 474000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.93    |\n",
      "|    critic_loss     | 0.00246  |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.00453  |\n",
      "|    n_updates       | 2310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=-311.16 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -311     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 475000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=-351.40 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -351     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 476000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.96    |\n",
      "|    critic_loss     | 0.00489  |\n",
      "|    ent_coef        | 0.000588 |\n",
      "|    ent_coef_loss   | 15.9     |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2320     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -99.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 476      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 759      |\n",
      "|    total_timesteps | 476000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477000, episode_reward=-350.38 +/- 1.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 477000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=-366.69 +/- 1.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -367     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 478000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.89    |\n",
      "|    critic_loss     | 0.00369  |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=479000, episode_reward=-363.78 +/- 2.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -364     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 479000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-478.01 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -478     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.86    |\n",
      "|    critic_loss     | 0.00474  |\n",
      "|    ent_coef        | 0.000592 |\n",
      "|    ent_coef_loss   | 36.1     |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2340     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -114     |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 766      |\n",
      "|    total_timesteps | 480000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=481000, episode_reward=-477.81 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -478     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 481000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=-543.30 +/- 359.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -543     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 482000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.68    |\n",
      "|    critic_loss     | 0.326    |\n",
      "|    ent_coef        | 0.000606 |\n",
      "|    ent_coef_loss   | 12.7     |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483000, episode_reward=-341.82 +/- 256.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -342     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 483000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=-163.60 +/- 122.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -164     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 484000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.56    |\n",
      "|    critic_loss     | 0.111    |\n",
      "|    ent_coef        | 0.000617 |\n",
      "|    ent_coef_loss   | 15.2     |\n",
      "|    learning_rate   | 0.00452  |\n",
      "|    n_updates       | 2360     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -110     |\n",
      "| time/              |          |\n",
      "|    episodes        | 484      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 772      |\n",
      "|    total_timesteps | 484000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=-345.04 +/- 167.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -345     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 485000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=-757.30 +/- 396.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -757     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 486000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.61    |\n",
      "|    critic_loss     | 0.0536   |\n",
      "|    ent_coef        | 0.000638 |\n",
      "|    ent_coef_loss   | 146      |\n",
      "|    learning_rate   | 0.00451  |\n",
      "|    n_updates       | 2370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487000, episode_reward=-912.21 +/- 298.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -912     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 487000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-1073.67 +/- 43.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -1.07e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 488000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.63     |\n",
      "|    critic_loss     | 0.0249    |\n",
      "|    ent_coef        | 0.000719  |\n",
      "|    ent_coef_loss   | 263       |\n",
      "|    learning_rate   | 0.00451   |\n",
      "|    n_updates       | 2380      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    episodes        | 488      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 778      |\n",
      "|    total_timesteps | 488000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=489000, episode_reward=-1100.03 +/- 49.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 489000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-566.01 +/- 93.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -566     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.85    |\n",
      "|    critic_loss     | 0.0312   |\n",
      "|    ent_coef        | 0.000866 |\n",
      "|    ent_coef_loss   | 224      |\n",
      "|    learning_rate   | 0.00451  |\n",
      "|    n_updates       | 2390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=491000, episode_reward=-683.84 +/- 181.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -684     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=-427.76 +/- 105.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -428     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 492000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.84    |\n",
      "|    critic_loss     | 0.0198   |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | 92.3     |\n",
      "|    learning_rate   | 0.00451  |\n",
      "|    n_updates       | 2400     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -115     |\n",
      "| time/              |          |\n",
      "|    episodes        | 492      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 785      |\n",
      "|    total_timesteps | 492000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=493000, episode_reward=-427.40 +/- 78.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -427     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=-287.04 +/- 59.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -287     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 494000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.89    |\n",
      "|    critic_loss     | 0.00877  |\n",
      "|    ent_coef        | 0.00116  |\n",
      "|    ent_coef_loss   | 90.2     |\n",
      "|    learning_rate   | 0.00451  |\n",
      "|    n_updates       | 2410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=-324.43 +/- 82.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -324     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 495000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-355.73 +/- 9.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -356     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 496000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.74    |\n",
      "|    critic_loss     | 0.00696  |\n",
      "|    ent_coef        | 0.00126  |\n",
      "|    ent_coef_loss   | 55.1     |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2420     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    episodes        | 496      |\n",
      "|    fps             | 626      |\n",
      "|    time_elapsed    | 791      |\n",
      "|    total_timesteps | 496000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=497000, episode_reward=-368.10 +/- 43.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -368     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 497000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=-468.25 +/- 101.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -468     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 498000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.65    |\n",
      "|    critic_loss     | 0.00696  |\n",
      "|    ent_coef        | 0.00134  |\n",
      "|    ent_coef_loss   | 10       |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=499000, episode_reward=-522.55 +/- 194.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -523     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-364.22 +/- 1.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -364     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.55    |\n",
      "|    critic_loss     | 0.00542  |\n",
      "|    ent_coef        | 0.00138  |\n",
      "|    ent_coef_loss   | 0.281    |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2440     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -158     |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 627      |\n",
      "|    time_elapsed    | 797      |\n",
      "|    total_timesteps | 500000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=501000, episode_reward=-364.51 +/- 3.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -365     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=502000, episode_reward=-344.89 +/- 96.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -345     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 502000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.85    |\n",
      "|    critic_loss     | 0.00463  |\n",
      "|    ent_coef        | 0.00139  |\n",
      "|    ent_coef_loss   | 17.2     |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=503000, episode_reward=-358.94 +/- 60.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -359     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 503000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-297.25 +/- 143.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -297     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 504000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.52    |\n",
      "|    critic_loss     | 0.00541  |\n",
      "|    ent_coef        | 0.00141  |\n",
      "|    ent_coef_loss   | 22.1     |\n",
      "|    learning_rate   | 0.0045   |\n",
      "|    n_updates       | 2460     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 504      |\n",
      "|    fps             | 627      |\n",
      "|    time_elapsed    | 803      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=-635.41 +/- 310.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -635     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=506000, episode_reward=-897.91 +/- 316.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -898     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 506000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.57    |\n",
      "|    critic_loss     | 0.00713  |\n",
      "|    ent_coef        | 0.00144  |\n",
      "|    ent_coef_loss   | 25.2     |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507000, episode_reward=-922.08 +/- 261.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -922     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 507000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=-512.52 +/- 192.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -513     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 508000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.06    |\n",
      "|    critic_loss     | 0.00744  |\n",
      "|    ent_coef        | 0.00146  |\n",
      "|    ent_coef_loss   | 12       |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -197     |\n",
      "| time/              |          |\n",
      "|    episodes        | 508      |\n",
      "|    fps             | 627      |\n",
      "|    time_elapsed    | 809      |\n",
      "|    total_timesteps | 508000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=509000, episode_reward=-447.26 +/- 204.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -447     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 509000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-778.38 +/- 79.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -778     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.59    |\n",
      "|    critic_loss     | 0.00645  |\n",
      "|    ent_coef        | 0.00149  |\n",
      "|    ent_coef_loss   | 70.6     |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=511000, episode_reward=-755.76 +/- 193.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -756     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=-886.74 +/- 24.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -887     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 512      |\n",
      "|    fps             | 627      |\n",
      "|    time_elapsed    | 815      |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=513000, episode_reward=-778.20 +/- 19.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -778     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 513000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.53    |\n",
      "|    critic_loss     | 0.00606  |\n",
      "|    ent_coef        | 0.00156  |\n",
      "|    ent_coef_loss   | 64.4     |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514000, episode_reward=-761.31 +/- 22.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -761     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 514000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=-511.99 +/- 2.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -512     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 515000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.59    |\n",
      "|    critic_loss     | 0.00796  |\n",
      "|    ent_coef        | 0.00165  |\n",
      "|    ent_coef_loss   | 69.8     |\n",
      "|    learning_rate   | 0.00449  |\n",
      "|    n_updates       | 2510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=-509.82 +/- 2.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -510     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 516000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -233     |\n",
      "| time/              |          |\n",
      "|    episodes        | 516      |\n",
      "|    fps             | 627      |\n",
      "|    time_elapsed    | 821      |\n",
      "|    total_timesteps | 516000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=517000, episode_reward=-321.97 +/- 7.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -322     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 517000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.41    |\n",
      "|    critic_loss     | 0.0311   |\n",
      "|    ent_coef        | 0.00174  |\n",
      "|    ent_coef_loss   | -18.4    |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518000, episode_reward=-325.73 +/- 4.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -326     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 518000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=519000, episode_reward=-164.53 +/- 200.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -165     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 519000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.54    |\n",
      "|    critic_loss     | 0.00819  |\n",
      "|    ent_coef        | 0.00177  |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-361.62 +/- 244.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -362     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 628      |\n",
      "|    time_elapsed    | 827      |\n",
      "|    total_timesteps | 520000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=521000, episode_reward=-147.80 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 521000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.55    |\n",
      "|    critic_loss     | 0.0079   |\n",
      "|    ent_coef        | 0.00178  |\n",
      "|    ent_coef_loss   | 16       |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=-142.41 +/- 6.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 522000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=523000, episode_reward=-232.01 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 523000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.58    |\n",
      "|    critic_loss     | 0.00437  |\n",
      "|    ent_coef        | 0.00179  |\n",
      "|    ent_coef_loss   | -6.6     |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=-231.02 +/- 2.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 524000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -243     |\n",
      "| time/              |          |\n",
      "|    episodes        | 524      |\n",
      "|    fps             | 628      |\n",
      "|    time_elapsed    | 834      |\n",
      "|    total_timesteps | 524000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=-312.99 +/- 2.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -313     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 525000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.63    |\n",
      "|    critic_loss     | 0.00317  |\n",
      "|    ent_coef        | 0.00178  |\n",
      "|    ent_coef_loss   | -6.08    |\n",
      "|    learning_rate   | 0.00448  |\n",
      "|    n_updates       | 2560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=526000, episode_reward=-311.76 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -312     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 526000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=527000, episode_reward=43.40 +/- 12.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 43.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 527000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.51    |\n",
      "|    critic_loss     | 0.00718  |\n",
      "|    ent_coef        | 0.00179  |\n",
      "|    ent_coef_loss   | 18.2     |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=82.51 +/- 34.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 82.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 528000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -254     |\n",
      "| time/              |          |\n",
      "|    episodes        | 528      |\n",
      "|    fps             | 628      |\n",
      "|    time_elapsed    | 840      |\n",
      "|    total_timesteps | 528000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=529000, episode_reward=-73.88 +/- 4.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -73.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 529000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.54    |\n",
      "|    critic_loss     | 0.00469  |\n",
      "|    ent_coef        | 0.0018   |\n",
      "|    ent_coef_loss   | 4.17     |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-69.94 +/- 15.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=531000, episode_reward=-99.81 +/- 30.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 531000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.55    |\n",
      "|    critic_loss     | 0.00407  |\n",
      "|    ent_coef        | 0.00181  |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=-91.51 +/- 10.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -91.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 532000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -264     |\n",
      "| time/              |          |\n",
      "|    episodes        | 532      |\n",
      "|    fps             | 628      |\n",
      "|    time_elapsed    | 846      |\n",
      "|    total_timesteps | 532000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=533000, episode_reward=-146.18 +/- 8.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 533000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.54    |\n",
      "|    critic_loss     | 0.00284  |\n",
      "|    ent_coef        | 0.0018   |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=-139.18 +/- 15.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=-102.30 +/- 4.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 535000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.52    |\n",
      "|    critic_loss     | 0.00479  |\n",
      "|    ent_coef        | 0.00178  |\n",
      "|    ent_coef_loss   | 18.3     |\n",
      "|    learning_rate   | 0.00447  |\n",
      "|    n_updates       | 2610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=-105.14 +/- 3.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -276     |\n",
      "| time/              |          |\n",
      "|    episodes        | 536      |\n",
      "|    fps             | 628      |\n",
      "|    time_elapsed    | 852      |\n",
      "|    total_timesteps | 536000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=537000, episode_reward=-218.47 +/- 19.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 537000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.57    |\n",
      "|    critic_loss     | 0.00299  |\n",
      "|    ent_coef        | 0.00179  |\n",
      "|    ent_coef_loss   | -14.6    |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=538000, episode_reward=-213.75 +/- 22.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -214     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 538000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=539000, episode_reward=-139.08 +/- 23.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 539000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.56    |\n",
      "|    critic_loss     | 0.00281  |\n",
      "|    ent_coef        | 0.00178  |\n",
      "|    ent_coef_loss   | -9.69    |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-136.83 +/- 14.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 628      |\n",
      "|    time_elapsed    | 858      |\n",
      "|    total_timesteps | 540000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=541000, episode_reward=-130.37 +/- 65.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 541000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.56    |\n",
      "|    critic_loss     | 0.00277  |\n",
      "|    ent_coef        | 0.00176  |\n",
      "|    ent_coef_loss   | -19.4    |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=542000, episode_reward=-66.16 +/- 92.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -66.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=543000, episode_reward=31.80 +/- 54.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 31.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 543000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.56    |\n",
      "|    critic_loss     | 0.00269  |\n",
      "|    ent_coef        | 0.00174  |\n",
      "|    ent_coef_loss   | -5.28    |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=68.89 +/- 57.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 68.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 544000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -287     |\n",
      "| time/              |          |\n",
      "|    episodes        | 544      |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 864      |\n",
      "|    total_timesteps | 544000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=-141.94 +/- 40.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 545000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.55    |\n",
      "|    critic_loss     | 0.00238  |\n",
      "|    ent_coef        | 0.00173  |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.00446  |\n",
      "|    n_updates       | 2660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=-165.76 +/- 36.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -166     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 546000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=547000, episode_reward=-312.51 +/- 20.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -313     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 547000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.52    |\n",
      "|    critic_loss     | 0.0014   |\n",
      "|    ent_coef        | 0.00172  |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00445  |\n",
      "|    n_updates       | 2670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=-314.76 +/- 17.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 548      |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 871      |\n",
      "|    total_timesteps | 548000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=549000, episode_reward=118.70 +/- 143.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 119      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 549000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.52    |\n",
      "|    critic_loss     | 0.00217  |\n",
      "|    ent_coef        | 0.00171  |\n",
      "|    ent_coef_loss   | -2.36    |\n",
      "|    learning_rate   | 0.00445  |\n",
      "|    n_updates       | 2680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=212.04 +/- 59.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 212      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=551000, episode_reward=-83.44 +/- 29.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -83.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 551000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.58    |\n",
      "|    critic_loss     | 0.00348  |\n",
      "|    ent_coef        | 0.0017   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.00445  |\n",
      "|    n_updates       | 2690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=-83.31 +/- 30.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -83.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 552000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -303     |\n",
      "| time/              |          |\n",
      "|    episodes        | 552      |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 877      |\n",
      "|    total_timesteps | 552000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=553000, episode_reward=75.08 +/- 39.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 75.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 553000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.57    |\n",
      "|    critic_loss     | 0.00284  |\n",
      "|    ent_coef        | 0.00169  |\n",
      "|    ent_coef_loss   | -23.8    |\n",
      "|    learning_rate   | 0.00445  |\n",
      "|    n_updates       | 2700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=554000, episode_reward=106.73 +/- 71.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=115.10 +/- 76.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 115      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 555000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=84.25 +/- 32.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 84.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 556000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.58    |\n",
      "|    critic_loss     | 0.00266  |\n",
      "|    ent_coef        | 0.00167  |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2710     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -300     |\n",
      "| time/              |          |\n",
      "|    episodes        | 556      |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 883      |\n",
      "|    total_timesteps | 556000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=557000, episode_reward=-49.92 +/- 245.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 557000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=36.92 +/- 36.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 558000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.54    |\n",
      "|    critic_loss     | 0.00258  |\n",
      "|    ent_coef        | 0.00165  |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=559000, episode_reward=55.70 +/- 18.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 55.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 559000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=198.53 +/- 82.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 199      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.54    |\n",
      "|    critic_loss     | 0.00243  |\n",
      "|    ent_coef        | 0.00163  |\n",
      "|    ent_coef_loss   | -0.518   |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2730     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -309     |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 889      |\n",
      "|    total_timesteps | 560000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=561000, episode_reward=92.57 +/- 240.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 92.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 561000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=562000, episode_reward=8.13 +/- 118.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 8.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 562000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.55    |\n",
      "|    critic_loss     | 0.00237  |\n",
      "|    ent_coef        | 0.00162  |\n",
      "|    ent_coef_loss   | -4.65    |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=563000, episode_reward=3.46 +/- 67.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.46     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 563000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=24.62 +/- 34.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 24.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 564000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.55    |\n",
      "|    critic_loss     | 0.00257  |\n",
      "|    ent_coef        | 0.00161  |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.00444  |\n",
      "|    n_updates       | 2750     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -316     |\n",
      "| time/              |          |\n",
      "|    episodes        | 564      |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 895      |\n",
      "|    total_timesteps | 564000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=31.75 +/- 32.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 31.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 565000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=566000, episode_reward=14.71 +/- 14.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 14.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 566000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.53    |\n",
      "|    critic_loss     | 0.00263  |\n",
      "|    ent_coef        | 0.0016   |\n",
      "|    ent_coef_loss   | -23.4    |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=567000, episode_reward=4.96 +/- 16.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.96     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 567000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=50.07 +/- 60.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 50.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 568000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.58    |\n",
      "|    critic_loss     | 0.00255  |\n",
      "|    ent_coef        | 0.00157  |\n",
      "|    ent_coef_loss   | -16.6    |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2770     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -325     |\n",
      "| time/              |          |\n",
      "|    episodes        | 568      |\n",
      "|    fps             | 629      |\n",
      "|    time_elapsed    | 901      |\n",
      "|    total_timesteps | 568000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=569000, episode_reward=-8.47 +/- 23.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.47    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 569000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-29.44 +/- 33.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -29.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.52    |\n",
      "|    critic_loss     | 0.00306  |\n",
      "|    ent_coef        | 0.00154  |\n",
      "|    ent_coef_loss   | -23.8    |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=571000, episode_reward=-27.01 +/- 19.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -27      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 571000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=97.70 +/- 93.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 97.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 572000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.54    |\n",
      "|    critic_loss     | 0.00297  |\n",
      "|    ent_coef        | 0.00151  |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2790     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -330     |\n",
      "| time/              |          |\n",
      "|    episodes        | 572      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 907      |\n",
      "|    total_timesteps | 572000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573000, episode_reward=-39.04 +/- 61.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -39      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 573000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=574000, episode_reward=-57.86 +/- 59.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -57.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 574000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.54    |\n",
      "|    critic_loss     | 0.0026   |\n",
      "|    ent_coef        | 0.00148  |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.00443  |\n",
      "|    n_updates       | 2800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=-52.28 +/- 101.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 575000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=-7.32 +/- 200.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.32    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 576000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.61    |\n",
      "|    critic_loss     | 0.0128   |\n",
      "|    ent_coef        | 0.00145  |\n",
      "|    ent_coef_loss   | -7.22    |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2810     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -331     |\n",
      "| time/              |          |\n",
      "|    episodes        | 576      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 913      |\n",
      "|    total_timesteps | 576000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=577000, episode_reward=-30.92 +/- 168.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -30.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 577000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=578000, episode_reward=-190.97 +/- 11.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 578000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.51    |\n",
      "|    critic_loss     | 0.00468  |\n",
      "|    ent_coef        | 0.00144  |\n",
      "|    ent_coef_loss   | 12       |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=579000, episode_reward=-191.56 +/- 12.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-58.67 +/- 16.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -58.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.51    |\n",
      "|    critic_loss     | 0.00247  |\n",
      "|    ent_coef        | 0.00144  |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2830     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -329     |\n",
      "| time/              |          |\n",
      "|    episodes        | 580      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 919      |\n",
      "|    total_timesteps | 580000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=581000, episode_reward=-53.10 +/- 14.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 581000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=-361.75 +/- 18.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -362     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 582000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.53    |\n",
      "|    critic_loss     | 0.00248  |\n",
      "|    ent_coef        | 0.00143  |\n",
      "|    ent_coef_loss   | -30.7    |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=583000, episode_reward=-327.63 +/- 55.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -328     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 583000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=-454.94 +/- 144.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -455     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 584000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.46    |\n",
      "|    critic_loss     | 0.00241  |\n",
      "|    ent_coef        | 0.00139  |\n",
      "|    ent_coef_loss   | -27.3    |\n",
      "|    learning_rate   | 0.00442  |\n",
      "|    n_updates       | 2850     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -336     |\n",
      "| time/              |          |\n",
      "|    episodes        | 584      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 926      |\n",
      "|    total_timesteps | 584000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=-451.43 +/- 64.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -451     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=586000, episode_reward=-129.53 +/- 33.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 586000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.44    |\n",
      "|    critic_loss     | 0.00279  |\n",
      "|    ent_coef        | 0.00136  |\n",
      "|    ent_coef_loss   | -19      |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=587000, episode_reward=-174.65 +/- 31.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -175     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 587000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=141.22 +/- 37.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 141      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 588000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.5     |\n",
      "|    critic_loss     | 0.00325  |\n",
      "|    ent_coef        | 0.00133  |\n",
      "|    ent_coef_loss   | 4.09     |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2870     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -341     |\n",
      "| time/              |          |\n",
      "|    episodes        | 588      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 932      |\n",
      "|    total_timesteps | 588000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=589000, episode_reward=130.87 +/- 54.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 131      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 589000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=1.35 +/- 13.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.53    |\n",
      "|    critic_loss     | 0.00249  |\n",
      "|    ent_coef        | 0.00133  |\n",
      "|    ent_coef_loss   | 8.79     |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=591000, episode_reward=-5.95 +/- 11.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.95    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=-272.11 +/- 472.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -272     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 592000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.69    |\n",
      "|    critic_loss     | 0.00866  |\n",
      "|    ent_coef        | 0.00134  |\n",
      "|    ent_coef_loss   | 17       |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2890     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -320     |\n",
      "| time/              |          |\n",
      "|    episodes        | 592      |\n",
      "|    fps             | 630      |\n",
      "|    time_elapsed    | 938      |\n",
      "|    total_timesteps | 592000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=593000, episode_reward=-707.52 +/- 548.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -708     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 593000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=-166.47 +/- 83.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -166     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 594000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.61    |\n",
      "|    critic_loss     | 0.00345  |\n",
      "|    ent_coef        | 0.00135  |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00441  |\n",
      "|    n_updates       | 2900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=-101.45 +/- 5.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 595000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=-62.35 +/- 11.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -62.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 596000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.49    |\n",
      "|    critic_loss     | 0.00291  |\n",
      "|    ent_coef        | 0.00134  |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -315     |\n",
      "| time/              |          |\n",
      "|    episodes        | 596      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 944      |\n",
      "|    total_timesteps | 596000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=597000, episode_reward=-64.67 +/- 8.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -64.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598000, episode_reward=-42.76 +/- 11.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 598000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=599000, episode_reward=-63.41 +/- 10.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 599000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.55    |\n",
      "|    critic_loss     | 0.00264  |\n",
      "|    ent_coef        | 0.00132  |\n",
      "|    ent_coef_loss   | -22.7    |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-48.95 +/- 22.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -301     |\n",
      "| time/              |          |\n",
      "|    episodes        | 600      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 950      |\n",
      "|    total_timesteps | 600000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=601000, episode_reward=-143.85 +/- 8.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 601000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.5     |\n",
      "|    critic_loss     | 0.00195  |\n",
      "|    ent_coef        | 0.00129  |\n",
      "|    ent_coef_loss   | -21.7    |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=602000, episode_reward=-139.44 +/- 16.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 602000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=603000, episode_reward=-175.37 +/- 50.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -175     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 603000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.52    |\n",
      "|    critic_loss     | 0.00182  |\n",
      "|    ent_coef        | 0.00127  |\n",
      "|    ent_coef_loss   | -22.2    |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=-153.33 +/- 34.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 604000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -286     |\n",
      "| time/              |          |\n",
      "|    episodes        | 604      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 956      |\n",
      "|    total_timesteps | 604000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=-158.17 +/- 31.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -158     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 605000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.00192  |\n",
      "|    ent_coef        | 0.00124  |\n",
      "|    ent_coef_loss   | -20.3    |\n",
      "|    learning_rate   | 0.0044   |\n",
      "|    n_updates       | 2950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=-128.00 +/- 32.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 606000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=607000, episode_reward=-186.88 +/- 87.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 607000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.44    |\n",
      "|    critic_loss     | 0.0021   |\n",
      "|    ent_coef        | 0.00121  |\n",
      "|    ent_coef_loss   | -8.61    |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 2960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=-180.67 +/- 112.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -181     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 608000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -271     |\n",
      "| time/              |          |\n",
      "|    episodes        | 608      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 962      |\n",
      "|    total_timesteps | 608000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=609000, episode_reward=-84.85 +/- 79.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 609000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.52    |\n",
      "|    critic_loss     | 0.00292  |\n",
      "|    ent_coef        | 0.00119  |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 2970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-187.94 +/- 21.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -188     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=611000, episode_reward=-123.58 +/- 16.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 611000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.5     |\n",
      "|    critic_loss     | 0.00236  |\n",
      "|    ent_coef        | 0.00118  |\n",
      "|    ent_coef_loss   | -8.31    |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 2980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=-142.03 +/- 30.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 612000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -245     |\n",
      "| time/              |          |\n",
      "|    episodes        | 612      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 969      |\n",
      "|    total_timesteps | 612000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=613000, episode_reward=-41.52 +/- 68.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 613000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.5     |\n",
      "|    critic_loss     | 0.00173  |\n",
      "|    ent_coef        | 0.00117  |\n",
      "|    ent_coef_loss   | -8.6     |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 2990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=614000, episode_reward=-93.06 +/- 79.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -93.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 614000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-76.01 +/- 34.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -76      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 615000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.76    |\n",
      "|    critic_loss     | 0.00237  |\n",
      "|    ent_coef        | 0.00115  |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.00439  |\n",
      "|    n_updates       | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=-143.37 +/- 85.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 616000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -232     |\n",
      "| time/              |          |\n",
      "|    episodes        | 616      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 975      |\n",
      "|    total_timesteps | 616000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=617000, episode_reward=-146.61 +/- 58.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 617000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.48    |\n",
      "|    critic_loss     | 0.00224  |\n",
      "|    ent_coef        | 0.00114  |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=-67.47 +/- 56.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -67.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 618000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=619000, episode_reward=64.64 +/- 52.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 64.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 619000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.58    |\n",
      "|    critic_loss     | 0.00238  |\n",
      "|    ent_coef        | 0.00112  |\n",
      "|    ent_coef_loss   | -19.7    |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=63.74 +/- 52.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 63.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -220     |\n",
      "| time/              |          |\n",
      "|    episodes        | 620      |\n",
      "|    fps             | 631      |\n",
      "|    time_elapsed    | 981      |\n",
      "|    total_timesteps | 620000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=621000, episode_reward=245.14 +/- 29.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 245      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 621000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.46    |\n",
      "|    critic_loss     | 0.00192  |\n",
      "|    ent_coef        | 0.00111  |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=622000, episode_reward=276.00 +/- 36.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 276      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=623000, episode_reward=200.69 +/- 40.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 623000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.00185  |\n",
      "|    ent_coef        | 0.00109  |\n",
      "|    ent_coef_loss   | -16.6    |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=184.61 +/- 30.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 185      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 624000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -205     |\n",
      "| time/              |          |\n",
      "|    episodes        | 624      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 987      |\n",
      "|    total_timesteps | 624000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=132.76 +/- 29.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 133      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 625000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.46    |\n",
      "|    critic_loss     | 0.00175  |\n",
      "|    ent_coef        | 0.00107  |\n",
      "|    ent_coef_loss   | -17.8    |\n",
      "|    learning_rate   | 0.00438  |\n",
      "|    n_updates       | 3050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626000, episode_reward=110.33 +/- 16.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 626000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=627000, episode_reward=168.07 +/- 25.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 168      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 627000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.48    |\n",
      "|    critic_loss     | 0.00177  |\n",
      "|    ent_coef        | 0.00105  |\n",
      "|    ent_coef_loss   | -5.56    |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=193.30 +/- 46.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -180     |\n",
      "| time/              |          |\n",
      "|    episodes        | 628      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 993      |\n",
      "|    total_timesteps | 628000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=629000, episode_reward=73.01 +/- 71.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 73       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 629000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.48    |\n",
      "|    critic_loss     | 0.00148  |\n",
      "|    ent_coef        | 0.00104  |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=106.72 +/- 40.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=631000, episode_reward=167.16 +/- 40.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 167      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 631000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.00164  |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=195.72 +/- 55.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 196      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 632000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -157     |\n",
      "| time/              |          |\n",
      "|    episodes        | 632      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 999      |\n",
      "|    total_timesteps | 632000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=633000, episode_reward=217.07 +/- 29.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 217      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 633000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.48    |\n",
      "|    critic_loss     | 0.00185  |\n",
      "|    ent_coef        | 0.00101  |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=634000, episode_reward=226.66 +/- 12.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 227      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=294.85 +/- 29.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 295      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 635000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.48    |\n",
      "|    critic_loss     | 0.00182  |\n",
      "|    ent_coef        | 0.000998 |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.00437  |\n",
      "|    n_updates       | 3100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=308.73 +/- 18.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 636000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    episodes        | 636      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 1005     |\n",
      "|    total_timesteps | 636000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=637000, episode_reward=276.62 +/- 28.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 277      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 637000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.48    |\n",
      "|    critic_loss     | 0.00183  |\n",
      "|    ent_coef        | 0.000983 |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.00436  |\n",
      "|    n_updates       | 3110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638000, episode_reward=285.05 +/- 46.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 285      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 638000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=639000, episode_reward=461.52 +/- 29.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 639000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.00179  |\n",
      "|    ent_coef        | 0.00097  |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.00436  |\n",
      "|    n_updates       | 3120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=416.57 +/- 29.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 417      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -119     |\n",
      "| time/              |          |\n",
      "|    episodes        | 640      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 1011     |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641000, episode_reward=466.45 +/- 17.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 466      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 641000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=493.55 +/- 26.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 494      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 642000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.49    |\n",
      "|    critic_loss     | 0.0018   |\n",
      "|    ent_coef        | 0.000958 |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.00436  |\n",
      "|    n_updates       | 3130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=643000, episode_reward=511.78 +/- 14.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 512      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 643000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=200.45 +/- 30.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 644000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.5     |\n",
      "|    critic_loss     | 0.00169  |\n",
      "|    ent_coef        | 0.000947 |\n",
      "|    ent_coef_loss   | -6.95    |\n",
      "|    learning_rate   | 0.00436  |\n",
      "|    n_updates       | 3140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    episodes        | 644      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 1017     |\n",
      "|    total_timesteps | 644000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=193.04 +/- 21.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 645000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=646000, episode_reward=250.78 +/- 19.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 251      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 646000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.00153  |\n",
      "|    ent_coef        | 0.000938 |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=647000, episode_reward=248.99 +/- 18.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 249      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 647000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=258.11 +/- 54.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 258      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 648000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.00115  |\n",
      "|    ent_coef        | 0.000927 |\n",
      "|    ent_coef_loss   | -15.8    |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3160     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -89.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 648      |\n",
      "|    fps             | 632      |\n",
      "|    time_elapsed    | 1023     |\n",
      "|    total_timesteps | 648000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649000, episode_reward=236.30 +/- 51.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 649000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=194.55 +/- 20.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 195      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.00121  |\n",
      "|    ent_coef        | 0.000914 |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=651000, episode_reward=212.04 +/- 29.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 212      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 651000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=223.00 +/- 27.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 223      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 652000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000901 |\n",
      "|    ent_coef_loss   | -18.2    |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3180     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -73.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 652      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1029     |\n",
      "|    total_timesteps | 652000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=653000, episode_reward=240.00 +/- 24.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 240      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 653000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=-290.91 +/- 5.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -291     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 654000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.45    |\n",
      "|    critic_loss     | 0.000752 |\n",
      "|    ent_coef        | 0.000885 |\n",
      "|    ent_coef_loss   | -20.6    |\n",
      "|    learning_rate   | 0.00435  |\n",
      "|    n_updates       | 3190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=-289.43 +/- 6.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -289     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 655000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=-360.40 +/- 9.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -360     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 656000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.44    |\n",
      "|    critic_loss     | 0.000388 |\n",
      "|    ent_coef        | 0.000869 |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3200     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -73.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 656      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1036     |\n",
      "|    total_timesteps | 656000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=657000, episode_reward=-340.04 +/- 10.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 657000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=658000, episode_reward=-136.21 +/- 20.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 658000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.41    |\n",
      "|    critic_loss     | 0.000275 |\n",
      "|    ent_coef        | 0.000862 |\n",
      "|    ent_coef_loss   | -5.23    |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=659000, episode_reward=-154.00 +/- 20.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 659000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-223.86 +/- 3.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.41    |\n",
      "|    critic_loss     | 0.000411 |\n",
      "|    ent_coef        | 0.000857 |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3220     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -82.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 660      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1042     |\n",
      "|    total_timesteps | 660000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=661000, episode_reward=-225.08 +/- 3.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 661000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=662000, episode_reward=-160.99 +/- 97.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -161     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 662000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.44    |\n",
      "|    critic_loss     | 0.000771 |\n",
      "|    ent_coef        | 0.000853 |\n",
      "|    ent_coef_loss   | -9.4     |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663000, episode_reward=-193.03 +/- 61.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -193     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 663000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=-120.39 +/- 4.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 664000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.5     |\n",
      "|    critic_loss     | 0.000741 |\n",
      "|    ent_coef        | 0.000848 |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.00434  |\n",
      "|    n_updates       | 3240     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -87.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 664      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1048     |\n",
      "|    total_timesteps | 664000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=-113.13 +/- 3.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -113     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=666000, episode_reward=-266.98 +/- 5.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 666000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.39    |\n",
      "|    critic_loss     | 0.000896 |\n",
      "|    ent_coef        | 0.000838 |\n",
      "|    ent_coef_loss   | -24.1    |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=667000, episode_reward=-270.02 +/- 4.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -270     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 667000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=-154.81 +/- 9.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 668000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.49    |\n",
      "|    critic_loss     | 0.00121  |\n",
      "|    ent_coef        | 0.000822 |\n",
      "|    ent_coef_loss   | -5.97    |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3260     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -91.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 668      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1054     |\n",
      "|    total_timesteps | 668000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669000, episode_reward=-125.77 +/- 33.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 669000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=382.71 +/- 25.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 383      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.62    |\n",
      "|    critic_loss     | 0.000957 |\n",
      "|    ent_coef        | 0.000811 |\n",
      "|    ent_coef_loss   | -16.6    |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=671000, episode_reward=416.71 +/- 26.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 417      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=230.17 +/- 8.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 230      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 672000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.44    |\n",
      "|    critic_loss     | 0.00157  |\n",
      "|    ent_coef        | 0.000799 |\n",
      "|    ent_coef_loss   | -5.8     |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3280     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -83.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 672      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1060     |\n",
      "|    total_timesteps | 672000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=673000, episode_reward=244.67 +/- 15.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 245      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 673000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=674000, episode_reward=-98.03 +/- 258.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 674000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.42    |\n",
      "|    critic_loss     | 0.0012   |\n",
      "|    ent_coef        | 0.00079  |\n",
      "|    ent_coef_loss   | -18.1    |\n",
      "|    learning_rate   | 0.00433  |\n",
      "|    n_updates       | 3290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-165.86 +/- 1.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -166     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 675000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=360.10 +/- 33.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 360      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 676000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.59    |\n",
      "|    critic_loss     | 0.00122  |\n",
      "|    ent_coef        | 0.000777 |\n",
      "|    ent_coef_loss   | -17.9    |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3300     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -81.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 676      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1066     |\n",
      "|    total_timesteps | 676000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=677000, episode_reward=329.68 +/- 28.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 330      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=-99.78 +/- 3.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 678000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.43    |\n",
      "|    critic_loss     | 0.00142  |\n",
      "|    ent_coef        | 0.000761 |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=679000, episode_reward=-99.84 +/- 4.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 679000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-256.57 +/- 3.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -257     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.00183  |\n",
      "|    ent_coef        | 0.000745 |\n",
      "|    ent_coef_loss   | -28.4    |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3320     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -69.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 680      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1073     |\n",
      "|    total_timesteps | 680000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681000, episode_reward=-257.88 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 681000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=682000, episode_reward=-92.36 +/- 7.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -92.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 682000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.53    |\n",
      "|    critic_loss     | 0.00072  |\n",
      "|    ent_coef        | 0.000727 |\n",
      "|    ent_coef_loss   | -25.9    |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=683000, episode_reward=-89.16 +/- 7.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -89.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=-90.43 +/- 2.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 684000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -67.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 684      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1079     |\n",
      "|    total_timesteps | 684000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=-286.73 +/- 1.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -287     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 685000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.45    |\n",
      "|    critic_loss     | 0.000922 |\n",
      "|    ent_coef        | 0.000711 |\n",
      "|    ent_coef_loss   | 27.4     |\n",
      "|    learning_rate   | 0.00432  |\n",
      "|    n_updates       | 3340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686000, episode_reward=-288.29 +/- 2.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -288     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 686000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=687000, episode_reward=-989.09 +/- 240.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -989     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 687000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.46    |\n",
      "|    critic_loss     | 0.00244  |\n",
      "|    ent_coef        | 0.000715 |\n",
      "|    ent_coef_loss   | 4.59     |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=-1122.76 +/- 70.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -1.12e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 688000    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -68.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 688      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1085     |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=689000, episode_reward=-194.06 +/- 2.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 689000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.48    |\n",
      "|    critic_loss     | 0.00275  |\n",
      "|    ent_coef        | 0.000722 |\n",
      "|    ent_coef_loss   | 23.1     |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-194.11 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=691000, episode_reward=297.73 +/- 22.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 298      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 691000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.5     |\n",
      "|    critic_loss     | 0.00212  |\n",
      "|    ent_coef        | 0.000733 |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=292.25 +/- 20.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 692000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -61.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 692      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1091     |\n",
      "|    total_timesteps | 692000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=693000, episode_reward=256.09 +/- 24.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 693000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.43    |\n",
      "|    critic_loss     | 0.0018   |\n",
      "|    ent_coef        | 0.000735 |\n",
      "|    ent_coef_loss   | -5.25    |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=694000, episode_reward=274.09 +/- 50.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 274      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 694000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=118.17 +/- 76.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 118      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 695000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.41    |\n",
      "|    critic_loss     | 0.00149  |\n",
      "|    ent_coef        | 0.000731 |\n",
      "|    ent_coef_loss   | -17.8    |\n",
      "|    learning_rate   | 0.00431  |\n",
      "|    n_updates       | 3390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=182.21 +/- 45.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 182      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 696000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -37.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 696      |\n",
      "|    fps             | 633      |\n",
      "|    time_elapsed    | 1097     |\n",
      "|    total_timesteps | 696000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=697000, episode_reward=57.29 +/- 120.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 57.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 697000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.42    |\n",
      "|    critic_loss     | 0.00115  |\n",
      "|    ent_coef        | 0.00072  |\n",
      "|    ent_coef_loss   | -15.9    |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=698000, episode_reward=132.22 +/- 68.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 132      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 698000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699000, episode_reward=294.83 +/- 73.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 295      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 699000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.37    |\n",
      "|    critic_loss     | 0.00157  |\n",
      "|    ent_coef        | 0.000707 |\n",
      "|    ent_coef_loss   | -26.1    |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=303.35 +/- 31.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 303      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -27      |\n",
      "| time/              |          |\n",
      "|    episodes        | 700      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1103     |\n",
      "|    total_timesteps | 700000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=701000, episode_reward=661.99 +/- 62.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 662      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 701000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.41    |\n",
      "|    critic_loss     | 0.00128  |\n",
      "|    ent_coef        | 0.000691 |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3420     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=702000, episode_reward=721.45 +/- 16.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 721      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 702000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=703000, episode_reward=626.64 +/- 49.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 627      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 703000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.44    |\n",
      "|    critic_loss     | 0.00119  |\n",
      "|    ent_coef        | 0.000679 |\n",
      "|    ent_coef_loss   | -7.54    |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=635.82 +/- 20.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 636      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 704000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -5.77    |\n",
      "| time/              |          |\n",
      "|    episodes        | 704      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1110     |\n",
      "|    total_timesteps | 704000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=660.72 +/- 22.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 661      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 705000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.000999 |\n",
      "|    ent_coef        | 0.000674 |\n",
      "|    ent_coef_loss   | 9.49     |\n",
      "|    learning_rate   | 0.0043   |\n",
      "|    n_updates       | 3440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=706000, episode_reward=618.53 +/- 36.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 619      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 706000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=707000, episode_reward=698.78 +/- 19.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 699      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 707000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.47    |\n",
      "|    critic_loss     | 0.0011   |\n",
      "|    ent_coef        | 0.000674 |\n",
      "|    ent_coef_loss   | 2.7      |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=702.11 +/- 36.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 702      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 708      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1116     |\n",
      "|    total_timesteps | 708000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=709000, episode_reward=661.09 +/- 40.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 661      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 709000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.48    |\n",
      "|    critic_loss     | 0.00105  |\n",
      "|    ent_coef        | 0.000677 |\n",
      "|    ent_coef_loss   | 3.12     |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=686.83 +/- 22.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 687      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=711000, episode_reward=517.46 +/- 16.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 517      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 711000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.45    |\n",
      "|    critic_loss     | 0.000918 |\n",
      "|    ent_coef        | 0.000678 |\n",
      "|    ent_coef_loss   | -7.08    |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=517.86 +/- 36.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 518      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 712000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 42.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 712      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1122     |\n",
      "|    total_timesteps | 712000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=713000, episode_reward=431.14 +/- 33.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 713000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.43    |\n",
      "|    critic_loss     | 0.00123  |\n",
      "|    ent_coef        | 0.000675 |\n",
      "|    ent_coef_loss   | -7.62    |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=422.78 +/- 25.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 423      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=502.00 +/- 28.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 502      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 715000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.4     |\n",
      "|    critic_loss     | 0.00145  |\n",
      "|    ent_coef        | 0.000671 |\n",
      "|    ent_coef_loss   | -5.01    |\n",
      "|    learning_rate   | 0.00429  |\n",
      "|    n_updates       | 3490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=498.89 +/- 36.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 499      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 716000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 64.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 716      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1128     |\n",
      "|    total_timesteps | 716000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=717000, episode_reward=538.91 +/- 23.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 539      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 717000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.42    |\n",
      "|    critic_loss     | 0.00116  |\n",
      "|    ent_coef        | 0.000665 |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=718000, episode_reward=546.69 +/- 26.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 547      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 718000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=719000, episode_reward=602.66 +/- 183.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 603      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 719000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.41    |\n",
      "|    critic_loss     | 0.00103  |\n",
      "|    ent_coef        | 0.000658 |\n",
      "|    ent_coef_loss   | -9.36    |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=495.92 +/- 247.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 496      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 84.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 720      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1134     |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=721000, episode_reward=523.08 +/- 158.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 523      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 721000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.4     |\n",
      "|    critic_loss     | 0.0011   |\n",
      "|    ent_coef        | 0.000649 |\n",
      "|    ent_coef_loss   | -19.3    |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=722000, episode_reward=593.08 +/- 20.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 593      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 722000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=723000, episode_reward=611.93 +/- 24.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 612      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 723000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.43    |\n",
      "|    critic_loss     | 0.00113  |\n",
      "|    ent_coef        | 0.00064  |\n",
      "|    ent_coef_loss   | 1.46     |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=414.13 +/- 366.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 414      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 724000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 98.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 724      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1140     |\n",
      "|    total_timesteps | 724000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=472.80 +/- 27.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 473      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 725000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.44    |\n",
      "|    critic_loss     | 0.00123  |\n",
      "|    ent_coef        | 0.000635 |\n",
      "|    ent_coef_loss   | -4.96    |\n",
      "|    learning_rate   | 0.00428  |\n",
      "|    n_updates       | 3540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=486.09 +/- 16.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 486      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727000, episode_reward=491.32 +/- 24.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 491      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 727000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=469.69 +/- 37.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 728000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.42    |\n",
      "|    critic_loss     | 0.0013   |\n",
      "|    ent_coef        | 0.000631 |\n",
      "|    ent_coef_loss   | -3.9     |\n",
      "|    learning_rate   | 0.00427  |\n",
      "|    n_updates       | 3550     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 113      |\n",
      "| time/              |          |\n",
      "|    episodes        | 728      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1146     |\n",
      "|    total_timesteps | 728000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729000, episode_reward=476.12 +/- 16.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 476      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 729000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=581.76 +/- 31.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 582      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.4     |\n",
      "|    critic_loss     | 0.00139  |\n",
      "|    ent_coef        | 0.000628 |\n",
      "|    ent_coef_loss   | -3.52    |\n",
      "|    learning_rate   | 0.00427  |\n",
      "|    n_updates       | 3560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=731000, episode_reward=426.33 +/- 248.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 426      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 731000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=40.14 +/- 416.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 732000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.43    |\n",
      "|    critic_loss     | 0.001    |\n",
      "|    ent_coef        | 0.000626 |\n",
      "|    ent_coef_loss   | 7.03     |\n",
      "|    learning_rate   | 0.00427  |\n",
      "|    n_updates       | 3570     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 127      |\n",
      "| time/              |          |\n",
      "|    episodes        | 732      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1153     |\n",
      "|    total_timesteps | 732000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=733000, episode_reward=-34.85 +/- 346.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -34.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 733000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=734000, episode_reward=66.60 +/- 424.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 66.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 734000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.43    |\n",
      "|    critic_loss     | 0.000907 |\n",
      "|    ent_coef        | 0.000628 |\n",
      "|    ent_coef_loss   | -5.11    |\n",
      "|    learning_rate   | 0.00427  |\n",
      "|    n_updates       | 3580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=236.93 +/- 427.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 237      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 735000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=361.09 +/- 403.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 361      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 736000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.41    |\n",
      "|    critic_loss     | 0.000896 |\n",
      "|    ent_coef        | 0.000625 |\n",
      "|    ent_coef_loss   | -19.8    |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3590     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 137      |\n",
      "| time/              |          |\n",
      "|    episodes        | 736      |\n",
      "|    fps             | 634      |\n",
      "|    time_elapsed    | 1159     |\n",
      "|    total_timesteps | 736000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=737000, episode_reward=369.40 +/- 452.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 369      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 737000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=570.03 +/- 17.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 570      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 738000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.38    |\n",
      "|    critic_loss     | 0.00113  |\n",
      "|    ent_coef        | 0.000615 |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=739000, episode_reward=611.60 +/- 27.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 612      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 739000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=215.90 +/- 409.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 216      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.42    |\n",
      "|    critic_loss     | 0.00137  |\n",
      "|    ent_coef        | 0.000607 |\n",
      "|    ent_coef_loss   | 1.69     |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3610     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    episodes        | 740      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1165     |\n",
      "|    total_timesteps | 740000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741000, episode_reward=362.18 +/- 321.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 362      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 741000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=742000, episode_reward=149.32 +/- 316.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 149      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 742000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.4     |\n",
      "|    critic_loss     | 0.00131  |\n",
      "|    ent_coef        | 0.000603 |\n",
      "|    ent_coef_loss   | -5.15    |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=743000, episode_reward=342.35 +/- 122.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 342      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 743000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=525.33 +/- 21.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 525      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 744000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.39    |\n",
      "|    critic_loss     | 0.00119  |\n",
      "|    ent_coef        | 0.000599 |\n",
      "|    ent_coef_loss   | -5.04    |\n",
      "|    learning_rate   | 0.00426  |\n",
      "|    n_updates       | 3630     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 147      |\n",
      "| time/              |          |\n",
      "|    episodes        | 744      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1171     |\n",
      "|    total_timesteps | 744000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=500.97 +/- 55.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 501      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 745000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=746000, episode_reward=444.59 +/- 25.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 445      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 746000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.39    |\n",
      "|    critic_loss     | 0.00135  |\n",
      "|    ent_coef        | 0.000596 |\n",
      "|    ent_coef_loss   | -3.62    |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=747000, episode_reward=472.75 +/- 61.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 473      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 747000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=665.48 +/- 15.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 665      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 748000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.39    |\n",
      "|    critic_loss     | 0.00141  |\n",
      "|    ent_coef        | 0.000594 |\n",
      "|    ent_coef_loss   | 4.54     |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3650     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 158      |\n",
      "| time/              |          |\n",
      "|    episodes        | 748      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1177     |\n",
      "|    total_timesteps | 748000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749000, episode_reward=651.64 +/- 15.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 652      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 749000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=560.99 +/- 60.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 561      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.39    |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.000594 |\n",
      "|    ent_coef_loss   | -5.04    |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=751000, episode_reward=585.41 +/- 85.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 585      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=347.23 +/- 226.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 347      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 752000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.38    |\n",
      "|    critic_loss     | 0.00137  |\n",
      "|    ent_coef        | 0.000592 |\n",
      "|    ent_coef_loss   | 0.477    |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3670     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 170      |\n",
      "| time/              |          |\n",
      "|    episodes        | 752      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1183     |\n",
      "|    total_timesteps | 752000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753000, episode_reward=361.08 +/- 89.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 361      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 753000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=754000, episode_reward=613.24 +/- 30.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 613      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 754000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.00175  |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | -8.22    |\n",
      "|    learning_rate   | 0.00425  |\n",
      "|    n_updates       | 3680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=610.30 +/- 32.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 610      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 755000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=515.59 +/- 13.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 516      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 756000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.4     |\n",
      "|    critic_loss     | 0.00146  |\n",
      "|    ent_coef        | 0.000587 |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3690     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 193      |\n",
      "| time/              |          |\n",
      "|    episodes        | 756      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1189     |\n",
      "|    total_timesteps | 756000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=757000, episode_reward=499.28 +/- 7.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 499      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=758000, episode_reward=285.49 +/- 371.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 285      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 758000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.37    |\n",
      "|    critic_loss     | 0.00107  |\n",
      "|    ent_coef        | 0.000586 |\n",
      "|    ent_coef_loss   | -4.1     |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=759000, episode_reward=267.11 +/- 352.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 267      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 759000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=302.17 +/- 113.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.37    |\n",
      "|    critic_loss     | 0.00121  |\n",
      "|    ent_coef        | 0.000583 |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3710     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 223      |\n",
      "| time/              |          |\n",
      "|    episodes        | 760      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1195     |\n",
      "|    total_timesteps | 760000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=761000, episode_reward=152.27 +/- 174.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 152      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 761000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=620.30 +/- 32.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 620      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 762000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.37    |\n",
      "|    critic_loss     | 0.00142  |\n",
      "|    ent_coef        | 0.000576 |\n",
      "|    ent_coef_loss   | -8.55    |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=763000, episode_reward=106.64 +/- 387.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=643.31 +/- 25.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 643      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 764000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.38    |\n",
      "|    critic_loss     | 0.00131  |\n",
      "|    ent_coef        | 0.00057  |\n",
      "|    ent_coef_loss   | 4.16     |\n",
      "|    learning_rate   | 0.00424  |\n",
      "|    n_updates       | 3730     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 250      |\n",
      "| time/              |          |\n",
      "|    episodes        | 764      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1201     |\n",
      "|    total_timesteps | 764000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=589.43 +/- 154.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 589      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 765000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766000, episode_reward=462.04 +/- 17.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 766000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.4     |\n",
      "|    critic_loss     | 0.00125  |\n",
      "|    ent_coef        | 0.000568 |\n",
      "|    ent_coef_loss   | -6.88    |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=767000, episode_reward=464.86 +/- 52.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 767000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=461.48 +/- 39.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 461      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 283      |\n",
      "| time/              |          |\n",
      "|    episodes        | 768      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1207     |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=769000, episode_reward=373.55 +/- 24.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 769000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.35    |\n",
      "|    critic_loss     | 0.00139  |\n",
      "|    ent_coef        | 0.000565 |\n",
      "|    ent_coef_loss   | -8.03    |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=381.57 +/- 13.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 382      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=771000, episode_reward=464.60 +/- 21.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 771000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.34    |\n",
      "|    critic_loss     | 0.00131  |\n",
      "|    ent_coef        | 0.000561 |\n",
      "|    ent_coef_loss   | 6.4      |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=441.62 +/- 53.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 442      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 772000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 299      |\n",
      "| time/              |          |\n",
      "|    episodes        | 772      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1213     |\n",
      "|    total_timesteps | 772000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=773000, episode_reward=173.79 +/- 34.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 174      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 773000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.00123  |\n",
      "|    ent_coef        | 0.000563 |\n",
      "|    ent_coef_loss   | 4.97     |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=185.69 +/- 32.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 186      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 774000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=291.85 +/- 29.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 775000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 0.0017   |\n",
      "|    ent_coef        | 0.000567 |\n",
      "|    ent_coef_loss   | 13.4     |\n",
      "|    learning_rate   | 0.00423  |\n",
      "|    n_updates       | 3780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=249.68 +/- 39.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 250      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 776000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 313      |\n",
      "| time/              |          |\n",
      "|    episodes        | 776      |\n",
      "|    fps             | 635      |\n",
      "|    time_elapsed    | 1220     |\n",
      "|    total_timesteps | 776000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=777000, episode_reward=527.55 +/- 20.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 528      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 777000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.34    |\n",
      "|    critic_loss     | 0.00144  |\n",
      "|    ent_coef        | 0.000574 |\n",
      "|    ent_coef_loss   | 16.7     |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778000, episode_reward=517.88 +/- 33.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 518      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 778000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=779000, episode_reward=506.05 +/- 48.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 506      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 779000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.00138  |\n",
      "|    ent_coef        | 0.000583 |\n",
      "|    ent_coef_loss   | 2.13     |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=466.94 +/- 26.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 467      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 332      |\n",
      "| time/              |          |\n",
      "|    episodes        | 780      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1226     |\n",
      "|    total_timesteps | 780000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=781000, episode_reward=509.69 +/- 36.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 510      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 781000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.00158  |\n",
      "|    ent_coef        | 0.000589 |\n",
      "|    ent_coef_loss   | 7.99     |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782000, episode_reward=518.40 +/- 40.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 518      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 782000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=783000, episode_reward=387.04 +/- 22.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 387      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 783000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.34    |\n",
      "|    critic_loss     | 0.00134  |\n",
      "|    ent_coef        | 0.000594 |\n",
      "|    ent_coef_loss   | 9.66     |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=336.71 +/- 56.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 337      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 784000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 359      |\n",
      "| time/              |          |\n",
      "|    episodes        | 784      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1232     |\n",
      "|    total_timesteps | 784000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=623.59 +/- 41.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 624      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 785000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.34    |\n",
      "|    critic_loss     | 0.0012   |\n",
      "|    ent_coef        | 0.000601 |\n",
      "|    ent_coef_loss   | 2.7      |\n",
      "|    learning_rate   | 0.00422  |\n",
      "|    n_updates       | 3830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=625.30 +/- 45.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 625      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 786000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=787000, episode_reward=306.15 +/- 29.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 306      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 787000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.00111  |\n",
      "|    ent_coef        | 0.000604 |\n",
      "|    ent_coef_loss   | -7.1     |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=343.14 +/- 42.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 343      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 788000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 389      |\n",
      "| time/              |          |\n",
      "|    episodes        | 788      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1238     |\n",
      "|    total_timesteps | 788000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=789000, episode_reward=392.05 +/- 46.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 392      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 789000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 0.00143  |\n",
      "|    ent_coef        | 0.000603 |\n",
      "|    ent_coef_loss   | 3.13     |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=394.45 +/- 57.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 394      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=791000, episode_reward=630.35 +/- 13.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 630      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 791000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.00126  |\n",
      "|    ent_coef        | 0.000603 |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=558.76 +/- 42.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 559      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 792000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 408      |\n",
      "| time/              |          |\n",
      "|    episodes        | 792      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1244     |\n",
      "|    total_timesteps | 792000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=793000, episode_reward=478.76 +/- 313.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 479      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 793000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.35    |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.000602 |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794000, episode_reward=436.08 +/- 312.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 436      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=497.04 +/- 41.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 497      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 795000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.35    |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.000595 |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.00421  |\n",
      "|    n_updates       | 3880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=491.70 +/- 40.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 492      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 796000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 420      |\n",
      "| time/              |          |\n",
      "|    episodes        | 796      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1250     |\n",
      "|    total_timesteps | 796000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=797000, episode_reward=559.70 +/- 94.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 560      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 797000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.00121  |\n",
      "|    ent_coef        | 0.000588 |\n",
      "|    ent_coef_loss   | 0.386    |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=577.64 +/- 61.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 578      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 798000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=799000, episode_reward=557.41 +/- 33.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 557      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 799000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.0013   |\n",
      "|    ent_coef        | 0.000586 |\n",
      "|    ent_coef_loss   | 1.51     |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=491.48 +/- 44.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 491      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 434      |\n",
      "| time/              |          |\n",
      "|    episodes        | 800      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1256     |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=801000, episode_reward=696.74 +/- 38.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 697      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 801000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.00126  |\n",
      "|    ent_coef        | 0.000585 |\n",
      "|    ent_coef_loss   | -5.21    |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802000, episode_reward=661.85 +/- 27.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 662      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 802000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=803000, episode_reward=493.03 +/- 43.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 493      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 803000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.00124  |\n",
      "|    ent_coef        | 0.000583 |\n",
      "|    ent_coef_loss   | 0.972    |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=514.85 +/- 33.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 515      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 804000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 440      |\n",
      "| time/              |          |\n",
      "|    episodes        | 804      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1262     |\n",
      "|    total_timesteps | 804000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=205.21 +/- 53.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 205      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 805000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.00101  |\n",
      "|    ent_coef        | 0.000582 |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 3930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806000, episode_reward=215.52 +/- 56.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 216      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=807000, episode_reward=320.72 +/- 48.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 807000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 0.0018   |\n",
      "|    ent_coef        | 0.000582 |\n",
      "|    ent_coef_loss   | 13       |\n",
      "|    learning_rate   | 0.00419  |\n",
      "|    n_updates       | 3940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=316.35 +/- 21.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 316      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 808000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 429      |\n",
      "| time/              |          |\n",
      "|    episodes        | 808      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1268     |\n",
      "|    total_timesteps | 808000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=809000, episode_reward=345.34 +/- 24.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 345      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 809000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 0.00182  |\n",
      "|    ent_coef        | 0.000587 |\n",
      "|    ent_coef_loss   | 7.25     |\n",
      "|    learning_rate   | 0.00419  |\n",
      "|    n_updates       | 3950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=355.36 +/- 32.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 355      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=811000, episode_reward=351.86 +/- 28.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 352      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 811000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=256.83 +/- 48.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 257      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 812000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 0.00155  |\n",
      "|    ent_coef        | 0.000594 |\n",
      "|    ent_coef_loss   | 6.21     |\n",
      "|    learning_rate   | 0.00419  |\n",
      "|    n_updates       | 3960     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 424      |\n",
      "| time/              |          |\n",
      "|    episodes        | 812      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1274     |\n",
      "|    total_timesteps | 812000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813000, episode_reward=311.80 +/- 95.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 312      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 813000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=814000, episode_reward=164.69 +/- 32.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 165      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 814000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.00159  |\n",
      "|    ent_coef        | 0.000599 |\n",
      "|    ent_coef_loss   | 6.54     |\n",
      "|    learning_rate   | 0.00419  |\n",
      "|    n_updates       | 3970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=148.42 +/- 36.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 148      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 815000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=399.01 +/- 35.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 816000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 0.00157  |\n",
      "|    ent_coef        | 0.000605 |\n",
      "|    ent_coef_loss   | 6.66     |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 3980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 420      |\n",
      "| time/              |          |\n",
      "|    episodes        | 816      |\n",
      "|    fps             | 636      |\n",
      "|    time_elapsed    | 1281     |\n",
      "|    total_timesteps | 816000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=817000, episode_reward=408.65 +/- 36.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 409      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 817000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818000, episode_reward=426.62 +/- 32.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 818000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.00137  |\n",
      "|    ent_coef        | 0.00061  |\n",
      "|    ent_coef_loss   | -0.498   |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 3990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=819000, episode_reward=409.81 +/- 18.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 410      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 819000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=431.78 +/- 41.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 432      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.29    |\n",
      "|    critic_loss     | 0.00141  |\n",
      "|    ent_coef        | 0.000611 |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 421      |\n",
      "| time/              |          |\n",
      "|    episodes        | 820      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1287     |\n",
      "|    total_timesteps | 820000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=821000, episode_reward=390.44 +/- 17.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 390      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 821000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=441.32 +/- 31.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 441      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 822000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 0.00168  |\n",
      "|    ent_coef        | 0.000607 |\n",
      "|    ent_coef_loss   | 3.43     |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 4010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=823000, episode_reward=468.97 +/- 52.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 469      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 823000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=668.28 +/- 30.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 668      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 824000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.00166  |\n",
      "|    ent_coef        | 0.000607 |\n",
      "|    ent_coef_loss   | 6.64     |\n",
      "|    learning_rate   | 0.00418  |\n",
      "|    n_updates       | 4020     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 422      |\n",
      "| time/              |          |\n",
      "|    episodes        | 824      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1293     |\n",
      "|    total_timesteps | 824000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=625.57 +/- 20.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 626      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 825000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826000, episode_reward=548.93 +/- 25.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 549      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 826000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.00114  |\n",
      "|    ent_coef        | 0.000608 |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=827000, episode_reward=539.53 +/- 26.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 540      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 827000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=488.90 +/- 67.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 489      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 828000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3     |\n",
      "|    critic_loss     | 0.00114  |\n",
      "|    ent_coef        | 0.000602 |\n",
      "|    ent_coef_loss   | -6.7     |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4040     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 424      |\n",
      "| time/              |          |\n",
      "|    episodes        | 828      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1299     |\n",
      "|    total_timesteps | 828000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=829000, episode_reward=614.08 +/- 78.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 614      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 829000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=879.59 +/- 65.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 880      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.29    |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.000597 |\n",
      "|    ent_coef_loss   | 3.27     |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4050     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=831000, episode_reward=883.35 +/- 43.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 883      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 831000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=832000, episode_reward=690.43 +/- 7.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 690      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 832000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.35    |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000597 |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4060     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 433      |\n",
      "| time/              |          |\n",
      "|    episodes        | 832      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1305     |\n",
      "|    total_timesteps | 832000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=833000, episode_reward=693.30 +/- 18.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 693      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 833000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=501.10 +/- 81.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 501      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 834000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.34    |\n",
      "|    critic_loss     | 0.00102  |\n",
      "|    ent_coef        | 0.000598 |\n",
      "|    ent_coef_loss   | 1.81     |\n",
      "|    learning_rate   | 0.00417  |\n",
      "|    n_updates       | 4070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=533.43 +/- 68.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 533      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 835000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=836000, episode_reward=473.19 +/- 42.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 473      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 836000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.00141  |\n",
      "|    ent_coef        | 0.000599 |\n",
      "|    ent_coef_loss   | 3.03     |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4080     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 437      |\n",
      "| time/              |          |\n",
      "|    episodes        | 836      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1311     |\n",
      "|    total_timesteps | 836000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=837000, episode_reward=508.46 +/- 30.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 508      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838000, episode_reward=586.25 +/- 49.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 586      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 838000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.25    |\n",
      "|    critic_loss     | 0.0014   |\n",
      "|    ent_coef        | 0.000602 |\n",
      "|    ent_coef_loss   | 13.4     |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=839000, episode_reward=620.91 +/- 30.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 621      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 839000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=605.65 +/- 70.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 606      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3     |\n",
      "|    critic_loss     | 0.00122  |\n",
      "|    ent_coef        | 0.00061  |\n",
      "|    ent_coef_loss   | 0.943    |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4100     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 439      |\n",
      "| time/              |          |\n",
      "|    episodes        | 840      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1317     |\n",
      "|    total_timesteps | 840000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=841000, episode_reward=505.16 +/- 70.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 505      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 841000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842000, episode_reward=461.93 +/- 98.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 842000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.29    |\n",
      "|    critic_loss     | 0.00151  |\n",
      "|    ent_coef        | 0.000613 |\n",
      "|    ent_coef_loss   | -3.48    |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=843000, episode_reward=523.60 +/- 79.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 524      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=820.69 +/- 13.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 821      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 844000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 0.00127  |\n",
      "|    ent_coef        | 0.000612 |\n",
      "|    ent_coef_loss   | -6.29    |\n",
      "|    learning_rate   | 0.00416  |\n",
      "|    n_updates       | 4120     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 449      |\n",
      "| time/              |          |\n",
      "|    episodes        | 844      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1324     |\n",
      "|    total_timesteps | 844000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=833.92 +/- 17.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 834      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 845000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=706.73 +/- 25.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 707      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 846000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.00111  |\n",
      "|    ent_coef        | 0.000608 |\n",
      "|    ent_coef_loss   | -2.09    |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=847000, episode_reward=714.21 +/- 25.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 714      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 847000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=626.15 +/- 85.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 626      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 848000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.32    |\n",
      "|    critic_loss     | 0.00121  |\n",
      "|    ent_coef        | 0.000605 |\n",
      "|    ent_coef_loss   | -4.91    |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 459      |\n",
      "| time/              |          |\n",
      "|    episodes        | 848      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1330     |\n",
      "|    total_timesteps | 848000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849000, episode_reward=587.37 +/- 52.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 587      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=721.96 +/- 43.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 722      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.29    |\n",
      "|    critic_loss     | 0.00158  |\n",
      "|    ent_coef        | 0.000602 |\n",
      "|    ent_coef_loss   | -3.08    |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=851000, episode_reward=647.02 +/- 38.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 647      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 851000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=844.12 +/- 20.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 844      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 852000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3     |\n",
      "|    critic_loss     | 0.00151  |\n",
      "|    ent_coef        | 0.000599 |\n",
      "|    ent_coef_loss   | 4.23     |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4160     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 466      |\n",
      "| time/              |          |\n",
      "|    episodes        | 852      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1336     |\n",
      "|    total_timesteps | 852000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=853000, episode_reward=853.88 +/- 17.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 854      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 853000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854000, episode_reward=843.61 +/- 17.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 844      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 854000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=695.25 +/- 46.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 695      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 855000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.34    |\n",
      "|    critic_loss     | 0.00121  |\n",
      "|    ent_coef        | 0.0006   |\n",
      "|    ent_coef_loss   | 0.767    |\n",
      "|    learning_rate   | 0.00415  |\n",
      "|    n_updates       | 4170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=636.34 +/- 41.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 636      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 856000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 475      |\n",
      "| time/              |          |\n",
      "|    episodes        | 856      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1342     |\n",
      "|    total_timesteps | 856000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=857000, episode_reward=318.92 +/- 291.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 319      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 857000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.00145  |\n",
      "|    ent_coef        | 0.000601 |\n",
      "|    ent_coef_loss   | -3.99    |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=424.08 +/- 32.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 424      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 858000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=859000, episode_reward=594.03 +/- 28.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 594      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 859000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.25    |\n",
      "|    critic_loss     | 0.00132  |\n",
      "|    ent_coef        | 0.000599 |\n",
      "|    ent_coef_loss   | 2.58     |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=582.29 +/- 79.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 582      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 476      |\n",
      "| time/              |          |\n",
      "|    episodes        | 860      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1348     |\n",
      "|    total_timesteps | 860000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=861000, episode_reward=668.16 +/- 59.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 668      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 861000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 0.00119  |\n",
      "|    ent_coef        | 0.0006   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862000, episode_reward=686.86 +/- 72.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 687      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 862000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=863000, episode_reward=740.52 +/- 54.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 741      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 863000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3     |\n",
      "|    critic_loss     | 0.00105  |\n",
      "|    ent_coef        | 0.000601 |\n",
      "|    ent_coef_loss   | -5.51    |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=717.41 +/- 59.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 717      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 864000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 481      |\n",
      "| time/              |          |\n",
      "|    episodes        | 864      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1354     |\n",
      "|    total_timesteps | 864000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=769.04 +/- 17.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 769      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 865000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.29    |\n",
      "|    critic_loss     | 0.00112  |\n",
      "|    ent_coef        | 0.000598 |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.00414  |\n",
      "|    n_updates       | 4220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866000, episode_reward=792.25 +/- 32.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 792      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 866000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=867000, episode_reward=655.66 +/- 64.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 656      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 867000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 0.00137  |\n",
      "|    ent_coef        | 0.000596 |\n",
      "|    ent_coef_loss   | 2.46     |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=619.67 +/- 35.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 620      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 868000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 484      |\n",
      "| time/              |          |\n",
      "|    episodes        | 868      |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1360     |\n",
      "|    total_timesteps | 868000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=869000, episode_reward=652.16 +/- 34.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 652      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 869000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.00118  |\n",
      "|    ent_coef        | 0.000597 |\n",
      "|    ent_coef_loss   | 7.79     |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=646.15 +/- 102.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 646      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=871000, episode_reward=605.62 +/- 43.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 606      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 871000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 0.00125  |\n",
      "|    ent_coef        | 0.000603 |\n",
      "|    ent_coef_loss   | 13.1     |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=672.30 +/- 33.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 672      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 872000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 488      |\n",
      "| time/              |          |\n",
      "|    episodes        | 872      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1366     |\n",
      "|    total_timesteps | 872000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=873000, episode_reward=778.87 +/- 62.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 779      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 873000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.25    |\n",
      "|    critic_loss     | 0.00125  |\n",
      "|    ent_coef        | 0.000612 |\n",
      "|    ent_coef_loss   | 2.57     |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874000, episode_reward=760.35 +/- 69.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 760      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 874000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=788.77 +/- 75.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 789      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 875000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3     |\n",
      "|    critic_loss     | 0.0013   |\n",
      "|    ent_coef        | 0.000618 |\n",
      "|    ent_coef_loss   | 3.16     |\n",
      "|    learning_rate   | 0.00413  |\n",
      "|    n_updates       | 4270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=846.81 +/- 55.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 847      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 876000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 505      |\n",
      "| time/              |          |\n",
      "|    episodes        | 876      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1372     |\n",
      "|    total_timesteps | 876000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=877000, episode_reward=676.38 +/- 66.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 676      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 877000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.29    |\n",
      "|    critic_loss     | 0.00136  |\n",
      "|    ent_coef        | 0.000621 |\n",
      "|    ent_coef_loss   | 3.48     |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=878000, episode_reward=632.52 +/- 29.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 633      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 878000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=879000, episode_reward=608.46 +/- 65.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 608      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 879000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 0.00139  |\n",
      "|    ent_coef        | 0.000626 |\n",
      "|    ent_coef_loss   | 8.03     |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=567.12 +/- 94.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 567      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 507      |\n",
      "| time/              |          |\n",
      "|    episodes        | 880      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1378     |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=881000, episode_reward=676.19 +/- 75.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 676      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 881000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.25    |\n",
      "|    critic_loss     | 0.00127  |\n",
      "|    ent_coef        | 0.000633 |\n",
      "|    ent_coef_loss   | 8.58     |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882000, episode_reward=746.91 +/- 52.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 747      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 882000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=883000, episode_reward=812.97 +/- 20.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 813      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 883000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.00064  |\n",
      "|    ent_coef_loss   | 5.09     |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=787.37 +/- 75.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 787      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 884000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 519      |\n",
      "| time/              |          |\n",
      "|    episodes        | 884      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1384     |\n",
      "|    total_timesteps | 884000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=741.84 +/- 62.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 742      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 885000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3     |\n",
      "|    critic_loss     | 0.00112  |\n",
      "|    ent_coef        | 0.000647 |\n",
      "|    ent_coef_loss   | 4.13     |\n",
      "|    learning_rate   | 0.00412  |\n",
      "|    n_updates       | 4320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=886000, episode_reward=691.02 +/- 124.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 691      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887000, episode_reward=524.93 +/- 60.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 525      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 887000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.00124  |\n",
      "|    ent_coef        | 0.000652 |\n",
      "|    ent_coef_loss   | 3.34     |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=353.80 +/- 382.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 354      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 888000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 527      |\n",
      "| time/              |          |\n",
      "|    episodes        | 888      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1390     |\n",
      "|    total_timesteps | 888000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=889000, episode_reward=439.12 +/- 71.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 439      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 889000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.23    |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.000656 |\n",
      "|    ent_coef_loss   | 8.91     |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=554.62 +/- 20.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 555      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=891000, episode_reward=750.27 +/- 25.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 750      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 891000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.22    |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000665 |\n",
      "|    ent_coef_loss   | 9.55     |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=719.13 +/- 61.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 719      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 530      |\n",
      "| time/              |          |\n",
      "|    episodes        | 892      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1396     |\n",
      "|    total_timesteps | 892000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=893000, episode_reward=-63.26 +/- 195.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -63.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 893000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.00133  |\n",
      "|    ent_coef        | 0.000673 |\n",
      "|    ent_coef_loss   | -2.33    |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=-95.18 +/- 154.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 894000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=367.87 +/- 282.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 368      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 895000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 0.0015   |\n",
      "|    ent_coef        | 0.000675 |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.00411  |\n",
      "|    n_updates       | 4370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=62.69 +/- 271.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 62.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 523      |\n",
      "| time/              |          |\n",
      "|    episodes        | 896      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1403     |\n",
      "|    total_timesteps | 896000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=897000, episode_reward=263.16 +/- 367.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 263      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 897000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=898000, episode_reward=81.28 +/- 362.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 81.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 898000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.22    |\n",
      "|    critic_loss     | 0.00156  |\n",
      "|    ent_coef        | 0.000676 |\n",
      "|    ent_coef_loss   | 13.4     |\n",
      "|    learning_rate   | 0.0041   |\n",
      "|    n_updates       | 4380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899000, episode_reward=140.27 +/- 408.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 140      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 899000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-211.37 +/- 3.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 0.00142  |\n",
      "|    ent_coef        | 0.000685 |\n",
      "|    ent_coef_loss   | 3.26     |\n",
      "|    learning_rate   | 0.0041   |\n",
      "|    n_updates       | 4390     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 528      |\n",
      "| time/              |          |\n",
      "|    episodes        | 900      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1409     |\n",
      "|    total_timesteps | 900000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=901000, episode_reward=-212.82 +/- 5.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 901000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=902000, episode_reward=-117.95 +/- 4.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 902000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.22    |\n",
      "|    critic_loss     | 0.00128  |\n",
      "|    ent_coef        | 0.000691 |\n",
      "|    ent_coef_loss   | 1.31     |\n",
      "|    learning_rate   | 0.0041   |\n",
      "|    n_updates       | 4400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903000, episode_reward=-117.07 +/- 4.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 903000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=24.08 +/- 294.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 904000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.23    |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.000693 |\n",
      "|    ent_coef_loss   | -4.09    |\n",
      "|    learning_rate   | 0.0041   |\n",
      "|    n_updates       | 4410     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 526      |\n",
      "| time/              |          |\n",
      "|    episodes        | 904      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1415     |\n",
      "|    total_timesteps | 904000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=905000, episode_reward=-123.07 +/- 3.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 905000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=-133.07 +/- 3.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 906000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 0.00115  |\n",
      "|    ent_coef        | 0.000692 |\n",
      "|    ent_coef_loss   | -3.83    |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907000, episode_reward=-130.21 +/- 5.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 907000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=908000, episode_reward=-103.79 +/- 4.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 908000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 0.00115  |\n",
      "|    ent_coef        | 0.000689 |\n",
      "|    ent_coef_loss   | 0.207    |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4430     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 543      |\n",
      "| time/              |          |\n",
      "|    episodes        | 908      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1421     |\n",
      "|    total_timesteps | 908000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=909000, episode_reward=-104.49 +/- 8.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 909000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=0.35 +/- 131.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 0.354    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 0.00114  |\n",
      "|    ent_coef        | 0.000686 |\n",
      "|    ent_coef_loss   | -7.14    |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=911000, episode_reward=-69.07 +/- 7.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -69.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 911000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=800.46 +/- 77.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 800      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 912000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.22    |\n",
      "|    critic_loss     | 0.00122  |\n",
      "|    ent_coef        | 0.000681 |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4450     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 549      |\n",
      "| time/              |          |\n",
      "|    episodes        | 912      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1427     |\n",
      "|    total_timesteps | 912000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=913000, episode_reward=726.32 +/- 84.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 726      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 913000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=914000, episode_reward=720.86 +/- 34.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 721      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 914000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.22    |\n",
      "|    critic_loss     | 0.00126  |\n",
      "|    ent_coef        | 0.00068  |\n",
      "|    ent_coef_loss   | 5.38     |\n",
      "|    learning_rate   | 0.00409  |\n",
      "|    n_updates       | 4460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=547.38 +/- 368.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 547      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 915000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=916000, episode_reward=-169.35 +/- 3.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -169     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 916000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 0.00124  |\n",
      "|    ent_coef        | 0.000685 |\n",
      "|    ent_coef_loss   | 7.33     |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4470     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 566      |\n",
      "| time/              |          |\n",
      "|    episodes        | 916      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1433     |\n",
      "|    total_timesteps | 916000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=917000, episode_reward=-170.21 +/- 4.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -170     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 917000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=-382.03 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 918000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 0.000669 |\n",
      "|    ent_coef        | 0.000689 |\n",
      "|    ent_coef_loss   | -0.0354  |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=919000, episode_reward=-380.69 +/- 2.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 919000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-488.07 +/- 13.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -488     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.34    |\n",
      "|    critic_loss     | 0.000233 |\n",
      "|    ent_coef        | 0.000692 |\n",
      "|    ent_coef_loss   | 11       |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4490     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 538      |\n",
      "| time/              |          |\n",
      "|    episodes        | 920      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1440     |\n",
      "|    total_timesteps | 920000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921000, episode_reward=-492.01 +/- 14.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -492     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 921000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=922000, episode_reward=-513.63 +/- 11.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -514     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 922000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.32    |\n",
      "|    critic_loss     | 0.00033  |\n",
      "|    ent_coef        | 0.000704 |\n",
      "|    ent_coef_loss   | 26.6     |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=923000, episode_reward=-504.47 +/- 8.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -504     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=-550.73 +/- 1.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -551     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 924000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.000126 |\n",
      "|    ent_coef        | 0.000732 |\n",
      "|    ent_coef_loss   | 41.2     |\n",
      "|    learning_rate   | 0.00408  |\n",
      "|    n_updates       | 4510     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 502      |\n",
      "| time/              |          |\n",
      "|    episodes        | 924      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1446     |\n",
      "|    total_timesteps | 924000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=-551.27 +/- 3.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -551     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 925000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=926000, episode_reward=-536.51 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -537     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 926000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 9.95e-05 |\n",
      "|    ent_coef        | 0.000779 |\n",
      "|    ent_coef_loss   | 36.6     |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927000, episode_reward=-533.47 +/- 3.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -533     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 927000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=-500.45 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -500     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 928000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.000114 |\n",
      "|    ent_coef        | 0.000829 |\n",
      "|    ent_coef_loss   | 29.9     |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4530     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 460      |\n",
      "| time/              |          |\n",
      "|    episodes        | 928      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1452     |\n",
      "|    total_timesteps | 928000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=929000, episode_reward=-500.51 +/- 2.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -501     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-462.95 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -463     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.32    |\n",
      "|    critic_loss     | 9.03e-05 |\n",
      "|    ent_coef        | 0.000875 |\n",
      "|    ent_coef_loss   | 20.7     |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=931000, episode_reward=-462.53 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -463     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 931000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=-462.22 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -462     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 932000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.32    |\n",
      "|    critic_loss     | 6.23e-05 |\n",
      "|    ent_coef        | 0.000914 |\n",
      "|    ent_coef_loss   | 18.2     |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4550     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 415      |\n",
      "| time/              |          |\n",
      "|    episodes        | 932      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1458     |\n",
      "|    total_timesteps | 932000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933000, episode_reward=-462.73 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -463     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 933000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=934000, episode_reward=-460.01 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -460     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 934000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.32    |\n",
      "|    critic_loss     | 7.32e-05 |\n",
      "|    ent_coef        | 0.000948 |\n",
      "|    ent_coef_loss   | 14.9     |\n",
      "|    learning_rate   | 0.00407  |\n",
      "|    n_updates       | 4560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=-459.67 +/- 2.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -460     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=-448.91 +/- 1.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -449     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 936000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.32    |\n",
      "|    critic_loss     | 5.3e-05  |\n",
      "|    ent_coef        | 0.000977 |\n",
      "|    ent_coef_loss   | 12       |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4570     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    episodes        | 936      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1464     |\n",
      "|    total_timesteps | 936000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=937000, episode_reward=-448.82 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -449     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 937000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=938000, episode_reward=-439.19 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -439     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 938000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 5.99e-05 |\n",
      "|    ent_coef        | 0.001    |\n",
      "|    ent_coef_loss   | 7.12     |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939000, episode_reward=-436.13 +/- 2.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -436     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 939000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-439.00 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -439     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 342      |\n",
      "| time/              |          |\n",
      "|    episodes        | 940      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1471     |\n",
      "|    total_timesteps | 940000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=941000, episode_reward=-422.18 +/- 1.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -422     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 941000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 6.17e-05 |\n",
      "|    ent_coef        | 0.00102  |\n",
      "|    ent_coef_loss   | 4.35     |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=-423.67 +/- 2.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -424     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 942000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=943000, episode_reward=-399.55 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -400     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 943000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 3.37e-05 |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=-399.44 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -399     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 944000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 303      |\n",
      "| time/              |          |\n",
      "|    episodes        | 944      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1477     |\n",
      "|    total_timesteps | 944000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=-355.84 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -356     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 945000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 3.99e-05 |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | -2.51    |\n",
      "|    learning_rate   | 0.00406  |\n",
      "|    n_updates       | 4610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=946000, episode_reward=-356.29 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -356     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 946000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947000, episode_reward=-334.17 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -334     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 947000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 5.52e-05 |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | -3.71    |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=-332.47 +/- 2.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -332     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 948000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 261      |\n",
      "| time/              |          |\n",
      "|    episodes        | 948      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1483     |\n",
      "|    total_timesteps | 948000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=949000, episode_reward=-335.57 +/- 2.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 949000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 6.14e-05 |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=-335.32 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=951000, episode_reward=-309.82 +/- 2.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -310     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 951000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3     |\n",
      "|    critic_loss     | 3.96e-05 |\n",
      "|    ent_coef        | 0.00102  |\n",
      "|    ent_coef_loss   | -4.52    |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=-310.78 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -311     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 952000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 224      |\n",
      "| time/              |          |\n",
      "|    episodes        | 952      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1490     |\n",
      "|    total_timesteps | 952000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=953000, episode_reward=-296.71 +/- 2.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -297     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 953000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3     |\n",
      "|    critic_loss     | 4.99e-05 |\n",
      "|    ent_coef        | 0.00102  |\n",
      "|    ent_coef_loss   | -6.08    |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954000, episode_reward=-297.49 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -297     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 954000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=-287.04 +/- 2.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -287     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 955000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3     |\n",
      "|    critic_loss     | 6.66e-05 |\n",
      "|    ent_coef        | 0.001    |\n",
      "|    ent_coef_loss   | -8.43    |\n",
      "|    learning_rate   | 0.00405  |\n",
      "|    n_updates       | 4660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=-285.90 +/- 2.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -286     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 956000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 184      |\n",
      "| time/              |          |\n",
      "|    episodes        | 956      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1496     |\n",
      "|    total_timesteps | 956000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=957000, episode_reward=-265.56 +/- 1.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -266     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 957000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.29    |\n",
      "|    critic_loss     | 6.87e-05 |\n",
      "|    ent_coef        | 0.00099  |\n",
      "|    ent_coef_loss   | -9.75    |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=958000, episode_reward=-262.73 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -263     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 958000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=959000, episode_reward=-259.96 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -260     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 959000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.29    |\n",
      "|    critic_loss     | 6.17e-05 |\n",
      "|    ent_coef        | 0.000973 |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-259.13 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -259     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    episodes        | 960      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1502     |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=961000, episode_reward=-245.22 +/- 3.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 961000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 6.99e-05 |\n",
      "|    ent_coef        | 0.000954 |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962000, episode_reward=-245.32 +/- 2.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 962000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=963000, episode_reward=-263.03 +/- 3.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -263     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 963000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 6.22e-05 |\n",
      "|    ent_coef        | 0.000933 |\n",
      "|    ent_coef_loss   | -9.37    |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=-261.86 +/- 3.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -262     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 964000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 120      |\n",
      "| time/              |          |\n",
      "|    episodes        | 964      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1509     |\n",
      "|    total_timesteps | 964000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=-270.64 +/- 2.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -271     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 965000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 5.68e-05 |\n",
      "|    ent_coef        | 0.000915 |\n",
      "|    ent_coef_loss   | -9.37    |\n",
      "|    learning_rate   | 0.00404  |\n",
      "|    n_updates       | 4710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=-270.30 +/- 2.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -270     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967000, episode_reward=-258.95 +/- 3.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -259     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 967000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 6.61e-05 |\n",
      "|    ent_coef        | 0.000898 |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=-259.34 +/- 1.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -259     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 968000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 85.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 968      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1515     |\n",
      "|    total_timesteps | 968000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=969000, episode_reward=-258.48 +/- 3.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 969000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 4.89e-05 |\n",
      "|    ent_coef        | 0.00088  |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=-255.18 +/- 2.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=971000, episode_reward=-246.62 +/- 2.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -247     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 971000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 4.22e-05 |\n",
      "|    ent_coef        | 0.000864 |\n",
      "|    ent_coef_loss   | -8.5     |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=-244.60 +/- 2.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 55.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 972      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1521     |\n",
      "|    total_timesteps | 972000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=973000, episode_reward=-233.80 +/- 2.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 973000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 4.39e-05 |\n",
      "|    ent_coef        | 0.000848 |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974000, episode_reward=-232.53 +/- 2.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 974000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=-243.51 +/- 2.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 975000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 3.9e-05  |\n",
      "|    ent_coef        | 0.000831 |\n",
      "|    ent_coef_loss   | -9.62    |\n",
      "|    learning_rate   | 0.00403  |\n",
      "|    n_updates       | 4760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=-241.14 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 976000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 976      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1527     |\n",
      "|    total_timesteps | 976000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=977000, episode_reward=-244.07 +/- 2.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 977000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 4.72e-05 |\n",
      "|    ent_coef        | 0.000815 |\n",
      "|    ent_coef_loss   | -6.73    |\n",
      "|    learning_rate   | 0.00402  |\n",
      "|    n_updates       | 4770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=-243.03 +/- 1.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=979000, episode_reward=-220.65 +/- 2.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -221     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 979000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 3.78e-05 |\n",
      "|    ent_coef        | 0.000802 |\n",
      "|    ent_coef_loss   | -8.83    |\n",
      "|    learning_rate   | 0.00402  |\n",
      "|    n_updates       | 4780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-222.69 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -223     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -13.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 980      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1534     |\n",
      "|    total_timesteps | 980000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=981000, episode_reward=-217.61 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 981000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 4.9e-05  |\n",
      "|    ent_coef        | 0.000789 |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.00402  |\n",
      "|    n_updates       | 4790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982000, episode_reward=-214.16 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -214     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 982000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=983000, episode_reward=-216.16 +/- 2.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -216     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 983000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=-223.37 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -223     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 984000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 5.45e-05 |\n",
      "|    ent_coef        | 0.000774 |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.00402  |\n",
      "|    n_updates       | 4800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -50.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 984      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1540     |\n",
      "|    total_timesteps | 984000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=-223.02 +/- 2.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -223     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 985000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=986000, episode_reward=-253.44 +/- 2.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -253     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 986000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 4.52e-05 |\n",
      "|    ent_coef        | 0.000758 |\n",
      "|    ent_coef_loss   | -5.35    |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987000, episode_reward=-251.35 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -251     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 987000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=-256.64 +/- 3.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -257     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 988000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 3.94e-05 |\n",
      "|    ent_coef        | 0.000746 |\n",
      "|    ent_coef_loss   | -6.15    |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4820     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -84.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 988      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1547     |\n",
      "|    total_timesteps | 988000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=989000, episode_reward=-255.12 +/- 2.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 989000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-242.23 +/- 2.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 2.75e-05 |\n",
      "|    ent_coef        | 0.000737 |\n",
      "|    ent_coef_loss   | -5.98    |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991000, episode_reward=-241.60 +/- 3.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 991000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=-229.34 +/- 3.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 992000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 2e-05    |\n",
      "|    ent_coef        | 0.000728 |\n",
      "|    ent_coef_loss   | -6.45    |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4840     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -114     |\n",
      "| time/              |          |\n",
      "|    episodes        | 992      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1553     |\n",
      "|    total_timesteps | 992000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=993000, episode_reward=-228.41 +/- 1.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 993000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=994000, episode_reward=-219.40 +/- 3.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 994000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.25    |\n",
      "|    critic_loss     | 2.36e-05 |\n",
      "|    ent_coef        | 0.000719 |\n",
      "|    ent_coef_loss   | -9.31    |\n",
      "|    learning_rate   | 0.00401  |\n",
      "|    n_updates       | 4850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=-217.29 +/- 2.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -217     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 995000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=-192.57 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -193     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 996000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.25    |\n",
      "|    critic_loss     | 2.84e-05 |\n",
      "|    ent_coef        | 0.000708 |\n",
      "|    ent_coef_loss   | -9.71    |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4860     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -136     |\n",
      "| time/              |          |\n",
      "|    episodes        | 996      |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1559     |\n",
      "|    total_timesteps | 996000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=997000, episode_reward=-193.75 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 997000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=998000, episode_reward=-187.36 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 998000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 4.08e-05 |\n",
      "|    ent_coef        | 0.000695 |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999000, episode_reward=-186.77 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 999000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-231.89 +/- 1.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 2.63e-05 |\n",
      "|    ent_coef        | 0.000681 |\n",
      "|    ent_coef_loss   | -6.59    |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4880     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -169     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1000     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1566     |\n",
      "|    total_timesteps | 1000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1001000, episode_reward=-231.76 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1001000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1002000, episode_reward=-247.12 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -247     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1002000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 3.86e-05 |\n",
      "|    ent_coef        | 0.00067  |\n",
      "|    ent_coef_loss   | -6.24    |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1003000, episode_reward=-247.53 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1003000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1004000, episode_reward=-229.47 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1004000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 2.93e-05 |\n",
      "|    ent_coef        | 0.000661 |\n",
      "|    ent_coef_loss   | -9.29    |\n",
      "|    learning_rate   | 0.004    |\n",
      "|    n_updates       | 4900     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1004     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1572     |\n",
      "|    total_timesteps | 1004000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1005000, episode_reward=-229.07 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1005000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1006000, episode_reward=-212.88 +/- 2.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1006000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 2.37e-05 |\n",
      "|    ent_coef        | 0.000651 |\n",
      "|    ent_coef_loss   | -8.44    |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1007000, episode_reward=-210.63 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1007000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-199.10 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -199     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1008000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 3.25e-05 |\n",
      "|    ent_coef        | 0.00064  |\n",
      "|    ent_coef_loss   | -9.12    |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4920     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -233     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1008     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1578     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1009000, episode_reward=-200.91 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -201     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1009000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=-224.10 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.23    |\n",
      "|    critic_loss     | 3.02e-05 |\n",
      "|    ent_coef        | 0.000629 |\n",
      "|    ent_coef_loss   | -7.26    |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1011000, episode_reward=-223.05 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -223     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1011000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1012000, episode_reward=-238.76 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -239     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1012000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.23    |\n",
      "|    critic_loss     | 2.34e-05 |\n",
      "|    ent_coef        | 0.000619 |\n",
      "|    ent_coef_loss   | -7.35    |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4940     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -260     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1012     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1584     |\n",
      "|    total_timesteps | 1012000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1013000, episode_reward=-238.17 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1013000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1014000, episode_reward=-231.73 +/- 2.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1014000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.23    |\n",
      "|    critic_loss     | 2.82e-05 |\n",
      "|    ent_coef        | 0.00061  |\n",
      "|    ent_coef_loss   | -8.6     |\n",
      "|    learning_rate   | 0.00399  |\n",
      "|    n_updates       | 4950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1015000, episode_reward=-231.61 +/- 1.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1015000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1016000, episode_reward=-225.24 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1016000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.22    |\n",
      "|    critic_loss     | 2.42e-05 |\n",
      "|    ent_coef        | 0.0006   |\n",
      "|    ent_coef_loss   | -8.95    |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 4960     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1016     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1591     |\n",
      "|    total_timesteps | 1016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1017000, episode_reward=-225.59 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -226     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1017000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1018000, episode_reward=-217.53 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1018000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.22    |\n",
      "|    critic_loss     | 2.31e-05 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | -8.6     |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 4970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1019000, episode_reward=-217.17 +/- 1.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -217     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1019000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=-211.98 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -212     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.22    |\n",
      "|    critic_loss     | 2.45e-05 |\n",
      "|    ent_coef        | 0.00058  |\n",
      "|    ent_coef_loss   | -9.12    |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 4980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -291     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1020     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1597     |\n",
      "|    total_timesteps | 1020000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1021000, episode_reward=-211.13 +/- 2.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1021000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1022000, episode_reward=-236.36 +/- 2.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1022000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.21    |\n",
      "|    critic_loss     | 2.61e-05 |\n",
      "|    ent_coef        | 0.00057  |\n",
      "|    ent_coef_loss   | -6.8     |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 4990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1023000, episode_reward=-234.00 +/- 1.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1023000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1024000, episode_reward=-232.04 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1024000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -280     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1024     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1603     |\n",
      "|    total_timesteps | 1024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1025000, episode_reward=-248.14 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1025000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.21    |\n",
      "|    critic_loss     | 2.25e-05 |\n",
      "|    ent_coef        | 0.000561 |\n",
      "|    ent_coef_loss   | -6.37    |\n",
      "|    learning_rate   | 0.00398  |\n",
      "|    n_updates       | 5000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1026000, episode_reward=-248.26 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1026000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1027000, episode_reward=-254.98 +/- 1.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1027000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.21    |\n",
      "|    critic_loss     | 2.62e-05 |\n",
      "|    ent_coef        | 0.000554 |\n",
      "|    ent_coef_loss   | -5.1     |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028000, episode_reward=-253.79 +/- 2.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -254     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1028000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -269     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1028     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1610     |\n",
      "|    total_timesteps | 1028000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1029000, episode_reward=-254.89 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1029000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.21    |\n",
      "|    critic_loss     | 2.52e-05 |\n",
      "|    ent_coef        | 0.000547 |\n",
      "|    ent_coef_loss   | -6.6     |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=-254.66 +/- 2.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1030000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1031000, episode_reward=-247.66 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1031000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.2     |\n",
      "|    critic_loss     | 2.18e-05 |\n",
      "|    ent_coef        | 0.000541 |\n",
      "|    ent_coef_loss   | -6.89    |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1032000, episode_reward=-246.71 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -247     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1032000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -260     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1032     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1616     |\n",
      "|    total_timesteps | 1032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1033000, episode_reward=-234.96 +/- 1.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -235     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1033000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.2     |\n",
      "|    critic_loss     | 2.6e-05  |\n",
      "|    ent_coef        | 0.000534 |\n",
      "|    ent_coef_loss   | -6.88    |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1034000, episode_reward=-231.70 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1034000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1035000, episode_reward=-253.79 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -254     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1035000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.2     |\n",
      "|    critic_loss     | 1.95e-05 |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | -6.18    |\n",
      "|    learning_rate   | 0.00397  |\n",
      "|    n_updates       | 5050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1036000, episode_reward=-254.97 +/- 1.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1036000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -252     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1036     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1622     |\n",
      "|    total_timesteps | 1036000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1037000, episode_reward=-249.39 +/- 3.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -249     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1037000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.2     |\n",
      "|    critic_loss     | 1.8e-05  |\n",
      "|    ent_coef        | 0.00052  |\n",
      "|    ent_coef_loss   | -5.59    |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1038000, episode_reward=-250.67 +/- 2.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -251     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1038000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1039000, episode_reward=-233.93 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1039000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.2     |\n",
      "|    critic_loss     | 2.13e-05 |\n",
      "|    ent_coef        | 0.000514 |\n",
      "|    ent_coef_loss   | -6.48    |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=-232.79 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1040000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -245     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1040     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1629     |\n",
      "|    total_timesteps | 1040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1041000, episode_reward=-237.52 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1041000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.19    |\n",
      "|    critic_loss     | 2.15e-05 |\n",
      "|    ent_coef        | 0.000507 |\n",
      "|    ent_coef_loss   | -7.91    |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1042000, episode_reward=-237.78 +/- 2.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1042000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1043000, episode_reward=-241.29 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1043000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.19    |\n",
      "|    critic_loss     | 2.9e-05  |\n",
      "|    ent_coef        | 0.000499 |\n",
      "|    ent_coef_loss   | -6.69    |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1044000, episode_reward=-240.88 +/- 2.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1044000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -238     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1044     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1635     |\n",
      "|    total_timesteps | 1044000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1045000, episode_reward=-231.41 +/- 2.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1045000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.19    |\n",
      "|    critic_loss     | 3.43e-05 |\n",
      "|    ent_coef        | 0.000493 |\n",
      "|    ent_coef_loss   | -6.36    |\n",
      "|    learning_rate   | 0.00396  |\n",
      "|    n_updates       | 5100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1046000, episode_reward=-230.50 +/- 3.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1046000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1047000, episode_reward=-223.43 +/- 1.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -223     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1047000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.18    |\n",
      "|    critic_loss     | 3.04e-05 |\n",
      "|    ent_coef        | 0.000486 |\n",
      "|    ent_coef_loss   | -7.64    |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048000, episode_reward=-223.96 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1048000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -234     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1048     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1641     |\n",
      "|    total_timesteps | 1048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1049000, episode_reward=-215.26 +/- 1.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1049000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.18    |\n",
      "|    critic_loss     | 2.3e-05  |\n",
      "|    ent_coef        | 0.000479 |\n",
      "|    ent_coef_loss   | -6.12    |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=-215.02 +/- 3.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1050000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1051000, episode_reward=-228.53 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1051000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.18    |\n",
      "|    critic_loss     | 2.52e-05 |\n",
      "|    ent_coef        | 0.000472 |\n",
      "|    ent_coef_loss   | -7.19    |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1052000, episode_reward=-228.13 +/- 1.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1052000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1052     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1648     |\n",
      "|    total_timesteps | 1052000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1053000, episode_reward=-242.81 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1053000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.18    |\n",
      "|    critic_loss     | 1.74e-05 |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | -7.18    |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1054000, episode_reward=-242.71 +/- 2.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1054000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1055000, episode_reward=-252.28 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -252     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1055000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.18    |\n",
      "|    critic_loss     | 2.43e-05 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | -5.32    |\n",
      "|    learning_rate   | 0.00395  |\n",
      "|    n_updates       | 5150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1056000, episode_reward=-250.25 +/- 2.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -250     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1056000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -228     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1056     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1654     |\n",
      "|    total_timesteps | 1056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1057000, episode_reward=-236.16 +/- 2.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1057000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.18    |\n",
      "|    critic_loss     | 2.76e-05 |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | -5.8     |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1058000, episode_reward=-234.56 +/- 1.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -235     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1058000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1059000, episode_reward=-234.28 +/- 2.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1059000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.17    |\n",
      "|    critic_loss     | 2.5e-05  |\n",
      "|    ent_coef        | 0.000448 |\n",
      "|    ent_coef_loss   | -5.7     |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=-234.13 +/- 2.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1060000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1060     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1660     |\n",
      "|    total_timesteps | 1060000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1061000, episode_reward=-236.49 +/- 2.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1061000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.17    |\n",
      "|    critic_loss     | 1.77e-05 |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | -6.32    |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1062000, episode_reward=-237.79 +/- 3.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1062000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1063000, episode_reward=-202.31 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -202     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1063000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.17    |\n",
      "|    critic_loss     | 2.89e-05 |\n",
      "|    ent_coef        | 0.000437 |\n",
      "|    ent_coef_loss   | -7.63    |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1064000, episode_reward=-203.35 +/- 1.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1064000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1064     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 1667     |\n",
      "|    total_timesteps | 1064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1065000, episode_reward=-228.03 +/- 1.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1065000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.16    |\n",
      "|    critic_loss     | 2.67e-05 |\n",
      "|    ent_coef        | 0.000431 |\n",
      "|    ent_coef_loss   | -5.58    |\n",
      "|    learning_rate   | 0.00394  |\n",
      "|    n_updates       | 5200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1066000, episode_reward=-225.79 +/- 1.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -226     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1066000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1067000, episode_reward=-226.78 +/- 2.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -227     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1067000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068000, episode_reward=-252.43 +/- 2.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -252     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1068000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.17    |\n",
      "|    critic_loss     | 2.41e-05 |\n",
      "|    ent_coef        | 0.000425 |\n",
      "|    ent_coef_loss   | -4.91    |\n",
      "|    learning_rate   | 0.00393  |\n",
      "|    n_updates       | 5210     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1068     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1674     |\n",
      "|    total_timesteps | 1068000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1069000, episode_reward=-250.08 +/- 3.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -250     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1069000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=-247.73 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.16    |\n",
      "|    critic_loss     | 2.58e-05 |\n",
      "|    ent_coef        | 0.000421 |\n",
      "|    ent_coef_loss   | -3.38    |\n",
      "|    learning_rate   | 0.00393  |\n",
      "|    n_updates       | 5220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1071000, episode_reward=-247.98 +/- 2.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -248     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1071000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1072000, episode_reward=-232.36 +/- 2.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1072000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.16    |\n",
      "|    critic_loss     | 2.05e-05 |\n",
      "|    ent_coef        | 0.000417 |\n",
      "|    ent_coef_loss   | -6.76    |\n",
      "|    learning_rate   | 0.00393  |\n",
      "|    n_updates       | 5230     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1072     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1680     |\n",
      "|    total_timesteps | 1072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1073000, episode_reward=-232.45 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1073000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1074000, episode_reward=-218.87 +/- 1.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1074000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.16    |\n",
      "|    critic_loss     | 2.35e-05 |\n",
      "|    ent_coef        | 0.000412 |\n",
      "|    ent_coef_loss   | -5.7     |\n",
      "|    learning_rate   | 0.00393  |\n",
      "|    n_updates       | 5240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1075000, episode_reward=-220.04 +/- 2.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -220     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1075000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1076000, episode_reward=-237.79 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1076000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 2.01e-05 |\n",
      "|    ent_coef        | 0.000407 |\n",
      "|    ent_coef_loss   | -6.06    |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5250     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1076     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1686     |\n",
      "|    total_timesteps | 1076000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1077000, episode_reward=-236.44 +/- 3.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1077000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1078000, episode_reward=-259.50 +/- 1.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -259     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1078000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.16    |\n",
      "|    critic_loss     | 1.62e-05 |\n",
      "|    ent_coef        | 0.000402 |\n",
      "|    ent_coef_loss   | -4.24    |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1079000, episode_reward=-258.27 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1079000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=-249.13 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -249     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.16    |\n",
      "|    critic_loss     | 1.5e-05  |\n",
      "|    ent_coef        | 0.000398 |\n",
      "|    ent_coef_loss   | -3.19    |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5270     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1080     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1693     |\n",
      "|    total_timesteps | 1080000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1081000, episode_reward=-252.21 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -252     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1081000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1082000, episode_reward=-214.30 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -214     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1082000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 2.03e-05 |\n",
      "|    ent_coef        | 0.000395 |\n",
      "|    ent_coef_loss   | -6.06    |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1083000, episode_reward=-213.19 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1083000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1084000, episode_reward=-214.64 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1084000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 1.95e-05 |\n",
      "|    ent_coef        | 0.00039  |\n",
      "|    ent_coef_loss   | -4.35    |\n",
      "|    learning_rate   | 0.00392  |\n",
      "|    n_updates       | 5290     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1084     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1699     |\n",
      "|    total_timesteps | 1084000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1085000, episode_reward=-217.72 +/- 2.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1085000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1086000, episode_reward=-232.45 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1086000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.14    |\n",
      "|    critic_loss     | 1.97e-05 |\n",
      "|    ent_coef        | 0.000386 |\n",
      "|    ent_coef_loss   | -4.95    |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1087000, episode_reward=-230.69 +/- 1.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1087000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088000, episode_reward=-242.02 +/- 2.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1088000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 1.09e-05 |\n",
      "|    ent_coef        | 0.000383 |\n",
      "|    ent_coef_loss   | -4.55    |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5310     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1088     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1705     |\n",
      "|    total_timesteps | 1088000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1089000, episode_reward=-243.56 +/- 1.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1089000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=-241.01 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 1.8e-05  |\n",
      "|    ent_coef        | 0.000379 |\n",
      "|    ent_coef_loss   | -2.98    |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1091000, episode_reward=-240.63 +/- 2.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1091000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1092000, episode_reward=-210.80 +/- 3.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1092000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.14    |\n",
      "|    critic_loss     | 1.64e-05 |\n",
      "|    ent_coef        | 0.000376 |\n",
      "|    ent_coef_loss   | -7.37    |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5330     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1092     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1712     |\n",
      "|    total_timesteps | 1092000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1093000, episode_reward=-209.72 +/- 1.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -210     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1093000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1094000, episode_reward=-229.01 +/- 3.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1094000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.14    |\n",
      "|    critic_loss     | 2.18e-05 |\n",
      "|    ent_coef        | 0.000371 |\n",
      "|    ent_coef_loss   | -3.91    |\n",
      "|    learning_rate   | 0.00391  |\n",
      "|    n_updates       | 5340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1095000, episode_reward=-227.66 +/- 4.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1095000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1096000, episode_reward=-245.76 +/- 2.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -246     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1096000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.14    |\n",
      "|    critic_loss     | 1.43e-05 |\n",
      "|    ent_coef        | 0.000367 |\n",
      "|    ent_coef_loss   | -4.2     |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5350     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1096     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1718     |\n",
      "|    total_timesteps | 1096000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1097000, episode_reward=-243.54 +/- 2.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1097000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1098000, episode_reward=-232.16 +/- 2.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1098000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.14    |\n",
      "|    critic_loss     | 1.53e-05 |\n",
      "|    ent_coef        | 0.000364 |\n",
      "|    ent_coef_loss   | -4.72    |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1099000, episode_reward=-233.65 +/- 2.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1099000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=-210.08 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -210     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.14    |\n",
      "|    critic_loss     | 1.41e-05 |\n",
      "|    ent_coef        | 0.00036  |\n",
      "|    ent_coef_loss   | -6.46    |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5370     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1100     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1724     |\n",
      "|    total_timesteps | 1100000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1101000, episode_reward=-211.18 +/- 3.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1101000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1102000, episode_reward=-205.70 +/- 2.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1102000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.13    |\n",
      "|    critic_loss     | 1.79e-05 |\n",
      "|    ent_coef        | 0.000356 |\n",
      "|    ent_coef_loss   | -3.56    |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1103000, episode_reward=-206.49 +/- 2.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1103000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1104000, episode_reward=-229.01 +/- 2.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1104000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.13    |\n",
      "|    critic_loss     | 1.92e-05 |\n",
      "|    ent_coef        | 0.000352 |\n",
      "|    ent_coef_loss   | -5.39    |\n",
      "|    learning_rate   | 0.0039   |\n",
      "|    n_updates       | 5390     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1104     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1730     |\n",
      "|    total_timesteps | 1104000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1105000, episode_reward=-230.66 +/- 1.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1105000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1106000, episode_reward=-229.84 +/- 2.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -230     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1106000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.13    |\n",
      "|    critic_loss     | 2.11e-05 |\n",
      "|    ent_coef        | 0.000349 |\n",
      "|    ent_coef_loss   | -4.25    |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1107000, episode_reward=-229.74 +/- 2.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -230     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1107000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108000, episode_reward=-209.19 +/- 2.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -209     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1108000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.13    |\n",
      "|    critic_loss     | 1.57e-05 |\n",
      "|    ent_coef        | 0.000345 |\n",
      "|    ent_coef_loss   | -6.95    |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5410     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1108     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1737     |\n",
      "|    total_timesteps | 1108000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1109000, episode_reward=-212.53 +/- 2.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -213     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1109000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=-209.30 +/- 2.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -209     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1110000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1111000, episode_reward=-219.28 +/- 1.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1111000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.12    |\n",
      "|    critic_loss     | 2.42e-05 |\n",
      "|    ent_coef        | 0.00034  |\n",
      "|    ent_coef_loss   | -5.12    |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1112000, episode_reward=-220.96 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -221     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1112000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1112     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1743     |\n",
      "|    total_timesteps | 1112000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1113000, episode_reward=-222.05 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -222     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1113000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.12    |\n",
      "|    critic_loss     | 1.54e-05 |\n",
      "|    ent_coef        | 0.000336 |\n",
      "|    ent_coef_loss   | -2.31    |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1114000, episode_reward=-221.89 +/- 3.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -222     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1114000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1115000, episode_reward=-194.91 +/- 2.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -195     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1115000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.12    |\n",
      "|    critic_loss     | 1.94e-05 |\n",
      "|    ent_coef        | 0.000334 |\n",
      "|    ent_coef_loss   | -4.76    |\n",
      "|    learning_rate   | 0.00389  |\n",
      "|    n_updates       | 5440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1116000, episode_reward=-196.46 +/- 2.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -196     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1116000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1116     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1749     |\n",
      "|    total_timesteps | 1116000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1117000, episode_reward=-203.64 +/- 3.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -204     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1117000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.11    |\n",
      "|    critic_loss     | 1.8e-05  |\n",
      "|    ent_coef        | 0.000331 |\n",
      "|    ent_coef_loss   | -2.81    |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1118000, episode_reward=-202.57 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1118000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1119000, episode_reward=-225.95 +/- 3.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -226     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1119000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.11    |\n",
      "|    critic_loss     | 1.62e-05 |\n",
      "|    ent_coef        | 0.000328 |\n",
      "|    ent_coef_loss   | -4.87    |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=-224.63 +/- 2.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1120000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1120     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1756     |\n",
      "|    total_timesteps | 1120000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1121000, episode_reward=-244.32 +/- 2.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1121000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.11    |\n",
      "|    critic_loss     | 1.8e-05  |\n",
      "|    ent_coef        | 0.000325 |\n",
      "|    ent_coef_loss   | -5.21    |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1122000, episode_reward=-240.95 +/- 3.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1122000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1123000, episode_reward=-233.18 +/- 2.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1123000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.11    |\n",
      "|    critic_loss     | 1.5e-05  |\n",
      "|    ent_coef        | 0.000321 |\n",
      "|    ent_coef_loss   | -6.82    |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1124000, episode_reward=-230.83 +/- 2.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1124000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1124     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1762     |\n",
      "|    total_timesteps | 1124000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1125000, episode_reward=-197.96 +/- 2.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -198     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1125000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.12    |\n",
      "|    critic_loss     | 1.76e-05 |\n",
      "|    ent_coef        | 0.000317 |\n",
      "|    ent_coef_loss   | -5.76    |\n",
      "|    learning_rate   | 0.00388  |\n",
      "|    n_updates       | 5490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1126000, episode_reward=-200.09 +/- 2.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1126000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1127000, episode_reward=-202.22 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -202     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1127000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.11    |\n",
      "|    critic_loss     | 2.48e-05 |\n",
      "|    ent_coef        | 0.000313 |\n",
      "|    ent_coef_loss   | -3.88    |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128000, episode_reward=-203.44 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1128000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1128     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1768     |\n",
      "|    total_timesteps | 1128000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1129000, episode_reward=-225.75 +/- 2.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -226     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1129000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.11    |\n",
      "|    critic_loss     | 1.39e-05 |\n",
      "|    ent_coef        | 0.00031  |\n",
      "|    ent_coef_loss   | -4.28    |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=-222.57 +/- 3.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -223     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1130000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1131000, episode_reward=-215.59 +/- 2.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -216     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1131000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.11    |\n",
      "|    critic_loss     | 1.53e-05 |\n",
      "|    ent_coef        | 0.000306 |\n",
      "|    ent_coef_loss   | -6.05    |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1132000, episode_reward=-218.36 +/- 2.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1132000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1132     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1774     |\n",
      "|    total_timesteps | 1132000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1133000, episode_reward=-200.47 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1133000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.11    |\n",
      "|    critic_loss     | 1.51e-05 |\n",
      "|    ent_coef        | 0.000303 |\n",
      "|    ent_coef_loss   | -5.95    |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1134000, episode_reward=-199.16 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -199     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1134000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1135000, episode_reward=-190.43 +/- 2.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1135000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.1     |\n",
      "|    critic_loss     | 1.69e-05 |\n",
      "|    ent_coef        | 0.000299 |\n",
      "|    ent_coef_loss   | -6.43    |\n",
      "|    learning_rate   | 0.00387  |\n",
      "|    n_updates       | 5540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1136000, episode_reward=-188.56 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -189     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1136000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1136     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1781     |\n",
      "|    total_timesteps | 1136000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1137000, episode_reward=-200.14 +/- 1.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1137000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.1     |\n",
      "|    critic_loss     | 2.02e-05 |\n",
      "|    ent_coef        | 0.000295 |\n",
      "|    ent_coef_loss   | -6.43    |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1138000, episode_reward=-200.28 +/- 2.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1138000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1139000, episode_reward=-218.25 +/- 3.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1139000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.1     |\n",
      "|    critic_loss     | 1.78e-05 |\n",
      "|    ent_coef        | 0.00029  |\n",
      "|    ent_coef_loss   | -3.71    |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=-219.38 +/- 3.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1140000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -222     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1140     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1787     |\n",
      "|    total_timesteps | 1140000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1141000, episode_reward=-218.39 +/- 2.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1141000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.1     |\n",
      "|    critic_loss     | 1.12e-05 |\n",
      "|    ent_coef        | 0.000287 |\n",
      "|    ent_coef_loss   | -2.17    |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1142000, episode_reward=-218.34 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1142000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1143000, episode_reward=-204.70 +/- 3.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -205     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1143000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.09    |\n",
      "|    critic_loss     | 1.59e-05 |\n",
      "|    ent_coef        | 0.000285 |\n",
      "|    ent_coef_loss   | -6.7     |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1144000, episode_reward=-205.88 +/- 2.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1144000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -222     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1144     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1793     |\n",
      "|    total_timesteps | 1144000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1145000, episode_reward=-175.46 +/- 2.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -175     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1145000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.09    |\n",
      "|    critic_loss     | 1.35e-05 |\n",
      "|    ent_coef        | 0.000282 |\n",
      "|    ent_coef_loss   | -6.73    |\n",
      "|    learning_rate   | 0.00386  |\n",
      "|    n_updates       | 5590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1146000, episode_reward=-173.44 +/- 2.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -173     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1146000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1147000, episode_reward=-202.72 +/- 2.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1147000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.08    |\n",
      "|    critic_loss     | 1.26e-05 |\n",
      "|    ent_coef        | 0.000278 |\n",
      "|    ent_coef_loss   | -4.45    |\n",
      "|    learning_rate   | 0.00385  |\n",
      "|    n_updates       | 5600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1148000, episode_reward=-202.53 +/- 2.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1148000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -221     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1148     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1800     |\n",
      "|    total_timesteps | 1148000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149000, episode_reward=-230.86 +/- 1.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1149000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.09    |\n",
      "|    critic_loss     | 1.82e-05 |\n",
      "|    ent_coef        | 0.000275 |\n",
      "|    ent_coef_loss   | -3.52    |\n",
      "|    learning_rate   | 0.00385  |\n",
      "|    n_updates       | 5610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=-231.03 +/- 2.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1150000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1151000, episode_reward=-206.85 +/- 3.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -207     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1151000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.09    |\n",
      "|    critic_loss     | 2.59e-05 |\n",
      "|    ent_coef        | 0.000272 |\n",
      "|    ent_coef_loss   | -4.8     |\n",
      "|    learning_rate   | 0.00385  |\n",
      "|    n_updates       | 5620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1152000, episode_reward=-204.99 +/- 2.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -205     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1152000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -221     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1152     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1806     |\n",
      "|    total_timesteps | 1152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1153000, episode_reward=-205.34 +/- 2.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -205     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1153000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1154000, episode_reward=-191.52 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1154000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.09    |\n",
      "|    critic_loss     | 2.42e-05 |\n",
      "|    ent_coef        | 0.000269 |\n",
      "|    ent_coef_loss   | -6.1     |\n",
      "|    learning_rate   | 0.00385  |\n",
      "|    n_updates       | 5630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1155000, episode_reward=-190.41 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1155000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1156000, episode_reward=-224.62 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1156000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.08    |\n",
      "|    critic_loss     | 1.39e-05 |\n",
      "|    ent_coef        | 0.000265 |\n",
      "|    ent_coef_loss   | -6.28    |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5640     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -219     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1156     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1812     |\n",
      "|    total_timesteps | 1156000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1157000, episode_reward=-225.65 +/- 1.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -226     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1157000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1158000, episode_reward=-227.90 +/- 1.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1158000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.09    |\n",
      "|    critic_loss     | 1.23e-05 |\n",
      "|    ent_coef        | 0.000262 |\n",
      "|    ent_coef_loss   | -4.75    |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1159000, episode_reward=-230.02 +/- 2.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -230     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1159000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=-198.18 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -198     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.09    |\n",
      "|    critic_loss     | 2.16e-05 |\n",
      "|    ent_coef        | 0.000259 |\n",
      "|    ent_coef_loss   | -6.03    |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5660     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -219     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1160     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1818     |\n",
      "|    total_timesteps | 1160000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1161000, episode_reward=-197.98 +/- 2.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -198     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1161000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1162000, episode_reward=-181.41 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -181     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1162000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.08    |\n",
      "|    critic_loss     | 1.36e-05 |\n",
      "|    ent_coef        | 0.000255 |\n",
      "|    ent_coef_loss   | -3.67    |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1163000, episode_reward=-179.90 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -180     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1163000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1164000, episode_reward=-194.46 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1164000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 1.08e-05 |\n",
      "|    ent_coef        | 0.000252 |\n",
      "|    ent_coef_loss   | -3.11    |\n",
      "|    learning_rate   | 0.00384  |\n",
      "|    n_updates       | 5680     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -217     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1164     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1825     |\n",
      "|    total_timesteps | 1164000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1165000, episode_reward=-195.89 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -196     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1165000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1166000, episode_reward=-202.34 +/- 2.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -202     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1166000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.08    |\n",
      "|    critic_loss     | 9.69e-06 |\n",
      "|    ent_coef        | 0.00025  |\n",
      "|    ent_coef_loss   | -6.02    |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1167000, episode_reward=-201.24 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -201     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1167000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1168000, episode_reward=-204.23 +/- 2.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -204     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1168000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.08    |\n",
      "|    critic_loss     | 7.67e-06 |\n",
      "|    ent_coef        | 0.000247 |\n",
      "|    ent_coef_loss   | -5.95    |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5700     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1168     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1831     |\n",
      "|    total_timesteps | 1168000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1169000, episode_reward=-202.15 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -202     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1169000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=-181.18 +/- 2.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -181     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.08    |\n",
      "|    critic_loss     | 1.3e-05  |\n",
      "|    ent_coef        | 0.000244 |\n",
      "|    ent_coef_loss   | -3.56    |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1171000, episode_reward=-182.73 +/- 2.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1171000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1172000, episode_reward=-173.48 +/- 2.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -173     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1172000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 1.13e-05 |\n",
      "|    ent_coef        | 0.000242 |\n",
      "|    ent_coef_loss   | -3.18    |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5720     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1172     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1837     |\n",
      "|    total_timesteps | 1172000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1173000, episode_reward=-172.21 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -172     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1173000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1174000, episode_reward=-170.32 +/- 2.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -170     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1174000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 1.13e-05 |\n",
      "|    ent_coef        | 0.00024  |\n",
      "|    ent_coef_loss   | -0.481   |\n",
      "|    learning_rate   | 0.00383  |\n",
      "|    n_updates       | 5730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1175000, episode_reward=-169.25 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -169     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1175000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1176000, episode_reward=-191.24 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1176000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 1.11e-05 |\n",
      "|    ent_coef        | 0.000239 |\n",
      "|    ent_coef_loss   | -3.17    |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5740     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1176     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1843     |\n",
      "|    total_timesteps | 1176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1177000, episode_reward=-191.99 +/- 2.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1177000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1178000, episode_reward=-191.24 +/- 2.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1178000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 1.26e-05 |\n",
      "|    ent_coef        | 0.000237 |\n",
      "|    ent_coef_loss   | -1.88    |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1179000, episode_reward=-189.78 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1179000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=-206.06 +/- 2.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 1.63e-05 |\n",
      "|    ent_coef        | 0.000236 |\n",
      "|    ent_coef_loss   | -2.5     |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5760     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1180     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1850     |\n",
      "|    total_timesteps | 1180000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1181000, episode_reward=-206.79 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -207     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1181000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1182000, episode_reward=-187.30 +/- 1.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1182000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 1.84e-05 |\n",
      "|    ent_coef        | 0.000235 |\n",
      "|    ent_coef_loss   | -4.69    |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1183000, episode_reward=-186.94 +/- 1.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1183000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1184000, episode_reward=-155.59 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1184000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 1.21e-05 |\n",
      "|    ent_coef        | 0.000232 |\n",
      "|    ent_coef_loss   | -5.45    |\n",
      "|    learning_rate   | 0.00382  |\n",
      "|    n_updates       | 5780     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1184     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1856     |\n",
      "|    total_timesteps | 1184000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1185000, episode_reward=-157.18 +/- 2.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1185000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1186000, episode_reward=-188.70 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -189     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1186000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.05    |\n",
      "|    critic_loss     | 1.62e-05 |\n",
      "|    ent_coef        | 0.00023  |\n",
      "|    ent_coef_loss   | 1.92     |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1187000, episode_reward=-187.07 +/- 1.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1187000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1188000, episode_reward=-207.75 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -208     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1188000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 2.79e-05 |\n",
      "|    ent_coef        | 0.00023  |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -207     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1188     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1862     |\n",
      "|    total_timesteps | 1188000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189000, episode_reward=-209.13 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -209     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1189000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=-207.42 +/- 2.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -207     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 2.01e-05 |\n",
      "|    ent_coef        | 0.00023  |\n",
      "|    ent_coef_loss   | -3.1     |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1191000, episode_reward=-207.05 +/- 1.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -207     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1191000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1192000, episode_reward=-184.43 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -184     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1192000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 1.27e-05 |\n",
      "|    ent_coef        | 0.000229 |\n",
      "|    ent_coef_loss   | -0.84    |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5820     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -206     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1192     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1869     |\n",
      "|    total_timesteps | 1192000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1193000, episode_reward=-184.08 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -184     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1193000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1194000, episode_reward=-161.51 +/- 1.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1194000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 2.1e-05  |\n",
      "|    ent_coef        | 0.000228 |\n",
      "|    ent_coef_loss   | -7.05    |\n",
      "|    learning_rate   | 0.00381  |\n",
      "|    n_updates       | 5830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1195000, episode_reward=-161.70 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1195000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1196000, episode_reward=-162.04 +/- 1.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1196000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -205     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1196     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1875     |\n",
      "|    total_timesteps | 1196000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1197000, episode_reward=-148.51 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -149     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1197000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 1.64e-05 |\n",
      "|    ent_coef        | 0.000225 |\n",
      "|    ent_coef_loss   | -4.81    |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1198000, episode_reward=-146.18 +/- 3.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1198000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1199000, episode_reward=-190.15 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1199000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.05    |\n",
      "|    critic_loss     | 1.48e-05 |\n",
      "|    ent_coef        | 0.000223 |\n",
      "|    ent_coef_loss   | -0.0698  |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=-189.90 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -202     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1200     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1881     |\n",
      "|    total_timesteps | 1200000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1201000, episode_reward=-205.67 +/- 2.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1201000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 1.61e-05 |\n",
      "|    ent_coef        | 0.000222 |\n",
      "|    ent_coef_loss   | 2.33     |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1202000, episode_reward=-205.61 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1202000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1203000, episode_reward=-187.42 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1203000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 2.33e-05 |\n",
      "|    ent_coef        | 0.000222 |\n",
      "|    ent_coef_loss   | -3.28    |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1204000, episode_reward=-189.08 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -189     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1204000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -201     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1204     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1887     |\n",
      "|    total_timesteps | 1204000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1205000, episode_reward=-157.57 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -158     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1205000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 8.49e-06 |\n",
      "|    ent_coef        | 0.000221 |\n",
      "|    ent_coef_loss   | -2.71    |\n",
      "|    learning_rate   | 0.0038   |\n",
      "|    n_updates       | 5880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1206000, episode_reward=-156.45 +/- 1.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1206000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1207000, episode_reward=-159.00 +/- 1.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1207000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.05    |\n",
      "|    critic_loss     | 1.96e-05 |\n",
      "|    ent_coef        | 0.00022  |\n",
      "|    ent_coef_loss   | -4.17    |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1208000, episode_reward=-158.69 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1208000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1208     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1894     |\n",
      "|    total_timesteps | 1208000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209000, episode_reward=-166.50 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -167     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1209000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.05    |\n",
      "|    critic_loss     | 1.35e-05 |\n",
      "|    ent_coef        | 0.000218 |\n",
      "|    ent_coef_loss   | -2.81    |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=-165.67 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -166     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1210000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1211000, episode_reward=-155.11 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1211000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.05    |\n",
      "|    critic_loss     | 1.41e-05 |\n",
      "|    ent_coef        | 0.000216 |\n",
      "|    ent_coef_loss   | -4.82    |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1212000, episode_reward=-157.08 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1212000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -196     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1212     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1900     |\n",
      "|    total_timesteps | 1212000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1213000, episode_reward=-147.23 +/- 2.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1213000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.05    |\n",
      "|    critic_loss     | 1.36e-05 |\n",
      "|    ent_coef        | 0.000214 |\n",
      "|    ent_coef_loss   | -4.69    |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1214000, episode_reward=-148.13 +/- 2.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1214000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1215000, episode_reward=-151.09 +/- 1.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1215000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.04    |\n",
      "|    critic_loss     | 2.69e-05 |\n",
      "|    ent_coef        | 0.000211 |\n",
      "|    ent_coef_loss   | -2.03    |\n",
      "|    learning_rate   | 0.00379  |\n",
      "|    n_updates       | 5930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1216000, episode_reward=-151.73 +/- 2.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1216000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -194     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1216     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1906     |\n",
      "|    total_timesteps | 1216000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1217000, episode_reward=-152.78 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1217000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.04    |\n",
      "|    critic_loss     | 1.57e-05 |\n",
      "|    ent_coef        | 0.00021  |\n",
      "|    ent_coef_loss   | -4.37    |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1218000, episode_reward=-151.89 +/- 2.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1218000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1219000, episode_reward=-159.05 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1219000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.04    |\n",
      "|    critic_loss     | 1.51e-05 |\n",
      "|    ent_coef        | 0.000208 |\n",
      "|    ent_coef_loss   | -3.36    |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=-157.82 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -158     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1220000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -191     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1220     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1912     |\n",
      "|    total_timesteps | 1220000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1221000, episode_reward=-151.23 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1221000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.04    |\n",
      "|    critic_loss     | 1.31e-05 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | -0.00023 |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1222000, episode_reward=-149.37 +/- 2.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -149     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1222000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1223000, episode_reward=-173.28 +/- 2.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -173     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1223000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 2.36e-05 |\n",
      "|    ent_coef        | 0.000205 |\n",
      "|    ent_coef_loss   | -4.29    |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1224000, episode_reward=-173.08 +/- 3.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -173     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1224000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1224     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1919     |\n",
      "|    total_timesteps | 1224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1225000, episode_reward=-162.21 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1225000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.04    |\n",
      "|    critic_loss     | 2.45e-05 |\n",
      "|    ent_coef        | 0.000203 |\n",
      "|    ent_coef_loss   | -6.83    |\n",
      "|    learning_rate   | 0.00378  |\n",
      "|    n_updates       | 5980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1226000, episode_reward=-162.58 +/- 2.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1226000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1227000, episode_reward=-145.33 +/- 1.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1227000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.04    |\n",
      "|    critic_loss     | 2.76e-05 |\n",
      "|    ent_coef        | 0.000201 |\n",
      "|    ent_coef_loss   | -4.03    |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 5990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1228000, episode_reward=-148.77 +/- 2.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -149     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1228000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1228     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1925     |\n",
      "|    total_timesteps | 1228000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229000, episode_reward=-143.12 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1229000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 2.53e-05 |\n",
      "|    ent_coef        | 0.000198 |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=-145.35 +/- 3.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1230000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1231000, episode_reward=-136.89 +/- 1.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1231000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 4.76e-05 |\n",
      "|    ent_coef        | 0.000197 |\n",
      "|    ent_coef_loss   | -0.566   |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 6010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1232000, episode_reward=-138.22 +/- 2.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1232000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -183     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1232     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1931     |\n",
      "|    total_timesteps | 1232000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1233000, episode_reward=-143.27 +/- 1.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1233000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.04    |\n",
      "|    critic_loss     | 3.41e-05 |\n",
      "|    ent_coef        | 0.000197 |\n",
      "|    ent_coef_loss   | -0.225   |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 6020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1234000, episode_reward=-141.94 +/- 2.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1234000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1235000, episode_reward=-121.79 +/- 5.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1235000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.04    |\n",
      "|    critic_loss     | 2.66e-05 |\n",
      "|    ent_coef        | 0.000196 |\n",
      "|    ent_coef_loss   | -1.7     |\n",
      "|    learning_rate   | 0.00377  |\n",
      "|    n_updates       | 6030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1236000, episode_reward=-120.91 +/- 2.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1236000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -181     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1236     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1937     |\n",
      "|    total_timesteps | 1236000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1237000, episode_reward=-139.23 +/- 2.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1237000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.02    |\n",
      "|    critic_loss     | 5.01e-05 |\n",
      "|    ent_coef        | 0.000195 |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.00376  |\n",
      "|    n_updates       | 6040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1238000, episode_reward=-137.79 +/- 3.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1238000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1239000, episode_reward=-140.19 +/- 6.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1239000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=-196.59 +/- 5.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 6.42e-05 |\n",
      "|    ent_coef        | 0.000196 |\n",
      "|    ent_coef_loss   | 4.08     |\n",
      "|    learning_rate   | 0.00376  |\n",
      "|    n_updates       | 6050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -179     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1240     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1944     |\n",
      "|    total_timesteps | 1240000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1241000, episode_reward=-194.85 +/- 7.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -195     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1241000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1242000, episode_reward=-198.74 +/- 5.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -199     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1242000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 0.000173 |\n",
      "|    ent_coef        | 0.000198 |\n",
      "|    ent_coef_loss   | 4.79     |\n",
      "|    learning_rate   | 0.00376  |\n",
      "|    n_updates       | 6060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1243000, episode_reward=-198.03 +/- 4.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -198     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1243000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1244000, episode_reward=-166.92 +/- 4.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -167     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1244000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 0.000115 |\n",
      "|    ent_coef        | 0.000199 |\n",
      "|    ent_coef_loss   | -6.55    |\n",
      "|    learning_rate   | 0.00376  |\n",
      "|    n_updates       | 6070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -178     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1244     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1950     |\n",
      "|    total_timesteps | 1244000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1245000, episode_reward=-171.29 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -171     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1245000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1246000, episode_reward=-182.06 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -182     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1246000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 5.37e-05 |\n",
      "|    ent_coef        | 0.000198 |\n",
      "|    ent_coef_loss   | 6.01     |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1247000, episode_reward=-181.82 +/- 2.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -182     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1247000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1248000, episode_reward=-145.77 +/- 2.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1248000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 5.17e-05 |\n",
      "|    ent_coef        | 0.0002   |\n",
      "|    ent_coef_loss   | 3.59     |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6090     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -177     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1248     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1956     |\n",
      "|    total_timesteps | 1248000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1249000, episode_reward=-147.05 +/- 3.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1249000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=-156.63 +/- 3.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 4.21e-05 |\n",
      "|    ent_coef        | 0.000201 |\n",
      "|    ent_coef_loss   | 4.63     |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1251000, episode_reward=-196.33 +/- 51.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -196     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1251000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1252000, episode_reward=-432.42 +/- 2.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -432     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1252000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.02    |\n",
      "|    critic_loss     | 8.28e-05 |\n",
      "|    ent_coef        | 0.000204 |\n",
      "|    ent_coef_loss   | 4.18     |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6110     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1252     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1963     |\n",
      "|    total_timesteps | 1252000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1253000, episode_reward=-431.47 +/- 2.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -431     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1253000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1254000, episode_reward=-583.33 +/- 3.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -583     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1254000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.94    |\n",
      "|    critic_loss     | 0.0642   |\n",
      "|    ent_coef        | 0.000212 |\n",
      "|    ent_coef_loss   | 104      |\n",
      "|    learning_rate   | 0.00375  |\n",
      "|    n_updates       | 6120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1255000, episode_reward=-585.62 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -586     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1255000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1256000, episode_reward=-336.23 +/- 3.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1256000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 0.0293   |\n",
      "|    ent_coef        | 0.000246 |\n",
      "|    ent_coef_loss   | 130      |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6130     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1256     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1969     |\n",
      "|    total_timesteps | 1256000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1257000, episode_reward=-335.88 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1257000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1258000, episode_reward=-586.16 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -586     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1258000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.89    |\n",
      "|    critic_loss     | 0.0548   |\n",
      "|    ent_coef        | 0.000297 |\n",
      "|    ent_coef_loss   | 263      |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1259000, episode_reward=-585.74 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -586     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1259000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=-589.82 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -590     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.78    |\n",
      "|    critic_loss     | 0.0518   |\n",
      "|    ent_coef        | 0.000374 |\n",
      "|    ent_coef_loss   | 222      |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6150     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -197     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1260     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1975     |\n",
      "|    total_timesteps | 1260000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1261000, episode_reward=-590.01 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -590     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1261000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1262000, episode_reward=-562.53 +/- 12.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -563     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1262000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 0.016    |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | 291      |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1263000, episode_reward=-567.75 +/- 8.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -568     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1263000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1264000, episode_reward=-583.20 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -583     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1264000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.89    |\n",
      "|    critic_loss     | 0.0094   |\n",
      "|    ent_coef        | 0.000544 |\n",
      "|    ent_coef_loss   | 153      |\n",
      "|    learning_rate   | 0.00374  |\n",
      "|    n_updates       | 6170     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1264     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1981     |\n",
      "|    total_timesteps | 1264000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1265000, episode_reward=-583.78 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -584     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1265000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1266000, episode_reward=-580.55 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -581     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1266000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 0.00228  |\n",
      "|    ent_coef        | 0.00062  |\n",
      "|    ent_coef_loss   | 110      |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1267000, episode_reward=-579.84 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -580     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1267000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1268000, episode_reward=-577.46 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -577     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1268000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.89    |\n",
      "|    critic_loss     | 0.00101  |\n",
      "|    ent_coef        | 0.000684 |\n",
      "|    ent_coef_loss   | 107      |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6190     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1268     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1987     |\n",
      "|    total_timesteps | 1268000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1269000, episode_reward=-576.98 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -577     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1269000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=-576.83 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -577     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.89    |\n",
      "|    critic_loss     | 0.000386 |\n",
      "|    ent_coef        | 0.000742 |\n",
      "|    ent_coef_loss   | 94.1     |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1271000, episode_reward=-576.59 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -577     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1271000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1272000, episode_reward=-574.51 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -575     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1272000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.87    |\n",
      "|    critic_loss     | 0.000202 |\n",
      "|    ent_coef        | 0.000797 |\n",
      "|    ent_coef_loss   | 90.8     |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6210     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -242     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1272     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 1994     |\n",
      "|    total_timesteps | 1272000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1273000, episode_reward=-575.12 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -575     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1273000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1274000, episode_reward=-579.82 +/- 2.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -580     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1274000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 0.000149 |\n",
      "|    ent_coef        | 0.000852 |\n",
      "|    ent_coef_loss   | 87.1     |\n",
      "|    learning_rate   | 0.00373  |\n",
      "|    n_updates       | 6220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1275000, episode_reward=-578.68 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -579     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1275000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1276000, episode_reward=-569.79 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -570     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1276000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.85    |\n",
      "|    critic_loss     | 0.000129 |\n",
      "|    ent_coef        | 0.000907 |\n",
      "|    ent_coef_loss   | 83.8     |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6230     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -257     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1276     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 2000     |\n",
      "|    total_timesteps | 1276000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1277000, episode_reward=-569.00 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -569     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1277000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1278000, episode_reward=-572.98 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -573     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1278000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.85    |\n",
      "|    critic_loss     | 7.15e-05 |\n",
      "|    ent_coef        | 0.000964 |\n",
      "|    ent_coef_loss   | 80.2     |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1279000, episode_reward=-573.27 +/- 1.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -573     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1279000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=-574.27 +/- 1.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -574     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1280000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -272     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1280     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 2006     |\n",
      "|    total_timesteps | 1280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1281000, episode_reward=-569.08 +/- 1.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -569     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1281000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.84    |\n",
      "|    critic_loss     | 8.55e-05 |\n",
      "|    ent_coef        | 0.00102  |\n",
      "|    ent_coef_loss   | 75.5     |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1282000, episode_reward=-569.90 +/- 1.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -570     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1282000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1283000, episode_reward=-571.40 +/- 2.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -571     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1283000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.85    |\n",
      "|    critic_loss     | 0.000104 |\n",
      "|    ent_coef        | 0.00108  |\n",
      "|    ent_coef_loss   | 72       |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1284000, episode_reward=-568.91 +/- 2.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -569     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1284000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -287     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1284     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 2012     |\n",
      "|    total_timesteps | 1284000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1285000, episode_reward=-565.32 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -565     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1285000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.82    |\n",
      "|    critic_loss     | 9.98e-05 |\n",
      "|    ent_coef        | 0.00114  |\n",
      "|    ent_coef_loss   | 69.8     |\n",
      "|    learning_rate   | 0.00372  |\n",
      "|    n_updates       | 6270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1286000, episode_reward=-568.82 +/- 2.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -569     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1286000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1287000, episode_reward=-564.23 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -564     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1287000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.8     |\n",
      "|    critic_loss     | 8.68e-05 |\n",
      "|    ent_coef        | 0.0012   |\n",
      "|    ent_coef_loss   | 67.4     |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1288000, episode_reward=-564.33 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -564     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1288000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -302     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1288     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 2018     |\n",
      "|    total_timesteps | 1288000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1289000, episode_reward=-560.65 +/- 4.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -561     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1289000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.82    |\n",
      "|    critic_loss     | 6.43e-05 |\n",
      "|    ent_coef        | 0.00126  |\n",
      "|    ent_coef_loss   | 65.1     |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=-563.05 +/- 4.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -563     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1290000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1291000, episode_reward=-564.61 +/- 4.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -565     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1291000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.76    |\n",
      "|    critic_loss     | 8.97e-05 |\n",
      "|    ent_coef        | 0.00132  |\n",
      "|    ent_coef_loss   | 60.6     |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1292000, episode_reward=-560.32 +/- 3.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1292000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -315     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1292     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 2025     |\n",
      "|    total_timesteps | 1292000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1293000, episode_reward=-558.45 +/- 5.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -558     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1293000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.8     |\n",
      "|    critic_loss     | 9.41e-05 |\n",
      "|    ent_coef        | 0.00138  |\n",
      "|    ent_coef_loss   | 59.4     |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1294000, episode_reward=-560.79 +/- 5.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -561     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1294000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1295000, episode_reward=-552.98 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -553     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1295000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.74    |\n",
      "|    critic_loss     | 0.000103 |\n",
      "|    ent_coef        | 0.00144  |\n",
      "|    ent_coef_loss   | 55       |\n",
      "|    learning_rate   | 0.00371  |\n",
      "|    n_updates       | 6320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1296000, episode_reward=-557.20 +/- 5.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -557     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1296000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -329     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1296     |\n",
      "|    fps             | 637      |\n",
      "|    time_elapsed    | 2031     |\n",
      "|    total_timesteps | 1296000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1297000, episode_reward=-553.91 +/- 4.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -554     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1297000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.77    |\n",
      "|    critic_loss     | 9.1e-05  |\n",
      "|    ent_coef        | 0.0015   |\n",
      "|    ent_coef_loss   | 54.8     |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1298000, episode_reward=-556.58 +/- 3.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -557     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1298000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1299000, episode_reward=-566.96 +/- 5.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -567     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1299000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.78    |\n",
      "|    critic_loss     | 0.000124 |\n",
      "|    ent_coef        | 0.00156  |\n",
      "|    ent_coef_loss   | 53.4     |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=-567.50 +/- 5.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -567     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -344     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1300     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2037     |\n",
      "|    total_timesteps | 1300000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1301000, episode_reward=-559.56 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1301000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.77    |\n",
      "|    critic_loss     | 8.08e-05 |\n",
      "|    ent_coef        | 0.00163  |\n",
      "|    ent_coef_loss   | 53.1     |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1302000, episode_reward=-559.65 +/- 0.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1302000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1303000, episode_reward=-559.98 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1303000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.75    |\n",
      "|    critic_loss     | 0.00011  |\n",
      "|    ent_coef        | 0.00169  |\n",
      "|    ent_coef_loss   | 49.4     |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1304000, episode_reward=-559.49 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -559     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1304000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -358     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1304     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2043     |\n",
      "|    total_timesteps | 1304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1305000, episode_reward=-557.68 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -558     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1305000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.75    |\n",
      "|    critic_loss     | 6.7e-05  |\n",
      "|    ent_coef        | 0.00175  |\n",
      "|    ent_coef_loss   | 48.5     |\n",
      "|    learning_rate   | 0.0037   |\n",
      "|    n_updates       | 6370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1306000, episode_reward=-557.24 +/- 1.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -557     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1306000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1307000, episode_reward=-556.18 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -556     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1307000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.74    |\n",
      "|    critic_loss     | 9.39e-05 |\n",
      "|    ent_coef        | 0.00182  |\n",
      "|    ent_coef_loss   | 47.2     |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1308000, episode_reward=-557.13 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -557     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1308000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -373     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1308     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2049     |\n",
      "|    total_timesteps | 1308000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1309000, episode_reward=-562.60 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -563     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1309000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.73    |\n",
      "|    critic_loss     | 0.000127 |\n",
      "|    ent_coef        | 0.00189  |\n",
      "|    ent_coef_loss   | 45.4     |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=-561.58 +/- 1.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -562     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1310000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1311000, episode_reward=-555.74 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -556     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1311000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.73    |\n",
      "|    critic_loss     | 8.24e-05 |\n",
      "|    ent_coef        | 0.00195  |\n",
      "|    ent_coef_loss   | 45       |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1312000, episode_reward=-555.21 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -555     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1312000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -388     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1312     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2056     |\n",
      "|    total_timesteps | 1312000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1313000, episode_reward=-561.33 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -561     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1313000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.72    |\n",
      "|    critic_loss     | 8.68e-05 |\n",
      "|    ent_coef        | 0.00202  |\n",
      "|    ent_coef_loss   | 42.8     |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1314000, episode_reward=-560.43 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -560     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1314000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1315000, episode_reward=-566.04 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -566     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1315000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.7     |\n",
      "|    critic_loss     | 8.3e-05  |\n",
      "|    ent_coef        | 0.00209  |\n",
      "|    ent_coef_loss   | 42.5     |\n",
      "|    learning_rate   | 0.00369  |\n",
      "|    n_updates       | 6420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1316000, episode_reward=-564.79 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -565     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1316000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -403     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1316     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2062     |\n",
      "|    total_timesteps | 1316000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1317000, episode_reward=-554.05 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -554     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1317000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.72    |\n",
      "|    critic_loss     | 6.15e-05 |\n",
      "|    ent_coef        | 0.00216  |\n",
      "|    ent_coef_loss   | 41.4     |\n",
      "|    learning_rate   | 0.00368  |\n",
      "|    n_updates       | 6430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1318000, episode_reward=-554.31 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -554     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1318000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1319000, episode_reward=-560.62 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -561     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1319000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.67    |\n",
      "|    critic_loss     | 8.12e-05 |\n",
      "|    ent_coef        | 0.00223  |\n",
      "|    ent_coef_loss   | 37.9     |\n",
      "|    learning_rate   | 0.00368  |\n",
      "|    n_updates       | 6440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=-561.43 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -561     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1320000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -419     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1320     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2068     |\n",
      "|    total_timesteps | 1320000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1321000, episode_reward=-551.67 +/- 2.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -552     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1321000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.68    |\n",
      "|    critic_loss     | 6.5e-05  |\n",
      "|    ent_coef        | 0.0023   |\n",
      "|    ent_coef_loss   | 38.8     |\n",
      "|    learning_rate   | 0.00368  |\n",
      "|    n_updates       | 6450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1322000, episode_reward=-551.79 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -552     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1322000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1323000, episode_reward=-552.24 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -552     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1323000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1324000, episode_reward=-552.48 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -552     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1324000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.65    |\n",
      "|    critic_loss     | 0.000101 |\n",
      "|    ent_coef        | 0.00237  |\n",
      "|    ent_coef_loss   | 33.9     |\n",
      "|    learning_rate   | 0.00368  |\n",
      "|    n_updates       | 6460     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -433     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1324     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2074     |\n",
      "|    total_timesteps | 1324000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1325000, episode_reward=-551.35 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -551     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1325000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1326000, episode_reward=-550.13 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -550     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1326000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.65    |\n",
      "|    critic_loss     | 7.03e-05 |\n",
      "|    ent_coef        | 0.00244  |\n",
      "|    ent_coef_loss   | 34.5     |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1327000, episode_reward=-548.83 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -549     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1327000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1328000, episode_reward=-549.85 +/- 1.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -550     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1328000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.65    |\n",
      "|    critic_loss     | 7.46e-05 |\n",
      "|    ent_coef        | 0.00251  |\n",
      "|    ent_coef_loss   | 31.2     |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -447     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1328     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2080     |\n",
      "|    total_timesteps | 1328000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1329000, episode_reward=-548.88 +/- 2.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -549     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1329000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=-548.34 +/- 1.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -548     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.64    |\n",
      "|    critic_loss     | 6.65e-05 |\n",
      "|    ent_coef        | 0.00257  |\n",
      "|    ent_coef_loss   | 30.1     |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1331000, episode_reward=-548.00 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -548     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1331000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1332000, episode_reward=-541.23 +/- 2.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -541     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1332000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.65    |\n",
      "|    critic_loss     | 8.14e-05 |\n",
      "|    ent_coef        | 0.00264  |\n",
      "|    ent_coef_loss   | 30.6     |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -462     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1332     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2086     |\n",
      "|    total_timesteps | 1332000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1333000, episode_reward=-540.95 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -541     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1333000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1334000, episode_reward=-541.40 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -541     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1334000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.62    |\n",
      "|    critic_loss     | 8.16e-05 |\n",
      "|    ent_coef        | 0.0027   |\n",
      "|    ent_coef_loss   | 25.8     |\n",
      "|    learning_rate   | 0.00367  |\n",
      "|    n_updates       | 6510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1335000, episode_reward=-539.89 +/- 3.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -540     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1335000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1336000, episode_reward=-547.21 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -547     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1336000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.62    |\n",
      "|    critic_loss     | 7.36e-05 |\n",
      "|    ent_coef        | 0.00277  |\n",
      "|    ent_coef_loss   | 27.1     |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6520     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -477     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1336     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2093     |\n",
      "|    total_timesteps | 1336000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1337000, episode_reward=-547.47 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -547     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1337000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1338000, episode_reward=-536.68 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -537     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1338000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.62    |\n",
      "|    critic_loss     | 8.42e-05 |\n",
      "|    ent_coef        | 0.00283  |\n",
      "|    ent_coef_loss   | 24.4     |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1339000, episode_reward=-535.03 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -535     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1339000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=-537.67 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -538     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.61    |\n",
      "|    critic_loss     | 8.88e-05 |\n",
      "|    ent_coef        | 0.00289  |\n",
      "|    ent_coef_loss   | 22.3     |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6540     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -490     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1340     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2099     |\n",
      "|    total_timesteps | 1340000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1341000, episode_reward=-538.46 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -538     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1341000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1342000, episode_reward=-536.19 +/- 0.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -536     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1342000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.6     |\n",
      "|    critic_loss     | 8.64e-05 |\n",
      "|    ent_coef        | 0.00294  |\n",
      "|    ent_coef_loss   | 20.4     |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1343000, episode_reward=-531.88 +/- 8.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -532     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1343000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1344000, episode_reward=-545.99 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -546     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1344000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.6     |\n",
      "|    critic_loss     | 8.47e-05 |\n",
      "|    ent_coef        | 0.003    |\n",
      "|    ent_coef_loss   | 18.8     |\n",
      "|    learning_rate   | 0.00366  |\n",
      "|    n_updates       | 6560     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -502     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1344     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2105     |\n",
      "|    total_timesteps | 1344000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1345000, episode_reward=-542.05 +/- 8.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -542     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1345000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1346000, episode_reward=-532.21 +/- 1.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -532     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1346000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.6     |\n",
      "|    critic_loss     | 8.09e-05 |\n",
      "|    ent_coef        | 0.00305  |\n",
      "|    ent_coef_loss   | 18.8     |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1347000, episode_reward=-532.17 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -532     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1347000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1348000, episode_reward=-529.97 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -530     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1348000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.59    |\n",
      "|    critic_loss     | 9.47e-05 |\n",
      "|    ent_coef        | 0.0031   |\n",
      "|    ent_coef_loss   | 16.3     |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6580     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -515     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1348     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2111     |\n",
      "|    total_timesteps | 1348000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1349000, episode_reward=-526.36 +/- 9.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -526     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1349000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=-522.79 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -523     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.59    |\n",
      "|    critic_loss     | 0.000115 |\n",
      "|    ent_coef        | 0.00314  |\n",
      "|    ent_coef_loss   | 14.6     |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1351000, episode_reward=-511.81 +/- 11.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -512     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1351000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1352000, episode_reward=-504.01 +/- 10.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -504     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1352000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.58    |\n",
      "|    critic_loss     | 0.000173 |\n",
      "|    ent_coef        | 0.00318  |\n",
      "|    ent_coef_loss   | 10.6     |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6600     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -527     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1352     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2117     |\n",
      "|    total_timesteps | 1352000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1353000, episode_reward=-513.30 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -513     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1353000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1354000, episode_reward=-509.51 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -510     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1354000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.58    |\n",
      "|    critic_loss     | 0.000154 |\n",
      "|    ent_coef        | 0.00322  |\n",
      "|    ent_coef_loss   | 7.63     |\n",
      "|    learning_rate   | 0.00365  |\n",
      "|    n_updates       | 6610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1355000, episode_reward=-509.34 +/- 10.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -509     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1355000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1356000, episode_reward=-503.67 +/- 11.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -504     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1356000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.57    |\n",
      "|    critic_loss     | 0.000163 |\n",
      "|    ent_coef        | 0.00324  |\n",
      "|    ent_coef_loss   | 5.59     |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6620     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -525     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1356     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2124     |\n",
      "|    total_timesteps | 1356000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1357000, episode_reward=-509.40 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -509     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1357000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1358000, episode_reward=-492.24 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -492     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1358000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.57    |\n",
      "|    critic_loss     | 0.000214 |\n",
      "|    ent_coef        | 0.00326  |\n",
      "|    ent_coef_loss   | 3.74     |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1359000, episode_reward=-492.15 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -492     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1359000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=-454.80 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -455     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 0.00148  |\n",
      "|    ent_coef        | 0.00328  |\n",
      "|    ent_coef_loss   | 1.31     |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6640     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -520     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1360     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2130     |\n",
      "|    total_timesteps | 1360000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1361000, episode_reward=-455.18 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -455     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1361000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1362000, episode_reward=-458.78 +/- 13.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -459     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1362000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.57    |\n",
      "|    critic_loss     | 0.00101  |\n",
      "|    ent_coef        | 0.00328  |\n",
      "|    ent_coef_loss   | -0.956   |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1363000, episode_reward=-451.95 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -452     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1363000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1364000, episode_reward=-468.25 +/- 0.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -468     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1364000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.57    |\n",
      "|    critic_loss     | 0.000386 |\n",
      "|    ent_coef        | 0.00328  |\n",
      "|    ent_coef_loss   | 0.681    |\n",
      "|    learning_rate   | 0.00364  |\n",
      "|    n_updates       | 6660     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -513     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1364     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2136     |\n",
      "|    total_timesteps | 1364000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1365000, episode_reward=-473.88 +/- 9.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -474     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1365000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1366000, episode_reward=-472.12 +/- 9.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -472     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1366000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1367000, episode_reward=-438.15 +/- 13.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -438     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1367000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.55    |\n",
      "|    critic_loss     | 0.000281 |\n",
      "|    ent_coef        | 0.00329  |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1368000, episode_reward=-427.07 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -427     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1368000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -505     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1368     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2142     |\n",
      "|    total_timesteps | 1368000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1369000, episode_reward=-397.61 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -398     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1369000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.55    |\n",
      "|    critic_loss     | 0.000386 |\n",
      "|    ent_coef        | 0.00328  |\n",
      "|    ent_coef_loss   | -6.46    |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=-398.16 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -398     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1370000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1371000, episode_reward=-352.58 +/- 0.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1371000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.55    |\n",
      "|    critic_loss     | 0.000507 |\n",
      "|    ent_coef        | 0.00327  |\n",
      "|    ent_coef_loss   | -9.66    |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1372000, episode_reward=-352.28 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -352     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1372000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -496     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1372     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2148     |\n",
      "|    total_timesteps | 1372000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1373000, episode_reward=-332.18 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -332     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1373000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.55    |\n",
      "|    critic_loss     | 0.000716 |\n",
      "|    ent_coef        | 0.00324  |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1374000, episode_reward=-332.63 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -333     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1374000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1375000, episode_reward=-291.06 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -291     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1375000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.54    |\n",
      "|    critic_loss     | 0.000815 |\n",
      "|    ent_coef        | 0.00321  |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00363  |\n",
      "|    n_updates       | 6710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1376000, episode_reward=-292.19 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -292     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1376000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -487     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1376     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2155     |\n",
      "|    total_timesteps | 1376000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1377000, episode_reward=-488.06 +/- 20.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -488     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1377000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.54    |\n",
      "|    critic_loss     | 0.00158  |\n",
      "|    ent_coef        | 0.00318  |\n",
      "|    ent_coef_loss   | -8.6     |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1378000, episode_reward=-497.47 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -497     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1378000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1379000, episode_reward=-532.12 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -532     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1379000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.65    |\n",
      "|    critic_loss     | 0.0066   |\n",
      "|    ent_coef        | 0.00316  |\n",
      "|    ent_coef_loss   | 12.1     |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=-531.27 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -531     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1380000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -483     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1380     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2161     |\n",
      "|    total_timesteps | 1380000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1381000, episode_reward=-442.39 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -442     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1381000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.65    |\n",
      "|    critic_loss     | 0.00132  |\n",
      "|    ent_coef        | 0.00317  |\n",
      "|    ent_coef_loss   | 20.2     |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1382000, episode_reward=-441.42 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -441     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1382000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1383000, episode_reward=-429.00 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -429     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1383000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.54    |\n",
      "|    critic_loss     | 0.00225  |\n",
      "|    ent_coef        | 0.00321  |\n",
      "|    ent_coef_loss   | 5.14     |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1384000, episode_reward=-429.10 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -429     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1384000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -477     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1384     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2167     |\n",
      "|    total_timesteps | 1384000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1385000, episode_reward=-328.27 +/- 0.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -328     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1385000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.54    |\n",
      "|    critic_loss     | 0.00392  |\n",
      "|    ent_coef        | 0.00323  |\n",
      "|    ent_coef_loss   | 2.1      |\n",
      "|    learning_rate   | 0.00362  |\n",
      "|    n_updates       | 6760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1386000, episode_reward=-328.28 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -328     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1386000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1387000, episode_reward=-430.69 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -431     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1387000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.61    |\n",
      "|    critic_loss     | 0.00302  |\n",
      "|    ent_coef        | 0.00325  |\n",
      "|    ent_coef_loss   | 16.4     |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1388000, episode_reward=-430.49 +/- 0.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -430     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1388000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -470     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1388     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2173     |\n",
      "|    total_timesteps | 1388000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1389000, episode_reward=-476.16 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -476     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1389000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.64    |\n",
      "|    critic_loss     | 0.00134  |\n",
      "|    ent_coef        | 0.00328  |\n",
      "|    ent_coef_loss   | 18.2     |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=-476.17 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -476     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1390000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391000, episode_reward=-384.80 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -385     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1391000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.59    |\n",
      "|    critic_loss     | 0.00145  |\n",
      "|    ent_coef        | 0.00333  |\n",
      "|    ent_coef_loss   | 3.37     |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1392000, episode_reward=-384.48 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -384     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1392000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -463     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1392     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2179     |\n",
      "|    total_timesteps | 1392000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1393000, episode_reward=-340.74 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -341     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1393000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.62    |\n",
      "|    critic_loss     | 0.00659  |\n",
      "|    ent_coef        | 0.00335  |\n",
      "|    ent_coef_loss   | 9.83     |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1394000, episode_reward=-340.98 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -341     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1394000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1395000, episode_reward=-458.18 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -458     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1395000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.6     |\n",
      "|    critic_loss     | 0.00317  |\n",
      "|    ent_coef        | 0.00338  |\n",
      "|    ent_coef_loss   | 10       |\n",
      "|    learning_rate   | 0.00361  |\n",
      "|    n_updates       | 6810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1396000, episode_reward=-457.84 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -458     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1396000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -455     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1396     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2185     |\n",
      "|    total_timesteps | 1396000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1397000, episode_reward=-319.12 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -319     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1397000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 0.00177  |\n",
      "|    ent_coef        | 0.00341  |\n",
      "|    ent_coef_loss   | -0.413   |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1398000, episode_reward=-320.05 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -320     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1398000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1399000, episode_reward=-561.42 +/- 51.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -561     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1399000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.51    |\n",
      "|    critic_loss     | 0.0147   |\n",
      "|    ent_coef        | 0.00342  |\n",
      "|    ent_coef_loss   | 9.99     |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=-468.58 +/- 115.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -469     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -451     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1400     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2192     |\n",
      "|    total_timesteps | 1400000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1401000, episode_reward=-370.83 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1401000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.58    |\n",
      "|    critic_loss     | 0.00542  |\n",
      "|    ent_coef        | 0.00344  |\n",
      "|    ent_coef_loss   | -0.67    |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1402000, episode_reward=-370.81 +/- 2.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1402000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1403000, episode_reward=-376.17 +/- 1.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1403000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.55    |\n",
      "|    critic_loss     | 0.0022   |\n",
      "|    ent_coef        | 0.00345  |\n",
      "|    ent_coef_loss   | -0.847   |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1404000, episode_reward=-375.95 +/- 1.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1404000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -446     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1404     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2198     |\n",
      "|    total_timesteps | 1404000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1405000, episode_reward=-402.94 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1405000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 0.00182  |\n",
      "|    ent_coef        | 0.00345  |\n",
      "|    ent_coef_loss   | -6.19    |\n",
      "|    learning_rate   | 0.0036   |\n",
      "|    n_updates       | 6860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1406000, episode_reward=-404.14 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -404     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1406000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1407000, episode_reward=-452.87 +/- 1.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -453     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1407000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.5     |\n",
      "|    critic_loss     | 0.00103  |\n",
      "|    ent_coef        | 0.00345  |\n",
      "|    ent_coef_loss   | 6.53     |\n",
      "|    learning_rate   | 0.00359  |\n",
      "|    n_updates       | 6870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1408000, episode_reward=-453.03 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -453     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1408000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -441     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1408     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2204     |\n",
      "|    total_timesteps | 1408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1409000, episode_reward=-452.48 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -452     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1409000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=-488.52 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -489     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.46    |\n",
      "|    critic_loss     | 0.000741 |\n",
      "|    ent_coef        | 0.00345  |\n",
      "|    ent_coef_loss   | 4.09     |\n",
      "|    learning_rate   | 0.00359  |\n",
      "|    n_updates       | 6880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1411000, episode_reward=-487.62 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -488     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1411000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1412000, episode_reward=-469.24 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -469     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1412000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.45    |\n",
      "|    critic_loss     | 0.000844 |\n",
      "|    ent_coef        | 0.00347  |\n",
      "|    ent_coef_loss   | 1.02     |\n",
      "|    learning_rate   | 0.00359  |\n",
      "|    n_updates       | 6890     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -439     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1412     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2210     |\n",
      "|    total_timesteps | 1412000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1413000, episode_reward=-468.78 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -469     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1413000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1414000, episode_reward=-420.53 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -421     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1414000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.46    |\n",
      "|    critic_loss     | 0.00118  |\n",
      "|    ent_coef        | 0.00347  |\n",
      "|    ent_coef_loss   | 0.0378   |\n",
      "|    learning_rate   | 0.00359  |\n",
      "|    n_updates       | 6900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1415000, episode_reward=-419.55 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -420     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1415000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1416000, episode_reward=-420.38 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -420     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1416000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.45    |\n",
      "|    critic_loss     | 0.0014   |\n",
      "|    ent_coef        | 0.00348  |\n",
      "|    ent_coef_loss   | -1.9     |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -432     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1416     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2216     |\n",
      "|    total_timesteps | 1416000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1417000, episode_reward=-420.65 +/- 1.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -421     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1417000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1418000, episode_reward=-433.31 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -433     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1418000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.46    |\n",
      "|    critic_loss     | 0.00184  |\n",
      "|    ent_coef        | 0.00347  |\n",
      "|    ent_coef_loss   | -2.06    |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1419000, episode_reward=-432.22 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -432     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1419000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=-329.16 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -329     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.47    |\n",
      "|    critic_loss     | 0.00175  |\n",
      "|    ent_coef        | 0.00347  |\n",
      "|    ent_coef_loss   | 4.91     |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6930     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -427     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1420     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2222     |\n",
      "|    total_timesteps | 1420000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1421000, episode_reward=-328.54 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -329     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1421000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1422000, episode_reward=-340.81 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -341     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1422000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.52    |\n",
      "|    critic_loss     | 0.00175  |\n",
      "|    ent_coef        | 0.00347  |\n",
      "|    ent_coef_loss   | -2.95    |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1423000, episode_reward=-340.97 +/- 2.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -341     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1423000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1424000, episode_reward=-326.99 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -327     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1424000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.53    |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.00347  |\n",
      "|    ent_coef_loss   | -3.38    |\n",
      "|    learning_rate   | 0.00358  |\n",
      "|    n_updates       | 6950     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -420     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1424     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2229     |\n",
      "|    total_timesteps | 1424000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1425000, episode_reward=-328.52 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -329     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1425000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1426000, episode_reward=-323.85 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -324     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1426000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.52    |\n",
      "|    critic_loss     | 0.00223  |\n",
      "|    ent_coef        | 0.00347  |\n",
      "|    ent_coef_loss   | 12.6     |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 6960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1427000, episode_reward=-322.92 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -323     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1427000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1428000, episode_reward=-391.56 +/- 0.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -392     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1428000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.52    |\n",
      "|    critic_loss     | 0.00163  |\n",
      "|    ent_coef        | 0.00349  |\n",
      "|    ent_coef_loss   | 0.703    |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 6970     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -412     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1428     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2235     |\n",
      "|    total_timesteps | 1428000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1429000, episode_reward=-390.86 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -391     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1429000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=-405.46 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -405     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.52    |\n",
      "|    critic_loss     | 0.00097  |\n",
      "|    ent_coef        | 0.0035   |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 6980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431000, episode_reward=-405.29 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -405     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1431000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1432000, episode_reward=-359.91 +/- 1.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -360     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1432000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.5     |\n",
      "|    critic_loss     | 0.000695 |\n",
      "|    ent_coef        | 0.0035   |\n",
      "|    ent_coef_loss   | -7.53    |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 6990     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1432     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2241     |\n",
      "|    total_timesteps | 1432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1433000, episode_reward=-360.20 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -360     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1433000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1434000, episode_reward=-332.29 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -332     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1434000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.49    |\n",
      "|    critic_loss     | 0.00176  |\n",
      "|    ent_coef        | 0.00349  |\n",
      "|    ent_coef_loss   | -5.22    |\n",
      "|    learning_rate   | 0.00357  |\n",
      "|    n_updates       | 7000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1435000, episode_reward=-333.55 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -334     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1435000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1436000, episode_reward=-343.83 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -344     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1436000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.51    |\n",
      "|    critic_loss     | 0.00116  |\n",
      "|    ent_coef        | 0.00347  |\n",
      "|    ent_coef_loss   | -7.48    |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7010     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -407     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1436     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2247     |\n",
      "|    total_timesteps | 1436000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1437000, episode_reward=-343.19 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -343     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1437000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1438000, episode_reward=-349.52 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1438000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.49    |\n",
      "|    critic_loss     | 0.00159  |\n",
      "|    ent_coef        | 0.00344  |\n",
      "|    ent_coef_loss   | 0.424    |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1439000, episode_reward=-348.98 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -349     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1439000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=-345.94 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -346     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.49    |\n",
      "|    critic_loss     | 0.00106  |\n",
      "|    ent_coef        | 0.00344  |\n",
      "|    ent_coef_loss   | -1.68    |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7030     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -405     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1440     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 2253     |\n",
      "|    total_timesteps | 1440000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1441000, episode_reward=-346.85 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1441000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1442000, episode_reward=-326.53 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -327     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1442000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.49    |\n",
      "|    critic_loss     | 0.000842 |\n",
      "|    ent_coef        | 0.00343  |\n",
      "|    ent_coef_loss   | 0.976    |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1443000, episode_reward=-326.49 +/- 2.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -326     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1443000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1444000, episode_reward=-354.52 +/- 30.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -355     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1444000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.48    |\n",
      "|    critic_loss     | 0.0015   |\n",
      "|    ent_coef        | 0.00343  |\n",
      "|    ent_coef_loss   | -0.986   |\n",
      "|    learning_rate   | 0.00356  |\n",
      "|    n_updates       | 7050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -404     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1444     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2259     |\n",
      "|    total_timesteps | 1444000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1445000, episode_reward=-362.86 +/- 47.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -363     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1445000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1446000, episode_reward=-316.18 +/- 1.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -316     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1446000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.46    |\n",
      "|    critic_loss     | 0.00242  |\n",
      "|    ent_coef        | 0.00343  |\n",
      "|    ent_coef_loss   | -6.61    |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1447000, episode_reward=-316.62 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -317     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1447000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1448000, episode_reward=-306.44 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -306     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1448000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.47    |\n",
      "|    critic_loss     | 0.00112  |\n",
      "|    ent_coef        | 0.00341  |\n",
      "|    ent_coef_loss   | -0.857   |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7070     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -399     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1448     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2265     |\n",
      "|    total_timesteps | 1448000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1449000, episode_reward=-306.83 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -307     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1449000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=-316.42 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -316     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.46    |\n",
      "|    critic_loss     | 0.00114  |\n",
      "|    ent_coef        | 0.0034   |\n",
      "|    ent_coef_loss   | -6.17    |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1451000, episode_reward=-316.70 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -317     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1451000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1452000, episode_reward=-318.00 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -318     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1452000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -399     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1452     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2272     |\n",
      "|    total_timesteps | 1452000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1453000, episode_reward=-315.45 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1453000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.45    |\n",
      "|    critic_loss     | 0.00188  |\n",
      "|    ent_coef        | 0.00339  |\n",
      "|    ent_coef_loss   | -4.97    |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1454000, episode_reward=-315.02 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1454000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1455000, episode_reward=-308.37 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1455000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.46    |\n",
      "|    critic_loss     | 0.00186  |\n",
      "|    ent_coef        | 0.00337  |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.00355  |\n",
      "|    n_updates       | 7100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1456000, episode_reward=-308.69 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -309     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1456000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -394     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1456     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2278     |\n",
      "|    total_timesteps | 1456000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1457000, episode_reward=-302.70 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -303     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1457000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.46    |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.00334  |\n",
      "|    ent_coef_loss   | -5.43    |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1458000, episode_reward=-302.81 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -303     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1458000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1459000, episode_reward=-296.10 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -296     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1459000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.45    |\n",
      "|    critic_loss     | 0.00104  |\n",
      "|    ent_coef        | 0.00331  |\n",
      "|    ent_coef_loss   | -10      |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=-296.53 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -297     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1460000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -396     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1460     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2284     |\n",
      "|    total_timesteps | 1460000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1461000, episode_reward=-301.65 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -302     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1461000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.44    |\n",
      "|    critic_loss     | 0.00102  |\n",
      "|    ent_coef        | 0.00328  |\n",
      "|    ent_coef_loss   | -7.19    |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1462000, episode_reward=-300.32 +/- 2.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -300     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1462000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1463000, episode_reward=-316.20 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -316     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1463000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.44    |\n",
      "|    critic_loss     | 0.000956 |\n",
      "|    ent_coef        | 0.00325  |\n",
      "|    ent_coef_loss   | -8.15    |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7140     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1464000, episode_reward=-315.74 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -316     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1464000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -396     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1464     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2290     |\n",
      "|    total_timesteps | 1464000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1465000, episode_reward=-329.99 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1465000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.44    |\n",
      "|    critic_loss     | 0.00089  |\n",
      "|    ent_coef        | 0.00323  |\n",
      "|    ent_coef_loss   | -6.75    |\n",
      "|    learning_rate   | 0.00354  |\n",
      "|    n_updates       | 7150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1466000, episode_reward=-331.24 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -331     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1466000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1467000, episode_reward=-315.63 +/- 2.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -316     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1467000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.44    |\n",
      "|    critic_loss     | 0.000763 |\n",
      "|    ent_coef        | 0.0032   |\n",
      "|    ent_coef_loss   | -3.84    |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7160     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1468000, episode_reward=-314.81 +/- 2.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1468000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -396     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1468     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2296     |\n",
      "|    total_timesteps | 1468000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1469000, episode_reward=-317.84 +/- 2.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -318     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1469000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.42    |\n",
      "|    critic_loss     | 0.000779 |\n",
      "|    ent_coef        | 0.00318  |\n",
      "|    ent_coef_loss   | -9.38    |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=-319.28 +/- 1.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -319     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1470000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471000, episode_reward=-332.90 +/- 1.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -333     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1471000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.43    |\n",
      "|    critic_loss     | 0.00111  |\n",
      "|    ent_coef        | 0.00316  |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1472000, episode_reward=-334.06 +/- 1.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -334     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1472000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -396     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1472     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2302     |\n",
      "|    total_timesteps | 1472000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1473000, episode_reward=-335.09 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1473000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.42    |\n",
      "|    critic_loss     | 0.000937 |\n",
      "|    ent_coef        | 0.00312  |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1474000, episode_reward=-335.85 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1474000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1475000, episode_reward=-338.84 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -339     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1475000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.41    |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.00308  |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.00353  |\n",
      "|    n_updates       | 7200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1476000, episode_reward=-339.73 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1476000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -395     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1476     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2309     |\n",
      "|    total_timesteps | 1476000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1477000, episode_reward=-329.71 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1477000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.42    |\n",
      "|    critic_loss     | 0.000746 |\n",
      "|    ent_coef        | 0.00304  |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1478000, episode_reward=-329.70 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1478000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1479000, episode_reward=-331.68 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -332     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1479000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.42    |\n",
      "|    critic_loss     | 0.000902 |\n",
      "|    ent_coef        | 0.003    |\n",
      "|    ent_coef_loss   | -14.5    |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=-331.64 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -332     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1480000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -390     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1480     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2315     |\n",
      "|    total_timesteps | 1480000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1481000, episode_reward=-146.94 +/- 311.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1481000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.41    |\n",
      "|    critic_loss     | 0.000931 |\n",
      "|    ent_coef        | 0.00295  |\n",
      "|    ent_coef_loss   | -14.9    |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1482000, episode_reward=-304.48 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -304     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1482000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1483000, episode_reward=-42.24 +/- 5.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1483000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.00112  |\n",
      "|    ent_coef        | 0.0029   |\n",
      "|    ent_coef_loss   | -16.2    |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1484000, episode_reward=-44.48 +/- 2.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -44.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1484000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -388     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1484     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2321     |\n",
      "|    total_timesteps | 1484000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1485000, episode_reward=-313.37 +/- 28.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -313     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1485000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.41    |\n",
      "|    critic_loss     | 0.00111  |\n",
      "|    ent_coef        | 0.00285  |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.00352  |\n",
      "|    n_updates       | 7250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1486000, episode_reward=-317.69 +/- 27.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -318     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1486000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1487000, episode_reward=-403.42 +/- 124.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1487000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.41    |\n",
      "|    critic_loss     | 0.0014   |\n",
      "|    ent_coef        | 0.0028   |\n",
      "|    ent_coef_loss   | -22.2    |\n",
      "|    learning_rate   | 0.00351  |\n",
      "|    n_updates       | 7260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1488000, episode_reward=-397.73 +/- 89.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -398     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1488000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -388     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1488     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2327     |\n",
      "|    total_timesteps | 1488000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1489000, episode_reward=-305.79 +/- 18.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -306     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1489000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.00088  |\n",
      "|    ent_coef        | 0.00274  |\n",
      "|    ent_coef_loss   | -18.6    |\n",
      "|    learning_rate   | 0.00351  |\n",
      "|    n_updates       | 7270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=-316.78 +/- 21.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -317     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1490000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1491000, episode_reward=-479.05 +/- 265.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -479     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1491000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.4     |\n",
      "|    critic_loss     | 0.00065  |\n",
      "|    ent_coef        | 0.00268  |\n",
      "|    ent_coef_loss   | -16      |\n",
      "|    learning_rate   | 0.00351  |\n",
      "|    n_updates       | 7280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1492000, episode_reward=-341.37 +/- 18.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -341     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1492000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -386     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1492     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2333     |\n",
      "|    total_timesteps | 1492000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1493000, episode_reward=-344.83 +/- 28.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -345     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1493000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000821 |\n",
      "|    ent_coef        | 0.00263  |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.00351  |\n",
      "|    n_updates       | 7290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1494000, episode_reward=-364.51 +/- 33.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -365     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1494000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1495000, episode_reward=-461.08 +/- 94.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -461     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1495000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1496000, episode_reward=-366.35 +/- 92.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -366     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1496000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000902 |\n",
      "|    ent_coef        | 0.00258  |\n",
      "|    ent_coef_loss   | -21      |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7300     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -391     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1496     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2339     |\n",
      "|    total_timesteps | 1496000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1497000, episode_reward=-381.89 +/- 70.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1497000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1498000, episode_reward=-348.27 +/- 43.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -348     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1498000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000802 |\n",
      "|    ent_coef        | 0.00253  |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1499000, episode_reward=-370.21 +/- 28.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1499000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=-425.32 +/- 65.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -425     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000583 |\n",
      "|    ent_coef        | 0.00248  |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7320     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -386     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1500     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2345     |\n",
      "|    total_timesteps | 1500000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1501000, episode_reward=-391.92 +/- 52.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -392     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1501000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1502000, episode_reward=-405.87 +/- 53.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -406     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1502000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000547 |\n",
      "|    ent_coef        | 0.00244  |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1503000, episode_reward=-472.68 +/- 160.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -473     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1503000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1504000, episode_reward=-376.69 +/- 95.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1504000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 0.000503 |\n",
      "|    ent_coef        | 0.0024   |\n",
      "|    ent_coef_loss   | -16      |\n",
      "|    learning_rate   | 0.0035   |\n",
      "|    n_updates       | 7340     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -386     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1504     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2352     |\n",
      "|    total_timesteps | 1504000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1505000, episode_reward=-333.27 +/- 14.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -333     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1505000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1506000, episode_reward=-347.38 +/- 44.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1506000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 0.000668 |\n",
      "|    ent_coef        | 0.00236  |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1507000, episode_reward=-349.73 +/- 46.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1507000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1508000, episode_reward=-594.59 +/- 260.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -595     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1508000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 0.000583 |\n",
      "|    ent_coef        | 0.00232  |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7360     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -385     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1508     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2358     |\n",
      "|    total_timesteps | 1508000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1509000, episode_reward=-390.05 +/- 29.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -390     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1509000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=-333.43 +/- 12.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -333     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 0.000563 |\n",
      "|    ent_coef        | 0.00228  |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1511000, episode_reward=-419.85 +/- 48.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -420     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1511000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-394.93 +/- 54.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -395     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1512000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 0.000609 |\n",
      "|    ent_coef        | 0.00225  |\n",
      "|    ent_coef_loss   | -16.8    |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7380     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -384     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1512     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2364     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1513000, episode_reward=-382.65 +/- 46.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1513000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1514000, episode_reward=-333.31 +/- 24.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -333     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1514000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 0.000606 |\n",
      "|    ent_coef        | 0.00221  |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.00349  |\n",
      "|    n_updates       | 7390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1515000, episode_reward=-338.09 +/- 38.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -338     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1515000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1516000, episode_reward=-347.18 +/- 21.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1516000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 0.000429 |\n",
      "|    ent_coef        | 0.00218  |\n",
      "|    ent_coef_loss   | -15.8    |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7400     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -383     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1516     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2370     |\n",
      "|    total_timesteps | 1516000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1517000, episode_reward=-402.05 +/- 63.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1517000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1518000, episode_reward=-363.16 +/- 20.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -363     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1518000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 0.000451 |\n",
      "|    ent_coef        | 0.00214  |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1519000, episode_reward=-378.32 +/- 31.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -378     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1519000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=-354.20 +/- 22.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -354     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 0.000521 |\n",
      "|    ent_coef        | 0.0021   |\n",
      "|    ent_coef_loss   | -16.8    |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7420     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -383     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1520     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2376     |\n",
      "|    total_timesteps | 1520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1521000, episode_reward=-369.86 +/- 52.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1521000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1522000, episode_reward=-380.95 +/- 60.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1522000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 0.000342 |\n",
      "|    ent_coef        | 0.00207  |\n",
      "|    ent_coef_loss   | -16.7    |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1523000, episode_reward=-361.77 +/- 30.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -362     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1523000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1524000, episode_reward=-354.35 +/- 21.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -354     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1524000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 0.00057  |\n",
      "|    ent_coef        | 0.00203  |\n",
      "|    ent_coef_loss   | -19.4    |\n",
      "|    learning_rate   | 0.00348  |\n",
      "|    n_updates       | 7440     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -384     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1524     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2382     |\n",
      "|    total_timesteps | 1524000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1525000, episode_reward=-342.58 +/- 10.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -343     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1525000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1526000, episode_reward=-326.12 +/- 12.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -326     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1526000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.000686 |\n",
      "|    ent_coef        | 0.00199  |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1527000, episode_reward=-329.00 +/- 7.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -329     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1527000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1528000, episode_reward=-388.54 +/- 70.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -389     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1528000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.35    |\n",
      "|    critic_loss     | 0.000627 |\n",
      "|    ent_coef        | 0.00195  |\n",
      "|    ent_coef_loss   | -15.7    |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7460     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -391     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1528     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2389     |\n",
      "|    total_timesteps | 1528000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1529000, episode_reward=-386.54 +/- 74.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -387     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1529000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=-372.60 +/- 9.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.000495 |\n",
      "|    ent_coef        | 0.00191  |\n",
      "|    ent_coef_loss   | -16      |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1531000, episode_reward=-375.54 +/- 48.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1531000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532000, episode_reward=-356.92 +/- 15.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -357     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1532000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.000439 |\n",
      "|    ent_coef        | 0.00188  |\n",
      "|    ent_coef_loss   | -18.1    |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -391     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1532     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2395     |\n",
      "|    total_timesteps | 1532000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1533000, episode_reward=-353.39 +/- 14.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1533000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1534000, episode_reward=-348.19 +/- 71.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -348     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1534000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000415 |\n",
      "|    ent_coef        | 0.00184  |\n",
      "|    ent_coef_loss   | -19.4    |\n",
      "|    learning_rate   | 0.00347  |\n",
      "|    n_updates       | 7490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1535000, episode_reward=-357.34 +/- 78.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -357     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1535000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1536000, episode_reward=-360.02 +/- 66.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -360     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1536000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -391     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1536     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2401     |\n",
      "|    total_timesteps | 1536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1537000, episode_reward=-327.60 +/- 91.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -328     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1537000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000625 |\n",
      "|    ent_coef        | 0.00181  |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1538000, episode_reward=-336.48 +/- 108.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1538000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1539000, episode_reward=-77.38 +/- 224.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1539000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.000839 |\n",
      "|    ent_coef        | 0.00178  |\n",
      "|    ent_coef_loss   | -23.3    |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=-82.97 +/- 196.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -83      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1540000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -389     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1540     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2407     |\n",
      "|    total_timesteps | 1540000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1541000, episode_reward=-401.05 +/- 96.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -401     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1541000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.00174  |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1542000, episode_reward=-397.22 +/- 109.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -397     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1542000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1543000, episode_reward=-382.83 +/- 125.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1543000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000961 |\n",
      "|    ent_coef        | 0.0017   |\n",
      "|    ent_coef_loss   | -18.2    |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1544000, episode_reward=-377.86 +/- 56.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -378     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1544000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -381     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1544     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2413     |\n",
      "|    total_timesteps | 1544000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1545000, episode_reward=-309.39 +/- 99.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -309     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1545000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.00074  |\n",
      "|    ent_coef        | 0.00166  |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.00346  |\n",
      "|    n_updates       | 7540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1546000, episode_reward=-244.96 +/- 44.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1546000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1547000, episode_reward=-379.77 +/- 72.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -380     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1547000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.00101  |\n",
      "|    ent_coef        | 0.00163  |\n",
      "|    ent_coef_loss   | -20.9    |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7550     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1548000, episode_reward=-376.09 +/- 40.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1548000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -380     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1548     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2419     |\n",
      "|    total_timesteps | 1548000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1549000, episode_reward=-243.49 +/- 20.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -243     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1549000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000957 |\n",
      "|    ent_coef        | 0.0016   |\n",
      "|    ent_coef_loss   | -15.7    |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=-336.77 +/- 145.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -337     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1550000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1551000, episode_reward=-387.60 +/- 132.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -388     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1551000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.00114  |\n",
      "|    ent_coef        | 0.00157  |\n",
      "|    ent_coef_loss   | -15.8    |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7570     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552000, episode_reward=-558.96 +/- 348.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -559     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1552000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -372     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1552     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2426     |\n",
      "|    total_timesteps | 1552000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1553000, episode_reward=-641.85 +/- 309.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -642     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1553000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.000974 |\n",
      "|    ent_coef        | 0.00154  |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1554000, episode_reward=-511.20 +/- 275.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -511     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1554000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1555000, episode_reward=-286.86 +/- 47.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -287     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1555000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.35    |\n",
      "|    critic_loss     | 0.000657 |\n",
      "|    ent_coef        | 0.00152  |\n",
      "|    ent_coef_loss   | -20.3    |\n",
      "|    learning_rate   | 0.00345  |\n",
      "|    n_updates       | 7590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1556000, episode_reward=-381.16 +/- 163.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1556000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -370     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1556     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2432     |\n",
      "|    total_timesteps | 1556000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1557000, episode_reward=-479.78 +/- 291.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -480     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1557000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000677 |\n",
      "|    ent_coef        | 0.00149  |\n",
      "|    ent_coef_loss   | -15.9    |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1558000, episode_reward=-401.67 +/- 100.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1558000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1559000, episode_reward=-285.80 +/- 17.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -286     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1559000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000608 |\n",
      "|    ent_coef        | 0.00146  |\n",
      "|    ent_coef_loss   | -19      |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=-311.00 +/- 48.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -311     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1560000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -367     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1560     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2438     |\n",
      "|    total_timesteps | 1560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1561000, episode_reward=-255.40 +/- 77.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1561000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.35    |\n",
      "|    critic_loss     | 0.000669 |\n",
      "|    ent_coef        | 0.00143  |\n",
      "|    ent_coef_loss   | -9.91    |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1562000, episode_reward=-291.13 +/- 68.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -291     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1562000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1563000, episode_reward=-316.60 +/- 13.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -317     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1563000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.35    |\n",
      "|    critic_loss     | 0.00063  |\n",
      "|    ent_coef        | 0.00141  |\n",
      "|    ent_coef_loss   | -5.02    |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1564000, episode_reward=-324.81 +/- 17.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -325     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1564000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -362     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1564     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2444     |\n",
      "|    total_timesteps | 1564000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1565000, episode_reward=-301.44 +/- 41.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -301     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1565000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.35    |\n",
      "|    critic_loss     | 0.000527 |\n",
      "|    ent_coef        | 0.0014   |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.00344  |\n",
      "|    n_updates       | 7640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1566000, episode_reward=-259.04 +/- 14.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -259     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1566000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1567000, episode_reward=-17.15 +/- 138.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -17.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1567000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000514 |\n",
      "|    ent_coef        | 0.00138  |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1568000, episode_reward=-241.47 +/- 116.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1568000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -356     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1568     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2450     |\n",
      "|    total_timesteps | 1568000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1569000, episode_reward=-10.30 +/- 49.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1569000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.35    |\n",
      "|    critic_loss     | 0.000589 |\n",
      "|    ent_coef        | 0.00136  |\n",
      "|    ent_coef_loss   | -17.3    |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=-48.52 +/- 43.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1570000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1571000, episode_reward=-21.93 +/- 15.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1571000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000557 |\n",
      "|    ent_coef        | 0.00134  |\n",
      "|    ent_coef_loss   | -7       |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1572000, episode_reward=-8.75 +/- 21.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -8.75    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1572000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -345     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1572     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2456     |\n",
      "|    total_timesteps | 1572000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1573000, episode_reward=-121.40 +/- 7.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1573000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000448 |\n",
      "|    ent_coef        | 0.00133  |\n",
      "|    ent_coef_loss   | -9.62    |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1574000, episode_reward=-119.39 +/- 16.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1574000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1575000, episode_reward=14.89 +/- 8.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 14.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1575000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.35    |\n",
      "|    critic_loss     | 0.000367 |\n",
      "|    ent_coef        | 0.00131  |\n",
      "|    ent_coef_loss   | -17.8    |\n",
      "|    learning_rate   | 0.00343  |\n",
      "|    n_updates       | 7690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1576000, episode_reward=24.04 +/- 8.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1576000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -336     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1576     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2462     |\n",
      "|    total_timesteps | 1576000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1577000, episode_reward=187.61 +/- 31.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 188      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1577000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 0.000333 |\n",
      "|    ent_coef        | 0.00129  |\n",
      "|    ent_coef_loss   | -20.1    |\n",
      "|    learning_rate   | 0.00342  |\n",
      "|    n_updates       | 7700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1578000, episode_reward=185.30 +/- 40.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 185      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1578000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1579000, episode_reward=209.03 +/- 5.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 209      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1579000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=-5.03 +/- 13.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -5.03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000366 |\n",
      "|    ent_coef        | 0.00126  |\n",
      "|    ent_coef_loss   | -16      |\n",
      "|    learning_rate   | 0.00342  |\n",
      "|    n_updates       | 7710     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -323     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1580     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2468     |\n",
      "|    total_timesteps | 1580000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1581000, episode_reward=-7.98 +/- 22.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.98    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1581000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1582000, episode_reward=234.99 +/- 10.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 235      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1582000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000439 |\n",
      "|    ent_coef        | 0.00124  |\n",
      "|    ent_coef_loss   | -9.16    |\n",
      "|    learning_rate   | 0.00342  |\n",
      "|    n_updates       | 7720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1583000, episode_reward=159.81 +/- 196.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1583000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1584000, episode_reward=-95.81 +/- 10.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1584000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 0.000398 |\n",
      "|    ent_coef        | 0.00123  |\n",
      "|    ent_coef_loss   | -7.57    |\n",
      "|    learning_rate   | 0.00342  |\n",
      "|    n_updates       | 7730     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -311     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1584     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 2475     |\n",
      "|    total_timesteps | 1584000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1585000, episode_reward=-135.14 +/- 57.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1585000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1586000, episode_reward=5.51 +/- 20.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.51     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1586000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.000611 |\n",
      "|    ent_coef        | 0.00121  |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1587000, episode_reward=-48.63 +/- 92.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1587000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1588000, episode_reward=291.28 +/- 3.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 291      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1588000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000351 |\n",
      "|    ent_coef        | 0.0012   |\n",
      "|    ent_coef_loss   | -14.7    |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7750     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -299     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1588     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2481     |\n",
      "|    total_timesteps | 1588000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1589000, episode_reward=284.97 +/- 9.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 285      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1589000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=277.52 +/- 11.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 278      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 0.000324 |\n",
      "|    ent_coef        | 0.00118  |\n",
      "|    ent_coef_loss   | -7.73    |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1591000, episode_reward=293.01 +/- 12.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 293      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1591000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592000, episode_reward=381.29 +/- 3.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 381      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1592000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000487 |\n",
      "|    ent_coef        | 0.00117  |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7770     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1592     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2487     |\n",
      "|    total_timesteps | 1592000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1593000, episode_reward=370.73 +/- 7.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 371      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1593000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1594000, episode_reward=512.51 +/- 20.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 513      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1594000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000424 |\n",
      "|    ent_coef        | 0.00115  |\n",
      "|    ent_coef_loss   | -19.8    |\n",
      "|    learning_rate   | 0.00341  |\n",
      "|    n_updates       | 7780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1595000, episode_reward=512.88 +/- 11.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 513      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1595000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1596000, episode_reward=256.52 +/- 9.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 257      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1596000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000622 |\n",
      "|    ent_coef        | 0.00113  |\n",
      "|    ent_coef_loss   | -14.9    |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7790     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -259     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1596     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2493     |\n",
      "|    total_timesteps | 1596000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1597000, episode_reward=227.73 +/- 21.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 228      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1597000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1598000, episode_reward=497.44 +/- 25.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 497      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1598000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.37    |\n",
      "|    critic_loss     | 0.000456 |\n",
      "|    ent_coef        | 0.00111  |\n",
      "|    ent_coef_loss   | -15.7    |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1599000, episode_reward=286.71 +/- 416.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 287      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1599000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=407.60 +/- 16.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 408      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000526 |\n",
      "|    ent_coef        | 0.00109  |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7810     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -236     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1600     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2499     |\n",
      "|    total_timesteps | 1600000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1601000, episode_reward=401.24 +/- 10.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 401      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1601000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1602000, episode_reward=362.50 +/- 13.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 363      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1602000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000552 |\n",
      "|    ent_coef        | 0.00108  |\n",
      "|    ent_coef_loss   | -18.4    |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1603000, episode_reward=356.17 +/- 12.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 356      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1603000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1604000, episode_reward=453.79 +/- 12.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 454      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1604000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000564 |\n",
      "|    ent_coef        | 0.00106  |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0034   |\n",
      "|    n_updates       | 7830     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -208     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1604     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2505     |\n",
      "|    total_timesteps | 1604000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1605000, episode_reward=452.50 +/- 15.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 452      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1605000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1606000, episode_reward=399.75 +/- 43.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 400      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1606000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.000793 |\n",
      "|    ent_coef        | 0.00104  |\n",
      "|    ent_coef_loss   | -21.4    |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1607000, episode_reward=406.55 +/- 22.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 407      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1607000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1608000, episode_reward=285.08 +/- 24.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 285      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1608000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 0.00111  |\n",
      "|    ent_coef        | 0.00102  |\n",
      "|    ent_coef_loss   | -16.6    |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7850     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -179     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1608     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2511     |\n",
      "|    total_timesteps | 1608000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1609000, episode_reward=301.48 +/- 35.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 301      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1609000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=-239.27 +/- 41.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -239     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 0.00113  |\n",
      "|    ent_coef        | 0.000997 |\n",
      "|    ent_coef_loss   | -6.69    |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1611000, episode_reward=-277.57 +/- 13.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -278     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1611000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612000, episode_reward=-22.80 +/- 30.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1612000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.32    |\n",
      "|    critic_loss     | 0.00212  |\n",
      "|    ent_coef        | 0.000985 |\n",
      "|    ent_coef_loss   | -7.37    |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7870     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -161     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1612     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2517     |\n",
      "|    total_timesteps | 1612000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1613000, episode_reward=-36.22 +/- 46.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1613000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1614000, episode_reward=-328.44 +/- 8.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -328     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1614000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.00203  |\n",
      "|    ent_coef        | 0.000978 |\n",
      "|    ent_coef_loss   | -0.269   |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 7880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1615000, episode_reward=-315.48 +/- 14.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1615000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1616000, episode_reward=-317.08 +/- 4.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -317     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1616000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.00158  |\n",
      "|    ent_coef        | 0.000973 |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7890     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -156     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1616     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2523     |\n",
      "|    total_timesteps | 1616000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1617000, episode_reward=-321.54 +/- 2.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -322     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1617000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1618000, episode_reward=-371.23 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1618000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.000603 |\n",
      "|    ent_coef        | 0.000963 |\n",
      "|    ent_coef_loss   | 0.823    |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1619000, episode_reward=-371.54 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -372     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1619000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=-369.01 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -369     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.32    |\n",
      "|    critic_loss     | 0.000613 |\n",
      "|    ent_coef        | 0.000958 |\n",
      "|    ent_coef_loss   | -6.21    |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7910     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -146     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1620     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2529     |\n",
      "|    total_timesteps | 1620000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1621000, episode_reward=-368.88 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -369     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1621000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1622000, episode_reward=-368.64 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -369     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1622000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1623000, episode_reward=-364.68 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -365     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1623000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.00135  |\n",
      "|    ent_coef        | 0.000955 |\n",
      "|    ent_coef_loss   | -2.25    |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1624000, episode_reward=-364.45 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -364     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1624000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -136     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1624     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2535     |\n",
      "|    total_timesteps | 1624000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1625000, episode_reward=-189.56 +/- 17.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1625000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.00137  |\n",
      "|    ent_coef        | 0.000949 |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.00338  |\n",
      "|    n_updates       | 7930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1626000, episode_reward=-196.66 +/- 44.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1626000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1627000, episode_reward=-158.26 +/- 20.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -158     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1627000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.0013   |\n",
      "|    ent_coef        | 0.000941 |\n",
      "|    ent_coef_loss   | -3.95    |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1628000, episode_reward=-155.13 +/- 19.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1628000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1628     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2541     |\n",
      "|    total_timesteps | 1628000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1629000, episode_reward=-319.04 +/- 126.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -319     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1629000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.31    |\n",
      "|    critic_loss     | 0.000808 |\n",
      "|    ent_coef        | 0.000934 |\n",
      "|    ent_coef_loss   | -7.36    |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=-381.30 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1630000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1631000, episode_reward=-382.42 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1631000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000579 |\n",
      "|    ent_coef        | 0.000925 |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632000, episode_reward=-382.69 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1632000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1632     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2548     |\n",
      "|    total_timesteps | 1632000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1633000, episode_reward=-76.79 +/- 238.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -76.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1633000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.000599 |\n",
      "|    ent_coef        | 0.000911 |\n",
      "|    ent_coef_loss   | -18.2    |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1634000, episode_reward=-176.94 +/- 234.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -177     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1634000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1635000, episode_reward=-293.11 +/- 162.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -293     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1635000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.32    |\n",
      "|    critic_loss     | 0.000652 |\n",
      "|    ent_coef        | 0.000893 |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.00337  |\n",
      "|    n_updates       | 7980     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1636000, episode_reward=-190.23 +/- 224.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -190     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1636000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -89.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1636     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2554     |\n",
      "|    total_timesteps | 1636000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1637000, episode_reward=-305.53 +/- 121.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -306     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1637000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000651 |\n",
      "|    ent_coef        | 0.000879 |\n",
      "|    ent_coef_loss   | -5.73    |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 7990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1638000, episode_reward=-230.97 +/- 131.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1638000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1639000, episode_reward=-383.35 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1639000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.32    |\n",
      "|    critic_loss     | 0.000301 |\n",
      "|    ent_coef        | 0.00087  |\n",
      "|    ent_coef_loss   | -0.719   |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=-382.81 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1640000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -77.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1640     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2560     |\n",
      "|    total_timesteps | 1640000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1641000, episode_reward=-401.92 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1641000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000482 |\n",
      "|    ent_coef        | 0.000867 |\n",
      "|    ent_coef_loss   | -2.03    |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 8010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1642000, episode_reward=-402.50 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1642000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1643000, episode_reward=-455.16 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -455     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1643000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.00047  |\n",
      "|    ent_coef        | 0.000864 |\n",
      "|    ent_coef_loss   | -3.93    |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 8020     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1644000, episode_reward=-454.19 +/- 1.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -454     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1644000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -69.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1644     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2566     |\n",
      "|    total_timesteps | 1644000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1645000, episode_reward=-424.23 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -424     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1645000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000444 |\n",
      "|    ent_coef        | 0.000861 |\n",
      "|    ent_coef_loss   | -5.3     |\n",
      "|    learning_rate   | 0.00336  |\n",
      "|    n_updates       | 8030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1646000, episode_reward=-424.45 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -424     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1646000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1647000, episode_reward=-52.36 +/- 201.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1647000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.000851 |\n",
      "|    ent_coef        | 0.000856 |\n",
      "|    ent_coef_loss   | -6.66    |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8040     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1648000, episode_reward=191.71 +/- 11.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 192      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1648000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -51.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1648     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2572     |\n",
      "|    total_timesteps | 1648000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1649000, episode_reward=207.18 +/- 357.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 207      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1649000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000687 |\n",
      "|    ent_coef        | 0.000848 |\n",
      "|    ent_coef_loss   | -23.1    |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=216.10 +/- 363.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 216      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1650000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1651000, episode_reward=-77.67 +/- 320.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -77.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1651000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000645 |\n",
      "|    ent_coef        | 0.000832 |\n",
      "|    ent_coef_loss   | -20.4    |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8060     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1652000, episode_reward=328.69 +/- 9.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 329      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1652000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -41.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1652     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2579     |\n",
      "|    total_timesteps | 1652000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1653000, episode_reward=497.98 +/- 35.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 498      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1653000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000539 |\n",
      "|    ent_coef        | 0.000812 |\n",
      "|    ent_coef_loss   | -22.3    |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1654000, episode_reward=338.53 +/- 340.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 339      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1654000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1655000, episode_reward=436.73 +/- 10.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 437      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1655000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.35    |\n",
      "|    critic_loss     | 0.00082  |\n",
      "|    ent_coef        | 0.000793 |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.00335  |\n",
      "|    n_updates       | 8080     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1656000, episode_reward=426.46 +/- 17.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 426      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1656000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -22      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1656     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2585     |\n",
      "|    total_timesteps | 1656000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1657000, episode_reward=465.35 +/- 25.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1657000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000756 |\n",
      "|    ent_coef        | 0.000776 |\n",
      "|    ent_coef_loss   | -21      |\n",
      "|    learning_rate   | 0.00334  |\n",
      "|    n_updates       | 8090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1658000, episode_reward=481.11 +/- 20.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 481      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1658000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1659000, episode_reward=210.95 +/- 476.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1659000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.000951 |\n",
      "|    ent_coef        | 0.000759 |\n",
      "|    ent_coef_loss   | -20.5    |\n",
      "|    learning_rate   | 0.00334  |\n",
      "|    n_updates       | 8100     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=208.30 +/- 476.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 208      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1660000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.8      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1660     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2591     |\n",
      "|    total_timesteps | 1660000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1661000, episode_reward=-87.33 +/- 513.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1661000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.35    |\n",
      "|    critic_loss     | 0.000753 |\n",
      "|    ent_coef        | 0.00074  |\n",
      "|    ent_coef_loss   | -27.9    |\n",
      "|    learning_rate   | 0.00334  |\n",
      "|    n_updates       | 8110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1662000, episode_reward=-93.61 +/- 490.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -93.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1662000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1663000, episode_reward=-310.97 +/- 292.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -311     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1663000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 0.000687 |\n",
      "|    ent_coef        | 0.00072  |\n",
      "|    ent_coef_loss   | -9.55    |\n",
      "|    learning_rate   | 0.00334  |\n",
      "|    n_updates       | 8120     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1664000, episode_reward=-127.55 +/- 403.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1664000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 30.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1664     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2597     |\n",
      "|    total_timesteps | 1664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1665000, episode_reward=-152.41 +/- 372.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1665000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1666000, episode_reward=-240.40 +/- 321.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -240     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1666000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.00139  |\n",
      "|    ent_coef        | 0.000708 |\n",
      "|    ent_coef_loss   | -6.8     |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1667000, episode_reward=69.68 +/- 387.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 69.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1667000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1668000, episode_reward=201.59 +/- 350.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 202      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1668000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.31    |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.000699 |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 49.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1668     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2603     |\n",
      "|    total_timesteps | 1668000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1669000, episode_reward=71.07 +/- 368.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 71.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1669000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=-153.35 +/- 267.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.000789 |\n",
      "|    ent_coef        | 0.000685 |\n",
      "|    ent_coef_loss   | -29.5    |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1671000, episode_reward=-11.95 +/- 335.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1671000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1672000, episode_reward=-154.54 +/- 213.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1672000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.000753 |\n",
      "|    ent_coef        | 0.000666 |\n",
      "|    ent_coef_loss   | -18.1    |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8160     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1672     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2609     |\n",
      "|    total_timesteps | 1672000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673000, episode_reward=-262.59 +/- 6.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -263     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1673000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1674000, episode_reward=-28.67 +/- 304.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -28.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1674000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.33    |\n",
      "|    critic_loss     | 0.00102  |\n",
      "|    ent_coef        | 0.000652 |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 8170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1675000, episode_reward=-167.11 +/- 243.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -167     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1675000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1676000, episode_reward=32.38 +/- 250.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 32.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1676000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.00059  |\n",
      "|    ent_coef        | 0.000638 |\n",
      "|    ent_coef_loss   | -21      |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8180     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1676     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2615     |\n",
      "|    total_timesteps | 1676000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1677000, episode_reward=-177.18 +/- 7.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -177     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1677000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1678000, episode_reward=77.30 +/- 178.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 77.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1678000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2     |\n",
      "|    critic_loss     | 0.000566 |\n",
      "|    ent_coef        | 0.000625 |\n",
      "|    ent_coef_loss   | -6.05    |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1679000, episode_reward=147.91 +/- 146.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 148      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1679000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=270.99 +/- 38.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 271      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.000502 |\n",
      "|    ent_coef        | 0.000616 |\n",
      "|    ent_coef_loss   | -18.3    |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8200     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 58       |\n",
      "| time/              |          |\n",
      "|    episodes        | 1680     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2622     |\n",
      "|    total_timesteps | 1680000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1681000, episode_reward=131.06 +/- 269.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 131      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1681000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1682000, episode_reward=-68.36 +/- 4.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -68.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1682000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.38    |\n",
      "|    critic_loss     | 0.00725  |\n",
      "|    ent_coef        | 0.000606 |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1683000, episode_reward=-78.83 +/- 10.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -78.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1683000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1684000, episode_reward=-303.62 +/- 411.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -304     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1684000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.00175  |\n",
      "|    ent_coef        | 0.0006   |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.00332  |\n",
      "|    n_updates       | 8220     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 54.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1684     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2628     |\n",
      "|    total_timesteps | 1684000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1685000, episode_reward=-126.36 +/- 274.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1685000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1686000, episode_reward=-125.75 +/- 412.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1686000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.00237  |\n",
      "|    ent_coef        | 0.000596 |\n",
      "|    ent_coef_loss   | 0.158    |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1687000, episode_reward=7.72 +/- 364.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 7.72     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1687000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1688000, episode_reward=-216.44 +/- 43.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -216     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1688000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.00123  |\n",
      "|    ent_coef        | 0.000595 |\n",
      "|    ent_coef_loss   | 19.1     |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8240     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 53.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1688     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2634     |\n",
      "|    total_timesteps | 1688000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1689000, episode_reward=-219.15 +/- 38.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1689000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=-192.48 +/- 35.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.00116  |\n",
      "|    ent_coef        | 0.000601 |\n",
      "|    ent_coef_loss   | -3.51    |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1691000, episode_reward=-182.80 +/- 31.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1691000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1692000, episode_reward=-295.07 +/- 10.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -295     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1692000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000992 |\n",
      "|    ent_coef        | 0.000602 |\n",
      "|    ent_coef_loss   | -16.5    |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8260     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 36.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1692     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2640     |\n",
      "|    total_timesteps | 1692000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693000, episode_reward=-293.26 +/- 19.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -293     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1693000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1694000, episode_reward=-435.55 +/- 1.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -436     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1694000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.0016   |\n",
      "|    ent_coef        | 0.000597 |\n",
      "|    ent_coef_loss   | 0.168    |\n",
      "|    learning_rate   | 0.00331  |\n",
      "|    n_updates       | 8270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1695000, episode_reward=-435.80 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -436     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1695000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1696000, episode_reward=-201.24 +/- 109.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -201     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1696000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2     |\n",
      "|    critic_loss     | 0.000666 |\n",
      "|    ent_coef        | 0.000595 |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8280     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1696     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2646     |\n",
      "|    total_timesteps | 1696000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1697000, episode_reward=-224.74 +/- 119.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1697000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1698000, episode_reward=-181.48 +/- 177.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -181     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1698000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.00173  |\n",
      "|    ent_coef        | 0.000593 |\n",
      "|    ent_coef_loss   | -7.02    |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1699000, episode_reward=-128.07 +/- 148.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1699000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=-421.00 +/- 42.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -421     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.00112  |\n",
      "|    ent_coef        | 0.000592 |\n",
      "|    ent_coef_loss   | 4.73     |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8300     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.86     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1700     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2652     |\n",
      "|    total_timesteps | 1700000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1701000, episode_reward=-456.40 +/- 31.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -456     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1701000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1702000, episode_reward=103.20 +/- 72.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 103      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1702000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.00086  |\n",
      "|    ent_coef        | 0.000593 |\n",
      "|    ent_coef_loss   | 10.4     |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1703000, episode_reward=-110.73 +/- 280.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1703000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1704000, episode_reward=-344.39 +/- 120.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -344     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1704000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.00122  |\n",
      "|    ent_coef        | 0.000599 |\n",
      "|    ent_coef_loss   | 12.4     |\n",
      "|    learning_rate   | 0.0033   |\n",
      "|    n_updates       | 8320     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -9.51    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1704     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2658     |\n",
      "|    total_timesteps | 1704000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1705000, episode_reward=-277.87 +/- 99.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -278     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1705000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1706000, episode_reward=-195.67 +/- 327.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -196     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1706000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.000832 |\n",
      "|    ent_coef        | 0.000605 |\n",
      "|    ent_coef_loss   | -8.35    |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8330     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1707000, episode_reward=-191.44 +/- 332.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1707000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1708000, episode_reward=-464.83 +/- 2.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -465     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1708000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -25.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1708     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2664     |\n",
      "|    total_timesteps | 1708000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1709000, episode_reward=-311.43 +/- 244.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -311     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1709000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000653 |\n",
      "|    ent_coef        | 0.000603 |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=-301.65 +/- 287.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -302     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1710000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1711000, episode_reward=-245.06 +/- 236.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1711000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.18    |\n",
      "|    critic_loss     | 0.000543 |\n",
      "|    ent_coef        | 0.000598 |\n",
      "|    ent_coef_loss   | -6.45    |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1712000, episode_reward=-364.81 +/- 6.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -365     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1712000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -33.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1712     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2671     |\n",
      "|    total_timesteps | 1712000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713000, episode_reward=-98.34 +/- 277.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -98.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1713000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.19    |\n",
      "|    critic_loss     | 0.00076  |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | -24      |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1714000, episode_reward=-109.30 +/- 264.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1714000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1715000, episode_reward=-75.76 +/- 297.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -75.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1715000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.000748 |\n",
      "|    ent_coef        | 0.000579 |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 8370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1716000, episode_reward=295.68 +/- 134.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 296      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1716000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -27.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1716     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2677     |\n",
      "|    total_timesteps | 1716000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1717000, episode_reward=-231.23 +/- 177.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1717000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.17    |\n",
      "|    critic_loss     | 0.000547 |\n",
      "|    ent_coef        | 0.000569 |\n",
      "|    ent_coef_loss   | -9.84    |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1718000, episode_reward=82.14 +/- 205.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 82.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1718000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1719000, episode_reward=-182.25 +/- 321.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -182     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1719000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.19    |\n",
      "|    critic_loss     | 0.000563 |\n",
      "|    ent_coef        | 0.00056  |\n",
      "|    ent_coef_loss   | -8.27    |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8390     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=-495.78 +/- 465.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -496     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1720000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -21.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1720     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 2683     |\n",
      "|    total_timesteps | 1720000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1721000, episode_reward=-62.35 +/- 370.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -62.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1721000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000744 |\n",
      "|    ent_coef        | 0.000555 |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1722000, episode_reward=40.05 +/- 335.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 40.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1722000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1723000, episode_reward=347.11 +/- 347.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 347      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1723000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000971 |\n",
      "|    ent_coef        | 0.000554 |\n",
      "|    ent_coef_loss   | 0.248    |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8410     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1724000, episode_reward=506.89 +/- 55.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 507      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1724000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -1.18    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1724     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2689     |\n",
      "|    total_timesteps | 1724000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1725000, episode_reward=465.88 +/- 59.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 466      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1725000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.00099  |\n",
      "|    ent_coef        | 0.000553 |\n",
      "|    ent_coef_loss   | -6.64    |\n",
      "|    learning_rate   | 0.00328  |\n",
      "|    n_updates       | 8420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1726000, episode_reward=288.20 +/- 326.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 288      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1726000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1727000, episode_reward=180.60 +/- 294.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 181      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1727000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.000915 |\n",
      "|    ent_coef        | 0.00055  |\n",
      "|    ent_coef_loss   | -2.57    |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8430     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1728000, episode_reward=308.46 +/- 34.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 308      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1728000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 11.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1728     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2695     |\n",
      "|    total_timesteps | 1728000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1729000, episode_reward=219.00 +/- 270.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 219      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1729000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.000786 |\n",
      "|    ent_coef        | 0.000548 |\n",
      "|    ent_coef_loss   | 0.17     |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=89.43 +/- 225.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 89.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1730000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1731000, episode_reward=517.95 +/- 11.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 518      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1731000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.000905 |\n",
      "|    ent_coef        | 0.000547 |\n",
      "|    ent_coef_loss   | -3.3     |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8450     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1732000, episode_reward=519.33 +/- 21.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 519      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1732000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 29.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1732     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2701     |\n",
      "|    total_timesteps | 1732000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1733000, episode_reward=390.66 +/- 18.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 391      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1733000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.000731 |\n",
      "|    ent_coef        | 0.000544 |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1734000, episode_reward=375.63 +/- 25.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 376      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1734000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1735000, episode_reward=300.72 +/- 35.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 301      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1735000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.000683 |\n",
      "|    ent_coef        | 0.000539 |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 8470     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1736000, episode_reward=293.95 +/- 13.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 294      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1736000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1736     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2707     |\n",
      "|    total_timesteps | 1736000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1737000, episode_reward=541.49 +/- 34.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 541      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1737000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.18    |\n",
      "|    critic_loss     | 0.000493 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | -5       |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1738000, episode_reward=546.48 +/- 37.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 546      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1738000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1739000, episode_reward=598.82 +/- 26.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 599      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1739000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.19    |\n",
      "|    critic_loss     | 0.00058  |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | -1.75    |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8490     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=552.89 +/- 22.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 553      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1740000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 33.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1740     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2713     |\n",
      "|    total_timesteps | 1740000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1741000, episode_reward=320.77 +/- 361.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1741000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.22    |\n",
      "|    critic_loss     | 0.000681 |\n",
      "|    ent_coef        | 0.000524 |\n",
      "|    ent_coef_loss   | -8.55    |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1742000, episode_reward=322.54 +/- 362.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 323      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1742000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1743000, episode_reward=-137.11 +/- 318.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1743000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000659 |\n",
      "|    ent_coef        | 0.000519 |\n",
      "|    ent_coef_loss   | -14.5    |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8510     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1744000, episode_reward=453.18 +/- 38.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 453      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1744000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 44.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1744     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2720     |\n",
      "|    total_timesteps | 1744000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1745000, episode_reward=592.85 +/- 28.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 593      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1745000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.000897 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.00326  |\n",
      "|    n_updates       | 8520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1746000, episode_reward=619.50 +/- 36.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 620      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1746000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1747000, episode_reward=-378.54 +/- 2.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -379     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1747000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.00106  |\n",
      "|    ent_coef        | 0.000505 |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.00325  |\n",
      "|    n_updates       | 8530     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1748000, episode_reward=-377.33 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1748000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 59.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1748     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2726     |\n",
      "|    total_timesteps | 1748000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1749000, episode_reward=-406.24 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -406     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1749000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000787 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | -7.26    |\n",
      "|    learning_rate   | 0.00325  |\n",
      "|    n_updates       | 8540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=-406.27 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -406     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1750000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1751000, episode_reward=-406.43 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -406     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1751000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1752000, episode_reward=-422.58 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -423     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1752000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000791 |\n",
      "|    ent_coef        | 0.000489 |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00325  |\n",
      "|    n_updates       | 8550     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 75.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1752     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2732     |\n",
      "|    total_timesteps | 1752000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753000, episode_reward=-423.12 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -423     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1753000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1754000, episode_reward=-414.74 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -415     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1754000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000613 |\n",
      "|    ent_coef        | 0.000483 |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.00325  |\n",
      "|    n_updates       | 8560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1755000, episode_reward=-415.25 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -415     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1755000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1756000, episode_reward=-413.81 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -414     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1756000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.18    |\n",
      "|    critic_loss     | 0.000362 |\n",
      "|    ent_coef        | 0.000478 |\n",
      "|    ent_coef_loss   | -0.666   |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8570     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 69.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1756     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2738     |\n",
      "|    total_timesteps | 1756000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1757000, episode_reward=-413.00 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -413     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1757000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1758000, episode_reward=-34.37 +/- 483.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -34.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1758000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.000528 |\n",
      "|    ent_coef        | 0.000473 |\n",
      "|    ent_coef_loss   | -17.5    |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1759000, episode_reward=-219.97 +/- 433.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -220     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1759000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=-209.46 +/- 293.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -209     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000769 |\n",
      "|    ent_coef        | 0.000467 |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8590     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 70.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1760     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2744     |\n",
      "|    total_timesteps | 1760000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1761000, episode_reward=52.20 +/- 397.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 52.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1761000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1762000, episode_reward=436.11 +/- 426.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 436      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1762000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000654 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | -19.5    |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1763000, episode_reward=431.44 +/- 423.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1763000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1764000, episode_reward=5.40 +/- 428.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1764000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.000873 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | -7.03    |\n",
      "|    learning_rate   | 0.00324  |\n",
      "|    n_updates       | 8610     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 74.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1764     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2750     |\n",
      "|    total_timesteps | 1764000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1765000, episode_reward=-170.26 +/- 358.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -170     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1765000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1766000, episode_reward=294.14 +/- 586.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 294      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1766000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.17    |\n",
      "|    critic_loss     | 0.000572 |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | -6.27    |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1767000, episode_reward=523.71 +/- 473.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 524      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1767000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1768000, episode_reward=444.99 +/- 423.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 445      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1768000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.19    |\n",
      "|    critic_loss     | 0.00059  |\n",
      "|    ent_coef        | 0.000439 |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8630     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 70.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1768     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2756     |\n",
      "|    total_timesteps | 1768000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1769000, episode_reward=17.78 +/- 499.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 17.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1769000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=601.21 +/- 33.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 601      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.00092  |\n",
      "|    ent_coef        | 0.000437 |\n",
      "|    ent_coef_loss   | -8.44    |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1771000, episode_reward=557.03 +/- 17.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 557      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1771000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1772000, episode_reward=683.28 +/- 52.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 683      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1772000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000869 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -15.6    |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8650     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 83.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1772     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2763     |\n",
      "|    total_timesteps | 1772000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1773000, episode_reward=665.99 +/- 16.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 666      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1773000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774000, episode_reward=591.49 +/- 458.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 591      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1774000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000783 |\n",
      "|    ent_coef        | 0.000426 |\n",
      "|    ent_coef_loss   | -9.47    |\n",
      "|    learning_rate   | 0.00323  |\n",
      "|    n_updates       | 8660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1775000, episode_reward=605.13 +/- 466.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 605      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1775000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1776000, episode_reward=102.13 +/- 535.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 102      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1776000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.000841 |\n",
      "|    ent_coef        | 0.00042  |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8670     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 115      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1776     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2769     |\n",
      "|    total_timesteps | 1776000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1777000, episode_reward=341.18 +/- 541.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 341      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1777000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1778000, episode_reward=-322.81 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -323     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1778000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.000695 |\n",
      "|    ent_coef        | 0.000414 |\n",
      "|    ent_coef_loss   | -8.67    |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1779000, episode_reward=-323.72 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -324     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1779000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=-323.29 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -323     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000969 |\n",
      "|    ent_coef        | 0.000409 |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8690     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 139      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1780     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2775     |\n",
      "|    total_timesteps | 1780000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1781000, episode_reward=-323.01 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -323     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1781000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1782000, episode_reward=-132.13 +/- 410.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1782000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000998 |\n",
      "|    ent_coef        | 0.000407 |\n",
      "|    ent_coef_loss   | -7.39    |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1783000, episode_reward=73.92 +/- 498.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 73.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1783000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1784000, episode_reward=-118.48 +/- 443.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1784000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.000879 |\n",
      "|    ent_coef        | 0.000405 |\n",
      "|    ent_coef_loss   | -0.749   |\n",
      "|    learning_rate   | 0.00322  |\n",
      "|    n_updates       | 8710     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 170      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1784     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2781     |\n",
      "|    total_timesteps | 1784000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1785000, episode_reward=-117.73 +/- 445.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1785000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1786000, episode_reward=466.03 +/- 416.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 466      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1786000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.000799 |\n",
      "|    ent_coef        | 0.000403 |\n",
      "|    ent_coef_loss   | -8.12    |\n",
      "|    learning_rate   | 0.00321  |\n",
      "|    n_updates       | 8720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1787000, episode_reward=-160.34 +/- 385.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1787000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1788000, episode_reward=-26.82 +/- 432.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -26.8     |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1788000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.29     |\n",
      "|    critic_loss     | 0.000902  |\n",
      "|    ent_coef        | 0.0004    |\n",
      "|    ent_coef_loss   | -0.000119 |\n",
      "|    learning_rate   | 0.00321   |\n",
      "|    n_updates       | 8730      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 201      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1788     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2787     |\n",
      "|    total_timesteps | 1788000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1789000, episode_reward=-22.75 +/- 433.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1789000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=-381.59 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.00073  |\n",
      "|    ent_coef        | 0.000399 |\n",
      "|    ent_coef_loss   | -3.94    |\n",
      "|    learning_rate   | 0.00321  |\n",
      "|    n_updates       | 8740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1791000, episode_reward=-173.18 +/- 415.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -173     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1791000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1792000, episode_reward=-382.43 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1792000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 231      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1792     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2793     |\n",
      "|    total_timesteps | 1792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1793000, episode_reward=-391.95 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -392     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1793000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000666 |\n",
      "|    ent_coef        | 0.000397 |\n",
      "|    ent_coef_loss   | -3.86    |\n",
      "|    learning_rate   | 0.00321  |\n",
      "|    n_updates       | 8750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794000, episode_reward=-392.96 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -393     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1794000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1795000, episode_reward=583.34 +/- 17.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 583      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1795000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000883 |\n",
      "|    ent_coef        | 0.000395 |\n",
      "|    ent_coef_loss   | -3.52    |\n",
      "|    learning_rate   | 0.00321  |\n",
      "|    n_updates       | 8760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1796000, episode_reward=398.57 +/- 387.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1796000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 264      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1796     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2799     |\n",
      "|    total_timesteps | 1796000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1797000, episode_reward=616.53 +/- 62.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 617      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1797000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000914 |\n",
      "|    ent_coef        | 0.000393 |\n",
      "|    ent_coef_loss   | -6.32    |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1798000, episode_reward=631.50 +/- 27.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 632      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1798000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1799000, episode_reward=646.79 +/- 21.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 647      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1799000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000873 |\n",
      "|    ent_coef        | 0.00039  |\n",
      "|    ent_coef_loss   | -3.4     |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=642.39 +/- 18.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 642      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 293      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1800     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2806     |\n",
      "|    total_timesteps | 1800000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1801000, episode_reward=700.84 +/- 20.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 701      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1801000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000977 |\n",
      "|    ent_coef        | 0.000388 |\n",
      "|    ent_coef_loss   | -2.32    |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1802000, episode_reward=671.73 +/- 49.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 672      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1802000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1803000, episode_reward=803.79 +/- 40.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 804      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1803000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000979 |\n",
      "|    ent_coef        | 0.000387 |\n",
      "|    ent_coef_loss   | -4.7     |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8800     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1804000, episode_reward=821.17 +/- 50.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 821      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1804000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 327      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1804     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2812     |\n",
      "|    total_timesteps | 1804000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1805000, episode_reward=538.33 +/- 461.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 538      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1805000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.000876 |\n",
      "|    ent_coef        | 0.000384 |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.0032   |\n",
      "|    n_updates       | 8810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1806000, episode_reward=791.70 +/- 40.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 792      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1806000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1807000, episode_reward=-395.95 +/- 1.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -396     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1807000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000931 |\n",
      "|    ent_coef        | 0.00038  |\n",
      "|    ent_coef_loss   | -3.82    |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8820     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1808000, episode_reward=-395.54 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -396     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1808000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 359      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1808     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2818     |\n",
      "|    total_timesteps | 1808000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1809000, episode_reward=-391.73 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -392     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1809000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3     |\n",
      "|    critic_loss     | 0.000796 |\n",
      "|    ent_coef        | 0.000377 |\n",
      "|    ent_coef_loss   | -2.47    |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=-393.13 +/- 1.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -393     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1810000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1811000, episode_reward=-158.31 +/- 453.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -158     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1811000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000684 |\n",
      "|    ent_coef        | 0.000375 |\n",
      "|    ent_coef_loss   | -6.3     |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8840     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1812000, episode_reward=-160.48 +/- 453.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1812000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 395      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1812     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2824     |\n",
      "|    total_timesteps | 1812000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1813000, episode_reward=-389.91 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -390     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1813000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000707 |\n",
      "|    ent_coef        | 0.000373 |\n",
      "|    ent_coef_loss   | -4.92    |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1814000, episode_reward=-155.39 +/- 470.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1814000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1815000, episode_reward=-395.74 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -396     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1815000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000773 |\n",
      "|    ent_coef        | 0.00037  |\n",
      "|    ent_coef_loss   | -5.44    |\n",
      "|    learning_rate   | 0.00319  |\n",
      "|    n_updates       | 8860     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1816000, episode_reward=-394.64 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -395     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1816000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 421      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1816     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2830     |\n",
      "|    total_timesteps | 1816000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1817000, episode_reward=-407.27 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -407     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1817000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000657 |\n",
      "|    ent_coef        | 0.000367 |\n",
      "|    ent_coef_loss   | -3.53    |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1818000, episode_reward=-406.45 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -406     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1818000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1819000, episode_reward=-404.67 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -405     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1819000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000712 |\n",
      "|    ent_coef        | 0.000365 |\n",
      "|    ent_coef_loss   | -3.77    |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8880     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=70.07 +/- 581.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 70.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1820000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 449      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1820     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2836     |\n",
      "|    total_timesteps | 1820000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1821000, episode_reward=322.45 +/- 596.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 322      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1821000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000864 |\n",
      "|    ent_coef        | 0.000363 |\n",
      "|    ent_coef_loss   | -2.86    |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1822000, episode_reward=-167.08 +/- 480.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -167     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1822000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1823000, episode_reward=790.05 +/- 21.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 790      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1823000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000794 |\n",
      "|    ent_coef        | 0.000362 |\n",
      "|    ent_coef_loss   | -3.51    |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8900     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1824000, episode_reward=783.20 +/- 28.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 783      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1824000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 462      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1824     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2843     |\n",
      "|    total_timesteps | 1824000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1825000, episode_reward=758.31 +/- 29.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 758      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1825000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000763 |\n",
      "|    ent_coef        | 0.00036  |\n",
      "|    ent_coef_loss   | 0.979    |\n",
      "|    learning_rate   | 0.00318  |\n",
      "|    n_updates       | 8910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1826000, episode_reward=710.63 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 711      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1826000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1827000, episode_reward=47.49 +/- 552.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 47.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1827000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000727 |\n",
      "|    ent_coef        | 0.00036  |\n",
      "|    ent_coef_loss   | 2.72     |\n",
      "|    learning_rate   | 0.00317  |\n",
      "|    n_updates       | 8920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1828000, episode_reward=50.52 +/- 556.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 50.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1828000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 486      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1828     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2849     |\n",
      "|    total_timesteps | 1828000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1829000, episode_reward=-402.98 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1829000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000764 |\n",
      "|    ent_coef        | 0.00036  |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 0.00317  |\n",
      "|    n_updates       | 8930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=-165.23 +/- 478.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -165     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1830000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1831000, episode_reward=-423.74 +/- 1.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -424     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1831000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000686 |\n",
      "|    ent_coef        | 0.00036  |\n",
      "|    ent_coef_loss   | -2.91    |\n",
      "|    learning_rate   | 0.00317  |\n",
      "|    n_updates       | 8940     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1832000, episode_reward=-426.11 +/- 1.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -426     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1832000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 500      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1832     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2855     |\n",
      "|    total_timesteps | 1832000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1833000, episode_reward=-412.45 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -412     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1833000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000804 |\n",
      "|    ent_coef        | 0.000359 |\n",
      "|    ent_coef_loss   | 0.825    |\n",
      "|    learning_rate   | 0.00317  |\n",
      "|    n_updates       | 8950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834000, episode_reward=-412.24 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -412     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1834000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1835000, episode_reward=-412.20 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -412     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1835000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1836000, episode_reward=706.59 +/- 30.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 707      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1836000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.000764 |\n",
      "|    ent_coef        | 0.000359 |\n",
      "|    ent_coef_loss   | 3.08     |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 8960     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 528      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1836     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2861     |\n",
      "|    total_timesteps | 1836000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1837000, episode_reward=707.89 +/- 23.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 708      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1837000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1838000, episode_reward=538.78 +/- 447.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 539      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1838000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000682 |\n",
      "|    ent_coef        | 0.00036  |\n",
      "|    ent_coef_loss   | -0.77    |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 8970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1839000, episode_reward=758.66 +/- 60.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 759      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1839000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=663.08 +/- 44.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 663      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.0009   |\n",
      "|    ent_coef        | 0.00036  |\n",
      "|    ent_coef_loss   | -2.82    |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 8980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 549      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1840     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2867     |\n",
      "|    total_timesteps | 1840000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1841000, episode_reward=713.67 +/- 24.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 714      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1841000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1842000, episode_reward=838.79 +/- 27.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 839      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1842000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000709 |\n",
      "|    ent_coef        | 0.000359 |\n",
      "|    ent_coef_loss   | -0.806   |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 8990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1843000, episode_reward=858.84 +/- 30.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 859      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1843000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1844000, episode_reward=766.40 +/- 26.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 766      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1844000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.000895 |\n",
      "|    ent_coef        | 0.000358 |\n",
      "|    ent_coef_loss   | 1.12     |\n",
      "|    learning_rate   | 0.00316  |\n",
      "|    n_updates       | 9000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 570      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1844     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2873     |\n",
      "|    total_timesteps | 1844000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1845000, episode_reward=783.85 +/- 13.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 784      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1845000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1846000, episode_reward=808.10 +/- 24.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 808      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1846000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000809 |\n",
      "|    ent_coef        | 0.000359 |\n",
      "|    ent_coef_loss   | 0.212    |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9010     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1847000, episode_reward=786.35 +/- 25.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 786      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1847000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1848000, episode_reward=896.06 +/- 20.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 896      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1848000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000754 |\n",
      "|    ent_coef        | 0.000358 |\n",
      "|    ent_coef_loss   | -3.98    |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9020     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 580      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1848     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2879     |\n",
      "|    total_timesteps | 1848000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1849000, episode_reward=898.57 +/- 19.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 899      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1849000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1850000, episode_reward=719.28 +/- 54.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 719      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 0.000771 |\n",
      "|    ent_coef        | 0.000357 |\n",
      "|    ent_coef_loss   | -7.06    |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9030     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1851000, episode_reward=755.01 +/- 31.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 755      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1851000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1852000, episode_reward=641.29 +/- 29.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 641      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1852000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000662 |\n",
      "|    ent_coef        | 0.000355 |\n",
      "|    ent_coef_loss   | 8.45     |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9040     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 594      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1852     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2886     |\n",
      "|    total_timesteps | 1852000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1853000, episode_reward=668.89 +/- 50.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 669      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1853000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854000, episode_reward=630.22 +/- 22.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 630      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1854000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000835 |\n",
      "|    ent_coef        | 0.000356 |\n",
      "|    ent_coef_loss   | 0.243    |\n",
      "|    learning_rate   | 0.00315  |\n",
      "|    n_updates       | 9050     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1855000, episode_reward=674.97 +/- 38.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 675      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1855000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1856000, episode_reward=790.46 +/- 22.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 790      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1856000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.000732 |\n",
      "|    ent_coef        | 0.000357 |\n",
      "|    ent_coef_loss   | -2.57    |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9060     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 618      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1856     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2892     |\n",
      "|    total_timesteps | 1856000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1857000, episode_reward=791.27 +/- 24.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 791      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1857000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1858000, episode_reward=776.29 +/- 25.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 776      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1858000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000759 |\n",
      "|    ent_coef        | 0.000356 |\n",
      "|    ent_coef_loss   | -2.86    |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9070     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1859000, episode_reward=764.32 +/- 23.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 764      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1859000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=743.72 +/- 45.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 744      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000765 |\n",
      "|    ent_coef        | 0.000355 |\n",
      "|    ent_coef_loss   | -0.804   |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9080     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 632      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1860     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2898     |\n",
      "|    total_timesteps | 1860000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1861000, episode_reward=748.19 +/- 23.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 748      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1861000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1862000, episode_reward=865.08 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 865      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1862000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.000833 |\n",
      "|    ent_coef        | 0.000354 |\n",
      "|    ent_coef_loss   | -2.16    |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9090     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1863000, episode_reward=849.32 +/- 24.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 849      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1863000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1864000, episode_reward=825.11 +/- 15.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 825      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1864000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.27    |\n",
      "|    critic_loss     | 0.000762 |\n",
      "|    ent_coef        | 0.000353 |\n",
      "|    ent_coef_loss   | -4.77    |\n",
      "|    learning_rate   | 0.00314  |\n",
      "|    n_updates       | 9100     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 642      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1864     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2904     |\n",
      "|    total_timesteps | 1864000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1865000, episode_reward=856.01 +/- 27.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 856      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1865000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1866000, episode_reward=785.54 +/- 60.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 786      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1866000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000666 |\n",
      "|    ent_coef        | 0.000351 |\n",
      "|    ent_coef_loss   | -7.16    |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9110     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1867000, episode_reward=762.26 +/- 62.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 762      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1867000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1868000, episode_reward=794.25 +/- 43.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 794      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1868000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.000857 |\n",
      "|    ent_coef        | 0.000348 |\n",
      "|    ent_coef_loss   | -2.38    |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9120     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 667      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1868     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2910     |\n",
      "|    total_timesteps | 1868000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1869000, episode_reward=789.99 +/- 24.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 790      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1869000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=811.35 +/- 40.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 811      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000901 |\n",
      "|    ent_coef        | 0.000347 |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9130     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1871000, episode_reward=798.20 +/- 32.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 798      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1871000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1872000, episode_reward=809.18 +/- 27.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 809      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1872000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000939 |\n",
      "|    ent_coef        | 0.000346 |\n",
      "|    ent_coef_loss   | 0.623    |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9140     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 675      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1872     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2916     |\n",
      "|    total_timesteps | 1872000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1873000, episode_reward=810.91 +/- 27.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 811      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1873000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874000, episode_reward=821.72 +/- 35.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 822      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1874000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.000893 |\n",
      "|    ent_coef        | 0.000346 |\n",
      "|    ent_coef_loss   | -3.99    |\n",
      "|    learning_rate   | 0.00313  |\n",
      "|    n_updates       | 9150     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1875000, episode_reward=808.99 +/- 52.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 809      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1875000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1876000, episode_reward=865.38 +/- 52.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 865      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1876000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.00077  |\n",
      "|    ent_coef        | 0.000344 |\n",
      "|    ent_coef_loss   | -8.55    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9160     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 678      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1876     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2922     |\n",
      "|    total_timesteps | 1876000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1877000, episode_reward=850.57 +/- 17.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 851      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1877000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1878000, episode_reward=883.23 +/- 20.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 883      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1878000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1879000, episode_reward=741.20 +/- 35.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 741      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1879000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.00091  |\n",
      "|    ent_coef        | 0.000341 |\n",
      "|    ent_coef_loss   | -2.55    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9170     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=367.46 +/- 517.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 367      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1880000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 688      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1880     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2928     |\n",
      "|    total_timesteps | 1880000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1881000, episode_reward=780.50 +/- 25.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 781      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1881000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.000934 |\n",
      "|    ent_coef        | 0.000339 |\n",
      "|    ent_coef_loss   | -2.37    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9180     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1882000, episode_reward=775.49 +/- 23.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 775      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1882000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1883000, episode_reward=-298.64 +/- 2.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -299     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1883000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.000742 |\n",
      "|    ent_coef        | 0.000338 |\n",
      "|    ent_coef_loss   | -4.75    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9190     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1884000, episode_reward=-299.45 +/- 3.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -299     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1884000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 690      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1884     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2934     |\n",
      "|    total_timesteps | 1884000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1885000, episode_reward=738.79 +/- 15.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 739      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1885000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.000613 |\n",
      "|    ent_coef        | 0.000335 |\n",
      "|    ent_coef_loss   | -8.14    |\n",
      "|    learning_rate   | 0.00312  |\n",
      "|    n_updates       | 9200     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1886000, episode_reward=735.69 +/- 34.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 736      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1886000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1887000, episode_reward=742.63 +/- 7.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 743      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1887000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.000927 |\n",
      "|    ent_coef        | 0.000333 |\n",
      "|    ent_coef_loss   | 2.58     |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9210     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1888000, episode_reward=781.45 +/- 30.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 781      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1888000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 690      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1888     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2941     |\n",
      "|    total_timesteps | 1888000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1889000, episode_reward=702.94 +/- 42.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 703      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1889000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.000884 |\n",
      "|    ent_coef        | 0.000332 |\n",
      "|    ent_coef_loss   | 3        |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9220     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=709.35 +/- 36.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 709      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1890000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1891000, episode_reward=773.60 +/- 18.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 774      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1891000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.000719 |\n",
      "|    ent_coef        | 0.000333 |\n",
      "|    ent_coef_loss   | -1.62    |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9230     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1892000, episode_reward=805.24 +/- 39.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 805      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1892000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 696      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1892     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2947     |\n",
      "|    total_timesteps | 1892000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1893000, episode_reward=667.85 +/- 30.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 668      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1893000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.000804 |\n",
      "|    ent_coef        | 0.000333 |\n",
      "|    ent_coef_loss   | -3.16    |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9240     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1894000, episode_reward=650.64 +/- 50.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 651      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1894000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1895000, episode_reward=782.39 +/- 18.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 782      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1895000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.22    |\n",
      "|    critic_loss     | 0.000871 |\n",
      "|    ent_coef        | 0.000332 |\n",
      "|    ent_coef_loss   | -0.143   |\n",
      "|    learning_rate   | 0.00311  |\n",
      "|    n_updates       | 9250     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1896000, episode_reward=749.83 +/- 32.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 750      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1896000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 701      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1896     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 2953     |\n",
      "|    total_timesteps | 1896000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1897000, episode_reward=807.45 +/- 31.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 807      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1897000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.000842 |\n",
      "|    ent_coef        | 0.000331 |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9260     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1898000, episode_reward=798.85 +/- 25.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 799      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1898000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1899000, episode_reward=849.22 +/- 40.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 849      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1899000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.000951 |\n",
      "|    ent_coef        | 0.00033  |\n",
      "|    ent_coef_loss   | -4.49    |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9270     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=810.56 +/- 38.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 811      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 707      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1900     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 2959     |\n",
      "|    total_timesteps | 1900000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1901000, episode_reward=764.72 +/- 38.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 765      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1901000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.000853 |\n",
      "|    ent_coef        | 0.000328 |\n",
      "|    ent_coef_loss   | -5.21    |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9280     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1902000, episode_reward=756.01 +/- 28.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 756      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1902000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1903000, episode_reward=721.36 +/- 16.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 721      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1903000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.000755 |\n",
      "|    ent_coef        | 0.000326 |\n",
      "|    ent_coef_loss   | -2.95    |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9290     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1904000, episode_reward=684.02 +/- 29.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 684      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1904000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 706      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1904     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 2965     |\n",
      "|    total_timesteps | 1904000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1905000, episode_reward=785.82 +/- 17.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 786      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1905000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.22    |\n",
      "|    critic_loss     | 0.000877 |\n",
      "|    ent_coef        | 0.000324 |\n",
      "|    ent_coef_loss   | 2.85     |\n",
      "|    learning_rate   | 0.0031   |\n",
      "|    n_updates       | 9300     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1906000, episode_reward=801.28 +/- 16.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 801      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1906000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1907000, episode_reward=635.00 +/- 32.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 635      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1907000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.000781 |\n",
      "|    ent_coef        | 0.000325 |\n",
      "|    ent_coef_loss   | 6.09     |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9310     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1908000, episode_reward=615.69 +/- 68.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 616      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1908000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 706      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1908     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 2971     |\n",
      "|    total_timesteps | 1908000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1909000, episode_reward=825.64 +/- 36.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 826      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1909000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2     |\n",
      "|    critic_loss     | 0.000898 |\n",
      "|    ent_coef        | 0.000327 |\n",
      "|    ent_coef_loss   | 5.3      |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9320     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=800.55 +/- 19.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 801      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1910000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1911000, episode_reward=923.33 +/- 327.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 923      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1911000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.000879 |\n",
      "|    ent_coef        | 0.000329 |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9330     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1912000, episode_reward=1254.60 +/- 400.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1912000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 706      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1912     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 2977     |\n",
      "|    total_timesteps | 1912000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1913000, episode_reward=589.93 +/- 10.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 590      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1913000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.00364  |\n",
      "|    ent_coef        | 0.000332 |\n",
      "|    ent_coef_loss   | 22.8     |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9340     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1914000, episode_reward=594.81 +/- 18.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 595      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1914000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915000, episode_reward=-235.57 +/- 4.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1915000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2     |\n",
      "|    critic_loss     | 0.00242  |\n",
      "|    ent_coef        | 0.000341 |\n",
      "|    ent_coef_loss   | 36.1     |\n",
      "|    learning_rate   | 0.00309  |\n",
      "|    n_updates       | 9350     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1916000, episode_reward=-238.91 +/- 3.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -239     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1916000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 710      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1916     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 2984     |\n",
      "|    total_timesteps | 1916000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1917000, episode_reward=279.71 +/- 412.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1917000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2     |\n",
      "|    critic_loss     | 0.0011   |\n",
      "|    ent_coef        | 0.000354 |\n",
      "|    ent_coef_loss   | 4.81     |\n",
      "|    learning_rate   | 0.00308  |\n",
      "|    n_updates       | 9360     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1918000, episode_reward=142.51 +/- 395.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 143      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1918000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1919000, episode_reward=-415.11 +/- 55.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -415     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1919000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.21    |\n",
      "|    critic_loss     | 0.00126  |\n",
      "|    ent_coef        | 0.000363 |\n",
      "|    ent_coef_loss   | 17.6     |\n",
      "|    learning_rate   | 0.00308  |\n",
      "|    n_updates       | 9370     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=-387.11 +/- 68.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -387     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1920000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 697      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1920     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 2990     |\n",
      "|    total_timesteps | 1920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1921000, episode_reward=-442.72 +/- 0.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -443     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1921000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1922000, episode_reward=-374.79 +/- 3.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -375     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1922000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000293 |\n",
      "|    ent_coef        | 0.000373 |\n",
      "|    ent_coef_loss   | 25.7     |\n",
      "|    learning_rate   | 0.00308  |\n",
      "|    n_updates       | 9380     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1923000, episode_reward=-374.46 +/- 3.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -374     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1923000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1924000, episode_reward=-403.12 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1924000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.13    |\n",
      "|    critic_loss     | 0.00295  |\n",
      "|    ent_coef        | 0.000386 |\n",
      "|    ent_coef_loss   | 17.7     |\n",
      "|    learning_rate   | 0.00308  |\n",
      "|    n_updates       | 9390     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 665      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1924     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 2996     |\n",
      "|    total_timesteps | 1924000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1925000, episode_reward=-403.94 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -404     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1925000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1926000, episode_reward=-503.59 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -504     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1926000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000233 |\n",
      "|    ent_coef        | 0.000398 |\n",
      "|    ent_coef_loss   | 25.2     |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9400     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1927000, episode_reward=-504.47 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -504     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1927000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1928000, episode_reward=509.86 +/- 49.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 510      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1928000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000235 |\n",
      "|    ent_coef        | 0.000415 |\n",
      "|    ent_coef_loss   | 28.6     |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9410     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 626      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1928     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3003     |\n",
      "|    total_timesteps | 1928000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1929000, episode_reward=501.54 +/- 33.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 502      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1929000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=677.22 +/- 21.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 677      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.18    |\n",
      "|    critic_loss     | 0.0236   |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 21.3     |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9420     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1931000, episode_reward=672.91 +/- 42.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 673      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1931000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1932000, episode_reward=-405.38 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -405     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1932000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.15    |\n",
      "|    critic_loss     | 0.0062   |\n",
      "|    ent_coef        | 0.000448 |\n",
      "|    ent_coef_loss   | 8.69     |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9430     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 619      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1932     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3009     |\n",
      "|    total_timesteps | 1932000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1933000, episode_reward=-404.18 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -404     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1933000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1934000, episode_reward=-266.24 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -266     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1934000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.04    |\n",
      "|    critic_loss     | 0.00124  |\n",
      "|    ent_coef        | 0.000461 |\n",
      "|    ent_coef_loss   | 25.2     |\n",
      "|    learning_rate   | 0.00307  |\n",
      "|    n_updates       | 9440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1935000, episode_reward=-266.66 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1935000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1936000, episode_reward=-305.81 +/- 1.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -306     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1936000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.07    |\n",
      "|    critic_loss     | 0.00168  |\n",
      "|    ent_coef        | 0.000472 |\n",
      "|    ent_coef_loss   | -0.763   |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9450     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 591      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1936     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3015     |\n",
      "|    total_timesteps | 1936000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1937000, episode_reward=-304.45 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -304     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1937000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1938000, episode_reward=-584.41 +/- 0.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -584     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1938000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.09    |\n",
      "|    critic_loss     | 0.00168  |\n",
      "|    ent_coef        | 0.000482 |\n",
      "|    ent_coef_loss   | 64.6     |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9460     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1939000, episode_reward=-584.27 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -584     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1939000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=-475.94 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -476     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000155 |\n",
      "|    ent_coef        | 0.000521 |\n",
      "|    ent_coef_loss   | 56.7     |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9470     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 552      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1940     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3021     |\n",
      "|    total_timesteps | 1940000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1941000, episode_reward=-476.53 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -477     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1941000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1942000, episode_reward=-456.29 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -456     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1942000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.04    |\n",
      "|    critic_loss     | 0.00024  |\n",
      "|    ent_coef        | 0.000557 |\n",
      "|    ent_coef_loss   | 17.3     |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1943000, episode_reward=-457.02 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -457     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1943000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1944000, episode_reward=-479.97 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -480     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1944000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000163 |\n",
      "|    ent_coef        | 0.000578 |\n",
      "|    ent_coef_loss   | 22.8     |\n",
      "|    learning_rate   | 0.00306  |\n",
      "|    n_updates       | 9490     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 505      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1944     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3027     |\n",
      "|    total_timesteps | 1944000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1945000, episode_reward=-480.26 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -480     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1945000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1946000, episode_reward=-489.40 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -489     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1946000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.04    |\n",
      "|    critic_loss     | 9.98e-05 |\n",
      "|    ent_coef        | 0.0006   |\n",
      "|    ent_coef_loss   | 39.3     |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1947000, episode_reward=-489.97 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -490     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1947000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1948000, episode_reward=-542.51 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -543     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1948000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000101 |\n",
      "|    ent_coef        | 0.000628 |\n",
      "|    ent_coef_loss   | 37.5     |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9510     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 459      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1948     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3034     |\n",
      "|    total_timesteps | 1948000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1949000, episode_reward=-542.49 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -542     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1949000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=-512.87 +/- 1.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -513     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 5.58e-05 |\n",
      "|    ent_coef        | 0.00066  |\n",
      "|    ent_coef_loss   | 21.5     |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9520     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1951000, episode_reward=-513.49 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -513     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1951000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1952000, episode_reward=-487.84 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -488     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1952000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 4.14e-05 |\n",
      "|    ent_coef        | 0.000686 |\n",
      "|    ent_coef_loss   | 14.9     |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9530     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 409      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1952     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3040     |\n",
      "|    total_timesteps | 1952000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1953000, episode_reward=-487.46 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -487     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1953000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1954000, episode_reward=-462.03 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -462     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1954000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 5.69e-05 |\n",
      "|    ent_coef        | 0.000705 |\n",
      "|    ent_coef_loss   | 14       |\n",
      "|    learning_rate   | 0.00305  |\n",
      "|    n_updates       | 9540     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955000, episode_reward=-461.31 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -461     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1955000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1956000, episode_reward=-460.89 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -461     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1956000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 5.76e-05 |\n",
      "|    ent_coef        | 0.00072  |\n",
      "|    ent_coef_loss   | 13.9     |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9550     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 366      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1956     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3046     |\n",
      "|    total_timesteps | 1956000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1957000, episode_reward=-461.12 +/- 1.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -461     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1957000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1958000, episode_reward=-487.18 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -487     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1958000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 5.24e-05 |\n",
      "|    ent_coef        | 0.000736 |\n",
      "|    ent_coef_loss   | 14.5     |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9560     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1959000, episode_reward=-486.34 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -486     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1959000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=-498.32 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -498     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 3.63e-05 |\n",
      "|    ent_coef        | 0.000752 |\n",
      "|    ent_coef_loss   | 16.2     |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9570     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 320      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1960     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3052     |\n",
      "|    total_timesteps | 1960000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1961000, episode_reward=-498.35 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -498     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1961000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1962000, episode_reward=-497.39 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -497     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1962000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 3.39e-05 |\n",
      "|    ent_coef        | 0.000769 |\n",
      "|    ent_coef_loss   | 12.8     |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9580     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1963000, episode_reward=-497.31 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -497     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1963000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1964000, episode_reward=-497.49 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -497     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1964000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 272      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1964     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3058     |\n",
      "|    total_timesteps | 1964000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1965000, episode_reward=-479.74 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -480     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1965000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 4.23e-05 |\n",
      "|    ent_coef        | 0.000784 |\n",
      "|    ent_coef_loss   | 8.7      |\n",
      "|    learning_rate   | 0.00304  |\n",
      "|    n_updates       | 9590     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1966000, episode_reward=-479.82 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -480     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1966000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1967000, episode_reward=-468.45 +/- 0.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -468     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1967000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 4e-05    |\n",
      "|    ent_coef        | 0.000796 |\n",
      "|    ent_coef_loss   | 5.59     |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9600     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1968000, episode_reward=-467.55 +/- 1.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -468     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1968000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 226      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1968     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3064     |\n",
      "|    total_timesteps | 1968000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1969000, episode_reward=-460.27 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -460     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1969000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 4.21e-05 |\n",
      "|    ent_coef        | 0.000804 |\n",
      "|    ent_coef_loss   | 2.49     |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9610     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=-459.87 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -460     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1970000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1971000, episode_reward=-448.16 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -448     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1971000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 4.63e-05 |\n",
      "|    ent_coef        | 0.000809 |\n",
      "|    ent_coef_loss   | 1.07     |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9620     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1972000, episode_reward=-448.05 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -448     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1972000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 179      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1972     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3071     |\n",
      "|    total_timesteps | 1972000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1973000, episode_reward=-437.99 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -438     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1973000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 5.68e-05 |\n",
      "|    ent_coef        | 0.000811 |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9630     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1974000, episode_reward=-437.66 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -438     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1974000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1975000, episode_reward=-420.43 +/- 1.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -420     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1975000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 6.21e-05 |\n",
      "|    ent_coef        | 0.000811 |\n",
      "|    ent_coef_loss   | -4.59    |\n",
      "|    learning_rate   | 0.00303  |\n",
      "|    n_updates       | 9640     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1976000, episode_reward=-421.05 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -421     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1976000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 135      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1976     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3077     |\n",
      "|    total_timesteps | 1976000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1977000, episode_reward=-376.42 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1977000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 7.14e-05 |\n",
      "|    ent_coef        | 0.000807 |\n",
      "|    ent_coef_loss   | -8.74    |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9650     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1978000, episode_reward=-375.57 +/- 0.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1978000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1979000, episode_reward=-363.32 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -363     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1979000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 9.62e-05 |\n",
      "|    ent_coef        | 0.000799 |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9660     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=-363.34 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -363     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1980000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 93.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1980     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3083     |\n",
      "|    total_timesteps | 1980000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1981000, episode_reward=-375.98 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1981000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 9.58e-05 |\n",
      "|    ent_coef        | 0.000788 |\n",
      "|    ent_coef_loss   | -9.92    |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9670     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1982000, episode_reward=-374.96 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -375     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1982000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1983000, episode_reward=-367.61 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -368     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1983000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 8.39e-05 |\n",
      "|    ent_coef        | 0.000776 |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1984000, episode_reward=-366.79 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -367     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1984000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 56.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1984     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3090     |\n",
      "|    total_timesteps | 1984000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1985000, episode_reward=-329.54 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1985000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 9.71e-05 |\n",
      "|    ent_coef        | 0.000765 |\n",
      "|    ent_coef_loss   | -8.43    |\n",
      "|    learning_rate   | 0.00302  |\n",
      "|    n_updates       | 9690     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1986000, episode_reward=-329.73 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1986000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1987000, episode_reward=-315.34 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1987000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 0.000103 |\n",
      "|    ent_coef        | 0.000754 |\n",
      "|    ent_coef_loss   | -9.25    |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9700     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1988000, episode_reward=-315.39 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1988000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 19.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1988     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3096     |\n",
      "|    total_timesteps | 1988000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1989000, episode_reward=-376.08 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1989000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 8.9e-05  |\n",
      "|    ent_coef        | 0.000744 |\n",
      "|    ent_coef_loss   | -8.57    |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9710     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=-375.85 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1990000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1991000, episode_reward=-360.09 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -360     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1991000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 8.82e-05 |\n",
      "|    ent_coef        | 0.000734 |\n",
      "|    ent_coef_loss   | -9.02    |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9720     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1992000, episode_reward=-359.84 +/- 2.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -360     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1992000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -15.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1992     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3102     |\n",
      "|    total_timesteps | 1992000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1993000, episode_reward=-363.39 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -363     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1993000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 0.000101 |\n",
      "|    ent_coef        | 0.000724 |\n",
      "|    ent_coef_loss   | -6.68    |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9730     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1994000, episode_reward=-364.26 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -364     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1994000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995000, episode_reward=-354.04 +/- 2.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -354     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1995000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 8.78e-05 |\n",
      "|    ent_coef        | 0.000715 |\n",
      "|    ent_coef_loss   | -7.94    |\n",
      "|    learning_rate   | 0.00301  |\n",
      "|    n_updates       | 9740     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1996000, episode_reward=-354.55 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -355     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1996000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -49.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1996     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3108     |\n",
      "|    total_timesteps | 1996000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1997000, episode_reward=-241.86 +/- 2.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1997000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 0.000102 |\n",
      "|    ent_coef        | 0.000706 |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9750     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1998000, episode_reward=-242.09 +/- 2.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1998000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1999000, episode_reward=-183.19 +/- 6.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1999000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.00012  |\n",
      "|    ent_coef        | 0.000696 |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9760     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=-185.62 +/- 3.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -186     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -80.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2000     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3114     |\n",
      "|    total_timesteps | 2000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2001000, episode_reward=88.19 +/- 22.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 88.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2001000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.996   |\n",
      "|    critic_loss     | 0.000244 |\n",
      "|    ent_coef        | 0.000682 |\n",
      "|    ent_coef_loss   | -22.1    |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9770     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2002000, episode_reward=106.28 +/- 34.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 106      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2002000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2003000, episode_reward=-310.87 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -311     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2003000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.966   |\n",
      "|    critic_loss     | 0.000355 |\n",
      "|    ent_coef        | 0.000663 |\n",
      "|    ent_coef_loss   | -17      |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9780     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2004000, episode_reward=-309.17 +/- 1.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -309     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2004000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -113     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2004     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3121     |\n",
      "|    total_timesteps | 2004000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2005000, episode_reward=-403.02 +/- 13.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -403     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2005000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.957   |\n",
      "|    critic_loss     | 0.000498 |\n",
      "|    ent_coef        | 0.000646 |\n",
      "|    ent_coef_loss   | -17      |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 9790     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2006000, episode_reward=-404.47 +/- 15.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -404     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2006000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2007000, episode_reward=-410.60 +/- 2.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -411     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2007000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2008000, episode_reward=-476.34 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -476     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2008000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.959   |\n",
      "|    critic_loss     | 0.00035  |\n",
      "|    ent_coef        | 0.000631 |\n",
      "|    ent_coef_loss   | -3.14    |\n",
      "|    learning_rate   | 0.00299  |\n",
      "|    n_updates       | 9800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -156     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2008     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3127     |\n",
      "|    total_timesteps | 2008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2009000, episode_reward=-475.54 +/- 2.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -476     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2009000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=-446.70 +/- 2.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -447     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.962   |\n",
      "|    critic_loss     | 0.000103 |\n",
      "|    ent_coef        | 0.000623 |\n",
      "|    ent_coef_loss   | -4.41    |\n",
      "|    learning_rate   | 0.00299  |\n",
      "|    n_updates       | 9810     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2011000, episode_reward=-446.02 +/- 1.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -446     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2011000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2012000, episode_reward=-358.39 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -358     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2012000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.956   |\n",
      "|    critic_loss     | 9.69e-05 |\n",
      "|    ent_coef        | 0.000616 |\n",
      "|    ent_coef_loss   | -19.4    |\n",
      "|    learning_rate   | 0.00299  |\n",
      "|    n_updates       | 9820     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2012     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3133     |\n",
      "|    total_timesteps | 2012000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2013000, episode_reward=-356.79 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -357     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2013000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2014000, episode_reward=-258.68 +/- 1.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -259     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2014000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.954   |\n",
      "|    critic_loss     | 0.000155 |\n",
      "|    ent_coef        | 0.000602 |\n",
      "|    ent_coef_loss   | -33.9    |\n",
      "|    learning_rate   | 0.00299  |\n",
      "|    n_updates       | 9830     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2015000, episode_reward=-259.63 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -260     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2015000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-367.33 +/- 14.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -367     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2016000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.948   |\n",
      "|    critic_loss     | 0.000234 |\n",
      "|    ent_coef        | 0.000578 |\n",
      "|    ent_coef_loss   | -37.9    |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9840     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2016     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3139     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2017000, episode_reward=-375.51 +/- 11.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -376     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2017000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2018000, episode_reward=-225.25 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2018000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.939   |\n",
      "|    critic_loss     | 0.0004   |\n",
      "|    ent_coef        | 0.000549 |\n",
      "|    ent_coef_loss   | -35.4    |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9850     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2019000, episode_reward=-227.13 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -227     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2019000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=-402.10 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.941   |\n",
      "|    critic_loss     | 0.000279 |\n",
      "|    ent_coef        | 0.000522 |\n",
      "|    ent_coef_loss   | -18.2    |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9860     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -270     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2020     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3146     |\n",
      "|    total_timesteps | 2020000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2021000, episode_reward=-402.37 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2021000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2022000, episode_reward=-287.55 +/- 2.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -288     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2022000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.95    |\n",
      "|    critic_loss     | 8e-05    |\n",
      "|    ent_coef        | 0.000504 |\n",
      "|    ent_coef_loss   | -21.7    |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9870     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2023000, episode_reward=-286.90 +/- 2.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -287     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2023000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2024000, episode_reward=-111.29 +/- 245.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2024000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.954   |\n",
      "|    critic_loss     | 0.000199 |\n",
      "|    ent_coef        | 0.000488 |\n",
      "|    ent_coef_loss   | -42.9    |\n",
      "|    learning_rate   | 0.00298  |\n",
      "|    n_updates       | 9880     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -276     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2024     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3152     |\n",
      "|    total_timesteps | 2024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2025000, episode_reward=-379.99 +/- 275.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -380     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2025000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2026000, episode_reward=-106.11 +/- 244.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2026000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.956   |\n",
      "|    critic_loss     | 0.000611 |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | -9.13    |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9890     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2027000, episode_reward=-298.87 +/- 208.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -299     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2027000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2028000, episode_reward=-620.93 +/- 65.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -621     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2028000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.987   |\n",
      "|    critic_loss     | 0.00404  |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9900     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -267     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2028     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3158     |\n",
      "|    total_timesteps | 2028000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2029000, episode_reward=-632.56 +/- 27.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -633     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2029000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=-244.55 +/- 3.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.977   |\n",
      "|    critic_loss     | 0.00222  |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | 86.1     |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2031000, episode_reward=-239.69 +/- 2.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -240     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2031000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2032000, episode_reward=-442.93 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -443     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2032000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.963   |\n",
      "|    critic_loss     | 0.000835 |\n",
      "|    ent_coef        | 0.000487 |\n",
      "|    ent_coef_loss   | 70.2     |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9920     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -314     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2032     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3164     |\n",
      "|    total_timesteps | 2032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2033000, episode_reward=-441.86 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -442     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2033000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2034000, episode_reward=-517.10 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -517     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2034000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.975   |\n",
      "|    critic_loss     | 0.000173 |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | 40       |\n",
      "|    learning_rate   | 0.00297  |\n",
      "|    n_updates       | 9930     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2035000, episode_reward=-517.41 +/- 0.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -517     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2035000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036000, episode_reward=-519.30 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -519     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2036000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.976   |\n",
      "|    critic_loss     | 4.39e-05 |\n",
      "|    ent_coef        | 0.00056  |\n",
      "|    ent_coef_loss   | 33.5     |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9940     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -329     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2036     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3170     |\n",
      "|    total_timesteps | 2036000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2037000, episode_reward=-518.41 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -518     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2037000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2038000, episode_reward=-515.79 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -516     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2038000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.968   |\n",
      "|    critic_loss     | 5.13e-05 |\n",
      "|    ent_coef        | 0.000588 |\n",
      "|    ent_coef_loss   | 23.4     |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9950     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2039000, episode_reward=-515.71 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -516     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2039000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=-487.75 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -488     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.968   |\n",
      "|    critic_loss     | 4e-05    |\n",
      "|    ent_coef        | 0.000609 |\n",
      "|    ent_coef_loss   | 18.5     |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9960     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -337     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2040     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3177     |\n",
      "|    total_timesteps | 2040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2041000, episode_reward=-487.58 +/- 0.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -488     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2041000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2042000, episode_reward=-423.87 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -424     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2042000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.966   |\n",
      "|    critic_loss     | 5.07e-05 |\n",
      "|    ent_coef        | 0.000626 |\n",
      "|    ent_coef_loss   | 12.1     |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9970     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2043000, episode_reward=-424.20 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -424     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2043000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2044000, episode_reward=-359.82 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -360     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2044000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.965   |\n",
      "|    critic_loss     | 7.36e-05 |\n",
      "|    ent_coef        | 0.000637 |\n",
      "|    ent_coef_loss   | 2.1      |\n",
      "|    learning_rate   | 0.00296  |\n",
      "|    n_updates       | 9980     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -338     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2044     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3183     |\n",
      "|    total_timesteps | 2044000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2045000, episode_reward=-360.59 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -361     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2045000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2046000, episode_reward=-381.45 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2046000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.962   |\n",
      "|    critic_loss     | 0.000109 |\n",
      "|    ent_coef        | 0.000641 |\n",
      "|    ent_coef_loss   | -0.784   |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 9990     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2047000, episode_reward=-382.19 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2047000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2048000, episode_reward=-381.87 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2048000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2048     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3189     |\n",
      "|    total_timesteps | 2048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2049000, episode_reward=-464.84 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -465     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2049000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.943   |\n",
      "|    critic_loss     | 8.25e-05 |\n",
      "|    ent_coef        | 0.000643 |\n",
      "|    ent_coef_loss   | 12.6     |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 10000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=-465.22 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -465     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2050000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2051000, episode_reward=-441.74 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -442     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2051000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.932   |\n",
      "|    critic_loss     | 3.46e-05 |\n",
      "|    ent_coef        | 0.000651 |\n",
      "|    ent_coef_loss   | 8.4      |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 10010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2052000, episode_reward=-442.25 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -442     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2052000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -331     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2052     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3195     |\n",
      "|    total_timesteps | 2052000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2053000, episode_reward=-401.57 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2053000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.933   |\n",
      "|    critic_loss     | 4.18e-05 |\n",
      "|    ent_coef        | 0.000658 |\n",
      "|    ent_coef_loss   | 0.74     |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 10020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2054000, episode_reward=-401.02 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -401     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2054000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2055000, episode_reward=-377.48 +/- 1.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2055000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.937   |\n",
      "|    critic_loss     | 6.11e-05 |\n",
      "|    ent_coef        | 0.000661 |\n",
      "|    ent_coef_loss   | -6.68    |\n",
      "|    learning_rate   | 0.00295  |\n",
      "|    n_updates       | 10030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2056000, episode_reward=-377.71 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -378     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2056000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -329     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2056     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3201     |\n",
      "|    total_timesteps | 2056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2057000, episode_reward=-339.27 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -339     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2057000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.937   |\n",
      "|    critic_loss     | 7.37e-05 |\n",
      "|    ent_coef        | 0.000658 |\n",
      "|    ent_coef_loss   | -7.83    |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2058000, episode_reward=-339.69 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2058000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2059000, episode_reward=-327.52 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -328     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2059000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.941   |\n",
      "|    critic_loss     | 9.18e-05 |\n",
      "|    ent_coef        | 0.000653 |\n",
      "|    ent_coef_loss   | -9.67    |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=-327.35 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -327     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2060000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -324     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2060     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3208     |\n",
      "|    total_timesteps | 2060000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2061000, episode_reward=-338.82 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -339     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2061000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.942   |\n",
      "|    critic_loss     | 9.29e-05 |\n",
      "|    ent_coef        | 0.000646 |\n",
      "|    ent_coef_loss   | -8.8     |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2062000, episode_reward=-338.72 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -339     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2062000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2063000, episode_reward=-347.72 +/- 0.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -348     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2063000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.941   |\n",
      "|    critic_loss     | 8.84e-05 |\n",
      "|    ent_coef        | 0.000638 |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2064000, episode_reward=-347.16 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2064000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -318     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2064     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3214     |\n",
      "|    total_timesteps | 2064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2065000, episode_reward=-242.10 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2065000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.939   |\n",
      "|    critic_loss     | 9.58e-05 |\n",
      "|    ent_coef        | 0.00063  |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00294  |\n",
      "|    n_updates       | 10080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2066000, episode_reward=-242.03 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2066000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2067000, episode_reward=-236.37 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2067000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.944   |\n",
      "|    critic_loss     | 0.000103 |\n",
      "|    ent_coef        | 0.00062  |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2068000, episode_reward=-236.26 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2068000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -313     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2068     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3220     |\n",
      "|    total_timesteps | 2068000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2069000, episode_reward=-227.62 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2069000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.947   |\n",
      "|    critic_loss     | 9.11e-05 |\n",
      "|    ent_coef        | 0.000611 |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=-230.07 +/- 4.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -230     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2070000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2071000, episode_reward=-238.45 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2071000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.943   |\n",
      "|    critic_loss     | 9.78e-05 |\n",
      "|    ent_coef        | 0.000601 |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2072000, episode_reward=-237.54 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2072000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -307     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2072     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3226     |\n",
      "|    total_timesteps | 2072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2073000, episode_reward=-228.92 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2073000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.946   |\n",
      "|    critic_loss     | 0.000102 |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2074000, episode_reward=-228.20 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2074000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2075000, episode_reward=-259.67 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -260     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2075000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.948   |\n",
      "|    critic_loss     | 0.000116 |\n",
      "|    ent_coef        | 0.00058  |\n",
      "|    ent_coef_loss   | -18.7    |\n",
      "|    learning_rate   | 0.00293  |\n",
      "|    n_updates       | 10130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076000, episode_reward=-259.11 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -259     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2076000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -299     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2076     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3232     |\n",
      "|    total_timesteps | 2076000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2077000, episode_reward=-258.43 +/- 23.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2077000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.948   |\n",
      "|    critic_loss     | 0.000187 |\n",
      "|    ent_coef        | 0.000567 |\n",
      "|    ent_coef_loss   | -22.9    |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2078000, episode_reward=-254.81 +/- 24.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2078000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2079000, episode_reward=-262.36 +/- 2.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -262     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2079000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.901   |\n",
      "|    critic_loss     | 0.000621 |\n",
      "|    ent_coef        | 0.000553 |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=-262.52 +/- 3.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -263     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2080000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -298     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2080     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3239     |\n",
      "|    total_timesteps | 2080000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2081000, episode_reward=-267.16 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2081000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.926   |\n",
      "|    critic_loss     | 0.000441 |\n",
      "|    ent_coef        | 0.000546 |\n",
      "|    ent_coef_loss   | -0.577   |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2082000, episode_reward=-267.97 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -268     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2082000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2083000, episode_reward=-335.29 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2083000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.936   |\n",
      "|    critic_loss     | 0.000158 |\n",
      "|    ent_coef        | 0.000544 |\n",
      "|    ent_coef_loss   | 0.581    |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2084000, episode_reward=-334.77 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2084000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -302     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2084     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3245     |\n",
      "|    total_timesteps | 2084000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2085000, episode_reward=-378.41 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -378     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2085000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.929   |\n",
      "|    critic_loss     | 9.16e-05 |\n",
      "|    ent_coef        | 0.000543 |\n",
      "|    ent_coef_loss   | -7.92    |\n",
      "|    learning_rate   | 0.00292  |\n",
      "|    n_updates       | 10180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2086000, episode_reward=-378.40 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -378     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2086000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2087000, episode_reward=-292.81 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -293     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2087000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.902   |\n",
      "|    critic_loss     | 0.000152 |\n",
      "|    ent_coef        | 0.00054  |\n",
      "|    ent_coef_loss   | -23.6    |\n",
      "|    learning_rate   | 0.00291  |\n",
      "|    n_updates       | 10190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2088000, episode_reward=-291.77 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -292     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2088000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -308     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2088     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3251     |\n",
      "|    total_timesteps | 2088000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2089000, episode_reward=-363.86 +/- 1.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -364     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2089000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.919   |\n",
      "|    critic_loss     | 0.000124 |\n",
      "|    ent_coef        | 0.000529 |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.00291  |\n",
      "|    n_updates       | 10200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=-363.28 +/- 1.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -363     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2090000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2091000, episode_reward=-363.09 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -363     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2091000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2092000, episode_reward=-215.29 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2092000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.916   |\n",
      "|    critic_loss     | 0.00014  |\n",
      "|    ent_coef        | 0.000519 |\n",
      "|    ent_coef_loss   | -29.4    |\n",
      "|    learning_rate   | 0.00291  |\n",
      "|    n_updates       | 10210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -312     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2092     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3257     |\n",
      "|    total_timesteps | 2092000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2093000, episode_reward=-214.45 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -214     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2093000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2094000, episode_reward=-314.38 +/- 3.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -314     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2094000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.902   |\n",
      "|    critic_loss     | 0.000398 |\n",
      "|    ent_coef        | 0.000504 |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.00291  |\n",
      "|    n_updates       | 10220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2095000, episode_reward=-315.76 +/- 3.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -316     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2095000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096000, episode_reward=-416.40 +/- 0.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -416     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2096000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.922   |\n",
      "|    critic_loss     | 0.0007   |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | 13.9     |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -316     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2096     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3264     |\n",
      "|    total_timesteps | 2096000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2097000, episode_reward=-415.28 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -415     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2097000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2098000, episode_reward=-353.46 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2098000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.9     |\n",
      "|    critic_loss     | 0.000685 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | -2.25    |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2099000, episode_reward=-353.31 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2099000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=-341.04 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -341     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.903   |\n",
      "|    critic_loss     | 0.000374 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | 6.06     |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10250    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -329     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2100     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3270     |\n",
      "|    total_timesteps | 2100000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2101000, episode_reward=-340.02 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2101000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2102000, episode_reward=-344.43 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -344     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2102000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.906   |\n",
      "|    critic_loss     | 0.000839 |\n",
      "|    ent_coef        | 0.000498 |\n",
      "|    ent_coef_loss   | -6.16    |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2103000, episode_reward=-344.28 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -344     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2103000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2104000, episode_reward=-262.88 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -263     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2104000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.901   |\n",
      "|    critic_loss     | 0.000276 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | -14.7    |\n",
      "|    learning_rate   | 0.0029   |\n",
      "|    n_updates       | 10270    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -335     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2104     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3276     |\n",
      "|    total_timesteps | 2104000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2105000, episode_reward=-256.38 +/- 14.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -256     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2105000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2106000, episode_reward=-289.11 +/- 4.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -289     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2106000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.911   |\n",
      "|    critic_loss     | 0.000751 |\n",
      "|    ent_coef        | 0.000489 |\n",
      "|    ent_coef_loss   | -7.76    |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2107000, episode_reward=-288.56 +/- 6.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -289     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2107000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2108000, episode_reward=-248.56 +/- 60.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -249     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2108000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.93    |\n",
      "|    critic_loss     | 0.000347 |\n",
      "|    ent_coef        | 0.000485 |\n",
      "|    ent_coef_loss   | -4.03    |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10290    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2108     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3282     |\n",
      "|    total_timesteps | 2108000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2109000, episode_reward=-283.71 +/- 39.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -284     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2109000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=-330.43 +/- 7.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.957   |\n",
      "|    critic_loss     | 0.000357 |\n",
      "|    ent_coef        | 0.00048  |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2111000, episode_reward=-320.00 +/- 20.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -320     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2111000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2112000, episode_reward=-273.82 +/- 55.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -274     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2112000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.949   |\n",
      "|    critic_loss     | 0.000321 |\n",
      "|    ent_coef        | 0.000474 |\n",
      "|    ent_coef_loss   | -3.79    |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10310    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -317     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2112     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3289     |\n",
      "|    total_timesteps | 2112000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2113000, episode_reward=-256.45 +/- 68.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -256     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2113000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2114000, episode_reward=-225.57 +/- 14.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -226     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2114000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.933   |\n",
      "|    critic_loss     | 0.000174 |\n",
      "|    ent_coef        | 0.000469 |\n",
      "|    ent_coef_loss   | -24.8    |\n",
      "|    learning_rate   | 0.00289  |\n",
      "|    n_updates       | 10320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2115000, episode_reward=-214.69 +/- 12.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2115000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116000, episode_reward=-16.30 +/- 323.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2116000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.934   |\n",
      "|    critic_loss     | 0.000265 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | -16.8    |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10330    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -312     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2116     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3295     |\n",
      "|    total_timesteps | 2116000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2117000, episode_reward=-142.45 +/- 329.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -142     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2117000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2118000, episode_reward=-204.54 +/- 364.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -205     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2118000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.925   |\n",
      "|    critic_loss     | 0.000317 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2119000, episode_reward=-215.40 +/- 343.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2119000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=-215.42 +/- 386.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.00055  |\n",
      "|    ent_coef        | 0.000444 |\n",
      "|    ent_coef_loss   | 10.4     |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10350    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -290     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2120     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3301     |\n",
      "|    total_timesteps | 2120000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2121000, episode_reward=-431.92 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -432     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2121000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2122000, episode_reward=-383.36 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2122000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.855   |\n",
      "|    critic_loss     | 0.000266 |\n",
      "|    ent_coef        | 0.000444 |\n",
      "|    ent_coef_loss   | 3.07     |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2123000, episode_reward=-195.42 +/- 378.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -195     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2123000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2124000, episode_reward=-503.22 +/- 1.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -503     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2124000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.816   |\n",
      "|    critic_loss     | 0.000104 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | 6.67     |\n",
      "|    learning_rate   | 0.00288  |\n",
      "|    n_updates       | 10370    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -299     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2124     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3307     |\n",
      "|    total_timesteps | 2124000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2125000, episode_reward=-503.10 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -503     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2125000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2126000, episode_reward=-524.72 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -525     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2126000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.821   |\n",
      "|    critic_loss     | 5.72e-05 |\n",
      "|    ent_coef        | 0.000449 |\n",
      "|    ent_coef_loss   | 20.2     |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2127000, episode_reward=-108.69 +/- 509.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2127000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2128000, episode_reward=-441.36 +/- 117.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -441     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2128000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.971   |\n",
      "|    critic_loss     | 0.000544 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | -6.83    |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -301     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2128     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3313     |\n",
      "|    total_timesteps | 2128000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2129000, episode_reward=-487.76 +/- 97.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -488     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2129000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=-333.28 +/- 83.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -333     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.998   |\n",
      "|    critic_loss     | 0.000463 |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2131000, episode_reward=-373.11 +/- 103.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2131000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2132000, episode_reward=-401.00 +/- 69.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -401     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2132000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.826   |\n",
      "|    critic_loss     | 0.000105 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | 15.5     |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2132     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3320     |\n",
      "|    total_timesteps | 2132000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2133000, episode_reward=-312.19 +/- 4.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -312     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2133000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2134000, episode_reward=-346.83 +/- 55.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2134000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2135000, episode_reward=42.67 +/- 512.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 42.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2135000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.812   |\n",
      "|    critic_loss     | 0.000114 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | 9.41     |\n",
      "|    learning_rate   | 0.00287  |\n",
      "|    n_updates       | 10420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2136000, episode_reward=-166.41 +/- 425.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -166     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2136000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -267     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2136     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3326     |\n",
      "|    total_timesteps | 2136000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2137000, episode_reward=-357.60 +/- 64.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -358     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2137000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.919   |\n",
      "|    critic_loss     | 0.000347 |\n",
      "|    ent_coef        | 0.000464 |\n",
      "|    ent_coef_loss   | -2.69    |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2138000, episode_reward=-373.87 +/- 54.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -374     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2138000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2139000, episode_reward=686.87 +/- 39.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 687      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2139000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.817   |\n",
      "|    critic_loss     | 5.62e-05 |\n",
      "|    ent_coef        | 0.000465 |\n",
      "|    ent_coef_loss   | 7.53     |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=744.16 +/- 36.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 744      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2140000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -254     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2140     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3332     |\n",
      "|    total_timesteps | 2140000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2141000, episode_reward=801.13 +/- 24.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 801      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2141000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.915   |\n",
      "|    critic_loss     | 0.000336 |\n",
      "|    ent_coef        | 0.000468 |\n",
      "|    ent_coef_loss   | 3.79     |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2142000, episode_reward=793.20 +/- 32.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 793      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2142000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2143000, episode_reward=560.06 +/- 504.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 560      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2143000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.933   |\n",
      "|    critic_loss     | 0.000254 |\n",
      "|    ent_coef        | 0.00047  |\n",
      "|    ent_coef_loss   | -0.39    |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2144000, episode_reward=790.85 +/- 31.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 791      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2144000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -222     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2144     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3338     |\n",
      "|    total_timesteps | 2144000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2145000, episode_reward=569.67 +/- 497.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 570      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2145000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.00036  |\n",
      "|    ent_coef        | 0.00047  |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.00286  |\n",
      "|    n_updates       | 10470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2146000, episode_reward=595.06 +/- 512.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 595      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2146000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2147000, episode_reward=450.74 +/- 450.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2147000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 0.000388 |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | -6.88    |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2148000, episode_reward=388.37 +/- 420.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 388      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2148000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2148     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3344     |\n",
      "|    total_timesteps | 2148000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2149000, episode_reward=131.09 +/- 465.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 131      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2149000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.91    |\n",
      "|    critic_loss     | 0.000234 |\n",
      "|    ent_coef        | 0.000462 |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=-47.72 +/- 480.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -47.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2150000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2151000, episode_reward=-237.26 +/- 363.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -237     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2151000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.952   |\n",
      "|    critic_loss     | 0.00029  |\n",
      "|    ent_coef        | 0.000461 |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2152000, episode_reward=-233.95 +/- 369.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2152000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -170     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2152     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3350     |\n",
      "|    total_timesteps | 2152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2153000, episode_reward=-184.07 +/- 486.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -184     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2153000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000438 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | -6.57    |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2154000, episode_reward=-193.43 +/- 466.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -193     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2154000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2155000, episode_reward=693.27 +/- 28.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 693      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2155000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.861   |\n",
      "|    critic_loss     | 0.000155 |\n",
      "|    ent_coef        | 0.000458 |\n",
      "|    ent_coef_loss   | 0.579    |\n",
      "|    learning_rate   | 0.00285  |\n",
      "|    n_updates       | 10520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2156000, episode_reward=43.57 +/- 558.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 43.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2156000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -155     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2156     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3356     |\n",
      "|    total_timesteps | 2156000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157000, episode_reward=588.81 +/- 47.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 589      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2157000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.909   |\n",
      "|    critic_loss     | 0.000246 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | -6.51    |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2158000, episode_reward=639.65 +/- 20.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 640      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2158000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2159000, episode_reward=596.21 +/- 46.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 596      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2159000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.96    |\n",
      "|    critic_loss     | 0.000314 |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | -8.33    |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=551.74 +/- 41.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 552      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2160000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2160     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3362     |\n",
      "|    total_timesteps | 2160000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2161000, episode_reward=161.10 +/- 453.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 161      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2161000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000498 |\n",
      "|    ent_coef        | 0.000448 |\n",
      "|    ent_coef_loss   | -8.98    |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2162000, episode_reward=607.80 +/- 42.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 608      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2162000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2163000, episode_reward=648.19 +/- 47.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 648      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2163000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 0.000424 |\n",
      "|    ent_coef        | 0.000444 |\n",
      "|    ent_coef_loss   | -7.38    |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2164000, episode_reward=639.87 +/- 38.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 640      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2164000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2164     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3369     |\n",
      "|    total_timesteps | 2164000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2165000, episode_reward=723.36 +/- 38.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 723      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2165000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 0.00036  |\n",
      "|    ent_coef        | 0.000439 |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.00284  |\n",
      "|    n_updates       | 10570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2166000, episode_reward=696.40 +/- 59.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 696      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2166000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2167000, episode_reward=522.76 +/- 16.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 523      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2167000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000398 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -17.1    |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2168000, episode_reward=539.04 +/- 54.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 539      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2168000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -70.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2168     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3375     |\n",
      "|    total_timesteps | 2168000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2169000, episode_reward=342.80 +/- 33.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 343      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2169000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.915   |\n",
      "|    critic_loss     | 0.000282 |\n",
      "|    ent_coef        | 0.000425 |\n",
      "|    ent_coef_loss   | -7.96    |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=347.84 +/- 27.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 348      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2170000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2171000, episode_reward=363.17 +/- 31.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 363      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2171000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.879   |\n",
      "|    critic_loss     | 0.000275 |\n",
      "|    ent_coef        | 0.000419 |\n",
      "|    ent_coef_loss   | -5.6     |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2172000, episode_reward=340.57 +/- 32.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 341      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2172000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -62.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2172     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3381     |\n",
      "|    total_timesteps | 2172000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2173000, episode_reward=476.45 +/- 22.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 476      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2173000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.882   |\n",
      "|    critic_loss     | 0.000254 |\n",
      "|    ent_coef        | 0.000415 |\n",
      "|    ent_coef_loss   | -7.34    |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2174000, episode_reward=433.15 +/- 36.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 433      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2174000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2175000, episode_reward=-30.39 +/- 449.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -30.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2175000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.885   |\n",
      "|    critic_loss     | 0.000197 |\n",
      "|    ent_coef        | 0.00041  |\n",
      "|    ent_coef_loss   | -6.4     |\n",
      "|    learning_rate   | 0.00283  |\n",
      "|    n_updates       | 10620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2176000, episode_reward=-43.25 +/- 432.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -43.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2176000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -65.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2176     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3387     |\n",
      "|    total_timesteps | 2176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2177000, episode_reward=-26.79 +/- 446.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2177000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2178000, episode_reward=429.04 +/- 150.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 429      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2178000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.786   |\n",
      "|    critic_loss     | 5.51e-05 |\n",
      "|    ent_coef        | 0.000407 |\n",
      "|    ent_coef_loss   | 5.81     |\n",
      "|    learning_rate   | 0.00282  |\n",
      "|    n_updates       | 10630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2179000, episode_reward=309.66 +/- 386.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 310      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2179000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=-350.11 +/- 76.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000373 |\n",
      "|    ent_coef        | 0.000407 |\n",
      "|    ent_coef_loss   | -9.35    |\n",
      "|    learning_rate   | 0.00282  |\n",
      "|    n_updates       | 10640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -45.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2180     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3393     |\n",
      "|    total_timesteps | 2180000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2181000, episode_reward=-309.02 +/- 2.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -309     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2181000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2182000, episode_reward=-376.59 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2182000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.89    |\n",
      "|    critic_loss     | 0.000236 |\n",
      "|    ent_coef        | 0.000405 |\n",
      "|    ent_coef_loss   | 4.53     |\n",
      "|    learning_rate   | 0.00282  |\n",
      "|    n_updates       | 10650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2183000, episode_reward=-377.32 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2183000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2184000, episode_reward=-384.01 +/- 2.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -384     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2184000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.891   |\n",
      "|    critic_loss     | 0.00022  |\n",
      "|    ent_coef        | 0.000405 |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.00282  |\n",
      "|    n_updates       | 10660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -33.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2184     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3399     |\n",
      "|    total_timesteps | 2184000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2185000, episode_reward=-383.73 +/- 2.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -384     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2185000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2186000, episode_reward=-487.67 +/- 2.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -488     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2186000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.921   |\n",
      "|    critic_loss     | 0.000219 |\n",
      "|    ent_coef        | 0.000406 |\n",
      "|    ent_coef_loss   | 7.26     |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2187000, episode_reward=-487.78 +/- 2.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -488     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2187000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2188000, episode_reward=-502.00 +/- 2.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -502     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2188000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.859   |\n",
      "|    critic_loss     | 2.31e-05 |\n",
      "|    ent_coef        | 0.000409 |\n",
      "|    ent_coef_loss   | 29       |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10680    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -36.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2188     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3406     |\n",
      "|    total_timesteps | 2188000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2189000, episode_reward=-502.54 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -503     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2189000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=-487.40 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -487     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.857   |\n",
      "|    critic_loss     | 4.12e-05 |\n",
      "|    ent_coef        | 0.00042  |\n",
      "|    ent_coef_loss   | 12.1     |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2191000, episode_reward=-485.74 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -486     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2191000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2192000, episode_reward=-470.57 +/- 4.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -471     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2192000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.859   |\n",
      "|    critic_loss     | 3.17e-05 |\n",
      "|    ent_coef        | 0.000429 |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10700    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -36.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2192     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3412     |\n",
      "|    total_timesteps | 2192000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2193000, episode_reward=-470.38 +/- 6.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -470     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2193000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2194000, episode_reward=191.05 +/- 498.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 191      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2194000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.882   |\n",
      "|    critic_loss     | 0.000285 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -2.95    |\n",
      "|    learning_rate   | 0.00281  |\n",
      "|    n_updates       | 10710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2195000, episode_reward=-27.17 +/- 477.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -27.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2195000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2196000, episode_reward=169.59 +/- 413.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 170      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2196000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.995   |\n",
      "|    critic_loss     | 0.000406 |\n",
      "|    ent_coef        | 0.000431 |\n",
      "|    ent_coef_loss   | -15.7    |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10720    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -15.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2196     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3418     |\n",
      "|    total_timesteps | 2196000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197000, episode_reward=380.08 +/- 359.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 380      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2197000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2198000, episode_reward=578.88 +/- 23.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 579      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2198000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.997   |\n",
      "|    critic_loss     | 0.000514 |\n",
      "|    ent_coef        | 0.000426 |\n",
      "|    ent_coef_loss   | -7.98    |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2199000, episode_reward=190.90 +/- 421.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 191      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2199000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=534.63 +/- 16.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 535      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 0.000444 |\n",
      "|    ent_coef        | 0.00042  |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10740    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 16.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2200     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3424     |\n",
      "|    total_timesteps | 2200000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2201000, episode_reward=528.21 +/- 44.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 528      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2201000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2202000, episode_reward=256.42 +/- 292.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2202000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.996   |\n",
      "|    critic_loss     | 0.000476 |\n",
      "|    ent_coef        | 0.000414 |\n",
      "|    ent_coef_loss   | -8.49    |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2203000, episode_reward=274.86 +/- 298.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 275      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2203000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2204000, episode_reward=587.98 +/- 24.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 588      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2204000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.98    |\n",
      "|    critic_loss     | 0.000449 |\n",
      "|    ent_coef        | 0.000409 |\n",
      "|    ent_coef_loss   | -2.58    |\n",
      "|    learning_rate   | 0.0028   |\n",
      "|    n_updates       | 10760    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 44       |\n",
      "| time/              |          |\n",
      "|    episodes        | 2204     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3430     |\n",
      "|    total_timesteps | 2204000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2205000, episode_reward=210.47 +/- 442.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 210      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2205000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2206000, episode_reward=-158.74 +/- 393.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2206000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 0.000454 |\n",
      "|    ent_coef        | 0.000406 |\n",
      "|    ent_coef_loss   | -8.2     |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2207000, episode_reward=11.15 +/- 444.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 11.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2207000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2208000, episode_reward=659.49 +/- 39.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 659      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2208000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.995   |\n",
      "|    critic_loss     | 0.000439 |\n",
      "|    ent_coef        | 0.000402 |\n",
      "|    ent_coef_loss   | -8.62    |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10780    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 72.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2208     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3437     |\n",
      "|    total_timesteps | 2208000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2209000, episode_reward=683.35 +/- 42.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 683      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2209000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=589.92 +/- 39.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 590      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000432 |\n",
      "|    ent_coef        | 0.000397 |\n",
      "|    ent_coef_loss   | -10      |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2211000, episode_reward=605.89 +/- 19.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 606      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2211000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2212000, episode_reward=509.23 +/- 68.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 509      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2212000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.891   |\n",
      "|    critic_loss     | 0.000341 |\n",
      "|    ent_coef        | 0.000392 |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 90.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2212     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3443     |\n",
      "|    total_timesteps | 2212000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2213000, episode_reward=487.10 +/- 75.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 487      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2213000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2214000, episode_reward=-346.12 +/- 2.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -346     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2214000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.966   |\n",
      "|    critic_loss     | 0.000375 |\n",
      "|    ent_coef        | 0.000386 |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.00279  |\n",
      "|    n_updates       | 10810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2215000, episode_reward=-346.52 +/- 2.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2215000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2216000, episode_reward=-367.89 +/- 4.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -368     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2216000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.999   |\n",
      "|    critic_loss     | 0.000339 |\n",
      "|    ent_coef        | 0.000378 |\n",
      "|    ent_coef_loss   | -14.6    |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 117      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2216     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3449     |\n",
      "|    total_timesteps | 2216000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2217000, episode_reward=-367.87 +/- 2.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -368     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2217000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2218000, episode_reward=-340.58 +/- 3.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -341     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2218000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.985   |\n",
      "|    critic_loss     | 0.000329 |\n",
      "|    ent_coef        | 0.000371 |\n",
      "|    ent_coef_loss   | -9.74    |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2219000, episode_reward=-340.19 +/- 2.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2219000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=-340.24 +/- 1.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2220000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 130      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2220     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3455     |\n",
      "|    total_timesteps | 2220000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2221000, episode_reward=75.13 +/- 395.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 75.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2221000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.991   |\n",
      "|    critic_loss     | 0.000384 |\n",
      "|    ent_coef        | 0.000365 |\n",
      "|    ent_coef_loss   | -6.79    |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2222000, episode_reward=-267.06 +/- 53.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2222000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2223000, episode_reward=-377.47 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2223000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.994   |\n",
      "|    critic_loss     | 0.000346 |\n",
      "|    ent_coef        | 0.00036  |\n",
      "|    ent_coef_loss   | -7.21    |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2224000, episode_reward=-373.39 +/- 2.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2224000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 162      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2224     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3461     |\n",
      "|    total_timesteps | 2224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2225000, episode_reward=821.80 +/- 30.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 822      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2225000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.916   |\n",
      "|    critic_loss     | 0.000226 |\n",
      "|    ent_coef        | 0.000356 |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.00278  |\n",
      "|    n_updates       | 10860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2226000, episode_reward=768.79 +/- 18.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 769      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2226000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2227000, episode_reward=127.60 +/- 522.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 128      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2227000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000439 |\n",
      "|    ent_coef        | 0.000352 |\n",
      "|    ent_coef_loss   | -2.59    |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2228000, episode_reward=570.70 +/- 437.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 571      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2228000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 191      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2228     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3467     |\n",
      "|    total_timesteps | 2228000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2229000, episode_reward=835.32 +/- 54.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 835      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2229000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000401 |\n",
      "|    ent_coef        | 0.000349 |\n",
      "|    ent_coef_loss   | -7.4     |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=843.63 +/- 20.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 844      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2230000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2231000, episode_reward=792.50 +/- 16.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 792      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2231000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000405 |\n",
      "|    ent_coef        | 0.000345 |\n",
      "|    ent_coef_loss   | -8.3     |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2232000, episode_reward=749.20 +/- 57.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 749      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2232000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 223      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2232     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3473     |\n",
      "|    total_timesteps | 2232000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2233000, episode_reward=532.52 +/- 57.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 533      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2233000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 0.000465 |\n",
      "|    ent_coef        | 0.000342 |\n",
      "|    ent_coef_loss   | 0.124    |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2234000, episode_reward=561.06 +/- 38.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 561      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2234000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2235000, episode_reward=593.18 +/- 33.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 593      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2235000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.985   |\n",
      "|    critic_loss     | 0.000491 |\n",
      "|    ent_coef        | 0.00034  |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.00277  |\n",
      "|    n_updates       | 10910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2236000, episode_reward=590.56 +/- 38.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 591      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2236000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 253      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2236     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3480     |\n",
      "|    total_timesteps | 2236000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237000, episode_reward=727.05 +/- 34.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 727      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2237000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.998   |\n",
      "|    critic_loss     | 0.000448 |\n",
      "|    ent_coef        | 0.00034  |\n",
      "|    ent_coef_loss   | 0.892    |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2238000, episode_reward=778.85 +/- 53.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 779      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2238000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2239000, episode_reward=853.20 +/- 29.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 853      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2239000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000422 |\n",
      "|    ent_coef        | 0.00034  |\n",
      "|    ent_coef_loss   | -4.05    |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=876.01 +/- 67.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 876      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 287      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2240     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3486     |\n",
      "|    total_timesteps | 2240000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2241000, episode_reward=510.32 +/- 42.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 510      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2241000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000432 |\n",
      "|    ent_coef        | 0.000338 |\n",
      "|    ent_coef_loss   | -5.8     |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2242000, episode_reward=566.19 +/- 42.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 566      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2242000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2243000, episode_reward=821.66 +/- 33.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 822      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2243000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 0.000402 |\n",
      "|    ent_coef        | 0.000336 |\n",
      "|    ent_coef_loss   | -7.12    |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2244000, episode_reward=779.34 +/- 26.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 779      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2244000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 299      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2244     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3492     |\n",
      "|    total_timesteps | 2244000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2245000, episode_reward=759.28 +/- 51.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 759      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2245000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 0.000365 |\n",
      "|    ent_coef        | 0.000333 |\n",
      "|    ent_coef_loss   | -6.46    |\n",
      "|    learning_rate   | 0.00276  |\n",
      "|    n_updates       | 10960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2246000, episode_reward=725.43 +/- 54.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 725      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2246000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2247000, episode_reward=779.72 +/- 34.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 780      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2247000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000407 |\n",
      "|    ent_coef        | 0.00033  |\n",
      "|    ent_coef_loss   | -3.15    |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 10970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2248000, episode_reward=759.02 +/- 34.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 759      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2248000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 317      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2248     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3498     |\n",
      "|    total_timesteps | 2248000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2249000, episode_reward=-211.96 +/- 2.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -212     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2249000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 0.000502 |\n",
      "|    ent_coef        | 0.000328 |\n",
      "|    ent_coef_loss   | -0.788   |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 10980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=-211.45 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2250000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2251000, episode_reward=694.46 +/- 26.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 694      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2251000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 0.000382 |\n",
      "|    ent_coef        | 0.000326 |\n",
      "|    ent_coef_loss   | -8.77    |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 10990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2252000, episode_reward=704.41 +/- 25.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 704      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2252000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 333      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2252     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3504     |\n",
      "|    total_timesteps | 2252000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2253000, episode_reward=903.48 +/- 37.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 903      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2253000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1       |\n",
      "|    critic_loss     | 0.000474 |\n",
      "|    ent_coef        | 0.000323 |\n",
      "|    ent_coef_loss   | -5.12    |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 11000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2254000, episode_reward=902.26 +/- 41.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 902      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2254000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2255000, episode_reward=862.45 +/- 46.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 862      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2255000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000422 |\n",
      "|    ent_coef        | 0.000321 |\n",
      "|    ent_coef_loss   | -3.34    |\n",
      "|    learning_rate   | 0.00275  |\n",
      "|    n_updates       | 11010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2256000, episode_reward=841.26 +/- 24.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 841      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2256000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 365      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2256     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3510     |\n",
      "|    total_timesteps | 2256000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2257000, episode_reward=769.32 +/- 36.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 769      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2257000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.04    |\n",
      "|    critic_loss     | 0.000403 |\n",
      "|    ent_coef        | 0.000319 |\n",
      "|    ent_coef_loss   | -4.34    |\n",
      "|    learning_rate   | 0.00274  |\n",
      "|    n_updates       | 11020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2258000, episode_reward=807.98 +/- 32.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 808      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2258000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2259000, episode_reward=905.58 +/- 48.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 906      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2259000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 0.000421 |\n",
      "|    ent_coef        | 0.000317 |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 0.00274  |\n",
      "|    n_updates       | 11030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=914.93 +/- 56.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 915      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2260000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 389      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2260     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3516     |\n",
      "|    total_timesteps | 2260000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2261000, episode_reward=-208.84 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -209     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2261000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000465 |\n",
      "|    ent_coef        | 0.000315 |\n",
      "|    ent_coef_loss   | -1.18    |\n",
      "|    learning_rate   | 0.00274  |\n",
      "|    n_updates       | 11040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2262000, episode_reward=-206.55 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -207     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2262000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2263000, episode_reward=-207.45 +/- 2.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -207     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2263000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2264000, episode_reward=-252.86 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -253     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2264000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.92    |\n",
      "|    critic_loss     | 0.000309 |\n",
      "|    ent_coef        | 0.000314 |\n",
      "|    ent_coef_loss   | -9.04    |\n",
      "|    learning_rate   | 0.00274  |\n",
      "|    n_updates       | 11050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 381      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2264     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3523     |\n",
      "|    total_timesteps | 2264000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2265000, episode_reward=-253.96 +/- 2.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -254     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2265000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2266000, episode_reward=918.84 +/- 46.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 919      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2266000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.812   |\n",
      "|    critic_loss     | 7.99e-05 |\n",
      "|    ent_coef        | 0.00031  |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2267000, episode_reward=878.07 +/- 32.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 878      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2267000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2268000, episode_reward=-238.18 +/- 1.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2268000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000796 |\n",
      "|    ent_coef        | 0.000307 |\n",
      "|    ent_coef_loss   | 5.3      |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 385      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2268     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3529     |\n",
      "|    total_timesteps | 2268000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2269000, episode_reward=-238.42 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2269000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=-156.57 +/- 1.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.000556 |\n",
      "|    ent_coef        | 0.000306 |\n",
      "|    ent_coef_loss   | 2.69     |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2271000, episode_reward=-156.51 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2271000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2272000, episode_reward=-210.95 +/- 1.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2272000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.942   |\n",
      "|    critic_loss     | 0.000605 |\n",
      "|    ent_coef        | 0.000306 |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11090    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 403      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2272     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3535     |\n",
      "|    total_timesteps | 2272000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2273000, episode_reward=-209.96 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -210     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2273000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2274000, episode_reward=-307.64 +/- 0.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2274000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.853   |\n",
      "|    critic_loss     | 0.000125 |\n",
      "|    ent_coef        | 0.000302 |\n",
      "|    ent_coef_loss   | -14.5    |\n",
      "|    learning_rate   | 0.00273  |\n",
      "|    n_updates       | 11100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2275000, episode_reward=-308.11 +/- 0.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2275000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2276000, episode_reward=460.68 +/- 8.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 461      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2276000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.855   |\n",
      "|    critic_loss     | 0.000146 |\n",
      "|    ent_coef        | 0.000297 |\n",
      "|    ent_coef_loss   | -26.6    |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11110    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 403      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2276     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3541     |\n",
      "|    total_timesteps | 2276000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2277000, episode_reward=468.21 +/- 14.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 468      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2277000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278000, episode_reward=-390.49 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -390     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2278000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.899   |\n",
      "|    critic_loss     | 0.000771 |\n",
      "|    ent_coef        | 0.000288 |\n",
      "|    ent_coef_loss   | -2.05    |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2279000, episode_reward=-390.57 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -391     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2279000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=-485.28 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -485     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.846   |\n",
      "|    critic_loss     | 0.000219 |\n",
      "|    ent_coef        | 0.000287 |\n",
      "|    ent_coef_loss   | 69.4     |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11130    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 388      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2280     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3548     |\n",
      "|    total_timesteps | 2280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2281000, episode_reward=-486.00 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -486     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2281000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2282000, episode_reward=-489.19 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -489     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2282000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.831   |\n",
      "|    critic_loss     | 0.000194 |\n",
      "|    ent_coef        | 0.000303 |\n",
      "|    ent_coef_loss   | 40.4     |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2283000, episode_reward=-488.65 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -489     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2283000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2284000, episode_reward=-487.06 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -487     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2284000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.841   |\n",
      "|    critic_loss     | 0.000135 |\n",
      "|    ent_coef        | 0.00032  |\n",
      "|    ent_coef_loss   | 13.6     |\n",
      "|    learning_rate   | 0.00272  |\n",
      "|    n_updates       | 11150    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 372      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2284     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3554     |\n",
      "|    total_timesteps | 2284000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2285000, episode_reward=-486.21 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -486     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2285000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2286000, episode_reward=-386.18 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -386     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2286000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.839   |\n",
      "|    critic_loss     | 8.5e-05  |\n",
      "|    ent_coef        | 0.00033  |\n",
      "|    ent_coef_loss   | 11.3     |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2287000, episode_reward=-386.49 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -386     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2287000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2288000, episode_reward=-379.78 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -380     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2288000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.841   |\n",
      "|    critic_loss     | 0.000121 |\n",
      "|    ent_coef        | 0.000338 |\n",
      "|    ent_coef_loss   | 14.1     |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11170    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 374      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2288     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3560     |\n",
      "|    total_timesteps | 2288000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2289000, episode_reward=-379.74 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -380     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2289000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=-364.82 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -365     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.851   |\n",
      "|    critic_loss     | 0.000108 |\n",
      "|    ent_coef        | 0.000345 |\n",
      "|    ent_coef_loss   | 17       |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2291000, episode_reward=-365.00 +/- 0.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -365     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2291000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2292000, episode_reward=-375.18 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -375     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2292000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.845   |\n",
      "|    critic_loss     | 0.000115 |\n",
      "|    ent_coef        | 0.000354 |\n",
      "|    ent_coef_loss   | 35.2     |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11190    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 374      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2292     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3566     |\n",
      "|    total_timesteps | 2292000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2293000, episode_reward=-375.31 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -375     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2293000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2294000, episode_reward=-335.44 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2294000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.842   |\n",
      "|    critic_loss     | 5.54e-05 |\n",
      "|    ent_coef        | 0.000367 |\n",
      "|    ent_coef_loss   | 19.6     |\n",
      "|    learning_rate   | 0.00271  |\n",
      "|    n_updates       | 11200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2295000, episode_reward=-334.87 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2295000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2296000, episode_reward=-337.66 +/- 7.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -338     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2296000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.842   |\n",
      "|    critic_loss     | 5.75e-05 |\n",
      "|    ent_coef        | 0.000379 |\n",
      "|    ent_coef_loss   | 21.8     |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 351      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2296     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3573     |\n",
      "|    total_timesteps | 2296000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2297000, episode_reward=-332.73 +/- 7.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -333     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2297000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2298000, episode_reward=-258.66 +/- 1.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -259     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2298000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.849   |\n",
      "|    critic_loss     | 4.69e-05 |\n",
      "|    ent_coef        | 0.000392 |\n",
      "|    ent_coef_loss   | 18.8     |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2299000, episode_reward=-258.12 +/- 2.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2299000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=-229.38 +/- 4.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.844   |\n",
      "|    critic_loss     | 6.69e-05 |\n",
      "|    ent_coef        | 0.000403 |\n",
      "|    ent_coef_loss   | 20.1     |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 323      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2300     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3579     |\n",
      "|    total_timesteps | 2300000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2301000, episode_reward=-231.12 +/- 7.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2301000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2302000, episode_reward=-266.83 +/- 1.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2302000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.85    |\n",
      "|    critic_loss     | 6.26e-05 |\n",
      "|    ent_coef        | 0.000414 |\n",
      "|    ent_coef_loss   | 14.9     |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2303000, episode_reward=-267.42 +/- 2.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2303000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2304000, episode_reward=-268.53 +/- 2.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -269     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2304000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 296      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2304     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3585     |\n",
      "|    total_timesteps | 2304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2305000, episode_reward=-380.62 +/- 30.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2305000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.842   |\n",
      "|    critic_loss     | 0.000102 |\n",
      "|    ent_coef        | 0.000425 |\n",
      "|    ent_coef_loss   | 22.4     |\n",
      "|    learning_rate   | 0.0027   |\n",
      "|    n_updates       | 11250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2306000, episode_reward=-393.77 +/- 25.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -394     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2306000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2307000, episode_reward=-368.80 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -369     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2307000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.854   |\n",
      "|    critic_loss     | 6.65e-05 |\n",
      "|    ent_coef        | 0.000437 |\n",
      "|    ent_coef_loss   | 23.1     |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2308000, episode_reward=-367.58 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -368     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2308000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 262      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2308     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3591     |\n",
      "|    total_timesteps | 2308000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2309000, episode_reward=-402.27 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2309000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.852   |\n",
      "|    critic_loss     | 6.31e-05 |\n",
      "|    ent_coef        | 0.000451 |\n",
      "|    ent_coef_loss   | 22.8     |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310000, episode_reward=-402.05 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -402     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2310000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2311000, episode_reward=-406.89 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -407     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2311000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.849   |\n",
      "|    critic_loss     | 5.08e-05 |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | 19.9     |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2312000, episode_reward=-406.91 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -407     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2312000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 236      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2312     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3597     |\n",
      "|    total_timesteps | 2312000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2313000, episode_reward=-335.90 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2313000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.848   |\n",
      "|    critic_loss     | 4.33e-05 |\n",
      "|    ent_coef        | 0.00048  |\n",
      "|    ent_coef_loss   | 12.5     |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2314000, episode_reward=-336.39 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2314000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2315000, episode_reward=-339.74 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2315000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.84    |\n",
      "|    critic_loss     | 6.07e-05 |\n",
      "|    ent_coef        | 0.00049  |\n",
      "|    ent_coef_loss   | 6.32     |\n",
      "|    learning_rate   | 0.00269  |\n",
      "|    n_updates       | 11300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2316000, episode_reward=-338.70 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -339     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2316000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 207      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2316     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3604     |\n",
      "|    total_timesteps | 2316000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2317000, episode_reward=-266.19 +/- 14.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -266     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2317000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.846   |\n",
      "|    critic_loss     | 6.37e-05 |\n",
      "|    ent_coef        | 0.000497 |\n",
      "|    ent_coef_loss   | 6.64     |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318000, episode_reward=-257.43 +/- 6.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -257     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2318000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2319000, episode_reward=-240.92 +/- 5.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2319000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.842   |\n",
      "|    critic_loss     | 9e-05    |\n",
      "|    ent_coef        | 0.000502 |\n",
      "|    ent_coef_loss   | 12.4     |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=-243.74 +/- 4.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2320000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 177      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2320     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3610     |\n",
      "|    total_timesteps | 2320000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2321000, episode_reward=-318.10 +/- 20.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -318     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2321000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.836   |\n",
      "|    critic_loss     | 0.000114 |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | 16.5     |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2322000, episode_reward=-335.07 +/- 9.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2322000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2323000, episode_reward=-231.33 +/- 2.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2323000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.841   |\n",
      "|    critic_loss     | 8.27e-05 |\n",
      "|    ent_coef        | 0.000521 |\n",
      "|    ent_coef_loss   | 10.8     |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2324000, episode_reward=-231.06 +/- 8.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2324000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 154      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2324     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3616     |\n",
      "|    total_timesteps | 2324000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2325000, episode_reward=-203.29 +/- 6.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2325000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.832   |\n",
      "|    critic_loss     | 0.000129 |\n",
      "|    ent_coef        | 0.000531 |\n",
      "|    ent_coef_loss   | 16       |\n",
      "|    learning_rate   | 0.00268  |\n",
      "|    n_updates       | 11350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2326000, episode_reward=-205.54 +/- 11.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2326000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2327000, episode_reward=-220.64 +/- 8.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -221     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2327000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.832   |\n",
      "|    critic_loss     | 0.000145 |\n",
      "|    ent_coef        | 0.000542 |\n",
      "|    ent_coef_loss   | 11.4     |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2328000, episode_reward=-217.08 +/- 6.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -217     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2328000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 121      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2328     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3622     |\n",
      "|    total_timesteps | 2328000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2329000, episode_reward=-205.18 +/- 8.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -205     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2329000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.83    |\n",
      "|    critic_loss     | 0.000151 |\n",
      "|    ent_coef        | 0.000553 |\n",
      "|    ent_coef_loss   | 9.82     |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=-190.92 +/- 10.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2330000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2331000, episode_reward=-235.38 +/- 54.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -235     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2331000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.829   |\n",
      "|    critic_loss     | 0.000175 |\n",
      "|    ent_coef        | 0.000562 |\n",
      "|    ent_coef_loss   | 8.34     |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2332000, episode_reward=-183.13 +/- 13.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2332000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 83.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2332     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3629     |\n",
      "|    total_timesteps | 2332000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2333000, episode_reward=-196.74 +/- 2.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2333000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.817   |\n",
      "|    critic_loss     | 0.000283 |\n",
      "|    ent_coef        | 0.000569 |\n",
      "|    ent_coef_loss   | 10.4     |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2334000, episode_reward=-198.97 +/- 3.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -199     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2334000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2335000, episode_reward=-153.67 +/- 56.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2335000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.835   |\n",
      "|    critic_loss     | 0.000127 |\n",
      "|    ent_coef        | 0.000576 |\n",
      "|    ent_coef_loss   | -5.57    |\n",
      "|    learning_rate   | 0.00267  |\n",
      "|    n_updates       | 11400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2336000, episode_reward=-143.68 +/- 55.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2336000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2336     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3635     |\n",
      "|    total_timesteps | 2336000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2337000, episode_reward=4.18 +/- 12.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.18     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2337000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.82    |\n",
      "|    critic_loss     | 0.000492 |\n",
      "|    ent_coef        | 0.000577 |\n",
      "|    ent_coef_loss   | -6.2     |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2338000, episode_reward=-0.78 +/- 11.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.779   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2338000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2339000, episode_reward=-104.02 +/- 15.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2339000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.807   |\n",
      "|    critic_loss     | 0.000656 |\n",
      "|    ent_coef        | 0.000571 |\n",
      "|    ent_coef_loss   | -21.3    |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=-115.30 +/- 7.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2340000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 28.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2340     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3641     |\n",
      "|    total_timesteps | 2340000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2341000, episode_reward=-350.71 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -351     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2341000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.785   |\n",
      "|    critic_loss     | 0.000551 |\n",
      "|    ent_coef        | 0.000558 |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2342000, episode_reward=-350.96 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -351     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2342000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2343000, episode_reward=-168.18 +/- 2.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -168     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2343000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.775   |\n",
      "|    critic_loss     | 0.000174 |\n",
      "|    ent_coef        | 0.000548 |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2344000, episode_reward=-167.95 +/- 2.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -168     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2344000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -2.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2344     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3647     |\n",
      "|    total_timesteps | 2344000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2345000, episode_reward=64.69 +/- 285.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 64.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2345000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.831   |\n",
      "|    critic_loss     | 0.000958 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.00266  |\n",
      "|    n_updates       | 11450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2346000, episode_reward=-59.75 +/- 41.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -59.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2346000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2347000, episode_reward=-87.12 +/- 29.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2347000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2348000, episode_reward=892.85 +/- 54.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 893      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2348000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.89    |\n",
      "|    critic_loss     | 0.00153  |\n",
      "|    ent_coef        | 0.000528 |\n",
      "|    ent_coef_loss   | -8.72    |\n",
      "|    learning_rate   | 0.00265  |\n",
      "|    n_updates       | 11460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -22.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2348     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3653     |\n",
      "|    total_timesteps | 2348000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2349000, episode_reward=895.32 +/- 29.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 895      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2349000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=174.19 +/- 163.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 174      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.982   |\n",
      "|    critic_loss     | 0.000791 |\n",
      "|    ent_coef        | 0.000521 |\n",
      "|    ent_coef_loss   | -2.22    |\n",
      "|    learning_rate   | 0.00265  |\n",
      "|    n_updates       | 11470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2351000, episode_reward=-72.07 +/- 33.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -72.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2351000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2352000, episode_reward=-125.38 +/- 4.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2352000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.864   |\n",
      "|    critic_loss     | 0.000843 |\n",
      "|    ent_coef        | 0.000514 |\n",
      "|    ent_coef_loss   | -25.3    |\n",
      "|    learning_rate   | 0.00265  |\n",
      "|    n_updates       | 11480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -40.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2352     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3660     |\n",
      "|    total_timesteps | 2352000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2353000, episode_reward=-126.33 +/- 4.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2353000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2354000, episode_reward=-236.20 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -236     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2354000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.771   |\n",
      "|    critic_loss     | 0.000297 |\n",
      "|    ent_coef        | 0.000501 |\n",
      "|    ent_coef_loss   | -20.4    |\n",
      "|    learning_rate   | 0.00265  |\n",
      "|    n_updates       | 11490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2355000, episode_reward=-234.77 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -235     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2355000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2356000, episode_reward=-305.92 +/- 47.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -306     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2356000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.739   |\n",
      "|    critic_loss     | 0.000352 |\n",
      "|    ent_coef        | 0.000487 |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -81.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2356     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3666     |\n",
      "|    total_timesteps | 2356000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2357000, episode_reward=-211.72 +/- 170.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -212     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2357000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358000, episode_reward=588.88 +/- 104.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 589      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2358000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.75    |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000474 |\n",
      "|    ent_coef_loss   | -18.7    |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2359000, episode_reward=648.88 +/- 73.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 649      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2359000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=775.48 +/- 31.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 775      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.953   |\n",
      "|    critic_loss     | 0.000691 |\n",
      "|    ent_coef        | 0.000462 |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11520    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2360     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3672     |\n",
      "|    total_timesteps | 2360000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2361000, episode_reward=340.18 +/- 550.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 340      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2361000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2362000, episode_reward=818.09 +/- 14.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 818      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2362000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.991   |\n",
      "|    critic_loss     | 0.000629 |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | -0.00741 |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2363000, episode_reward=809.79 +/- 56.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 810      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2363000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2364000, episode_reward=-162.15 +/- 84.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -162     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2364000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.994   |\n",
      "|    critic_loss     | 0.000556 |\n",
      "|    ent_coef        | 0.000451 |\n",
      "|    ent_coef_loss   | 2.93     |\n",
      "|    learning_rate   | 0.00264  |\n",
      "|    n_updates       | 11540    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -87.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2364     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3678     |\n",
      "|    total_timesteps | 2364000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2365000, episode_reward=-219.43 +/- 37.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2365000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2366000, episode_reward=-146.90 +/- 10.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2366000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.826   |\n",
      "|    critic_loss     | 0.000981 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | -8.43    |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2367000, episode_reward=-157.09 +/- 7.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2367000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2368000, episode_reward=234.68 +/- 18.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 235      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2368000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.826   |\n",
      "|    critic_loss     | 0.000626 |\n",
      "|    ent_coef        | 0.000447 |\n",
      "|    ent_coef_loss   | -6.64    |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11560    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2368     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3684     |\n",
      "|    total_timesteps | 2368000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2369000, episode_reward=244.20 +/- 25.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 244      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2369000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=-237.76 +/- 11.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.78    |\n",
      "|    critic_loss     | 0.000613 |\n",
      "|    ent_coef        | 0.000442 |\n",
      "|    ent_coef_loss   | -17.5    |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2371000, episode_reward=-245.08 +/- 13.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -245     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2371000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2372000, episode_reward=-81.95 +/- 62.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -82      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2372000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.753   |\n",
      "|    critic_loss     | 0.000193 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | -15.8    |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11580    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -128     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2372     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3691     |\n",
      "|    total_timesteps | 2372000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2373000, episode_reward=-97.13 +/- 72.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2373000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2374000, episode_reward=-294.81 +/- 2.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -295     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2374000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.763   |\n",
      "|    critic_loss     | 0.000204 |\n",
      "|    ent_coef        | 0.000425 |\n",
      "|    ent_coef_loss   | -8.83    |\n",
      "|    learning_rate   | 0.00263  |\n",
      "|    n_updates       | 11590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2375000, episode_reward=-295.03 +/- 2.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -295     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2375000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2376000, episode_reward=-253.17 +/- 23.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -253     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2376000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.753   |\n",
      "|    critic_loss     | 0.000117 |\n",
      "|    ent_coef        | 0.000418 |\n",
      "|    ent_coef_loss   | -16.2    |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11600    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -126     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2376     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3697     |\n",
      "|    total_timesteps | 2376000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2377000, episode_reward=-239.76 +/- 30.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -240     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2377000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2378000, episode_reward=-261.40 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -261     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2378000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.705   |\n",
      "|    critic_loss     | 0.000301 |\n",
      "|    ent_coef        | 0.00041  |\n",
      "|    ent_coef_loss   | 2.41     |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2379000, episode_reward=-261.62 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -262     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2379000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=-272.38 +/- 7.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -272     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.749   |\n",
      "|    critic_loss     | 0.000265 |\n",
      "|    ent_coef        | 0.000406 |\n",
      "|    ent_coef_loss   | -27.5    |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11620    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2380     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3703     |\n",
      "|    total_timesteps | 2380000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2381000, episode_reward=-263.73 +/- 5.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -264     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2381000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2382000, episode_reward=-386.96 +/- 2.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -387     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2382000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.718   |\n",
      "|    critic_loss     | 0.000599 |\n",
      "|    ent_coef        | 0.000396 |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2383000, episode_reward=-385.46 +/- 1.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -385     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2383000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2384000, episode_reward=-453.61 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -454     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2384000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.705   |\n",
      "|    critic_loss     | 0.000289 |\n",
      "|    ent_coef        | 0.00039  |\n",
      "|    ent_coef_loss   | 29.4     |\n",
      "|    learning_rate   | 0.00262  |\n",
      "|    n_updates       | 11640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -129     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2384     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3709     |\n",
      "|    total_timesteps | 2384000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2385000, episode_reward=-452.51 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -453     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2385000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2386000, episode_reward=-442.72 +/- 2.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -443     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2386000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.809   |\n",
      "|    critic_loss     | 0.000425 |\n",
      "|    ent_coef        | 0.000397 |\n",
      "|    ent_coef_loss   | 19.3     |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2387000, episode_reward=-442.29 +/- 2.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -442     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2387000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2388000, episode_reward=-373.56 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -374     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2388000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.688   |\n",
      "|    critic_loss     | 0.000101 |\n",
      "|    ent_coef        | 0.000406 |\n",
      "|    ent_coef_loss   | 0.0567   |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -122     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2388     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3715     |\n",
      "|    total_timesteps | 2388000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2389000, episode_reward=-371.35 +/- 1.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2389000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=-373.24 +/- 1.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2390000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2391000, episode_reward=-413.27 +/- 2.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -413     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2391000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.696   |\n",
      "|    critic_loss     | 0.000104 |\n",
      "|    ent_coef        | 0.000409 |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2392000, episode_reward=-228.35 +/- 365.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2392000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2392     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3722     |\n",
      "|    total_timesteps | 2392000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2393000, episode_reward=-364.15 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -364     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2393000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.683   |\n",
      "|    critic_loss     | 0.000118 |\n",
      "|    ent_coef        | 0.000406 |\n",
      "|    ent_coef_loss   | -5.76    |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2394000, episode_reward=-363.81 +/- 3.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -364     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2394000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2395000, episode_reward=-474.83 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -475     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2395000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.809   |\n",
      "|    critic_loss     | 0.000458 |\n",
      "|    ent_coef        | 0.000402 |\n",
      "|    ent_coef_loss   | -4.71    |\n",
      "|    learning_rate   | 0.00261  |\n",
      "|    n_updates       | 11690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2396000, episode_reward=-476.41 +/- 3.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -476     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2396000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -110     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2396     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3728     |\n",
      "|    total_timesteps | 2396000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2397000, episode_reward=803.97 +/- 39.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 804      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2397000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.792   |\n",
      "|    critic_loss     | 0.000283 |\n",
      "|    ent_coef        | 0.000399 |\n",
      "|    ent_coef_loss   | 7.64     |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2398000, episode_reward=806.56 +/- 32.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 807      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2398000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399000, episode_reward=491.49 +/- 61.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 491      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2399000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.834   |\n",
      "|    critic_loss     | 0.00035  |\n",
      "|    ent_coef        | 0.000402 |\n",
      "|    ent_coef_loss   | 12       |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=504.34 +/- 41.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 504      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2400000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -87.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2400     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3734     |\n",
      "|    total_timesteps | 2400000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2401000, episode_reward=669.68 +/- 71.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 670      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2401000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.934   |\n",
      "|    critic_loss     | 0.000583 |\n",
      "|    ent_coef        | 0.000406 |\n",
      "|    ent_coef_loss   | -3.12    |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2402000, episode_reward=742.84 +/- 32.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 743      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2402000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2403000, episode_reward=550.81 +/- 51.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 551      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2403000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.922   |\n",
      "|    critic_loss     | 0.000462 |\n",
      "|    ent_coef        | 0.000407 |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2404000, episode_reward=546.39 +/- 27.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 546      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2404000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -63.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2404     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3740     |\n",
      "|    total_timesteps | 2404000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2405000, episode_reward=701.25 +/- 35.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 701      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2405000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.835   |\n",
      "|    critic_loss     | 0.000379 |\n",
      "|    ent_coef        | 0.000407 |\n",
      "|    ent_coef_loss   | -0.301   |\n",
      "|    learning_rate   | 0.0026   |\n",
      "|    n_updates       | 11740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2406000, episode_reward=725.60 +/- 59.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 726      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2406000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2407000, episode_reward=527.08 +/- 458.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 527      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2407000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.903   |\n",
      "|    critic_loss     | 0.000437 |\n",
      "|    ent_coef        | 0.000407 |\n",
      "|    ent_coef_loss   | -1.84    |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2408000, episode_reward=763.64 +/- 49.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 764      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2408000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -34.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2408     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3746     |\n",
      "|    total_timesteps | 2408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2409000, episode_reward=769.65 +/- 74.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 770      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2409000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.874   |\n",
      "|    critic_loss     | 0.000371 |\n",
      "|    ent_coef        | 0.000406 |\n",
      "|    ent_coef_loss   | 2.27     |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=745.35 +/- 49.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 745      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2410000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2411000, episode_reward=520.21 +/- 63.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 520      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2411000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.962   |\n",
      "|    critic_loss     | 0.000437 |\n",
      "|    ent_coef        | 0.000406 |\n",
      "|    ent_coef_loss   | -6.55    |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2412000, episode_reward=570.30 +/- 49.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 570      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2412000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.12     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2412     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3752     |\n",
      "|    total_timesteps | 2412000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2413000, episode_reward=335.82 +/- 57.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 336      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2413000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.934   |\n",
      "|    critic_loss     | 0.000459 |\n",
      "|    ent_coef        | 0.000405 |\n",
      "|    ent_coef_loss   | 0.374    |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2414000, episode_reward=371.80 +/- 103.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 372      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2414000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2415000, episode_reward=404.82 +/- 375.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 405      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2415000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.904   |\n",
      "|    critic_loss     | 0.00054  |\n",
      "|    ent_coef        | 0.000404 |\n",
      "|    ent_coef_loss   | -0.71    |\n",
      "|    learning_rate   | 0.00259  |\n",
      "|    n_updates       | 11790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2416000, episode_reward=614.51 +/- 76.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 615      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2416000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 30.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2416     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3758     |\n",
      "|    total_timesteps | 2416000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2417000, episode_reward=471.34 +/- 416.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 471      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2417000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.939   |\n",
      "|    critic_loss     | 0.000446 |\n",
      "|    ent_coef        | 0.000403 |\n",
      "|    ent_coef_loss   | -0.509   |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2418000, episode_reward=51.23 +/- 498.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 51.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2418000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2419000, episode_reward=269.46 +/- 506.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 269      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2419000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.951   |\n",
      "|    critic_loss     | 0.000464 |\n",
      "|    ent_coef        | 0.000403 |\n",
      "|    ent_coef_loss   | -5.8     |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=-172.15 +/- 359.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -172     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2420000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2420     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3765     |\n",
      "|    total_timesteps | 2420000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2421000, episode_reward=608.63 +/- 462.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 609      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2421000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.852   |\n",
      "|    critic_loss     | 0.000429 |\n",
      "|    ent_coef        | 0.0004   |\n",
      "|    ent_coef_loss   | -6.64    |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2422000, episode_reward=426.77 +/- 604.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2422000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2423000, episode_reward=825.47 +/- 54.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 825      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2423000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.77    |\n",
      "|    critic_loss     | 0.000252 |\n",
      "|    ent_coef        | 0.000397 |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2424000, episode_reward=828.64 +/- 47.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 829      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2424000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 79.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 2424     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3771     |\n",
      "|    total_timesteps | 2424000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2425000, episode_reward=780.76 +/- 47.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 781      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2425000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.969   |\n",
      "|    critic_loss     | 0.000511 |\n",
      "|    ent_coef        | 0.000391 |\n",
      "|    ent_coef_loss   | -5.36    |\n",
      "|    learning_rate   | 0.00258  |\n",
      "|    n_updates       | 11840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2426000, episode_reward=752.01 +/- 70.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 752      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2426000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2427000, episode_reward=874.99 +/- 21.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 875      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2427000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.96    |\n",
      "|    critic_loss     | 0.000444 |\n",
      "|    ent_coef        | 0.000387 |\n",
      "|    ent_coef_loss   | -4.4     |\n",
      "|    learning_rate   | 0.00257  |\n",
      "|    n_updates       | 11850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2428000, episode_reward=891.22 +/- 40.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 891      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2428000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2428     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3777     |\n",
      "|    total_timesteps | 2428000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2429000, episode_reward=896.32 +/- 41.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 896      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2429000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.802   |\n",
      "|    critic_loss     | 0.000281 |\n",
      "|    ent_coef        | 0.000384 |\n",
      "|    ent_coef_loss   | -3.08    |\n",
      "|    learning_rate   | 0.00257  |\n",
      "|    n_updates       | 11860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=910.31 +/- 55.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 910      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2430000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2431000, episode_reward=593.05 +/- 47.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 593      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2431000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.814   |\n",
      "|    critic_loss     | 0.000309 |\n",
      "|    ent_coef        | 0.000382 |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.00257  |\n",
      "|    n_updates       | 11870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2432000, episode_reward=623.37 +/- 54.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 623      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2432000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 128      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2432     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3783     |\n",
      "|    total_timesteps | 2432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2433000, episode_reward=649.39 +/- 55.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 649      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2433000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2434000, episode_reward=819.02 +/- 59.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 819      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2434000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.921   |\n",
      "|    critic_loss     | 0.000528 |\n",
      "|    ent_coef        | 0.000377 |\n",
      "|    ent_coef_loss   | -7.85    |\n",
      "|    learning_rate   | 0.00257  |\n",
      "|    n_updates       | 11880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2435000, episode_reward=854.26 +/- 43.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 854      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2435000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2436000, episode_reward=922.56 +/- 36.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 923      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2436000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.979   |\n",
      "|    critic_loss     | 0.000488 |\n",
      "|    ent_coef        | 0.000373 |\n",
      "|    ent_coef_loss   | 3.57     |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2436     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3789     |\n",
      "|    total_timesteps | 2436000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2437000, episode_reward=916.59 +/- 56.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 917      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2437000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2438000, episode_reward=576.87 +/- 54.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 577      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2438000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.973   |\n",
      "|    critic_loss     | 0.000426 |\n",
      "|    ent_coef        | 0.000372 |\n",
      "|    ent_coef_loss   | 0.203    |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439000, episode_reward=513.05 +/- 51.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 513      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2439000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=642.44 +/- 56.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 642      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.938   |\n",
      "|    critic_loss     | 0.00056  |\n",
      "|    ent_coef        | 0.000372 |\n",
      "|    ent_coef_loss   | 3.44     |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 188      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2440     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3795     |\n",
      "|    total_timesteps | 2440000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2441000, episode_reward=656.44 +/- 43.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 656      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2441000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2442000, episode_reward=1022.64 +/- 39.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2442000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.937   |\n",
      "|    critic_loss     | 0.000537 |\n",
      "|    ent_coef        | 0.000373 |\n",
      "|    ent_coef_loss   | -1.81    |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2443000, episode_reward=1054.41 +/- 22.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2443000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2444000, episode_reward=972.62 +/- 35.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 973      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2444000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.99    |\n",
      "|    critic_loss     | 0.000463 |\n",
      "|    ent_coef        | 0.000373 |\n",
      "|    ent_coef_loss   | -6.72    |\n",
      "|    learning_rate   | 0.00256  |\n",
      "|    n_updates       | 11930    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 224      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2444     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3802     |\n",
      "|    total_timesteps | 2444000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2445000, episode_reward=962.43 +/- 53.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 962      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2445000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2446000, episode_reward=733.42 +/- 50.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 733      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2446000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.983   |\n",
      "|    critic_loss     | 0.000494 |\n",
      "|    ent_coef        | 0.000371 |\n",
      "|    ent_coef_loss   | 1.82     |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2447000, episode_reward=672.10 +/- 66.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 672      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2447000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2448000, episode_reward=790.50 +/- 50.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 790      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2448000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.949   |\n",
      "|    critic_loss     | 0.000537 |\n",
      "|    ent_coef        | 0.00037  |\n",
      "|    ent_coef_loss   | 3.84     |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11950    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 245      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2448     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3808     |\n",
      "|    total_timesteps | 2448000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2449000, episode_reward=762.82 +/- 35.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 763      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2449000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2450000, episode_reward=1029.65 +/- 57.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.974   |\n",
      "|    critic_loss     | 0.000503 |\n",
      "|    ent_coef        | 0.000372 |\n",
      "|    ent_coef_loss   | 9.13     |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2451000, episode_reward=1046.57 +/- 12.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2451000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2452000, episode_reward=831.60 +/- 39.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 832      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2452000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.999   |\n",
      "|    critic_loss     | 0.000545 |\n",
      "|    ent_coef        | 0.000375 |\n",
      "|    ent_coef_loss   | -2.6     |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11970    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 271      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2452     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3814     |\n",
      "|    total_timesteps | 2452000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2453000, episode_reward=808.64 +/- 24.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 809      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2453000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2454000, episode_reward=750.19 +/- 35.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 750      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2454000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.968   |\n",
      "|    critic_loss     | 0.000539 |\n",
      "|    ent_coef        | 0.000376 |\n",
      "|    ent_coef_loss   | -1.52    |\n",
      "|    learning_rate   | 0.00255  |\n",
      "|    n_updates       | 11980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2455000, episode_reward=734.81 +/- 21.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 735      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2455000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2456000, episode_reward=741.99 +/- 16.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 742      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2456000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.953   |\n",
      "|    critic_loss     | 0.00052  |\n",
      "|    ent_coef        | 0.000375 |\n",
      "|    ent_coef_loss   | 0.765    |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 11990    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 308      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2456     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3820     |\n",
      "|    total_timesteps | 2456000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2457000, episode_reward=759.15 +/- 12.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 759      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2457000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2458000, episode_reward=925.78 +/- 29.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 926      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2458000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.959   |\n",
      "|    critic_loss     | 0.000512 |\n",
      "|    ent_coef        | 0.000376 |\n",
      "|    ent_coef_loss   | 3.24     |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2459000, episode_reward=935.39 +/- 39.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 935      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2459000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=882.16 +/- 29.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 882      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.992   |\n",
      "|    critic_loss     | 0.000542 |\n",
      "|    ent_coef        | 0.000377 |\n",
      "|    ent_coef_loss   | 5.59     |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 12010    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 328      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2460     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3826     |\n",
      "|    total_timesteps | 2460000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2461000, episode_reward=838.14 +/- 48.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 838      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2461000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2462000, episode_reward=612.75 +/- 46.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 613      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2462000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.986   |\n",
      "|    critic_loss     | 0.000466 |\n",
      "|    ent_coef        | 0.00038  |\n",
      "|    ent_coef_loss   | 8.61     |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 12020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2463000, episode_reward=588.36 +/- 49.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 588      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2463000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2464000, episode_reward=672.11 +/- 22.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 672      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2464000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.933   |\n",
      "|    critic_loss     | 0.000632 |\n",
      "|    ent_coef        | 0.000384 |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.00254  |\n",
      "|    n_updates       | 12030    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 330      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2464     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3832     |\n",
      "|    total_timesteps | 2464000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2465000, episode_reward=684.15 +/- 14.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 684      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2465000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2466000, episode_reward=768.97 +/- 41.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 769      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2466000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.944   |\n",
      "|    critic_loss     | 0.000588 |\n",
      "|    ent_coef        | 0.000385 |\n",
      "|    ent_coef_loss   | -1.09    |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2467000, episode_reward=778.56 +/- 17.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 779      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2467000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2468000, episode_reward=836.52 +/- 28.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 837      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2468000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.956   |\n",
      "|    critic_loss     | 0.000595 |\n",
      "|    ent_coef        | 0.000385 |\n",
      "|    ent_coef_loss   | 0.931    |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 353      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2468     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3838     |\n",
      "|    total_timesteps | 2468000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2469000, episode_reward=835.42 +/- 53.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 835      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2469000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=726.16 +/- 53.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 726      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.976   |\n",
      "|    critic_loss     | 0.000488 |\n",
      "|    ent_coef        | 0.000386 |\n",
      "|    ent_coef_loss   | 5.47     |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2471000, episode_reward=755.60 +/- 44.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 756      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2471000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2472000, episode_reward=431.01 +/- 564.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2472000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.955   |\n",
      "|    critic_loss     | 0.000499 |\n",
      "|    ent_coef        | 0.000388 |\n",
      "|    ent_coef_loss   | 2.79     |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 388      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2472     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3844     |\n",
      "|    total_timesteps | 2472000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2473000, episode_reward=905.97 +/- 37.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 906      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2473000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2474000, episode_reward=477.33 +/- 612.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 477      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2474000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.976   |\n",
      "|    critic_loss     | 0.000544 |\n",
      "|    ent_coef        | 0.00039  |\n",
      "|    ent_coef_loss   | 5.8      |\n",
      "|    learning_rate   | 0.00253  |\n",
      "|    n_updates       | 12080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2475000, episode_reward=965.78 +/- 33.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 966      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2475000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2476000, episode_reward=732.78 +/- 500.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 733      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2476000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 431      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2476     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3850     |\n",
      "|    total_timesteps | 2476000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2477000, episode_reward=903.27 +/- 64.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 903      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2477000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.988   |\n",
      "|    critic_loss     | 0.000452 |\n",
      "|    ent_coef        | 0.000393 |\n",
      "|    ent_coef_loss   | 8.94     |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2478000, episode_reward=421.23 +/- 573.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 421      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2478000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479000, episode_reward=945.57 +/- 37.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 946      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2479000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.976   |\n",
      "|    critic_loss     | 0.000507 |\n",
      "|    ent_coef        | 0.000398 |\n",
      "|    ent_coef_loss   | 6.21     |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=929.24 +/- 35.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 929      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 478      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2480     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3857     |\n",
      "|    total_timesteps | 2480000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2481000, episode_reward=959.95 +/- 21.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 960      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2481000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.978   |\n",
      "|    critic_loss     | 0.000502 |\n",
      "|    ent_coef        | 0.000402 |\n",
      "|    ent_coef_loss   | 4.25     |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2482000, episode_reward=906.76 +/- 61.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 907      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2482000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2483000, episode_reward=842.54 +/- 40.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 843      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2483000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.979   |\n",
      "|    critic_loss     | 0.000563 |\n",
      "|    ent_coef        | 0.000405 |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2484000, episode_reward=837.89 +/- 39.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 838      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2484000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 526      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2484     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3863     |\n",
      "|    total_timesteps | 2484000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2485000, episode_reward=1038.76 +/- 25.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2485000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.97    |\n",
      "|    critic_loss     | 0.000552 |\n",
      "|    ent_coef        | 0.000407 |\n",
      "|    ent_coef_loss   | 8.68     |\n",
      "|    learning_rate   | 0.00252  |\n",
      "|    n_updates       | 12130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2486000, episode_reward=1047.58 +/- 67.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2486000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2487000, episode_reward=961.82 +/- 46.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 962      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2487000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.992   |\n",
      "|    critic_loss     | 0.000475 |\n",
      "|    ent_coef        | 0.000411 |\n",
      "|    ent_coef_loss   | 4.89     |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2488000, episode_reward=970.06 +/- 20.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 970      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2488000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 569      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2488     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 3869     |\n",
      "|    total_timesteps | 2488000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2489000, episode_reward=897.92 +/- 37.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 898      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2489000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.963   |\n",
      "|    critic_loss     | 0.000544 |\n",
      "|    ent_coef        | 0.000415 |\n",
      "|    ent_coef_loss   | 0.132    |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=873.87 +/- 32.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 874      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2490000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2491000, episode_reward=989.30 +/- 39.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 989      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2491000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.968   |\n",
      "|    critic_loss     | 0.000521 |\n",
      "|    ent_coef        | 0.000417 |\n",
      "|    ent_coef_loss   | 5.14     |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2492000, episode_reward=980.30 +/- 53.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 980      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2492000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 620      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2492     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3875     |\n",
      "|    total_timesteps | 2492000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2493000, episode_reward=1028.11 +/- 13.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2493000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.985   |\n",
      "|    critic_loss     | 0.000482 |\n",
      "|    ent_coef        | 0.00042  |\n",
      "|    ent_coef_loss   | 8.87     |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2494000, episode_reward=1024.08 +/- 29.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2494000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2495000, episode_reward=980.07 +/- 72.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 980      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2495000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.994   |\n",
      "|    critic_loss     | 0.000507 |\n",
      "|    ent_coef        | 0.000425 |\n",
      "|    ent_coef_loss   | 4.83     |\n",
      "|    learning_rate   | 0.00251  |\n",
      "|    n_updates       | 12180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2496000, episode_reward=944.02 +/- 41.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 944      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2496000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 655      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2496     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3881     |\n",
      "|    total_timesteps | 2496000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2497000, episode_reward=881.20 +/- 40.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 881      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2497000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.977   |\n",
      "|    critic_loss     | 0.000585 |\n",
      "|    ent_coef        | 0.000429 |\n",
      "|    ent_coef_loss   | 3.1      |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2498000, episode_reward=896.24 +/- 19.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 896      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2498000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2499000, episode_reward=948.58 +/- 22.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 949      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2499000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.969   |\n",
      "|    critic_loss     | 0.000578 |\n",
      "|    ent_coef        | 0.000432 |\n",
      "|    ent_coef_loss   | 2.64     |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=971.49 +/- 29.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 971      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 678      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2500     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3887     |\n",
      "|    total_timesteps | 2500000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2501000, episode_reward=840.23 +/- 5.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 840      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2501000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.98    |\n",
      "|    critic_loss     | 0.000569 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | 0.806    |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2502000, episode_reward=837.29 +/- 24.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 837      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2502000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2503000, episode_reward=928.58 +/- 39.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 929      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2503000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.952   |\n",
      "|    critic_loss     | 0.000573 |\n",
      "|    ent_coef        | 0.000435 |\n",
      "|    ent_coef_loss   | 0.37     |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2504000, episode_reward=904.23 +/- 22.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 904      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2504000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 696      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2504     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3893     |\n",
      "|    total_timesteps | 2504000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2505000, episode_reward=1044.60 +/- 30.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2505000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.968   |\n",
      "|    critic_loss     | 0.000579 |\n",
      "|    ent_coef        | 0.000436 |\n",
      "|    ent_coef_loss   | -2.38    |\n",
      "|    learning_rate   | 0.0025   |\n",
      "|    n_updates       | 12230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2506000, episode_reward=1069.43 +/- 14.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2506000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2507000, episode_reward=840.02 +/- 26.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 840      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2507000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.993   |\n",
      "|    critic_loss     | 0.000507 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | -3.7     |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2508000, episode_reward=852.67 +/- 11.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 853      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2508000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 717      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2508     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3900     |\n",
      "|    total_timesteps | 2508000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2509000, episode_reward=805.22 +/- 40.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 805      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2509000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.963   |\n",
      "|    critic_loss     | 0.0006   |\n",
      "|    ent_coef        | 0.000432 |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=789.92 +/- 36.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 790      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2510000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2511000, episode_reward=964.75 +/- 35.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 965      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2511000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.953   |\n",
      "|    critic_loss     | 0.000563 |\n",
      "|    ent_coef        | 0.000431 |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2512000, episode_reward=988.58 +/- 32.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 989      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2512000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 723      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2512     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3906     |\n",
      "|    total_timesteps | 2512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2513000, episode_reward=833.87 +/- 43.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 834      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2513000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.97    |\n",
      "|    critic_loss     | 0.000585 |\n",
      "|    ent_coef        | 0.000429 |\n",
      "|    ent_coef_loss   | -5.15    |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2514000, episode_reward=874.04 +/- 46.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 874      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2514000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2515000, episode_reward=1001.56 +/- 41.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2515000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.962   |\n",
      "|    critic_loss     | 0.000543 |\n",
      "|    ent_coef        | 0.000427 |\n",
      "|    ent_coef_loss   | 4.69     |\n",
      "|    learning_rate   | 0.00249  |\n",
      "|    n_updates       | 12280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2516000, episode_reward=991.35 +/- 43.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 991      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2516000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 743      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2516     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3912     |\n",
      "|    total_timesteps | 2516000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2517000, episode_reward=1105.19 +/- 40.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2517000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.978   |\n",
      "|    critic_loss     | 0.000553 |\n",
      "|    ent_coef        | 0.000429 |\n",
      "|    ent_coef_loss   | 2.48     |\n",
      "|    learning_rate   | 0.00248  |\n",
      "|    n_updates       | 12290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2518000, episode_reward=1079.07 +/- 30.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2518000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2519000, episode_reward=1087.62 +/- 17.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2519000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=974.89 +/- 90.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 975      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.999   |\n",
      "|    critic_loss     | 0.000557 |\n",
      "|    ent_coef        | 0.00043  |\n",
      "|    ent_coef_loss   | -0.445   |\n",
      "|    learning_rate   | 0.00248  |\n",
      "|    n_updates       | 12300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 758      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2520     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3918     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2521000, episode_reward=989.92 +/- 39.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 990      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2521000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2522000, episode_reward=862.02 +/- 26.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 862      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2522000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.966   |\n",
      "|    critic_loss     | 0.000562 |\n",
      "|    ent_coef        | 0.00043  |\n",
      "|    ent_coef_loss   | 0.0856   |\n",
      "|    learning_rate   | 0.00248  |\n",
      "|    n_updates       | 12310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2523000, episode_reward=891.75 +/- 23.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 892      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2523000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2524000, episode_reward=849.72 +/- 20.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 850      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2524000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.956   |\n",
      "|    critic_loss     | 0.000592 |\n",
      "|    ent_coef        | 0.000431 |\n",
      "|    ent_coef_loss   | 8.28     |\n",
      "|    learning_rate   | 0.00248  |\n",
      "|    n_updates       | 12320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 785      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2524     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3924     |\n",
      "|    total_timesteps | 2524000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2525000, episode_reward=876.03 +/- 25.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 876      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2525000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2526000, episode_reward=791.05 +/- 14.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 791      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2526000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.958   |\n",
      "|    critic_loss     | 0.00061  |\n",
      "|    ent_coef        | 0.000435 |\n",
      "|    ent_coef_loss   | 3.3      |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2527000, episode_reward=800.54 +/- 10.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 801      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2527000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2528000, episode_reward=766.09 +/- 21.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 766      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2528000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.949   |\n",
      "|    critic_loss     | 0.000611 |\n",
      "|    ent_coef        | 0.000438 |\n",
      "|    ent_coef_loss   | 0.711    |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12340    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 798      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2528     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3930     |\n",
      "|    total_timesteps | 2528000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2529000, episode_reward=766.94 +/- 33.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 767      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2529000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=891.42 +/- 37.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 891      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.956   |\n",
      "|    critic_loss     | 0.000633 |\n",
      "|    ent_coef        | 0.00044  |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2531000, episode_reward=942.01 +/- 20.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 942      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2531000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2532000, episode_reward=1086.33 +/- 17.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2532000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.97    |\n",
      "|    critic_loss     | 0.000545 |\n",
      "|    ent_coef        | 0.000441 |\n",
      "|    ent_coef_loss   | 0.596    |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12360    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 818      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2532     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3936     |\n",
      "|    total_timesteps | 2532000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2533000, episode_reward=1048.86 +/- 23.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2533000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2534000, episode_reward=1016.76 +/- 23.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2534000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.99    |\n",
      "|    critic_loss     | 0.000551 |\n",
      "|    ent_coef        | 0.000442 |\n",
      "|    ent_coef_loss   | 2.27     |\n",
      "|    learning_rate   | 0.00247  |\n",
      "|    n_updates       | 12370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2535000, episode_reward=1034.02 +/- 41.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2535000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2536000, episode_reward=1046.28 +/- 11.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2536000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.974   |\n",
      "|    critic_loss     | 0.000617 |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12380    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 828      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2536     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3942     |\n",
      "|    total_timesteps | 2536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2537000, episode_reward=1016.78 +/- 28.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2537000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2538000, episode_reward=934.16 +/- 9.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 934      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2538000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.977   |\n",
      "|    critic_loss     | 0.000637 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | 5.39     |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2539000, episode_reward=941.87 +/- 50.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 942      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2539000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=717.27 +/- 14.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 717      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.96    |\n",
      "|    critic_loss     | 0.000723 |\n",
      "|    ent_coef        | 0.000447 |\n",
      "|    ent_coef_loss   | -1.66    |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12400    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 837      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2540     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3949     |\n",
      "|    total_timesteps | 2540000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2541000, episode_reward=701.26 +/- 32.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 701      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2541000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2542000, episode_reward=880.53 +/- 27.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 881      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2542000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.925   |\n",
      "|    critic_loss     | 0.00078  |\n",
      "|    ent_coef        | 0.000448 |\n",
      "|    ent_coef_loss   | 2.7      |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2543000, episode_reward=873.90 +/- 27.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 874      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2543000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2544000, episode_reward=879.66 +/- 48.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 880      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2544000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.962   |\n",
      "|    critic_loss     | 0.000675 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | 3.1      |\n",
      "|    learning_rate   | 0.00246  |\n",
      "|    n_updates       | 12420    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 838      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2544     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3955     |\n",
      "|    total_timesteps | 2544000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2545000, episode_reward=898.99 +/- 35.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 899      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2545000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2546000, episode_reward=875.02 +/- 40.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 875      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2546000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.946   |\n",
      "|    critic_loss     | 0.000712 |\n",
      "|    ent_coef        | 0.000452 |\n",
      "|    ent_coef_loss   | -0.085   |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2547000, episode_reward=885.33 +/- 26.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 885      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2547000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2548000, episode_reward=597.34 +/- 30.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 597      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2548000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.947   |\n",
      "|    critic_loss     | 0.000722 |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | 3.89     |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12440    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 840      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2548     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3961     |\n",
      "|    total_timesteps | 2548000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2549000, episode_reward=608.32 +/- 42.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 608      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2549000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=756.98 +/- 31.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 757      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.905   |\n",
      "|    critic_loss     | 0.000779 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | 0.629    |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2551000, episode_reward=796.54 +/- 26.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 797      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2551000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2552000, episode_reward=1011.90 +/- 18.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2552000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.941   |\n",
      "|    critic_loss     | 0.000697 |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | 3.48     |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 834      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2552     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3967     |\n",
      "|    total_timesteps | 2552000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2553000, episode_reward=989.89 +/- 48.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 990      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2553000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2554000, episode_reward=1013.72 +/- 63.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2554000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.968   |\n",
      "|    critic_loss     | 0.000653 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | -2.65    |\n",
      "|    learning_rate   | 0.00245  |\n",
      "|    n_updates       | 12470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2555000, episode_reward=1043.66 +/- 15.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2555000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2556000, episode_reward=1032.24 +/- 35.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2556000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.972   |\n",
      "|    critic_loss     | 0.000687 |\n",
      "|    ent_coef        | 0.000458 |\n",
      "|    ent_coef_loss   | -4.1     |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 843      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2556     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3973     |\n",
      "|    total_timesteps | 2556000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2557000, episode_reward=985.50 +/- 21.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 986      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2557000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2558000, episode_reward=979.22 +/- 53.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 979      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2558000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.974   |\n",
      "|    critic_loss     | 0.000641 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | 1.25     |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2559000, episode_reward=984.21 +/- 54.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 984      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2559000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=948.13 +/- 40.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 948      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2560000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 847      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2560     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3979     |\n",
      "|    total_timesteps | 2560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2561000, episode_reward=954.94 +/- 52.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 955      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2561000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.942   |\n",
      "|    critic_loss     | 0.000709 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | -0.963   |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2562000, episode_reward=919.13 +/- 39.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 919      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2562000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2563000, episode_reward=465.78 +/- 708.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 466      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2563000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.949   |\n",
      "|    critic_loss     | 0.000702 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | 1.81     |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2564000, episode_reward=-117.77 +/- 567.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2564000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 844      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2564     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3985     |\n",
      "|    total_timesteps | 2564000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2565000, episode_reward=677.79 +/- 463.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 678      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2565000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.83    |\n",
      "|    critic_loss     | 0.000772 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | -1.93    |\n",
      "|    learning_rate   | 0.00244  |\n",
      "|    n_updates       | 12520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2566000, episode_reward=943.31 +/- 49.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 943      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2566000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2567000, episode_reward=919.78 +/- 17.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 920      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2567000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.935   |\n",
      "|    critic_loss     | 0.000755 |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | 8.28     |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2568000, episode_reward=944.69 +/- 30.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 945      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2568000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 851      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2568     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3992     |\n",
      "|    total_timesteps | 2568000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2569000, episode_reward=261.74 +/- 646.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 262      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2569000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.954   |\n",
      "|    critic_loss     | 0.000723 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | 2.43     |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=796.28 +/- 530.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 796      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2570000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2571000, episode_reward=764.11 +/- 521.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 764      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2571000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.986   |\n",
      "|    critic_loss     | 0.000696 |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | 0.285    |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2572000, episode_reward=509.28 +/- 644.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 509      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2572000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 861      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2572     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 3998     |\n",
      "|    total_timesteps | 2572000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2573000, episode_reward=815.03 +/- 32.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 815      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2573000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.943   |\n",
      "|    critic_loss     | 0.00077  |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2574000, episode_reward=575.83 +/- 421.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 576      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2574000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2575000, episode_reward=820.16 +/- 33.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 820      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2575000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.925   |\n",
      "|    critic_loss     | 0.000813 |\n",
      "|    ent_coef        | 0.000464 |\n",
      "|    ent_coef_loss   | 5.66     |\n",
      "|    learning_rate   | 0.00243  |\n",
      "|    n_updates       | 12570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2576000, episode_reward=838.28 +/- 48.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 838      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2576000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 856      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2576     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4004     |\n",
      "|    total_timesteps | 2576000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2577000, episode_reward=1033.33 +/- 65.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2577000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.93    |\n",
      "|    critic_loss     | 0.00077  |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | 2.54     |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2578000, episode_reward=1010.93 +/- 72.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2578000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2579000, episode_reward=1010.53 +/- 26.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2579000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.975   |\n",
      "|    critic_loss     | 0.00064  |\n",
      "|    ent_coef        | 0.00047  |\n",
      "|    ent_coef_loss   | 9.35     |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=997.43 +/- 34.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 997      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2580000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 860      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2580     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4010     |\n",
      "|    total_timesteps | 2580000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2581000, episode_reward=965.89 +/- 42.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 966      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2581000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.965   |\n",
      "|    critic_loss     | 0.000635 |\n",
      "|    ent_coef        | 0.000476 |\n",
      "|    ent_coef_loss   | 1.87     |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2582000, episode_reward=948.31 +/- 22.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 948      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2582000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2583000, episode_reward=1019.54 +/- 36.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2583000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.929   |\n",
      "|    critic_loss     | 0.000701 |\n",
      "|    ent_coef        | 0.000479 |\n",
      "|    ent_coef_loss   | 1.86     |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2584000, episode_reward=1046.34 +/- 35.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2584000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 861      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2584     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4016     |\n",
      "|    total_timesteps | 2584000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2585000, episode_reward=1008.62 +/- 18.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2585000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.952   |\n",
      "|    critic_loss     | 0.000759 |\n",
      "|    ent_coef        | 0.000482 |\n",
      "|    ent_coef_loss   | -0.817   |\n",
      "|    learning_rate   | 0.00242  |\n",
      "|    n_updates       | 12620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2586000, episode_reward=1022.82 +/- 36.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2586000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2587000, episode_reward=1065.15 +/- 31.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2587000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.952   |\n",
      "|    critic_loss     | 0.000773 |\n",
      "|    ent_coef        | 0.000482 |\n",
      "|    ent_coef_loss   | -3.07    |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2588000, episode_reward=6.68 +/- 545.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 6.68     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2588000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 866      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2588     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4022     |\n",
      "|    total_timesteps | 2588000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2589000, episode_reward=-7.36 +/- 543.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -7.36    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2589000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.973   |\n",
      "|    critic_loss     | 0.000734 |\n",
      "|    ent_coef        | 0.000481 |\n",
      "|    ent_coef_loss   | 2.8      |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=516.72 +/- 649.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 517      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2590000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2591000, episode_reward=-307.89 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2591000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.724   |\n",
      "|    critic_loss     | 0.000418 |\n",
      "|    ent_coef        | 0.000479 |\n",
      "|    ent_coef_loss   | -14.5    |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2592000, episode_reward=176.19 +/- 593.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 176      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2592000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 846      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2592     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4029     |\n",
      "|    total_timesteps | 2592000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2593000, episode_reward=938.90 +/- 61.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 939      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2593000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.894   |\n",
      "|    critic_loss     | 0.000945 |\n",
      "|    ent_coef        | 0.000473 |\n",
      "|    ent_coef_loss   | -0.846   |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2594000, episode_reward=929.12 +/- 58.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 929      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2594000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2595000, episode_reward=838.19 +/- 43.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 838      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2595000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.948   |\n",
      "|    critic_loss     | 0.000945 |\n",
      "|    ent_coef        | 0.00047  |\n",
      "|    ent_coef_loss   | 6.7      |\n",
      "|    learning_rate   | 0.00241  |\n",
      "|    n_updates       | 12670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2596000, episode_reward=787.23 +/- 25.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 787      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2596000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 842      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2596     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4035     |\n",
      "|    total_timesteps | 2596000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2597000, episode_reward=869.30 +/- 88.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 869      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2597000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.923   |\n",
      "|    critic_loss     | 0.000893 |\n",
      "|    ent_coef        | 0.000472 |\n",
      "|    ent_coef_loss   | 0.0728   |\n",
      "|    learning_rate   | 0.0024   |\n",
      "|    n_updates       | 12680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2598000, episode_reward=883.84 +/- 45.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 884      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2598000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2599000, episode_reward=1072.16 +/- 61.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2599000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.928   |\n",
      "|    critic_loss     | 0.000819 |\n",
      "|    ent_coef        | 0.000472 |\n",
      "|    ent_coef_loss   | -5.32    |\n",
      "|    learning_rate   | 0.0024   |\n",
      "|    n_updates       | 12690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=1041.47 +/- 21.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2600000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 842      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2600     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4041     |\n",
      "|    total_timesteps | 2600000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2601000, episode_reward=891.49 +/- 32.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 891      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2601000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.962   |\n",
      "|    critic_loss     | 0.000817 |\n",
      "|    ent_coef        | 0.000471 |\n",
      "|    ent_coef_loss   | 5.54     |\n",
      "|    learning_rate   | 0.0024   |\n",
      "|    n_updates       | 12700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2602000, episode_reward=972.48 +/- 44.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 972      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2602000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2603000, episode_reward=908.49 +/- 58.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 908      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2603000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2604000, episode_reward=772.68 +/- 49.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 773      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2604000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.936   |\n",
      "|    critic_loss     | 0.000812 |\n",
      "|    ent_coef        | 0.000473 |\n",
      "|    ent_coef_loss   | 4.58     |\n",
      "|    learning_rate   | 0.0024   |\n",
      "|    n_updates       | 12710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 845      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2604     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4047     |\n",
      "|    total_timesteps | 2604000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2605000, episode_reward=812.70 +/- 73.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 813      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2605000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2606000, episode_reward=957.21 +/- 18.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 957      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2606000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.912   |\n",
      "|    critic_loss     | 0.000863 |\n",
      "|    ent_coef        | 0.000477 |\n",
      "|    ent_coef_loss   | 3.86     |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2607000, episode_reward=947.88 +/- 68.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 948      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2607000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2608000, episode_reward=1052.77 +/- 19.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2608000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.938   |\n",
      "|    critic_loss     | 0.00083  |\n",
      "|    ent_coef        | 0.00048  |\n",
      "|    ent_coef_loss   | -0.683   |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 845      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2608     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4053     |\n",
      "|    total_timesteps | 2608000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2609000, episode_reward=1061.28 +/- 35.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2609000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=1000.07 +/- 38.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.947   |\n",
      "|    critic_loss     | 0.000709 |\n",
      "|    ent_coef        | 0.000481 |\n",
      "|    ent_coef_loss   | 0.259    |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2611000, episode_reward=933.77 +/- 42.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 934      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2611000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2612000, episode_reward=1064.94 +/- 53.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2612000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.938   |\n",
      "|    critic_loss     | 0.000835 |\n",
      "|    ent_coef        | 0.000482 |\n",
      "|    ent_coef_loss   | 9.09     |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12750    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 849      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2612     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4059     |\n",
      "|    total_timesteps | 2612000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2613000, episode_reward=1075.24 +/- 27.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2613000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2614000, episode_reward=922.10 +/- 34.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 922      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2614000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.953   |\n",
      "|    critic_loss     | 0.000742 |\n",
      "|    ent_coef        | 0.000488 |\n",
      "|    ent_coef_loss   | 5.07     |\n",
      "|    learning_rate   | 0.00239  |\n",
      "|    n_updates       | 12760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2615000, episode_reward=977.56 +/- 19.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 978      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2615000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2616000, episode_reward=931.09 +/- 19.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 931      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2616000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.949   |\n",
      "|    critic_loss     | 0.00069  |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | 3.02     |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12770    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 850      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2616     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4066     |\n",
      "|    total_timesteps | 2616000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2617000, episode_reward=905.24 +/- 70.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 905      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2617000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2618000, episode_reward=999.68 +/- 23.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2618000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.917   |\n",
      "|    critic_loss     | 0.00081  |\n",
      "|    ent_coef        | 0.000498 |\n",
      "|    ent_coef_loss   | 7.66     |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2619000, episode_reward=1040.07 +/- 74.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2619000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=1074.60 +/- 25.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.946   |\n",
      "|    critic_loss     | 0.000777 |\n",
      "|    ent_coef        | 0.000505 |\n",
      "|    ent_coef_loss   | 1.95     |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12790    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 848      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2620     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4072     |\n",
      "|    total_timesteps | 2620000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2621000, episode_reward=1059.61 +/- 32.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2621000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2622000, episode_reward=994.95 +/- 31.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 995      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2622000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.954   |\n",
      "|    critic_loss     | 0.000804 |\n",
      "|    ent_coef        | 0.000508 |\n",
      "|    ent_coef_loss   | -3.41    |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2623000, episode_reward=988.12 +/- 62.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 988      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2623000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2624000, episode_reward=1076.19 +/- 37.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2624000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.939   |\n",
      "|    critic_loss     | 0.000858 |\n",
      "|    ent_coef        | 0.000507 |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.00238  |\n",
      "|    n_updates       | 12810    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 853      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2624     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4078     |\n",
      "|    total_timesteps | 2624000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2625000, episode_reward=1080.81 +/- 51.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2625000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2626000, episode_reward=967.76 +/- 95.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 968      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2626000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.968   |\n",
      "|    critic_loss     | 0.000878 |\n",
      "|    ent_coef        | 0.000506 |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2627000, episode_reward=987.81 +/- 38.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 988      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2627000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2628000, episode_reward=840.77 +/- 53.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 841      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2628000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.936   |\n",
      "|    critic_loss     | 0.000905 |\n",
      "|    ent_coef        | 0.000506 |\n",
      "|    ent_coef_loss   | 1.69     |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12830    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 861      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2628     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4084     |\n",
      "|    total_timesteps | 2628000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2629000, episode_reward=871.37 +/- 13.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 871      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2629000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2630000, episode_reward=860.75 +/- 39.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 861      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.919   |\n",
      "|    critic_loss     | 0.000966 |\n",
      "|    ent_coef        | 0.000508 |\n",
      "|    ent_coef_loss   | 0.222    |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2631000, episode_reward=850.71 +/- 64.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 851      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2631000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2632000, episode_reward=1008.73 +/- 17.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2632000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.916   |\n",
      "|    critic_loss     | 0.000895 |\n",
      "|    ent_coef        | 0.000508 |\n",
      "|    ent_coef_loss   | -3.37    |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12850    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 861      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2632     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4090     |\n",
      "|    total_timesteps | 2632000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2633000, episode_reward=1027.37 +/- 52.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2633000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2634000, episode_reward=1080.06 +/- 31.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2634000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.94    |\n",
      "|    critic_loss     | 0.000924 |\n",
      "|    ent_coef        | 0.000506 |\n",
      "|    ent_coef_loss   | -0.708   |\n",
      "|    learning_rate   | 0.00237  |\n",
      "|    n_updates       | 12860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2635000, episode_reward=1076.10 +/- 60.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2635000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2636000, episode_reward=898.22 +/- 66.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 898      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2636000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.95    |\n",
      "|    critic_loss     | 0.000833 |\n",
      "|    ent_coef        | 0.000505 |\n",
      "|    ent_coef_loss   | 1.68     |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12870    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 861      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2636     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4096     |\n",
      "|    total_timesteps | 2636000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2637000, episode_reward=893.15 +/- 53.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 893      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2637000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2638000, episode_reward=992.52 +/- 21.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 993      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2638000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.922   |\n",
      "|    critic_loss     | 0.000857 |\n",
      "|    ent_coef        | 0.000506 |\n",
      "|    ent_coef_loss   | 1.96     |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2639000, episode_reward=985.06 +/- 49.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 985      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2639000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=1001.28 +/- 30.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.927   |\n",
      "|    critic_loss     | 0.000854 |\n",
      "|    ent_coef        | 0.000508 |\n",
      "|    ent_coef_loss   | 4.14     |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 863      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2640     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4102     |\n",
      "|    total_timesteps | 2640000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2641000, episode_reward=1010.10 +/- 22.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2641000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2642000, episode_reward=831.41 +/- 16.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 831      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2642000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.939   |\n",
      "|    critic_loss     | 0.000841 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | 6        |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2643000, episode_reward=916.40 +/- 42.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 916      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2643000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2644000, episode_reward=1023.52 +/- 51.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2644000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.909   |\n",
      "|    critic_loss     | 0.000887 |\n",
      "|    ent_coef        | 0.000517 |\n",
      "|    ent_coef_loss   | 3.75     |\n",
      "|    learning_rate   | 0.00236  |\n",
      "|    n_updates       | 12910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 867      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2644     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4109     |\n",
      "|    total_timesteps | 2644000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2645000, episode_reward=1012.04 +/- 26.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2645000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2646000, episode_reward=998.52 +/- 55.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 999      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2646000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2647000, episode_reward=1093.98 +/- 15.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2647000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.942   |\n",
      "|    critic_loss     | 0.000747 |\n",
      "|    ent_coef        | 0.000522 |\n",
      "|    ent_coef_loss   | 3.11     |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2648000, episode_reward=1073.15 +/- 29.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2648000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 876      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2648     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4115     |\n",
      "|    total_timesteps | 2648000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2649000, episode_reward=900.63 +/- 32.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 901      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2649000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.945   |\n",
      "|    critic_loss     | 0.000853 |\n",
      "|    ent_coef        | 0.000526 |\n",
      "|    ent_coef_loss   | 2.35     |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=871.55 +/- 65.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 872      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2650000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2651000, episode_reward=1038.98 +/- 17.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2651000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.917   |\n",
      "|    critic_loss     | 0.000911 |\n",
      "|    ent_coef        | 0.000529 |\n",
      "|    ent_coef_loss   | 3.17     |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2652000, episode_reward=1063.99 +/- 34.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2652000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 885      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2652     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4121     |\n",
      "|    total_timesteps | 2652000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2653000, episode_reward=1070.91 +/- 47.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2653000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.938   |\n",
      "|    critic_loss     | 0.000842 |\n",
      "|    ent_coef        | 0.000533 |\n",
      "|    ent_coef_loss   | -0.894   |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2654000, episode_reward=1094.29 +/- 45.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2654000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2655000, episode_reward=1035.60 +/- 41.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2655000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.953   |\n",
      "|    critic_loss     | 0.000826 |\n",
      "|    ent_coef        | 0.000533 |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.00235  |\n",
      "|    n_updates       | 12960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2656000, episode_reward=1035.54 +/- 46.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2656000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 889      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2656     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4127     |\n",
      "|    total_timesteps | 2656000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2657000, episode_reward=1040.29 +/- 42.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2657000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.934   |\n",
      "|    critic_loss     | 0.000784 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 12970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2658000, episode_reward=1001.95 +/- 41.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2658000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2659000, episode_reward=1039.09 +/- 26.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2659000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.938   |\n",
      "|    critic_loss     | 0.000887 |\n",
      "|    ent_coef        | 0.000531 |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 12980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=990.10 +/- 34.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 990      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2660000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 891      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2660     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4133     |\n",
      "|    total_timesteps | 2660000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2661000, episode_reward=945.68 +/- 55.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 946      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2661000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.924   |\n",
      "|    critic_loss     | 0.000946 |\n",
      "|    ent_coef        | 0.00053  |\n",
      "|    ent_coef_loss   | 0.583    |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 12990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2662000, episode_reward=903.05 +/- 32.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 903      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2662000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2663000, episode_reward=1033.78 +/- 62.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2663000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.915   |\n",
      "|    critic_loss     | 0.000838 |\n",
      "|    ent_coef        | 0.000529 |\n",
      "|    ent_coef_loss   | 0.793    |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2664000, episode_reward=1016.87 +/- 25.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2664000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 903      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2664     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4139     |\n",
      "|    total_timesteps | 2664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2665000, episode_reward=1008.00 +/- 35.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2665000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.934   |\n",
      "|    critic_loss     | 0.000863 |\n",
      "|    ent_coef        | 0.00053  |\n",
      "|    ent_coef_loss   | 2.26     |\n",
      "|    learning_rate   | 0.00234  |\n",
      "|    n_updates       | 13010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2666000, episode_reward=1047.78 +/- 37.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2666000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2667000, episode_reward=1067.74 +/- 30.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2667000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.928   |\n",
      "|    critic_loss     | 0.000926 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2668000, episode_reward=1056.84 +/- 42.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2668000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 910      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2668     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4145     |\n",
      "|    total_timesteps | 2668000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2669000, episode_reward=998.45 +/- 25.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 998      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2669000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.934   |\n",
      "|    critic_loss     | 0.000812 |\n",
      "|    ent_coef        | 0.000533 |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=1041.59 +/- 29.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2670000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2671000, episode_reward=941.56 +/- 34.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 942      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2671000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.907   |\n",
      "|    critic_loss     | 0.000755 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | -5.29    |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2672000, episode_reward=928.24 +/- 12.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 928      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2672000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 907      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2672     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4152     |\n",
      "|    total_timesteps | 2672000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2673000, episode_reward=962.92 +/- 34.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 963      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2673000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.919   |\n",
      "|    critic_loss     | 0.000879 |\n",
      "|    ent_coef        | 0.000529 |\n",
      "|    ent_coef_loss   | 0.762    |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2674000, episode_reward=1016.98 +/- 26.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2674000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2675000, episode_reward=51.60 +/- 429.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 51.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2675000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.918   |\n",
      "|    critic_loss     | 0.000796 |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | -3.31    |\n",
      "|    learning_rate   | 0.00233  |\n",
      "|    n_updates       | 13060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2676000, episode_reward=285.09 +/- 550.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 285      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2676000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 917      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2676     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4158     |\n",
      "|    total_timesteps | 2676000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2677000, episode_reward=-210.66 +/- 0.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -211     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2677000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.904   |\n",
      "|    critic_loss     | 0.000773 |\n",
      "|    ent_coef        | 0.000524 |\n",
      "|    ent_coef_loss   | -3.31    |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2678000, episode_reward=-209.80 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -210     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2678000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2679000, episode_reward=-342.81 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -343     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2679000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.702   |\n",
      "|    critic_loss     | 0.00063  |\n",
      "|    ent_coef        | 0.000517 |\n",
      "|    ent_coef_loss   | -15.6    |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=-344.05 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -344     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2680000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 879      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2680     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4164     |\n",
      "|    total_timesteps | 2680000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681000, episode_reward=-578.60 +/- 18.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -579     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2681000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.673   |\n",
      "|    critic_loss     | 0.000219 |\n",
      "|    ent_coef        | 0.000506 |\n",
      "|    ent_coef_loss   | -14.5    |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2682000, episode_reward=-519.72 +/- 31.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -520     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2682000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2683000, episode_reward=-305.23 +/- 2.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -305     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2683000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.88    |\n",
      "|    critic_loss     | 0.00159  |\n",
      "|    ent_coef        | 0.000493 |\n",
      "|    ent_coef_loss   | -0.955   |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2684000, episode_reward=-296.40 +/- 18.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -296     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2684000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 865      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2684     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4171     |\n",
      "|    total_timesteps | 2684000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2685000, episode_reward=969.99 +/- 28.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 970      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2685000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.843   |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.000486 |\n",
      "|    ent_coef_loss   | 3.24     |\n",
      "|    learning_rate   | 0.00232  |\n",
      "|    n_updates       | 13110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2686000, episode_reward=1031.61 +/- 24.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2686000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2687000, episode_reward=1019.48 +/- 28.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2687000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.901   |\n",
      "|    critic_loss     | 0.000997 |\n",
      "|    ent_coef        | 0.000487 |\n",
      "|    ent_coef_loss   | 1.02     |\n",
      "|    learning_rate   | 0.00231  |\n",
      "|    n_updates       | 13120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2688000, episode_reward=1037.90 +/- 40.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2688000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 860      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2688     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4177     |\n",
      "|    total_timesteps | 2688000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2689000, episode_reward=1003.80 +/- 42.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2689000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=-155.39 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -155     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.909   |\n",
      "|    critic_loss     | 0.000932 |\n",
      "|    ent_coef        | 0.000487 |\n",
      "|    ent_coef_loss   | -3.29    |\n",
      "|    learning_rate   | 0.00231  |\n",
      "|    n_updates       | 13130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2691000, episode_reward=-155.56 +/- 1.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2691000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2692000, episode_reward=-192.69 +/- 2.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -193     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2692000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.915   |\n",
      "|    critic_loss     | 0.000794 |\n",
      "|    ent_coef        | 0.000485 |\n",
      "|    ent_coef_loss   | 3.22     |\n",
      "|    learning_rate   | 0.00231  |\n",
      "|    n_updates       | 13140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 886      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2692     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4183     |\n",
      "|    total_timesteps | 2692000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2693000, episode_reward=-191.55 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2693000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2694000, episode_reward=23.96 +/- 456.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2694000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.882   |\n",
      "|    critic_loss     | 0.000794 |\n",
      "|    ent_coef        | 0.000486 |\n",
      "|    ent_coef_loss   | -0.49    |\n",
      "|    learning_rate   | 0.00231  |\n",
      "|    n_updates       | 13150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2695000, episode_reward=21.04 +/- 450.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 21       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2695000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2696000, episode_reward=912.93 +/- 31.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 913      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2696000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.928   |\n",
      "|    critic_loss     | 0.000873 |\n",
      "|    ent_coef        | 0.000487 |\n",
      "|    ent_coef_loss   | 3.26     |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 888      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2696     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4190     |\n",
      "|    total_timesteps | 2696000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2697000, episode_reward=852.82 +/- 35.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 853      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2697000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2698000, episode_reward=810.73 +/- 71.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 811      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2698000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.894   |\n",
      "|    critic_loss     | 0.000857 |\n",
      "|    ent_coef        | 0.00049  |\n",
      "|    ent_coef_loss   | 7.58     |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2699000, episode_reward=830.82 +/- 39.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 831      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2699000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=852.37 +/- 34.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 852      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.889   |\n",
      "|    critic_loss     | 0.000812 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | 2.72     |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13180    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 888      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2700     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4196     |\n",
      "|    total_timesteps | 2700000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2701000, episode_reward=902.79 +/- 35.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 903      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2701000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2702000, episode_reward=987.79 +/- 34.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 988      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2702000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.89    |\n",
      "|    critic_loss     | 0.000775 |\n",
      "|    ent_coef        | 0.0005   |\n",
      "|    ent_coef_loss   | -4.92    |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2703000, episode_reward=986.37 +/- 31.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 986      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2703000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2704000, episode_reward=1109.97 +/- 57.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2704000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.895   |\n",
      "|    critic_loss     | 0.000816 |\n",
      "|    ent_coef        | 0.000498 |\n",
      "|    ent_coef_loss   | -2.36    |\n",
      "|    learning_rate   | 0.0023   |\n",
      "|    n_updates       | 13200    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 887      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2704     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4202     |\n",
      "|    total_timesteps | 2704000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2705000, episode_reward=1094.67 +/- 29.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2705000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2706000, episode_reward=1083.31 +/- 17.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2706000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.922   |\n",
      "|    critic_loss     | 0.000698 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | -2.1     |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2707000, episode_reward=1105.52 +/- 13.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2707000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2708000, episode_reward=1097.01 +/- 67.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2708000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.918   |\n",
      "|    critic_loss     | 0.000676 |\n",
      "|    ent_coef        | 0.000493 |\n",
      "|    ent_coef_loss   | 2.4      |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13220    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 892      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2708     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4208     |\n",
      "|    total_timesteps | 2708000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2709000, episode_reward=1083.95 +/- 29.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2709000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=1080.66 +/- 25.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.922   |\n",
      "|    critic_loss     | 0.000791 |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | 2.54     |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2711000, episode_reward=1110.87 +/- 19.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2711000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2712000, episode_reward=1031.21 +/- 50.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2712000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.91    |\n",
      "|    critic_loss     | 0.00075  |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | -0.749   |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13240    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 897      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2712     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4214     |\n",
      "|    total_timesteps | 2712000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2713000, episode_reward=1033.90 +/- 27.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2713000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2714000, episode_reward=1151.70 +/- 37.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2714000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.907   |\n",
      "|    critic_loss     | 0.000767 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | -0.112   |\n",
      "|    learning_rate   | 0.00229  |\n",
      "|    n_updates       | 13250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2715000, episode_reward=1112.16 +/- 40.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2715000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2716000, episode_reward=1178.52 +/- 37.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2716000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.927   |\n",
      "|    critic_loss     | 0.000785 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | -0.761   |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13260    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 902      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2716     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4220     |\n",
      "|    total_timesteps | 2716000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2717000, episode_reward=1192.52 +/- 46.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2717000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2718000, episode_reward=993.90 +/- 99.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 994      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2718000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.923   |\n",
      "|    critic_loss     | 0.000801 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2719000, episode_reward=1058.34 +/- 46.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2719000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=1029.01 +/- 49.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.897   |\n",
      "|    critic_loss     | 0.000845 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | 2.72     |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13280    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 907      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2720     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4226     |\n",
      "|    total_timesteps | 2720000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721000, episode_reward=981.23 +/- 35.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 981      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2721000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2722000, episode_reward=1086.24 +/- 41.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2722000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.901   |\n",
      "|    critic_loss     | 0.000895 |\n",
      "|    ent_coef        | 0.000497 |\n",
      "|    ent_coef_loss   | 2.36     |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2723000, episode_reward=1052.65 +/- 50.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2723000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2724000, episode_reward=1066.10 +/- 34.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2724000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.909   |\n",
      "|    critic_loss     | 0.000937 |\n",
      "|    ent_coef        | 0.000499 |\n",
      "|    ent_coef_loss   | 2.43     |\n",
      "|    learning_rate   | 0.00228  |\n",
      "|    n_updates       | 13300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 908      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2724     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4233     |\n",
      "|    total_timesteps | 2724000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2725000, episode_reward=1058.79 +/- 47.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2725000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2726000, episode_reward=1043.41 +/- 42.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2726000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.904   |\n",
      "|    critic_loss     | 0.000834 |\n",
      "|    ent_coef        | 0.000502 |\n",
      "|    ent_coef_loss   | 4.8      |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2727000, episode_reward=1060.00 +/- 44.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2727000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2728000, episode_reward=389.97 +/- 487.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 390      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2728000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.899   |\n",
      "|    critic_loss     | 0.000922 |\n",
      "|    ent_coef        | 0.000507 |\n",
      "|    ent_coef_loss   | 5.27     |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 908      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2728     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4239     |\n",
      "|    total_timesteps | 2728000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2729000, episode_reward=199.32 +/- 497.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 199      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2729000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=-225.38 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.867   |\n",
      "|    critic_loss     | 0.00093  |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | 0.889    |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2731000, episode_reward=-225.41 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2731000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2732000, episode_reward=-223.74 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -224     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2732000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 908      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2732     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4245     |\n",
      "|    total_timesteps | 2732000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2733000, episode_reward=-275.06 +/- 1.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -275     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2733000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.875   |\n",
      "|    critic_loss     | 0.001    |\n",
      "|    ent_coef        | 0.000516 |\n",
      "|    ent_coef_loss   | 2.72     |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2734000, episode_reward=-274.95 +/- 1.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -275     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2734000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2735000, episode_reward=1117.53 +/- 50.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2735000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.648   |\n",
      "|    critic_loss     | 0.000128 |\n",
      "|    ent_coef        | 0.000517 |\n",
      "|    ent_coef_loss   | -10      |\n",
      "|    learning_rate   | 0.00227  |\n",
      "|    n_updates       | 13350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2736000, episode_reward=1134.49 +/- 29.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2736000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 884      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2736     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4251     |\n",
      "|    total_timesteps | 2736000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2737000, episode_reward=1087.93 +/- 36.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2737000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.891   |\n",
      "|    critic_loss     | 0.00337  |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | 0.889    |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2738000, episode_reward=1090.60 +/- 47.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2738000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2739000, episode_reward=995.58 +/- 64.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 996      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2739000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.9     |\n",
      "|    critic_loss     | 0.00167  |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | 7.33     |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=948.53 +/- 49.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 949      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2740000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 888      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2740     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4257     |\n",
      "|    total_timesteps | 2740000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2741000, episode_reward=912.49 +/- 61.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 912      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2741000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.89    |\n",
      "|    critic_loss     | 0.00123  |\n",
      "|    ent_coef        | 0.000516 |\n",
      "|    ent_coef_loss   | 4.12     |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2742000, episode_reward=958.91 +/- 73.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 959      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2742000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2743000, episode_reward=900.61 +/- 19.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 901      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2743000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.856   |\n",
      "|    critic_loss     | 0.00104  |\n",
      "|    ent_coef        | 0.000521 |\n",
      "|    ent_coef_loss   | 2.14     |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2744000, episode_reward=905.00 +/- 59.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 905      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2744000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 885      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2744     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4263     |\n",
      "|    total_timesteps | 2744000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2745000, episode_reward=1076.34 +/- 62.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2745000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.864   |\n",
      "|    critic_loss     | 0.000832 |\n",
      "|    ent_coef        | 0.000524 |\n",
      "|    ent_coef_loss   | -4.13    |\n",
      "|    learning_rate   | 0.00226  |\n",
      "|    n_updates       | 13400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2746000, episode_reward=1068.96 +/- 49.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2746000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2747000, episode_reward=999.20 +/- 40.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 999      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2747000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.901   |\n",
      "|    critic_loss     | 0.000952 |\n",
      "|    ent_coef        | 0.000523 |\n",
      "|    ent_coef_loss   | 3.1      |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2748000, episode_reward=1021.28 +/- 39.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2748000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 883      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2748     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4270     |\n",
      "|    total_timesteps | 2748000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2749000, episode_reward=1084.98 +/- 30.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2749000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.887   |\n",
      "|    critic_loss     | 0.000924 |\n",
      "|    ent_coef        | 0.000524 |\n",
      "|    ent_coef_loss   | -0.605   |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2750000, episode_reward=1102.26 +/- 43.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2750000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2751000, episode_reward=1120.84 +/- 23.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2751000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.901   |\n",
      "|    critic_loss     | 0.000925 |\n",
      "|    ent_coef        | 0.000525 |\n",
      "|    ent_coef_loss   | 2.98     |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2752000, episode_reward=1091.48 +/- 19.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2752000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 889      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2752     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4276     |\n",
      "|    total_timesteps | 2752000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2753000, episode_reward=1158.83 +/- 18.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2753000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.908   |\n",
      "|    critic_loss     | 0.000739 |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2754000, episode_reward=1147.99 +/- 9.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2754000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2755000, episode_reward=1009.45 +/- 19.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2755000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.899   |\n",
      "|    critic_loss     | 0.000922 |\n",
      "|    ent_coef        | 0.000529 |\n",
      "|    ent_coef_loss   | -1.82    |\n",
      "|    learning_rate   | 0.00225  |\n",
      "|    n_updates       | 13450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2756000, episode_reward=979.83 +/- 68.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 980      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2756000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 888      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2756     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4282     |\n",
      "|    total_timesteps | 2756000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2757000, episode_reward=1090.17 +/- 15.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2757000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.874   |\n",
      "|    critic_loss     | 0.000892 |\n",
      "|    ent_coef        | 0.000528 |\n",
      "|    ent_coef_loss   | -0.177   |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2758000, episode_reward=1145.23 +/- 39.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2758000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2759000, episode_reward=1152.02 +/- 54.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2759000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.897   |\n",
      "|    critic_loss     | 0.000855 |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | -5.9     |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=1134.35 +/- 41.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2760000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 890      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2760     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4288     |\n",
      "|    total_timesteps | 2760000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761000, episode_reward=1092.65 +/- 26.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2761000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.903   |\n",
      "|    critic_loss     | 0.000863 |\n",
      "|    ent_coef        | 0.000522 |\n",
      "|    ent_coef_loss   | -3.1     |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2762000, episode_reward=1067.42 +/- 7.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2762000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2763000, episode_reward=1125.83 +/- 6.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2763000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.876   |\n",
      "|    critic_loss     | 0.000896 |\n",
      "|    ent_coef        | 0.000518 |\n",
      "|    ent_coef_loss   | -0.906   |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2764000, episode_reward=1095.74 +/- 33.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2764000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 892      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2764     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4294     |\n",
      "|    total_timesteps | 2764000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2765000, episode_reward=1203.63 +/- 30.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2765000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.887   |\n",
      "|    critic_loss     | 0.000799 |\n",
      "|    ent_coef        | 0.000516 |\n",
      "|    ent_coef_loss   | -1.94    |\n",
      "|    learning_rate   | 0.00224  |\n",
      "|    n_updates       | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2766000, episode_reward=1214.59 +/- 9.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2766000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2767000, episode_reward=113.91 +/- 560.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 114      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2767000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.912   |\n",
      "|    critic_loss     | 0.000838 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | -0.276   |\n",
      "|    learning_rate   | 0.00223  |\n",
      "|    n_updates       | 13510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2768000, episode_reward=-164.02 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -164     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2768000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 896      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2768     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4300     |\n",
      "|    total_timesteps | 2768000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2769000, episode_reward=104.10 +/- 528.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2769000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.9     |\n",
      "|    critic_loss     | 0.00085  |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | 0.449    |\n",
      "|    learning_rate   | 0.00223  |\n",
      "|    n_updates       | 13520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=-157.25 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2770000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2771000, episode_reward=1124.70 +/- 29.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2771000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.732   |\n",
      "|    critic_loss     | 0.000489 |\n",
      "|    ent_coef        | 0.000509 |\n",
      "|    ent_coef_loss   | -15.4    |\n",
      "|    learning_rate   | 0.00223  |\n",
      "|    n_updates       | 13530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2772000, episode_reward=1106.93 +/- 82.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2772000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 888      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2772     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4306     |\n",
      "|    total_timesteps | 2772000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2773000, episode_reward=882.70 +/- 101.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 883      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2773000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.905   |\n",
      "|    critic_loss     | 0.000869 |\n",
      "|    ent_coef        | 0.000499 |\n",
      "|    ent_coef_loss   | 1.74     |\n",
      "|    learning_rate   | 0.00223  |\n",
      "|    n_updates       | 13540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2774000, episode_reward=891.52 +/- 83.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 892      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2774000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2775000, episode_reward=907.16 +/- 62.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 907      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2775000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2776000, episode_reward=1051.62 +/- 37.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2776000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.865   |\n",
      "|    critic_loss     | 0.000808 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | -0.781   |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 890      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2776     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4313     |\n",
      "|    total_timesteps | 2776000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2777000, episode_reward=1072.26 +/- 84.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2777000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2778000, episode_reward=1089.55 +/- 50.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2778000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.899   |\n",
      "|    critic_loss     | 0.000831 |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2779000, episode_reward=1086.85 +/- 38.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2779000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=1080.35 +/- 33.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.673   |\n",
      "|    critic_loss     | 0.000635 |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | 3.67     |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 916      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2780     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4319     |\n",
      "|    total_timesteps | 2780000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2781000, episode_reward=1089.70 +/- 44.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2781000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2782000, episode_reward=275.02 +/- 629.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 275      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2782000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.871   |\n",
      "|    critic_loss     | 0.000804 |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | -8.03    |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2783000, episode_reward=15.71 +/- 506.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 15.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2783000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2784000, episode_reward=824.24 +/- 566.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 824      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2784000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.832   |\n",
      "|    critic_loss     | 0.000833 |\n",
      "|    ent_coef        | 0.000488 |\n",
      "|    ent_coef_loss   | -6.5     |\n",
      "|    learning_rate   | 0.00222  |\n",
      "|    n_updates       | 13590    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 932      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2784     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4325     |\n",
      "|    total_timesteps | 2784000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2785000, episode_reward=1130.41 +/- 44.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2785000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2786000, episode_reward=1231.39 +/- 20.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2786000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.872   |\n",
      "|    critic_loss     | 0.000934 |\n",
      "|    ent_coef        | 0.000482 |\n",
      "|    ent_coef_loss   | 2.17     |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2787000, episode_reward=1215.02 +/- 48.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2787000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2788000, episode_reward=1174.75 +/- 25.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2788000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.902   |\n",
      "|    critic_loss     | 0.000826 |\n",
      "|    ent_coef        | 0.00048  |\n",
      "|    ent_coef_loss   | -3.09    |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13610    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 941      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2788     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4331     |\n",
      "|    total_timesteps | 2788000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2789000, episode_reward=1178.50 +/- 32.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2789000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=1156.14 +/- 19.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.902   |\n",
      "|    critic_loss     | 0.000703 |\n",
      "|    ent_coef        | 0.000478 |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2791000, episode_reward=1180.12 +/- 16.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2791000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2792000, episode_reward=-146.86 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2792000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.893   |\n",
      "|    critic_loss     | 0.00069  |\n",
      "|    ent_coef        | 0.000476 |\n",
      "|    ent_coef_loss   | -2.52    |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13630    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 948      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2792     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4337     |\n",
      "|    total_timesteps | 2792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2793000, episode_reward=-147.23 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2793000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2794000, episode_reward=-404.97 +/- 1.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -405     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2794000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.686   |\n",
      "|    critic_loss     | 0.000464 |\n",
      "|    ent_coef        | 0.000469 |\n",
      "|    ent_coef_loss   | -16.8    |\n",
      "|    learning_rate   | 0.00221  |\n",
      "|    n_updates       | 13640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2795000, episode_reward=-406.30 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -406     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2795000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2796000, episode_reward=1243.42 +/- 19.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2796000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.622   |\n",
      "|    critic_loss     | 4.68e-05 |\n",
      "|    ent_coef        | 0.000461 |\n",
      "|    ent_coef_loss   | 2.18     |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13650    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 906      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2796     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4344     |\n",
      "|    total_timesteps | 2796000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2797000, episode_reward=1248.66 +/- 32.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2797000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2798000, episode_reward=1171.52 +/- 37.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2798000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.896   |\n",
      "|    critic_loss     | 0.000828 |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | 0.198    |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2799000, episode_reward=1187.01 +/- 31.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2799000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=217.22 +/- 465.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 217      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.882   |\n",
      "|    critic_loss     | 0.000833 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | -2.47    |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13670    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 916      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2800     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4350     |\n",
      "|    total_timesteps | 2800000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2801000, episode_reward=728.44 +/- 604.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 728      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2801000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802000, episode_reward=662.27 +/- 643.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 662      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2802000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.89    |\n",
      "|    critic_loss     | 0.0008   |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | 1.61     |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2803000, episode_reward=661.60 +/- 644.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 662      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2803000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2804000, episode_reward=1256.72 +/- 31.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2804000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.892   |\n",
      "|    critic_loss     | 0.000755 |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.0022   |\n",
      "|    n_updates       | 13690    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 927      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2804     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4356     |\n",
      "|    total_timesteps | 2804000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2805000, episode_reward=1247.52 +/- 22.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2805000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2806000, episode_reward=1263.86 +/- 45.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2806000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.888   |\n",
      "|    critic_loss     | 0.000747 |\n",
      "|    ent_coef        | 0.000451 |\n",
      "|    ent_coef_loss   | -6.06    |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13700    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2807000, episode_reward=1214.87 +/- 39.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2807000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2808000, episode_reward=1077.63 +/- 92.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2808000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.901   |\n",
      "|    critic_loss     | 0.000896 |\n",
      "|    ent_coef        | 0.000448 |\n",
      "|    ent_coef_loss   | 3.07     |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 932      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2808     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4363     |\n",
      "|    total_timesteps | 2808000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2809000, episode_reward=1038.37 +/- 82.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2809000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=1002.33 +/- 57.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.867   |\n",
      "|    critic_loss     | 0.000847 |\n",
      "|    ent_coef        | 0.000447 |\n",
      "|    ent_coef_loss   | -2.15    |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2811000, episode_reward=991.36 +/- 39.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 991      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2811000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2812000, episode_reward=1016.96 +/- 26.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2812000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.851   |\n",
      "|    critic_loss     | 0.000908 |\n",
      "|    ent_coef        | 0.000446 |\n",
      "|    ent_coef_loss   | -0.42    |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 930      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2812     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4369     |\n",
      "|    total_timesteps | 2812000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2813000, episode_reward=1043.35 +/- 33.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2813000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2814000, episode_reward=1191.80 +/- 54.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2814000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.855   |\n",
      "|    critic_loss     | 0.000877 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | -0.943   |\n",
      "|    learning_rate   | 0.00219  |\n",
      "|    n_updates       | 13740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2815000, episode_reward=1208.84 +/- 28.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2815000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2816000, episode_reward=1195.59 +/- 41.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2816000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 930      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2816     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4375     |\n",
      "|    total_timesteps | 2816000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2817000, episode_reward=1265.81 +/- 45.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2817000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.894   |\n",
      "|    critic_loss     | 0.000793 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13750    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2818000, episode_reward=1269.70 +/- 31.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2818000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2819000, episode_reward=1289.49 +/- 26.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2819000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.89    |\n",
      "|    critic_loss     | 0.000765 |\n",
      "|    ent_coef        | 0.000446 |\n",
      "|    ent_coef_loss   | 3.49     |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13760    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2820000, episode_reward=1308.48 +/- 29.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2820000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 938      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2820     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4381     |\n",
      "|    total_timesteps | 2820000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2821000, episode_reward=1192.29 +/- 79.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2821000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.906   |\n",
      "|    critic_loss     | 0.00081  |\n",
      "|    ent_coef        | 0.000449 |\n",
      "|    ent_coef_loss   | 3.24     |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2822000, episode_reward=1243.16 +/- 48.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2822000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2823000, episode_reward=1120.32 +/- 82.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2823000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.881   |\n",
      "|    critic_loss     | 0.000862 |\n",
      "|    ent_coef        | 0.000452 |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2824000, episode_reward=1050.34 +/- 74.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2824000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 942      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2824     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4388     |\n",
      "|    total_timesteps | 2824000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2825000, episode_reward=1275.95 +/- 25.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2825000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.862   |\n",
      "|    critic_loss     | 0.000882 |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | 3.07     |\n",
      "|    learning_rate   | 0.00218  |\n",
      "|    n_updates       | 13790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2826000, episode_reward=1281.33 +/- 28.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2826000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2827000, episode_reward=1318.89 +/- 16.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2827000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.902   |\n",
      "|    critic_loss     | 0.000867 |\n",
      "|    ent_coef        | 0.000458 |\n",
      "|    ent_coef_loss   | 5.76     |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13800    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2828000, episode_reward=1294.89 +/- 19.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2828000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 951      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2828     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4394     |\n",
      "|    total_timesteps | 2828000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2829000, episode_reward=1296.31 +/- 47.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2829000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.891   |\n",
      "|    critic_loss     | 0.000774 |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | 0.38     |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2830000, episode_reward=1295.90 +/- 38.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2830000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2831000, episode_reward=1287.56 +/- 19.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2831000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.885   |\n",
      "|    critic_loss     | 0.000826 |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | 4.81     |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2832000, episode_reward=1287.59 +/- 31.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2832000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 964      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2832     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4400     |\n",
      "|    total_timesteps | 2832000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2833000, episode_reward=1320.02 +/- 17.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2833000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.9     |\n",
      "|    critic_loss     | 0.00081  |\n",
      "|    ent_coef        | 0.000471 |\n",
      "|    ent_coef_loss   | 4.9      |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13830    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2834000, episode_reward=1325.65 +/- 25.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2834000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2835000, episode_reward=1272.60 +/- 14.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2835000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.903   |\n",
      "|    critic_loss     | 0.000758 |\n",
      "|    ent_coef        | 0.000476 |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.00217  |\n",
      "|    n_updates       | 13840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2836000, episode_reward=1288.74 +/- 20.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2836000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 998      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2836     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4407     |\n",
      "|    total_timesteps | 2836000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2837000, episode_reward=1292.18 +/- 17.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2837000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.894   |\n",
      "|    critic_loss     | 0.000698 |\n",
      "|    ent_coef        | 0.000479 |\n",
      "|    ent_coef_loss   | 2.49     |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2838000, episode_reward=1277.05 +/- 17.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2838000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2839000, episode_reward=1192.27 +/- 58.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2839000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.894   |\n",
      "|    critic_loss     | 0.00074  |\n",
      "|    ent_coef        | 0.000483 |\n",
      "|    ent_coef_loss   | 1.67     |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2840000, episode_reward=1110.47 +/- 79.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2840000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2840     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4413     |\n",
      "|    total_timesteps | 2840000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2841000, episode_reward=1100.99 +/- 61.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2841000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.852   |\n",
      "|    critic_loss     | 0.000753 |\n",
      "|    ent_coef        | 0.000484 |\n",
      "|    ent_coef_loss   | -2.24    |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842000, episode_reward=1100.95 +/- 59.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2842000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2843000, episode_reward=908.17 +/- 20.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 908      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2843000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.869   |\n",
      "|    critic_loss     | 0.000937 |\n",
      "|    ent_coef        | 0.000484 |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2844000, episode_reward=933.20 +/- 69.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 933      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2844000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2844     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4420     |\n",
      "|    total_timesteps | 2844000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2845000, episode_reward=1279.73 +/- 49.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2845000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.837   |\n",
      "|    critic_loss     | 0.000895 |\n",
      "|    ent_coef        | 0.000484 |\n",
      "|    ent_coef_loss   | -4.85    |\n",
      "|    learning_rate   | 0.00216  |\n",
      "|    n_updates       | 13890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2846000, episode_reward=1289.72 +/- 37.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2846000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2847000, episode_reward=1288.06 +/- 54.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2847000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.882   |\n",
      "|    critic_loss     | 0.000832 |\n",
      "|    ent_coef        | 0.000481 |\n",
      "|    ent_coef_loss   | -1.82    |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2848000, episode_reward=1290.33 +/- 55.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2848000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2848     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4426     |\n",
      "|    total_timesteps | 2848000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2849000, episode_reward=1241.87 +/- 22.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2849000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.894   |\n",
      "|    critic_loss     | 0.000828 |\n",
      "|    ent_coef        | 0.000478 |\n",
      "|    ent_coef_loss   | 2.18     |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=1186.38 +/- 42.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2850000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2851000, episode_reward=1170.91 +/- 30.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2851000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.861   |\n",
      "|    critic_loss     | 0.000868 |\n",
      "|    ent_coef        | 0.000479 |\n",
      "|    ent_coef_loss   | 2.58     |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2852000, episode_reward=1148.63 +/- 31.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2852000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2852     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4432     |\n",
      "|    total_timesteps | 2852000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2853000, episode_reward=1171.80 +/- 30.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2853000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.864   |\n",
      "|    critic_loss     | 0.000972 |\n",
      "|    ent_coef        | 0.000481 |\n",
      "|    ent_coef_loss   | 0.533    |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2854000, episode_reward=1180.07 +/- 143.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2854000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2855000, episode_reward=1304.53 +/- 48.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2855000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.876   |\n",
      "|    critic_loss     | 0.000845 |\n",
      "|    ent_coef        | 0.000483 |\n",
      "|    ent_coef_loss   | 2.46     |\n",
      "|    learning_rate   | 0.00215  |\n",
      "|    n_updates       | 13940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2856000, episode_reward=1302.45 +/- 28.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2856000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2856     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4438     |\n",
      "|    total_timesteps | 2856000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2857000, episode_reward=1178.29 +/- 33.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2857000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.888   |\n",
      "|    critic_loss     | 0.000724 |\n",
      "|    ent_coef        | 0.000486 |\n",
      "|    ent_coef_loss   | 1.81     |\n",
      "|    learning_rate   | 0.00214  |\n",
      "|    n_updates       | 13950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2858000, episode_reward=1167.42 +/- 40.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2858000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2859000, episode_reward=1194.44 +/- 41.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2859000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=1219.79 +/- 47.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.861   |\n",
      "|    critic_loss     | 0.000729 |\n",
      "|    ent_coef        | 0.000487 |\n",
      "|    ent_coef_loss   | -2.58    |\n",
      "|    learning_rate   | 0.00214  |\n",
      "|    n_updates       | 13960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2860     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4444     |\n",
      "|    total_timesteps | 2860000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2861000, episode_reward=1226.02 +/- 40.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2861000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2862000, episode_reward=1089.82 +/- 55.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2862000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.865   |\n",
      "|    critic_loss     | 0.000789 |\n",
      "|    ent_coef        | 0.000487 |\n",
      "|    ent_coef_loss   | 0.626    |\n",
      "|    learning_rate   | 0.00214  |\n",
      "|    n_updates       | 13970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2863000, episode_reward=1126.29 +/- 69.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2863000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2864000, episode_reward=1073.20 +/- 64.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2864000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.853   |\n",
      "|    critic_loss     | 0.000839 |\n",
      "|    ent_coef        | 0.000487 |\n",
      "|    ent_coef_loss   | -0.927   |\n",
      "|    learning_rate   | 0.00214  |\n",
      "|    n_updates       | 13980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2864     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4450     |\n",
      "|    total_timesteps | 2864000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2865000, episode_reward=1105.77 +/- 85.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2865000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2866000, episode_reward=1207.57 +/- 28.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2866000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.839   |\n",
      "|    critic_loss     | 0.000909 |\n",
      "|    ent_coef        | 0.000486 |\n",
      "|    ent_coef_loss   | -2.79    |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 13990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2867000, episode_reward=1180.99 +/- 87.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2867000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2868000, episode_reward=1254.65 +/- 41.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2868000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.857   |\n",
      "|    critic_loss     | 0.000858 |\n",
      "|    ent_coef        | 0.000483 |\n",
      "|    ent_coef_loss   | -2.07    |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2868     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4457     |\n",
      "|    total_timesteps | 2868000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2869000, episode_reward=1286.26 +/- 6.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2869000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=1280.99 +/- 40.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.876   |\n",
      "|    critic_loss     | 0.000911 |\n",
      "|    ent_coef        | 0.000481 |\n",
      "|    ent_coef_loss   | 3.9      |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 14010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2871000, episode_reward=1270.19 +/- 65.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2871000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2872000, episode_reward=1296.32 +/- 36.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2872000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.886   |\n",
      "|    critic_loss     | 0.000824 |\n",
      "|    ent_coef        | 0.000484 |\n",
      "|    ent_coef_loss   | 4.39     |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 14020    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2872     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4463     |\n",
      "|    total_timesteps | 2872000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2873000, episode_reward=1277.44 +/- 23.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2873000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2874000, episode_reward=1259.77 +/- 27.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2874000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.878   |\n",
      "|    critic_loss     | 0.000879 |\n",
      "|    ent_coef        | 0.000489 |\n",
      "|    ent_coef_loss   | 6.47     |\n",
      "|    learning_rate   | 0.00213  |\n",
      "|    n_updates       | 14030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2875000, episode_reward=1276.73 +/- 32.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2875000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2876000, episode_reward=1184.21 +/- 75.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2876000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.879   |\n",
      "|    critic_loss     | 0.000818 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | 5.43     |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14040    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2876     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4469     |\n",
      "|    total_timesteps | 2876000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2877000, episode_reward=1211.14 +/- 66.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2877000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2878000, episode_reward=1314.52 +/- 47.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2878000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.852   |\n",
      "|    critic_loss     | 0.000934 |\n",
      "|    ent_coef        | 0.000503 |\n",
      "|    ent_coef_loss   | 1.71     |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2879000, episode_reward=1330.61 +/- 30.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2879000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2880000, episode_reward=1319.14 +/- 48.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.887   |\n",
      "|    critic_loss     | 0.000843 |\n",
      "|    ent_coef        | 0.000508 |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14060    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2880     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4475     |\n",
      "|    total_timesteps | 2880000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2881000, episode_reward=1318.98 +/- 43.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2881000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882000, episode_reward=1235.56 +/- 25.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2882000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.867   |\n",
      "|    critic_loss     | 0.000867 |\n",
      "|    ent_coef        | 0.00051  |\n",
      "|    ent_coef_loss   | -6.03    |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2883000, episode_reward=1255.99 +/- 25.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2883000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2884000, episode_reward=1249.91 +/- 15.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2884000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.87    |\n",
      "|    critic_loss     | 0.000848 |\n",
      "|    ent_coef        | 0.000505 |\n",
      "|    ent_coef_loss   | -3.75    |\n",
      "|    learning_rate   | 0.00212  |\n",
      "|    n_updates       | 14080    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2884     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4481     |\n",
      "|    total_timesteps | 2884000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2885000, episode_reward=1268.57 +/- 26.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2885000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2886000, episode_reward=1313.26 +/- 60.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2886000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.862   |\n",
      "|    critic_loss     | 0.000784 |\n",
      "|    ent_coef        | 0.0005   |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2887000, episode_reward=1201.10 +/- 7.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2887000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2888000, episode_reward=1184.01 +/- 64.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2888000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.863   |\n",
      "|    critic_loss     | 0.000862 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | 0.477    |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14100    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2888     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4488     |\n",
      "|    total_timesteps | 2888000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2889000, episode_reward=1152.98 +/- 39.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2889000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=1229.24 +/- 33.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.86    |\n",
      "|    critic_loss     | 0.000998 |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | -0.626   |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2891000, episode_reward=1199.46 +/- 48.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2891000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2892000, episode_reward=1280.85 +/- 47.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2892000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.873   |\n",
      "|    critic_loss     | 0.000877 |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | 0.472    |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14120    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2892     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4494     |\n",
      "|    total_timesteps | 2892000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2893000, episode_reward=1298.49 +/- 67.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2893000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2894000, episode_reward=-358.80 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -359     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2894000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.871   |\n",
      "|    critic_loss     | 0.000892 |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | -0.205   |\n",
      "|    learning_rate   | 0.00211  |\n",
      "|    n_updates       | 14130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2895000, episode_reward=-358.73 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -359     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2895000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2896000, episode_reward=-307.44 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -307     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2896000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.591   |\n",
      "|    critic_loss     | 0.000345 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | 6.58     |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2896     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4500     |\n",
      "|    total_timesteps | 2896000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2897000, episode_reward=-307.31 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -307     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2897000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2898000, episode_reward=-263.94 +/- 1.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -264     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2898000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.543   |\n",
      "|    critic_loss     | 0.000275 |\n",
      "|    ent_coef        | 0.000493 |\n",
      "|    ent_coef_loss   | -32.4    |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2899000, episode_reward=-265.12 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2899000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=-345.18 +/- 2.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -345     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.549   |\n",
      "|    critic_loss     | 0.000116 |\n",
      "|    ent_coef        | 0.000472 |\n",
      "|    ent_coef_loss   | -6       |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2900     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4506     |\n",
      "|    total_timesteps | 2900000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2901000, episode_reward=-344.99 +/- 1.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -345     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2901000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2902000, episode_reward=-341.77 +/- 2.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -342     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2902000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2903000, episode_reward=-308.04 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2903000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.542   |\n",
      "|    critic_loss     | 7.76e-05 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2904000, episode_reward=-307.60 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2904000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 994      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2904     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4513     |\n",
      "|    total_timesteps | 2904000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2905000, episode_reward=-373.32 +/- 2.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2905000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.55    |\n",
      "|    critic_loss     | 0.000105 |\n",
      "|    ent_coef        | 0.000446 |\n",
      "|    ent_coef_loss   | -9.55    |\n",
      "|    learning_rate   | 0.0021   |\n",
      "|    n_updates       | 14180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2906000, episode_reward=-374.46 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -374     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2906000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2907000, episode_reward=-369.63 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2907000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.553   |\n",
      "|    critic_loss     | 5.67e-05 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | -6.35    |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2908000, episode_reward=-366.62 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -367     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2908000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 963      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2908     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4519     |\n",
      "|    total_timesteps | 2908000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2909000, episode_reward=1347.17 +/- 43.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2909000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.827   |\n",
      "|    critic_loss     | 0.00259  |\n",
      "|    ent_coef        | 0.000427 |\n",
      "|    ent_coef_loss   | 1.49     |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14200    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2910000, episode_reward=1316.77 +/- 42.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2910000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2911000, episode_reward=-194.28 +/- 2.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2911000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.844   |\n",
      "|    critic_loss     | 0.00149  |\n",
      "|    ent_coef        | 0.000425 |\n",
      "|    ent_coef_loss   | 4.88     |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2912000, episode_reward=213.25 +/- 365.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 213      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2912000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 965      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2912     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4525     |\n",
      "|    total_timesteps | 2912000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2913000, episode_reward=-177.92 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -178     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2913000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.000999 |\n",
      "|    ent_coef        | 0.000427 |\n",
      "|    ent_coef_loss   | -1.72    |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2914000, episode_reward=-177.30 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -177     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2914000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2915000, episode_reward=-227.35 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -227     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2915000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.754   |\n",
      "|    critic_loss     | 0.000895 |\n",
      "|    ent_coef        | 0.000426 |\n",
      "|    ent_coef_loss   | -9.57    |\n",
      "|    learning_rate   | 0.00209  |\n",
      "|    n_updates       | 14230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2916000, episode_reward=-226.64 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -227     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2916000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 935      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2916     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4532     |\n",
      "|    total_timesteps | 2916000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2917000, episode_reward=-217.68 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -218     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2917000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.543   |\n",
      "|    critic_loss     | 0.000106 |\n",
      "|    ent_coef        | 0.000416 |\n",
      "|    ent_coef_loss   | -34.6    |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2918000, episode_reward=-219.37 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2918000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2919000, episode_reward=-342.80 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -343     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2919000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.69    |\n",
      "|    critic_loss     | 0.000804 |\n",
      "|    ent_coef        | 0.000395 |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=-342.55 +/- 1.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -343     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2920000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 911      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2920     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4538     |\n",
      "|    total_timesteps | 2920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2921000, episode_reward=1075.35 +/- 67.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2921000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.796   |\n",
      "|    critic_loss     | 0.000906 |\n",
      "|    ent_coef        | 0.00038  |\n",
      "|    ent_coef_loss   | -3.3     |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2922000, episode_reward=964.99 +/- 75.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 965      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2922000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923000, episode_reward=1153.30 +/- 77.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2923000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.807   |\n",
      "|    critic_loss     | 0.000901 |\n",
      "|    ent_coef        | 0.000372 |\n",
      "|    ent_coef_loss   | 0.252    |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2924000, episode_reward=1202.83 +/- 85.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2924000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 909      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2924     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4544     |\n",
      "|    total_timesteps | 2924000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2925000, episode_reward=1216.20 +/- 74.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2925000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.824   |\n",
      "|    critic_loss     | 0.000945 |\n",
      "|    ent_coef        | 0.00037  |\n",
      "|    ent_coef_loss   | 4.13     |\n",
      "|    learning_rate   | 0.00208  |\n",
      "|    n_updates       | 14280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2926000, episode_reward=1226.41 +/- 44.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2926000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2927000, episode_reward=1159.76 +/- 103.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2927000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.843   |\n",
      "|    critic_loss     | 0.000914 |\n",
      "|    ent_coef        | 0.000371 |\n",
      "|    ent_coef_loss   | 1.89     |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2928000, episode_reward=1187.75 +/- 23.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2928000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 908      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2928     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4550     |\n",
      "|    total_timesteps | 2928000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2929000, episode_reward=1309.57 +/- 48.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2929000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.811   |\n",
      "|    critic_loss     | 0.001    |\n",
      "|    ent_coef        | 0.000373 |\n",
      "|    ent_coef_loss   | 5.87     |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=1283.67 +/- 46.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2930000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2931000, episode_reward=1265.40 +/- 52.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2931000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.843   |\n",
      "|    critic_loss     | 0.000936 |\n",
      "|    ent_coef        | 0.000377 |\n",
      "|    ent_coef_loss   | 7.36     |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2932000, episode_reward=1293.45 +/- 49.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2932000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 907      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2932     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4556     |\n",
      "|    total_timesteps | 2932000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2933000, episode_reward=270.56 +/- 525.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 271      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2933000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.842   |\n",
      "|    critic_loss     | 0.00093  |\n",
      "|    ent_coef        | 0.000382 |\n",
      "|    ent_coef_loss   | 5.64     |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2934000, episode_reward=743.79 +/- 695.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 744      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2934000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2935000, episode_reward=1336.24 +/- 17.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2935000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.82    |\n",
      "|    critic_loss     | 0.000859 |\n",
      "|    ent_coef        | 0.000387 |\n",
      "|    ent_coef_loss   | 3.5      |\n",
      "|    learning_rate   | 0.00207  |\n",
      "|    n_updates       | 14330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2936000, episode_reward=1327.32 +/- 12.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2936000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 906      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2936     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4563     |\n",
      "|    total_timesteps | 2936000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2937000, episode_reward=1206.63 +/- 51.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2937000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.826   |\n",
      "|    critic_loss     | 0.000896 |\n",
      "|    ent_coef        | 0.000391 |\n",
      "|    ent_coef_loss   | 2.3      |\n",
      "|    learning_rate   | 0.00206  |\n",
      "|    n_updates       | 14340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2938000, episode_reward=1234.08 +/- 21.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2938000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2939000, episode_reward=1329.07 +/- 30.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2939000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.817   |\n",
      "|    critic_loss     | 0.000982 |\n",
      "|    ent_coef        | 0.000395 |\n",
      "|    ent_coef_loss   | 8.94     |\n",
      "|    learning_rate   | 0.00206  |\n",
      "|    n_updates       | 14350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=1346.64 +/- 26.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2940000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 906      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2940     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4569     |\n",
      "|    total_timesteps | 2940000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2941000, episode_reward=1348.63 +/- 30.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2941000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.848   |\n",
      "|    critic_loss     | 0.00094  |\n",
      "|    ent_coef        | 0.000401 |\n",
      "|    ent_coef_loss   | 7.49     |\n",
      "|    learning_rate   | 0.00206  |\n",
      "|    n_updates       | 14360    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2942000, episode_reward=1318.55 +/- 32.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2942000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2943000, episode_reward=1323.59 +/- 35.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2943000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.845   |\n",
      "|    critic_loss     | 0.000877 |\n",
      "|    ent_coef        | 0.000408 |\n",
      "|    ent_coef_loss   | 7.99     |\n",
      "|    learning_rate   | 0.00206  |\n",
      "|    n_updates       | 14370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2944000, episode_reward=1308.51 +/- 36.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2944000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 916      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2944     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4575     |\n",
      "|    total_timesteps | 2944000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2945000, episode_reward=1330.70 +/- 48.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2945000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2946000, episode_reward=1305.08 +/- 18.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2946000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.838   |\n",
      "|    critic_loss     | 0.000924 |\n",
      "|    ent_coef        | 0.000416 |\n",
      "|    ent_coef_loss   | 9.61     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2947000, episode_reward=1283.19 +/- 24.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2947000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2948000, episode_reward=1320.55 +/- 20.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2948000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.84    |\n",
      "|    critic_loss     | 0.000926 |\n",
      "|    ent_coef        | 0.000425 |\n",
      "|    ent_coef_loss   | 11.3     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 921      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2948     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4582     |\n",
      "|    total_timesteps | 2948000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2949000, episode_reward=1353.84 +/- 41.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2949000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2950000, episode_reward=1321.77 +/- 23.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.833   |\n",
      "|    critic_loss     | 0.00103  |\n",
      "|    ent_coef        | 0.000436 |\n",
      "|    ent_coef_loss   | 9.97     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2951000, episode_reward=1329.93 +/- 10.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2951000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2952000, episode_reward=1306.98 +/- 26.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2952000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.846   |\n",
      "|    critic_loss     | 0.000918 |\n",
      "|    ent_coef        | 0.000446 |\n",
      "|    ent_coef_loss   | 6.95     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 927      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2952     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4589     |\n",
      "|    total_timesteps | 2952000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2953000, episode_reward=1306.40 +/- 16.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2953000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2954000, episode_reward=1305.95 +/- 21.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2954000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.847   |\n",
      "|    critic_loss     | 0.000927 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | 7.17     |\n",
      "|    learning_rate   | 0.00205  |\n",
      "|    n_updates       | 14420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2955000, episode_reward=1318.39 +/- 29.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2955000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2956000, episode_reward=1202.97 +/- 23.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2956000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.839   |\n",
      "|    critic_loss     | 0.000877 |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | 5.18     |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14430    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 933      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2956     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4595     |\n",
      "|    total_timesteps | 2956000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2957000, episode_reward=1193.44 +/- 35.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2957000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2958000, episode_reward=1063.46 +/- 25.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2958000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.819   |\n",
      "|    critic_loss     | 0.000915 |\n",
      "|    ent_coef        | 0.000469 |\n",
      "|    ent_coef_loss   | 3.76     |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2959000, episode_reward=1052.87 +/- 62.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2959000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2960000, episode_reward=957.41 +/- 29.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 957      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.8     |\n",
      "|    critic_loss     | 0.000947 |\n",
      "|    ent_coef        | 0.000475 |\n",
      "|    ent_coef_loss   | 5.45     |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14450    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 929      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2960     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4601     |\n",
      "|    total_timesteps | 2960000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2961000, episode_reward=915.15 +/- 36.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 915      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2961000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2962000, episode_reward=1256.86 +/- 36.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2962000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.768   |\n",
      "|    critic_loss     | 0.000963 |\n",
      "|    ent_coef        | 0.00048  |\n",
      "|    ent_coef_loss   | 0.192    |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963000, episode_reward=1296.71 +/- 27.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2963000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2964000, episode_reward=1347.69 +/- 25.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2964000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.818   |\n",
      "|    critic_loss     | 0.000828 |\n",
      "|    ent_coef        | 0.000482 |\n",
      "|    ent_coef_loss   | -2.79    |\n",
      "|    learning_rate   | 0.00204  |\n",
      "|    n_updates       | 14470    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 932      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2964     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4607     |\n",
      "|    total_timesteps | 2964000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2965000, episode_reward=1366.99 +/- 21.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2965000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2966000, episode_reward=1317.63 +/- 21.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2966000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.846   |\n",
      "|    critic_loss     | 0.000906 |\n",
      "|    ent_coef        | 0.000482 |\n",
      "|    ent_coef_loss   | 3.09     |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2967000, episode_reward=1305.78 +/- 29.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2967000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2968000, episode_reward=1345.00 +/- 33.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2968000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.838   |\n",
      "|    critic_loss     | 0.000825 |\n",
      "|    ent_coef        | 0.000483 |\n",
      "|    ent_coef_loss   | 2.19     |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14490    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 942      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2968     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4614     |\n",
      "|    total_timesteps | 2968000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2969000, episode_reward=1350.29 +/- 31.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2969000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=1393.35 +/- 56.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.85    |\n",
      "|    critic_loss     | 0.000892 |\n",
      "|    ent_coef        | 0.000486 |\n",
      "|    ent_coef_loss   | 4.9      |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2971000, episode_reward=1339.47 +/- 80.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2971000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2972000, episode_reward=1337.43 +/- 56.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2972000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.821   |\n",
      "|    critic_loss     | 0.000971 |\n",
      "|    ent_coef        | 0.00049  |\n",
      "|    ent_coef_loss   | 2.89     |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14510    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 941      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2972     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4620     |\n",
      "|    total_timesteps | 2972000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2973000, episode_reward=1337.67 +/- 98.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2973000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2974000, episode_reward=-97.46 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2974000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.829   |\n",
      "|    critic_loss     | 0.000942 |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | -0.258   |\n",
      "|    learning_rate   | 0.00203  |\n",
      "|    n_updates       | 14520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2975000, episode_reward=-97.90 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2975000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2976000, episode_reward=185.11 +/- 577.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 185      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2976000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.798   |\n",
      "|    critic_loss     | 0.00081  |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | -5.84    |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14530    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 941      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2976     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4627     |\n",
      "|    total_timesteps | 2976000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2977000, episode_reward=-105.52 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2977000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2978000, episode_reward=1171.69 +/- 58.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2978000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.00095  |\n",
      "|    ent_coef        | 0.000491 |\n",
      "|    ent_coef_loss   | -1.7     |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2979000, episode_reward=1120.05 +/- 15.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2979000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=1169.12 +/- 50.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.000987 |\n",
      "|    ent_coef        | 0.000488 |\n",
      "|    ent_coef_loss   | -0.273   |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 939      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2980     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4633     |\n",
      "|    total_timesteps | 2980000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2981000, episode_reward=1249.31 +/- 48.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2981000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2982000, episode_reward=1164.86 +/- 18.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2982000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.808   |\n",
      "|    critic_loss     | 0.000913 |\n",
      "|    ent_coef        | 0.000487 |\n",
      "|    ent_coef_loss   | -1.94    |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2983000, episode_reward=1203.57 +/- 22.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2983000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2984000, episode_reward=1175.73 +/- 47.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2984000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.825   |\n",
      "|    critic_loss     | 0.00104  |\n",
      "|    ent_coef        | 0.000485 |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.00202  |\n",
      "|    n_updates       | 14570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 939      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2984     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4639     |\n",
      "|    total_timesteps | 2984000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2985000, episode_reward=1167.67 +/- 34.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2985000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2986000, episode_reward=1177.08 +/- 59.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2986000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.794   |\n",
      "|    critic_loss     | 0.000905 |\n",
      "|    ent_coef        | 0.000486 |\n",
      "|    ent_coef_loss   | -2.16    |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2987000, episode_reward=1153.64 +/- 49.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2987000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2988000, episode_reward=1168.52 +/- 49.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2988000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 934      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2988     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4645     |\n",
      "|    total_timesteps | 2988000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2989000, episode_reward=1209.94 +/- 72.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2989000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.000844 |\n",
      "|    ent_coef        | 0.000483 |\n",
      "|    ent_coef_loss   | -6.75    |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=1184.61 +/- 72.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2990000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2991000, episode_reward=1369.58 +/- 81.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2991000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.812   |\n",
      "|    critic_loss     | 0.00089  |\n",
      "|    ent_coef        | 0.000478 |\n",
      "|    ent_coef_loss   | 0.6      |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2992000, episode_reward=1420.70 +/- 27.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2992000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 935      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2992     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4651     |\n",
      "|    total_timesteps | 2992000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2993000, episode_reward=1353.21 +/- 33.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2993000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.833   |\n",
      "|    critic_loss     | 0.000911 |\n",
      "|    ent_coef        | 0.000478 |\n",
      "|    ent_coef_loss   | 5.86     |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2994000, episode_reward=1413.58 +/- 49.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2994000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2995000, episode_reward=1224.35 +/- 30.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2995000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.837   |\n",
      "|    critic_loss     | 0.000976 |\n",
      "|    ent_coef        | 0.000481 |\n",
      "|    ent_coef_loss   | 3.45     |\n",
      "|    learning_rate   | 0.00201  |\n",
      "|    n_updates       | 14620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2996000, episode_reward=1263.64 +/- 38.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2996000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 965      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2996     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4658     |\n",
      "|    total_timesteps | 2996000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2997000, episode_reward=1285.50 +/- 70.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2997000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.816   |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000485 |\n",
      "|    ent_coef_loss   | 2.36     |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2998000, episode_reward=1279.84 +/- 56.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2998000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2999000, episode_reward=1235.13 +/- 25.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2999000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.826   |\n",
      "|    critic_loss     | 0.00104  |\n",
      "|    ent_coef        | 0.000488 |\n",
      "|    ent_coef_loss   | 2.07     |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=1220.17 +/- 21.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3000     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4664     |\n",
      "|    total_timesteps | 3000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3001000, episode_reward=1251.34 +/- 19.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3001000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.808   |\n",
      "|    critic_loss     | 0.00115  |\n",
      "|    ent_coef        | 0.000492 |\n",
      "|    ent_coef_loss   | 2.76     |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3002000, episode_reward=1237.82 +/- 36.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3002000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003000, episode_reward=1235.97 +/- 38.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3003000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.809   |\n",
      "|    critic_loss     | 0.000989 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | 1.72     |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3004000, episode_reward=1245.53 +/- 57.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3004000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3004     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4670     |\n",
      "|    total_timesteps | 3004000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3005000, episode_reward=1248.13 +/- 39.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3005000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.805   |\n",
      "|    critic_loss     | 0.000972 |\n",
      "|    ent_coef        | 0.000497 |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.002    |\n",
      "|    n_updates       | 14670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3006000, episode_reward=1231.14 +/- 25.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3006000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3007000, episode_reward=1256.93 +/- 91.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3007000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.809   |\n",
      "|    critic_loss     | 0.000972 |\n",
      "|    ent_coef        | 0.000499 |\n",
      "|    ent_coef_loss   | 3.84     |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3008000, episode_reward=1257.26 +/- 25.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3008000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3008     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4676     |\n",
      "|    total_timesteps | 3008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3009000, episode_reward=1212.54 +/- 40.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3009000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.001    |\n",
      "|    ent_coef        | 0.000503 |\n",
      "|    ent_coef_loss   | 1.71     |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3010000, episode_reward=1235.10 +/- 18.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3010000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3011000, episode_reward=773.26 +/- 741.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 773      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3011000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000931 |\n",
      "|    ent_coef        | 0.000506 |\n",
      "|    ent_coef_loss   | 5.87     |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3012000, episode_reward=1001.51 +/- 568.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3012000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3012     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4682     |\n",
      "|    total_timesteps | 3012000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3013000, episode_reward=-141.50 +/- 1.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3013000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.812   |\n",
      "|    critic_loss     | 0.000978 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | 5.03     |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3014000, episode_reward=-140.24 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3014000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3015000, episode_reward=394.96 +/- 664.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 395      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3015000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.747   |\n",
      "|    critic_loss     | 0.000826 |\n",
      "|    ent_coef        | 0.000517 |\n",
      "|    ent_coef_loss   | -6.44    |\n",
      "|    learning_rate   | 0.00199  |\n",
      "|    n_updates       | 14720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3016000, episode_reward=384.85 +/- 649.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 385      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3016000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3016     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4689     |\n",
      "|    total_timesteps | 3016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3017000, episode_reward=1380.64 +/- 34.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3017000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.00107  |\n",
      "|    ent_coef        | 0.000516 |\n",
      "|    ent_coef_loss   | 3.83     |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3018000, episode_reward=1390.12 +/- 37.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3018000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3019000, episode_reward=-136.73 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3019000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.813   |\n",
      "|    critic_loss     | 0.00111  |\n",
      "|    ent_coef        | 0.000517 |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3020000, episode_reward=-137.47 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -137     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3020000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3020     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4695     |\n",
      "|    total_timesteps | 3020000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3021000, episode_reward=1340.13 +/- 19.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3021000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.815   |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.000517 |\n",
      "|    ent_coef_loss   | -0.633   |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3022000, episode_reward=1373.71 +/- 35.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3022000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3023000, episode_reward=1322.44 +/- 34.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3023000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.813   |\n",
      "|    critic_loss     | 0.00113  |\n",
      "|    ent_coef        | 0.000516 |\n",
      "|    ent_coef_loss   | 3.36     |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3024000, episode_reward=1275.69 +/- 20.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3024000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3024     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4701     |\n",
      "|    total_timesteps | 3024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3025000, episode_reward=1319.75 +/- 35.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3025000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.821   |\n",
      "|    critic_loss     | 0.00106  |\n",
      "|    ent_coef        | 0.00052  |\n",
      "|    ent_coef_loss   | 5.86     |\n",
      "|    learning_rate   | 0.00198  |\n",
      "|    n_updates       | 14770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3026000, episode_reward=1346.63 +/- 75.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3026000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3027000, episode_reward=1212.27 +/- 96.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3027000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.804   |\n",
      "|    critic_loss     | 0.00101  |\n",
      "|    ent_coef        | 0.000526 |\n",
      "|    ent_coef_loss   | 4.49     |\n",
      "|    learning_rate   | 0.00197  |\n",
      "|    n_updates       | 14780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3028000, episode_reward=1193.76 +/- 61.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3028000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3028     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4707     |\n",
      "|    total_timesteps | 3028000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3029000, episode_reward=1257.46 +/- 40.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3029000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.00101  |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | 3.29     |\n",
      "|    learning_rate   | 0.00197  |\n",
      "|    n_updates       | 14790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3030000, episode_reward=1301.46 +/- 41.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3030000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3031000, episode_reward=1326.27 +/- 65.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3031000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3032000, episode_reward=1282.15 +/- 96.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3032000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.775   |\n",
      "|    critic_loss     | 0.000959 |\n",
      "|    ent_coef        | 0.000536 |\n",
      "|    ent_coef_loss   | -2.3     |\n",
      "|    learning_rate   | 0.00197  |\n",
      "|    n_updates       | 14800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3032     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4713     |\n",
      "|    total_timesteps | 3032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3033000, episode_reward=1273.51 +/- 87.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3033000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3034000, episode_reward=-115.59 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3034000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.698   |\n",
      "|    critic_loss     | 0.00069  |\n",
      "|    ent_coef        | 0.000535 |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.00197  |\n",
      "|    n_updates       | 14810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3035000, episode_reward=-115.73 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3035000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3036000, episode_reward=-145.52 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3036000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.49    |\n",
      "|    critic_loss     | 0.000152 |\n",
      "|    ent_coef        | 0.000519 |\n",
      "|    ent_coef_loss   | -37.2    |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3036     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4720     |\n",
      "|    total_timesteps | 3036000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3037000, episode_reward=-146.58 +/- 1.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3037000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3038000, episode_reward=-192.73 +/- 1.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -193     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3038000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.605   |\n",
      "|    critic_loss     | 0.000484 |\n",
      "|    ent_coef        | 0.00049  |\n",
      "|    ent_coef_loss   | -20.4    |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3039000, episode_reward=-193.38 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -193     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3039000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=-143.72 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.481   |\n",
      "|    critic_loss     | 6.39e-05 |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | -34.1    |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14840    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3040     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4726     |\n",
      "|    total_timesteps | 3040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3041000, episode_reward=-144.28 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3041000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3042000, episode_reward=1294.21 +/- 22.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3042000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.47    |\n",
      "|    critic_loss     | 4.48e-05 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | -35.4    |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3043000, episode_reward=1275.15 +/- 65.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3043000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3044000, episode_reward=163.17 +/- 546.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 163      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3044000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.757   |\n",
      "|    critic_loss     | 0.00134  |\n",
      "|    ent_coef        | 0.00041  |\n",
      "|    ent_coef_loss   | 1.62     |\n",
      "|    learning_rate   | 0.00196  |\n",
      "|    n_updates       | 14860    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3044     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4732     |\n",
      "|    total_timesteps | 3044000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3045000, episode_reward=-110.76 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3045000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3046000, episode_reward=1406.02 +/- 21.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3046000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.618   |\n",
      "|    critic_loss     | 0.000522 |\n",
      "|    ent_coef        | 0.000399 |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3047000, episode_reward=1368.00 +/- 71.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3047000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3048000, episode_reward=938.51 +/- 535.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 939      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3048000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.81    |\n",
      "|    critic_loss     | 0.0011   |\n",
      "|    ent_coef        | 0.000392 |\n",
      "|    ent_coef_loss   | 14       |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14880    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3048     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4738     |\n",
      "|    total_timesteps | 3048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3049000, episode_reward=407.40 +/- 657.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 407      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3049000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3050000, episode_reward=1242.20 +/- 56.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.745   |\n",
      "|    critic_loss     | 0.000797 |\n",
      "|    ent_coef        | 0.000394 |\n",
      "|    ent_coef_loss   | 2.35     |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3051000, episode_reward=1217.34 +/- 33.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3051000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3052000, episode_reward=1273.59 +/- 53.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3052000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000397 |\n",
      "|    ent_coef_loss   | 9.88     |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14900    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3052     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4745     |\n",
      "|    total_timesteps | 3052000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3053000, episode_reward=1262.90 +/- 57.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3053000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3054000, episode_reward=1306.18 +/- 48.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3054000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.802   |\n",
      "|    critic_loss     | 0.00119  |\n",
      "|    ent_coef        | 0.000403 |\n",
      "|    ent_coef_loss   | 11.3     |\n",
      "|    learning_rate   | 0.00195  |\n",
      "|    n_updates       | 14910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3055000, episode_reward=1324.30 +/- 60.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3055000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3056000, episode_reward=1118.47 +/- 29.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3056000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.802   |\n",
      "|    critic_loss     | 0.00103  |\n",
      "|    ent_coef        | 0.00041  |\n",
      "|    ent_coef_loss   | 9.44     |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14920    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3056     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4751     |\n",
      "|    total_timesteps | 3056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3057000, episode_reward=1153.69 +/- 46.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3057000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3058000, episode_reward=1227.40 +/- 19.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3058000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.763   |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.000418 |\n",
      "|    ent_coef_loss   | 5.78     |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3059000, episode_reward=1246.92 +/- 33.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3059000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060000, episode_reward=1293.84 +/- 43.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.0011   |\n",
      "|    ent_coef        | 0.000424 |\n",
      "|    ent_coef_loss   | 11.2     |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14940    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3060     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4757     |\n",
      "|    total_timesteps | 3060000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3061000, episode_reward=1252.40 +/- 25.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3061000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3062000, episode_reward=1336.19 +/- 39.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3062000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.786   |\n",
      "|    critic_loss     | 0.00111  |\n",
      "|    ent_coef        | 0.000432 |\n",
      "|    ent_coef_loss   | 11.6     |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3063000, episode_reward=1363.79 +/- 52.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3063000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3064000, episode_reward=1244.01 +/- 79.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3064000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.807   |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000441 |\n",
      "|    ent_coef_loss   | 5.93     |\n",
      "|    learning_rate   | 0.00194  |\n",
      "|    n_updates       | 14960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3064     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4763     |\n",
      "|    total_timesteps | 3064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3065000, episode_reward=1219.20 +/- 55.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3065000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3066000, episode_reward=1240.01 +/- 48.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3066000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.773   |\n",
      "|    critic_loss     | 0.00106  |\n",
      "|    ent_coef        | 0.000448 |\n",
      "|    ent_coef_loss   | 6.58     |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 14970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3067000, episode_reward=1295.65 +/- 27.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3067000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3068000, episode_reward=1141.94 +/- 51.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3068000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.00107  |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | 2.34     |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 14980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3068     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4769     |\n",
      "|    total_timesteps | 3068000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3069000, episode_reward=1092.38 +/- 39.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3069000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3070000, episode_reward=1120.84 +/- 18.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.76    |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.000458 |\n",
      "|    ent_coef_loss   | 5.74     |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 14990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3071000, episode_reward=1098.39 +/- 28.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3071000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3072000, episode_reward=1106.40 +/- 31.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3072000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3072     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4775     |\n",
      "|    total_timesteps | 3072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3073000, episode_reward=1231.77 +/- 40.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3073000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.751   |\n",
      "|    critic_loss     | 0.000918 |\n",
      "|    ent_coef        | 0.000462 |\n",
      "|    ent_coef_loss   | 3.15     |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 15000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3074000, episode_reward=1207.47 +/- 47.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3074000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3075000, episode_reward=-132.20 +/- 1.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -132     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3075000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.772   |\n",
      "|    critic_loss     | 0.00104  |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | 4.17     |\n",
      "|    learning_rate   | 0.00193  |\n",
      "|    n_updates       | 15010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3076000, episode_reward=-131.27 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3076000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3076     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4782     |\n",
      "|    total_timesteps | 3076000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3077000, episode_reward=-140.34 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3077000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.724   |\n",
      "|    critic_loss     | 0.000845 |\n",
      "|    ent_coef        | 0.000469 |\n",
      "|    ent_coef_loss   | -1.72    |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3078000, episode_reward=-141.18 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3078000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3079000, episode_reward=-162.88 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3079000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.55    |\n",
      "|    critic_loss     | 0.000349 |\n",
      "|    ent_coef        | 0.000467 |\n",
      "|    ent_coef_loss   | -19.9    |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3080000, episode_reward=-163.11 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3080000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3080     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4788     |\n",
      "|    total_timesteps | 3080000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3081000, episode_reward=-168.70 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -169     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3081000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.577   |\n",
      "|    critic_loss     | 0.000429 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3082000, episode_reward=-169.38 +/- 1.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -169     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3082000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3083000, episode_reward=-160.30 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -160     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3083000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.458   |\n",
      "|    critic_loss     | 2.82e-05 |\n",
      "|    ent_coef        | 0.000442 |\n",
      "|    ent_coef_loss   | -31.7    |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3084000, episode_reward=-163.11 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3084000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 961      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3084     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4794     |\n",
      "|    total_timesteps | 3084000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3085000, episode_reward=-140.52 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3085000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.452   |\n",
      "|    critic_loss     | 2.32e-05 |\n",
      "|    ent_coef        | 0.000422 |\n",
      "|    ent_coef_loss   | -36.1    |\n",
      "|    learning_rate   | 0.00192  |\n",
      "|    n_updates       | 15060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3086000, episode_reward=-140.92 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3086000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3087000, episode_reward=-102.82 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3087000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.451   |\n",
      "|    critic_loss     | 2.38e-05 |\n",
      "|    ent_coef        | 0.000398 |\n",
      "|    ent_coef_loss   | -37.5    |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3088000, episode_reward=-103.10 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3088000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 913      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3088     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4801     |\n",
      "|    total_timesteps | 3088000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3089000, episode_reward=-118.15 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3089000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.448   |\n",
      "|    critic_loss     | 3.43e-05 |\n",
      "|    ent_coef        | 0.000375 |\n",
      "|    ent_coef_loss   | -39.4    |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090000, episode_reward=-118.18 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3090000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3091000, episode_reward=-132.61 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3091000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.614   |\n",
      "|    critic_loss     | 0.000583 |\n",
      "|    ent_coef        | 0.000356 |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3092000, episode_reward=-131.12 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3092000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 882      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3092     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4807     |\n",
      "|    total_timesteps | 3092000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3093000, episode_reward=-232.01 +/- 2.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -232     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3093000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.528   |\n",
      "|    critic_loss     | 0.000311 |\n",
      "|    ent_coef        | 0.000344 |\n",
      "|    ent_coef_loss   | -16.1    |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3094000, episode_reward=-230.87 +/- 2.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -231     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3094000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3095000, episode_reward=-191.52 +/- 2.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3095000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.434   |\n",
      "|    critic_loss     | 1.59e-05 |\n",
      "|    ent_coef        | 0.000334 |\n",
      "|    ent_coef_loss   | -20.5    |\n",
      "|    learning_rate   | 0.00191  |\n",
      "|    n_updates       | 15110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3096000, episode_reward=-194.07 +/- 0.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3096000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 825      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3096     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4813     |\n",
      "|    total_timesteps | 3096000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3097000, episode_reward=-164.09 +/- 1.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -164     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3097000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.44    |\n",
      "|    critic_loss     | 1.28e-05 |\n",
      "|    ent_coef        | 0.000324 |\n",
      "|    ent_coef_loss   | -30.1    |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3098000, episode_reward=-165.05 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -165     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3098000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3099000, episode_reward=-180.31 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -180     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3099000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.442   |\n",
      "|    critic_loss     | 9.63e-06 |\n",
      "|    ent_coef        | 0.000312 |\n",
      "|    ent_coef_loss   | -24.4    |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3100000, episode_reward=-178.93 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -179     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3100000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 770      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3100     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4820     |\n",
      "|    total_timesteps | 3100000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3101000, episode_reward=-154.27 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3101000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.443   |\n",
      "|    critic_loss     | 7.56e-06 |\n",
      "|    ent_coef        | 0.000301 |\n",
      "|    ent_coef_loss   | -24.5    |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3102000, episode_reward=-153.73 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3102000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3103000, episode_reward=-127.78 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3103000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.534   |\n",
      "|    critic_loss     | 0.000358 |\n",
      "|    ent_coef        | 0.000291 |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3104000, episode_reward=-127.76 +/- 0.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3104000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 738      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3104     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4826     |\n",
      "|    total_timesteps | 3104000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3105000, episode_reward=1157.95 +/- 60.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3105000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.658   |\n",
      "|    critic_loss     | 0.000862 |\n",
      "|    ent_coef        | 0.000285 |\n",
      "|    ent_coef_loss   | 3.73     |\n",
      "|    learning_rate   | 0.0019   |\n",
      "|    n_updates       | 15160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3106000, episode_reward=1171.32 +/- 44.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3106000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3107000, episode_reward=1137.32 +/- 45.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3107000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.753   |\n",
      "|    critic_loss     | 0.00116  |\n",
      "|    ent_coef        | 0.000284 |\n",
      "|    ent_coef_loss   | 16.2     |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    n_updates       | 15170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3108000, episode_reward=1095.34 +/- 37.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3108000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 734      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3108     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4832     |\n",
      "|    total_timesteps | 3108000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3109000, episode_reward=1120.95 +/- 47.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3109000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.74    |\n",
      "|    critic_loss     | 0.00114  |\n",
      "|    ent_coef        | 0.000288 |\n",
      "|    ent_coef_loss   | 24.7     |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    n_updates       | 15180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3110000, episode_reward=1094.68 +/- 27.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3110000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3111000, episode_reward=1109.91 +/- 38.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3111000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.731   |\n",
      "|    critic_loss     | 0.00105  |\n",
      "|    ent_coef        | 0.000295 |\n",
      "|    ent_coef_loss   | 15.6     |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    n_updates       | 15190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3112000, episode_reward=1122.30 +/- 29.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3112000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 721      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3112     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4838     |\n",
      "|    total_timesteps | 3112000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3113000, episode_reward=1116.70 +/- 31.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3113000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 0.000364 |\n",
      "|    ent_coef        | 0.0003   |\n",
      "|    ent_coef_loss   | -17.1    |\n",
      "|    learning_rate   | 0.00189  |\n",
      "|    n_updates       | 15200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3114000, episode_reward=1097.40 +/- 50.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3114000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3115000, episode_reward=1106.50 +/- 33.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3115000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3116000, episode_reward=931.66 +/- 31.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 932      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3116000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.739   |\n",
      "|    critic_loss     | 0.00115  |\n",
      "|    ent_coef        | 0.0003   |\n",
      "|    ent_coef_loss   | 16.6     |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 714      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3116     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4844     |\n",
      "|    total_timesteps | 3116000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3117000, episode_reward=969.08 +/- 50.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 969      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3117000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3118000, episode_reward=1077.37 +/- 20.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3118000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.71    |\n",
      "|    critic_loss     | 0.00106  |\n",
      "|    ent_coef        | 0.000304 |\n",
      "|    ent_coef_loss   | 15.8     |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3119000, episode_reward=1121.67 +/- 64.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3119000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3120000, episode_reward=907.24 +/- 51.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 907      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.75    |\n",
      "|    critic_loss     | 0.00126  |\n",
      "|    ent_coef        | 0.00031  |\n",
      "|    ent_coef_loss   | 22.6     |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 704      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3120     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4850     |\n",
      "|    total_timesteps | 3120000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3121000, episode_reward=946.48 +/- 39.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 946      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3121000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3122000, episode_reward=1195.12 +/- 62.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3122000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.701   |\n",
      "|    critic_loss     | 0.00114  |\n",
      "|    ent_coef        | 0.000318 |\n",
      "|    ent_coef_loss   | 20.4     |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3123000, episode_reward=1210.35 +/- 25.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3123000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3124000, episode_reward=1217.98 +/- 42.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3124000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.748   |\n",
      "|    critic_loss     | 0.00105  |\n",
      "|    ent_coef        | 0.000327 |\n",
      "|    ent_coef_loss   | 20.1     |\n",
      "|    learning_rate   | 0.00188  |\n",
      "|    n_updates       | 15250    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 696      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3124     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4857     |\n",
      "|    total_timesteps | 3124000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3125000, episode_reward=1197.23 +/- 78.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3125000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3126000, episode_reward=1227.34 +/- 57.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3126000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.761   |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.000336 |\n",
      "|    ent_coef_loss   | 21.1     |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3127000, episode_reward=1157.31 +/- 31.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3127000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3128000, episode_reward=-100.61 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3128000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.727   |\n",
      "|    critic_loss     | 0.000987 |\n",
      "|    ent_coef        | 0.000345 |\n",
      "|    ent_coef_loss   | 9.19     |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15270    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 693      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3128     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4863     |\n",
      "|    total_timesteps | 3128000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3129000, episode_reward=-99.55 +/- 2.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -99.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3129000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3130000, episode_reward=-185.15 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -185     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.551   |\n",
      "|    critic_loss     | 0.000457 |\n",
      "|    ent_coef        | 0.00035  |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3131000, episode_reward=-186.74 +/- 1.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3131000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3132000, episode_reward=-163.26 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3132000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.515   |\n",
      "|    critic_loss     | 0.000296 |\n",
      "|    ent_coef        | 0.000348 |\n",
      "|    ent_coef_loss   | -17.3    |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15290    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 655      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3132     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4869     |\n",
      "|    total_timesteps | 3132000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3133000, episode_reward=-162.53 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -163     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3133000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3134000, episode_reward=-140.83 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3134000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.434   |\n",
      "|    critic_loss     | 2.1e-05  |\n",
      "|    ent_coef        | 0.000342 |\n",
      "|    ent_coef_loss   | -34.5    |\n",
      "|    learning_rate   | 0.00187  |\n",
      "|    n_updates       | 15300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3135000, episode_reward=-139.12 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3135000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3136000, episode_reward=-85.34 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3136000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.434   |\n",
      "|    critic_loss     | 1.24e-05 |\n",
      "|    ent_coef        | 0.00033  |\n",
      "|    ent_coef_loss   | -38.3    |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15310    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 629      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3136     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4876     |\n",
      "|    total_timesteps | 3136000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3137000, episode_reward=-85.58 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3137000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3138000, episode_reward=-90.73 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3138000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.433   |\n",
      "|    critic_loss     | 1.57e-05 |\n",
      "|    ent_coef        | 0.000316 |\n",
      "|    ent_coef_loss   | -36.1    |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3139000, episode_reward=-89.32 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -89.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3139000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3140000, episode_reward=-83.23 +/- 2.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -83.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.433   |\n",
      "|    critic_loss     | 1.19e-05 |\n",
      "|    ent_coef        | 0.000303 |\n",
      "|    ent_coef_loss   | -34.1    |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15330    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 635      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3140     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4882     |\n",
      "|    total_timesteps | 3140000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3141000, episode_reward=-81.63 +/- 1.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3141000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3142000, episode_reward=-81.98 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -82      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3142000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.679   |\n",
      "|    critic_loss     | 0.000876 |\n",
      "|    ent_coef        | 0.000292 |\n",
      "|    ent_coef_loss   | 1.63     |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3143000, episode_reward=-81.88 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -81.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3143000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3144000, episode_reward=1072.13 +/- 28.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3144000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.487   |\n",
      "|    critic_loss     | 0.000244 |\n",
      "|    ent_coef        | 0.000286 |\n",
      "|    ent_coef_loss   | -22.5    |\n",
      "|    learning_rate   | 0.00186  |\n",
      "|    n_updates       | 15350    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 629      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3144     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4888     |\n",
      "|    total_timesteps | 3144000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3145000, episode_reward=1056.67 +/- 56.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3145000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3146000, episode_reward=1276.88 +/- 27.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3146000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.721   |\n",
      "|    critic_loss     | 0.00115  |\n",
      "|    ent_coef        | 0.000282 |\n",
      "|    ent_coef_loss   | 19.8     |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3147000, episode_reward=1244.88 +/- 82.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3147000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3148000, episode_reward=1234.93 +/- 43.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3148000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.756   |\n",
      "|    critic_loss     | 0.00114  |\n",
      "|    ent_coef        | 0.000284 |\n",
      "|    ent_coef_loss   | 23.9     |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15370    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 640      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3148     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4894     |\n",
      "|    total_timesteps | 3148000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3149000, episode_reward=1269.73 +/- 28.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3149000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3150000, episode_reward=1251.45 +/- 30.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.748   |\n",
      "|    critic_loss     | 0.0011   |\n",
      "|    ent_coef        | 0.00029  |\n",
      "|    ent_coef_loss   | 13       |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3151000, episode_reward=1260.27 +/- 29.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3151000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3152000, episode_reward=1090.20 +/- 38.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3152000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.745   |\n",
      "|    critic_loss     | 0.00115  |\n",
      "|    ent_coef        | 0.000295 |\n",
      "|    ent_coef_loss   | 15.6     |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 645      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3152     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4900     |\n",
      "|    total_timesteps | 3152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3153000, episode_reward=1082.17 +/- 34.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3153000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3154000, episode_reward=1285.96 +/- 45.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3154000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.72    |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.0003   |\n",
      "|    ent_coef_loss   | 20.4     |\n",
      "|    learning_rate   | 0.00185  |\n",
      "|    n_updates       | 15400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3155000, episode_reward=1269.03 +/- 33.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3155000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3156000, episode_reward=1202.41 +/- 44.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3156000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.75    |\n",
      "|    critic_loss     | 0.00102  |\n",
      "|    ent_coef        | 0.000307 |\n",
      "|    ent_coef_loss   | 17       |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 642      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3156     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4907     |\n",
      "|    total_timesteps | 3156000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3157000, episode_reward=1193.60 +/- 49.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3157000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3158000, episode_reward=1167.12 +/- 22.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3158000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3159000, episode_reward=1232.11 +/- 35.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3159000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.729   |\n",
      "|    critic_loss     | 0.0011   |\n",
      "|    ent_coef        | 0.000313 |\n",
      "|    ent_coef_loss   | 15.7     |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3160000, episode_reward=1233.25 +/- 38.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3160000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 644      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3160     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4913     |\n",
      "|    total_timesteps | 3160000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3161000, episode_reward=1296.66 +/- 34.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3161000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.746   |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.000319 |\n",
      "|    ent_coef_loss   | 17.4     |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3162000, episode_reward=1288.34 +/- 65.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3162000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3163000, episode_reward=1230.88 +/- 46.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3163000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.758   |\n",
      "|    critic_loss     | 0.00103  |\n",
      "|    ent_coef        | 0.000325 |\n",
      "|    ent_coef_loss   | 14.1     |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3164000, episode_reward=1232.71 +/- 53.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3164000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 645      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3164     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4919     |\n",
      "|    total_timesteps | 3164000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3165000, episode_reward=1259.79 +/- 45.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3165000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.737   |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000331 |\n",
      "|    ent_coef_loss   | 14.1     |\n",
      "|    learning_rate   | 0.00184  |\n",
      "|    n_updates       | 15450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3166000, episode_reward=1291.69 +/- 18.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3166000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3167000, episode_reward=1291.74 +/- 14.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3167000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.737   |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.000337 |\n",
      "|    ent_coef_loss   | 13.3     |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3168000, episode_reward=1263.54 +/- 40.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3168000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 648      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3168     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4925     |\n",
      "|    total_timesteps | 3168000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3169000, episode_reward=1167.81 +/- 38.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3169000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.742   |\n",
      "|    critic_loss     | 0.00113  |\n",
      "|    ent_coef        | 0.000343 |\n",
      "|    ent_coef_loss   | 15.5     |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3170000, episode_reward=1168.49 +/- 51.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3170000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3171000, episode_reward=1286.11 +/- 63.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3171000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.721   |\n",
      "|    critic_loss     | 0.000951 |\n",
      "|    ent_coef        | 0.000349 |\n",
      "|    ent_coef_loss   | 13.2     |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3172000, episode_reward=1286.65 +/- 54.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3172000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 651      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3172     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4931     |\n",
      "|    total_timesteps | 3172000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3173000, episode_reward=1213.36 +/- 63.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3173000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.733   |\n",
      "|    critic_loss     | 0.00113  |\n",
      "|    ent_coef        | 0.000354 |\n",
      "|    ent_coef_loss   | 8.93     |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3174000, episode_reward=1210.96 +/- 28.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3174000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3175000, episode_reward=1236.27 +/- 38.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3175000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.708   |\n",
      "|    critic_loss     | 0.0011   |\n",
      "|    ent_coef        | 0.000359 |\n",
      "|    ent_coef_loss   | 10.9     |\n",
      "|    learning_rate   | 0.00183  |\n",
      "|    n_updates       | 15500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3176000, episode_reward=1268.61 +/- 47.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3176000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 653      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3176     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4937     |\n",
      "|    total_timesteps | 3176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3177000, episode_reward=1216.19 +/- 51.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3177000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.728   |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000364 |\n",
      "|    ent_coef_loss   | 9.49     |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3178000, episode_reward=1185.80 +/- 46.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3178000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3179000, episode_reward=1188.33 +/- 59.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3179000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.731   |\n",
      "|    critic_loss     | 0.00113  |\n",
      "|    ent_coef        | 0.000368 |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3180000, episode_reward=1180.88 +/- 61.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3180000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 685      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3180     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4943     |\n",
      "|    total_timesteps | 3180000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3181000, episode_reward=1302.83 +/- 49.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3181000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.705   |\n",
      "|    critic_loss     | 0.00108  |\n",
      "|    ent_coef        | 0.000372 |\n",
      "|    ent_coef_loss   | 7.7      |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3182000, episode_reward=1274.83 +/- 40.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3182000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3183000, episode_reward=1122.93 +/- 64.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3183000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.714   |\n",
      "|    critic_loss     | 0.00107  |\n",
      "|    ent_coef        | 0.000376 |\n",
      "|    ent_coef_loss   | 4.73     |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3184000, episode_reward=1156.53 +/- 55.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3184000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 737      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3184     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4949     |\n",
      "|    total_timesteps | 3184000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3185000, episode_reward=1181.91 +/- 19.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3185000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.697   |\n",
      "|    critic_loss     | 0.00105  |\n",
      "|    ent_coef        | 0.000379 |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 0.00182  |\n",
      "|    n_updates       | 15550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3186000, episode_reward=1155.40 +/- 36.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3186000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3187000, episode_reward=-84.19 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3187000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.678   |\n",
      "|    critic_loss     | 0.000919 |\n",
      "|    ent_coef        | 0.000383 |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3188000, episode_reward=-83.59 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -83.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3188000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 773      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3188     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4956     |\n",
      "|    total_timesteps | 3188000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3189000, episode_reward=-104.90 +/- 3.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3189000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.55    |\n",
      "|    critic_loss     | 0.000475 |\n",
      "|    ent_coef        | 0.000384 |\n",
      "|    ent_coef_loss   | -17      |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3190000, episode_reward=-105.50 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3190000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3191000, episode_reward=-139.52 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3191000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.443   |\n",
      "|    critic_loss     | 0.000123 |\n",
      "|    ent_coef        | 0.000379 |\n",
      "|    ent_coef_loss   | -30.2    |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3192000, episode_reward=-139.86 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3192000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 762      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3192     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4962     |\n",
      "|    total_timesteps | 3192000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3193000, episode_reward=-148.24 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3193000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.41    |\n",
      "|    critic_loss     | 3.22e-05 |\n",
      "|    ent_coef        | 0.000369 |\n",
      "|    ent_coef_loss   | -31.1    |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3194000, episode_reward=-148.45 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3194000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3195000, episode_reward=-243.63 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3195000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.413   |\n",
      "|    critic_loss     | 2.43e-05 |\n",
      "|    ent_coef        | 0.000357 |\n",
      "|    ent_coef_loss   | -34      |\n",
      "|    learning_rate   | 0.00181  |\n",
      "|    n_updates       | 15600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3196000, episode_reward=-241.67 +/- 4.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3196000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 763      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3196     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4968     |\n",
      "|    total_timesteps | 3196000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3197000, episode_reward=-165.77 +/- 2.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -166     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3197000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.416   |\n",
      "|    critic_loss     | 2.49e-05 |\n",
      "|    ent_coef        | 0.000344 |\n",
      "|    ent_coef_loss   | -30.9    |\n",
      "|    learning_rate   | 0.0018   |\n",
      "|    n_updates       | 15610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3198000, episode_reward=-164.63 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -165     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3198000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3199000, episode_reward=-176.17 +/- 2.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -176     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3199000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.415   |\n",
      "|    critic_loss     | 1.89e-05 |\n",
      "|    ent_coef        | 0.000332 |\n",
      "|    ent_coef_loss   | -30.4    |\n",
      "|    learning_rate   | 0.0018   |\n",
      "|    n_updates       | 15620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=-176.82 +/- 2.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -177     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3200000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 763      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3200     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4975     |\n",
      "|    total_timesteps | 3200000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3201000, episode_reward=-175.73 +/- 2.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -176     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3201000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3202000, episode_reward=-200.51 +/- 2.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -201     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3202000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.415   |\n",
      "|    critic_loss     | 1.07e-05 |\n",
      "|    ent_coef        | 0.000321 |\n",
      "|    ent_coef_loss   | -31.7    |\n",
      "|    learning_rate   | 0.0018   |\n",
      "|    n_updates       | 15630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3203000, episode_reward=-198.28 +/- 2.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -198     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3203000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3204000, episode_reward=-144.73 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3204000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.416   |\n",
      "|    critic_loss     | 1.33e-05 |\n",
      "|    ent_coef        | 0.00031  |\n",
      "|    ent_coef_loss   | -34      |\n",
      "|    learning_rate   | 0.0018   |\n",
      "|    n_updates       | 15640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 742      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3204     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4981     |\n",
      "|    total_timesteps | 3204000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3205000, episode_reward=-146.10 +/- 2.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3205000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3206000, episode_reward=-133.21 +/- 3.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3206000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.414   |\n",
      "|    critic_loss     | 1.09e-05 |\n",
      "|    ent_coef        | 0.000299 |\n",
      "|    ent_coef_loss   | -36      |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3207000, episode_reward=-134.96 +/- 1.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3207000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3208000, episode_reward=-110.18 +/- 2.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3208000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.412   |\n",
      "|    critic_loss     | 8.75e-06 |\n",
      "|    ent_coef        | 0.000289 |\n",
      "|    ent_coef_loss   | -36      |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 694      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3208     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4987     |\n",
      "|    total_timesteps | 3208000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3209000, episode_reward=-108.42 +/- 2.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3209000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210000, episode_reward=-111.23 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.411   |\n",
      "|    critic_loss     | 9.03e-06 |\n",
      "|    ent_coef        | 0.000278 |\n",
      "|    ent_coef_loss   | -37.8    |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3211000, episode_reward=-110.31 +/- 1.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3211000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3212000, episode_reward=-109.61 +/- 1.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3212000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.412   |\n",
      "|    critic_loss     | 8.33e-06 |\n",
      "|    ent_coef        | 0.000268 |\n",
      "|    ent_coef_loss   | -35.7    |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15680    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 656      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3212     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 4994     |\n",
      "|    total_timesteps | 3212000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3213000, episode_reward=-110.95 +/- 1.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -111     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3213000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3214000, episode_reward=-106.27 +/- 2.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3214000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.414   |\n",
      "|    critic_loss     | 6.93e-06 |\n",
      "|    ent_coef        | 0.000259 |\n",
      "|    ent_coef_loss   | -30.9    |\n",
      "|    learning_rate   | 0.00179  |\n",
      "|    n_updates       | 15690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3215000, episode_reward=-106.80 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3215000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3216000, episode_reward=-107.84 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3216000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.414   |\n",
      "|    critic_loss     | 9.12e-06 |\n",
      "|    ent_coef        | 0.000251 |\n",
      "|    ent_coef_loss   | -28.7    |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15700    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 618      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3216     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5000     |\n",
      "|    total_timesteps | 3216000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3217000, episode_reward=-108.42 +/- 1.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3217000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3218000, episode_reward=-104.40 +/- 0.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3218000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.413   |\n",
      "|    critic_loss     | 5.58e-06 |\n",
      "|    ent_coef        | 0.000244 |\n",
      "|    ent_coef_loss   | -29.9    |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3219000, episode_reward=-103.96 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3219000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3220000, episode_reward=-87.99 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -88      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.409   |\n",
      "|    critic_loss     | 5.68e-06 |\n",
      "|    ent_coef        | 0.000237 |\n",
      "|    ent_coef_loss   | -33.7    |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15720    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 573      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3220     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5006     |\n",
      "|    total_timesteps | 3220000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3221000, episode_reward=-87.14 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3221000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3222000, episode_reward=-104.72 +/- 1.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3222000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.408   |\n",
      "|    critic_loss     | 5.69e-06 |\n",
      "|    ent_coef        | 0.00023  |\n",
      "|    ent_coef_loss   | -34.6    |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3223000, episode_reward=-104.10 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3223000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3224000, episode_reward=-85.02 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3224000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.411   |\n",
      "|    critic_loss     | 4.95e-06 |\n",
      "|    ent_coef        | 0.000223 |\n",
      "|    ent_coef_loss   | -33.4    |\n",
      "|    learning_rate   | 0.00178  |\n",
      "|    n_updates       | 15740    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 525      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3224     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5013     |\n",
      "|    total_timesteps | 3224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3225000, episode_reward=-83.86 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -83.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3225000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3226000, episode_reward=-101.74 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3226000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.41    |\n",
      "|    critic_loss     | 6.16e-06 |\n",
      "|    ent_coef        | 0.000217 |\n",
      "|    ent_coef_loss   | -31.4    |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3227000, episode_reward=-101.33 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3227000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3228000, episode_reward=-108.97 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3228000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.41    |\n",
      "|    critic_loss     | 5.36e-06 |\n",
      "|    ent_coef        | 0.000211 |\n",
      "|    ent_coef_loss   | -18.7    |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15760    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 476      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3228     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5019     |\n",
      "|    total_timesteps | 3228000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3229000, episode_reward=-109.25 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3229000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3230000, episode_reward=-104.19 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.409   |\n",
      "|    critic_loss     | 5.77e-06 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | -19.9    |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3231000, episode_reward=-103.23 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3231000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3232000, episode_reward=-94.96 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3232000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.406   |\n",
      "|    critic_loss     | 4.85e-06 |\n",
      "|    ent_coef        | 0.000203 |\n",
      "|    ent_coef_loss   | -25.7    |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15780    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 470      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3232     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5025     |\n",
      "|    total_timesteps | 3232000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3233000, episode_reward=-95.66 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3233000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3234000, episode_reward=-72.40 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -72.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3234000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.405   |\n",
      "|    critic_loss     | 5.6e-06  |\n",
      "|    ent_coef        | 0.000198 |\n",
      "|    ent_coef_loss   | -31.3    |\n",
      "|    learning_rate   | 0.00177  |\n",
      "|    n_updates       | 15790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3235000, episode_reward=-73.76 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -73.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3235000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3236000, episode_reward=-39.97 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3236000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.405   |\n",
      "|    critic_loss     | 5.01e-06 |\n",
      "|    ent_coef        | 0.000193 |\n",
      "|    ent_coef_loss   | -33.4    |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 472      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3236     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5031     |\n",
      "|    total_timesteps | 3236000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3237000, episode_reward=-40.19 +/- 1.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3237000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3238000, episode_reward=-52.94 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3238000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.404   |\n",
      "|    critic_loss     | 5.64e-06 |\n",
      "|    ent_coef        | 0.000188 |\n",
      "|    ent_coef_loss   | -32.9    |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3239000, episode_reward=-52.43 +/- 1.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3239000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3240000, episode_reward=-53.07 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.403   |\n",
      "|    critic_loss     | 5.49e-06 |\n",
      "|    ent_coef        | 0.000183 |\n",
      "|    ent_coef_loss   | -29      |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 472      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3240     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5038     |\n",
      "|    total_timesteps | 3240000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3241000, episode_reward=-52.59 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3241000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3242000, episode_reward=-60.26 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -60.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3242000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.402   |\n",
      "|    critic_loss     | 5.22e-06 |\n",
      "|    ent_coef        | 0.000179 |\n",
      "|    ent_coef_loss   | -25.9    |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3243000, episode_reward=-60.36 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -60.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3243000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3244000, episode_reward=-60.29 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -60.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3244000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 447      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3244     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5044     |\n",
      "|    total_timesteps | 3244000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3245000, episode_reward=-37.89 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -37.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3245000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.401   |\n",
      "|    critic_loss     | 4.92e-06 |\n",
      "|    ent_coef        | 0.000175 |\n",
      "|    ent_coef_loss   | -23.2    |\n",
      "|    learning_rate   | 0.00176  |\n",
      "|    n_updates       | 15840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3246000, episode_reward=-37.04 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -37      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3246000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3247000, episode_reward=-23.79 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3247000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.399   |\n",
      "|    critic_loss     | 5.13e-06 |\n",
      "|    ent_coef        | 0.000171 |\n",
      "|    ent_coef_loss   | -25      |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3248000, episode_reward=-23.04 +/- 0.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3248000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 399      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3248     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5050     |\n",
      "|    total_timesteps | 3248000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3249000, episode_reward=-31.94 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -31.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3249000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.398   |\n",
      "|    critic_loss     | 6.03e-06 |\n",
      "|    ent_coef        | 0.000168 |\n",
      "|    ent_coef_loss   | -24.4    |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3250000, episode_reward=-31.73 +/- 0.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -31.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3250000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3251000, episode_reward=-42.27 +/- 1.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3251000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.398   |\n",
      "|    critic_loss     | 4.02e-06 |\n",
      "|    ent_coef        | 0.000164 |\n",
      "|    ent_coef_loss   | -22.3    |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3252000, episode_reward=-41.30 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3252000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 349      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3252     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5056     |\n",
      "|    total_timesteps | 3252000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3253000, episode_reward=-42.04 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3253000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.4     |\n",
      "|    critic_loss     | 3.71e-06 |\n",
      "|    ent_coef        | 0.000161 |\n",
      "|    ent_coef_loss   | -20.4    |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3254000, episode_reward=-41.33 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3254000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3255000, episode_reward=-36.54 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3255000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.401   |\n",
      "|    critic_loss     | 2.76e-06 |\n",
      "|    ent_coef        | 0.000159 |\n",
      "|    ent_coef_loss   | -18.8    |\n",
      "|    learning_rate   | 0.00175  |\n",
      "|    n_updates       | 15890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3256000, episode_reward=-36.00 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3256000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 302      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3256     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5063     |\n",
      "|    total_timesteps | 3256000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3257000, episode_reward=-32.57 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3257000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.398   |\n",
      "|    critic_loss     | 3.14e-06 |\n",
      "|    ent_coef        | 0.000156 |\n",
      "|    ent_coef_loss   | -19.6    |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3258000, episode_reward=-32.39 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -32.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3258000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3259000, episode_reward=-28.41 +/- 1.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -28.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3259000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.398   |\n",
      "|    critic_loss     | 3.06e-06 |\n",
      "|    ent_coef        | 0.000153 |\n",
      "|    ent_coef_loss   | -22.4    |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3260000, episode_reward=-28.47 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -28.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3260000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 253      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3260     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5069     |\n",
      "|    total_timesteps | 3260000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3261000, episode_reward=-22.63 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3261000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.398   |\n",
      "|    critic_loss     | 2.96e-06 |\n",
      "|    ent_coef        | 0.000151 |\n",
      "|    ent_coef_loss   | -19.3    |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3262000, episode_reward=-22.76 +/- 1.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3262000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3263000, episode_reward=-23.90 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3263000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.396   |\n",
      "|    critic_loss     | 3.05e-06 |\n",
      "|    ent_coef        | 0.000148 |\n",
      "|    ent_coef_loss   | -19.4    |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3264000, episode_reward=-22.74 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3264000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 202      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3264     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5075     |\n",
      "|    total_timesteps | 3264000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3265000, episode_reward=-20.76 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3265000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.396   |\n",
      "|    critic_loss     | 2.81e-06 |\n",
      "|    ent_coef        | 0.000146 |\n",
      "|    ent_coef_loss   | -17.1    |\n",
      "|    learning_rate   | 0.00174  |\n",
      "|    n_updates       | 15940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3266000, episode_reward=-19.73 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -19.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3266000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3267000, episode_reward=-18.55 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3267000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.397   |\n",
      "|    critic_loss     | 3.41e-06 |\n",
      "|    ent_coef        | 0.000144 |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3268000, episode_reward=-17.58 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -17.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3268000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3268     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5082     |\n",
      "|    total_timesteps | 3268000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3269000, episode_reward=-20.06 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3269000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.395   |\n",
      "|    critic_loss     | 3.43e-06 |\n",
      "|    ent_coef        | 0.000142 |\n",
      "|    ent_coef_loss   | -15.7    |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270000, episode_reward=-19.72 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -19.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3270000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3271000, episode_reward=-22.92 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3271000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.395   |\n",
      "|    critic_loss     | 3.72e-06 |\n",
      "|    ent_coef        | 0.000141 |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3272000, episode_reward=-22.91 +/- 1.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3272000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3272     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5088     |\n",
      "|    total_timesteps | 3272000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3273000, episode_reward=-20.60 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3273000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.396   |\n",
      "|    critic_loss     | 3.18e-06 |\n",
      "|    ent_coef        | 0.000139 |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3274000, episode_reward=-20.03 +/- 1.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3274000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3275000, episode_reward=-26.86 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3275000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.395   |\n",
      "|    critic_loss     | 2.53e-06 |\n",
      "|    ent_coef        | 0.000137 |\n",
      "|    ent_coef_loss   | -17.4    |\n",
      "|    learning_rate   | 0.00173  |\n",
      "|    n_updates       | 15990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3276000, episode_reward=-26.45 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3276000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3276     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5094     |\n",
      "|    total_timesteps | 3276000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3277000, episode_reward=-20.53 +/- 0.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3277000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.394   |\n",
      "|    critic_loss     | 3.48e-06 |\n",
      "|    ent_coef        | 0.000136 |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3278000, episode_reward=-20.64 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3278000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3279000, episode_reward=-16.87 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3279000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.392   |\n",
      "|    critic_loss     | 2.79e-06 |\n",
      "|    ent_coef        | 0.000134 |\n",
      "|    ent_coef_loss   | -15.9    |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3280000, episode_reward=-17.63 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -17.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3280000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 11.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3280     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5100     |\n",
      "|    total_timesteps | 3280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3281000, episode_reward=-17.68 +/- 1.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -17.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3281000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.393   |\n",
      "|    critic_loss     | 2.19e-06 |\n",
      "|    ent_coef        | 0.000132 |\n",
      "|    ent_coef_loss   | -15.6    |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3282000, episode_reward=-17.85 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -17.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3282000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3283000, episode_reward=-16.42 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3283000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.39    |\n",
      "|    critic_loss     | 2.35e-06 |\n",
      "|    ent_coef        | 0.000131 |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3284000, episode_reward=-16.21 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -16.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3284000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -33.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3284     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5107     |\n",
      "|    total_timesteps | 3284000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3285000, episode_reward=-18.89 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3285000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.391   |\n",
      "|    critic_loss     | 2.74e-06 |\n",
      "|    ent_coef        | 0.000129 |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00172  |\n",
      "|    n_updates       | 16040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3286000, episode_reward=-19.00 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -19      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3286000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3287000, episode_reward=-19.24 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -19.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3287000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3288000, episode_reward=-18.11 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3288000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.392   |\n",
      "|    critic_loss     | 3.72e-06 |\n",
      "|    ent_coef        | 0.000128 |\n",
      "|    ent_coef_loss   | -16.3    |\n",
      "|    learning_rate   | 0.00171  |\n",
      "|    n_updates       | 16050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -63.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3288     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5113     |\n",
      "|    total_timesteps | 3288000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3289000, episode_reward=-18.13 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3289000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3290000, episode_reward=-18.19 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.392   |\n",
      "|    critic_loss     | 4.59e-06 |\n",
      "|    ent_coef        | 0.000126 |\n",
      "|    ent_coef_loss   | -15.9    |\n",
      "|    learning_rate   | 0.00171  |\n",
      "|    n_updates       | 16060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3291000, episode_reward=-18.03 +/- 0.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3291000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3292000, episode_reward=-25.06 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -25.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3292000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.392   |\n",
      "|    critic_loss     | 4.18e-06 |\n",
      "|    ent_coef        | 0.000124 |\n",
      "|    ent_coef_loss   | -16.9    |\n",
      "|    learning_rate   | 0.00171  |\n",
      "|    n_updates       | 16070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -71.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3292     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5119     |\n",
      "|    total_timesteps | 3292000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3293000, episode_reward=-25.78 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -25.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3293000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3294000, episode_reward=-25.15 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -25.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3294000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.392   |\n",
      "|    critic_loss     | 2.98e-06 |\n",
      "|    ent_coef        | 0.000123 |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00171  |\n",
      "|    n_updates       | 16080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3295000, episode_reward=-25.69 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -25.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3295000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3296000, episode_reward=-26.03 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -26      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3296000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.392   |\n",
      "|    critic_loss     | 2.63e-06 |\n",
      "|    ent_coef        | 0.000121 |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16090    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -65.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3296     |\n",
      "|    fps             | 643      |\n",
      "|    time_elapsed    | 5125     |\n",
      "|    total_timesteps | 3296000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3297000, episode_reward=-27.21 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -27.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3297000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3298000, episode_reward=-21.76 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3298000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.393   |\n",
      "|    critic_loss     | 2.5e-06  |\n",
      "|    ent_coef        | 0.00012  |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3299000, episode_reward=-21.21 +/- 0.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3299000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300000, episode_reward=-21.53 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.391   |\n",
      "|    critic_loss     | 3.48e-06 |\n",
      "|    ent_coef        | 0.000119 |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16110    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -58.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3300     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5132     |\n",
      "|    total_timesteps | 3300000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3301000, episode_reward=-22.30 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3301000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3302000, episode_reward=-20.96 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3302000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.391   |\n",
      "|    critic_loss     | 3.93e-06 |\n",
      "|    ent_coef        | 0.000117 |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3303000, episode_reward=-20.89 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3303000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3304000, episode_reward=-20.83 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3304000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.392   |\n",
      "|    critic_loss     | 3.16e-06 |\n",
      "|    ent_coef        | 0.000116 |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.0017   |\n",
      "|    n_updates       | 16130    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -52.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3304     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5138     |\n",
      "|    total_timesteps | 3304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3305000, episode_reward=-21.00 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -21      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3305000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3306000, episode_reward=-18.58 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3306000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.388   |\n",
      "|    critic_loss     | 2.48e-06 |\n",
      "|    ent_coef        | 0.000114 |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3307000, episode_reward=-18.74 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3307000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3308000, episode_reward=-17.87 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -17.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3308000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.388   |\n",
      "|    critic_loss     | 2.79e-06 |\n",
      "|    ent_coef        | 0.000113 |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16150    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -47.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3308     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5144     |\n",
      "|    total_timesteps | 3308000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3309000, episode_reward=-18.30 +/- 0.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -18.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3309000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3310000, episode_reward=-19.69 +/- 2.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -19.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.386   |\n",
      "|    critic_loss     | 2.86e-06 |\n",
      "|    ent_coef        | 0.000112 |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3311000, episode_reward=-20.41 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -20.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3311000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3312000, episode_reward=-15.68 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -15.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3312000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.387   |\n",
      "|    critic_loss     | 3.22e-06 |\n",
      "|    ent_coef        | 0.000111 |\n",
      "|    ent_coef_loss   | -8.84    |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16170    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -44.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3312     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5151     |\n",
      "|    total_timesteps | 3312000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3313000, episode_reward=-15.41 +/- 0.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -15.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3313000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3314000, episode_reward=-12.78 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3314000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.385   |\n",
      "|    critic_loss     | 3.01e-06 |\n",
      "|    ent_coef        | 0.00011  |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.00169  |\n",
      "|    n_updates       | 16180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3315000, episode_reward=-13.86 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -13.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3315000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3316000, episode_reward=-13.35 +/- 1.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -13.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3316000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.385   |\n",
      "|    critic_loss     | 2.96e-06 |\n",
      "|    ent_coef        | 0.000109 |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16190    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -40.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3316     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5157     |\n",
      "|    total_timesteps | 3316000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3317000, episode_reward=-11.70 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3317000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3318000, episode_reward=-39.18 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -39.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3318000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.382   |\n",
      "|    critic_loss     | 4.92e-06 |\n",
      "|    ent_coef        | 0.000108 |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3319000, episode_reward=-41.11 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3319000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3320000, episode_reward=1247.03 +/- 31.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.378   |\n",
      "|    critic_loss     | 5.41e-06 |\n",
      "|    ent_coef        | 0.000107 |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -36.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3320     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5163     |\n",
      "|    total_timesteps | 3320000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3321000, episode_reward=1186.39 +/- 42.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3321000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3322000, episode_reward=1282.15 +/- 38.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3322000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.375   |\n",
      "|    critic_loss     | 8.02e-06 |\n",
      "|    ent_coef        | 0.000106 |\n",
      "|    ent_coef_loss   | -18.7    |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3323000, episode_reward=1294.18 +/- 52.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3323000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3324000, episode_reward=1341.11 +/- 56.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3324000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.673   |\n",
      "|    critic_loss     | 0.00131  |\n",
      "|    ent_coef        | 0.000105 |\n",
      "|    ent_coef_loss   | 27.8     |\n",
      "|    learning_rate   | 0.00168  |\n",
      "|    n_updates       | 16230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -5.92    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3324     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5169     |\n",
      "|    total_timesteps | 3324000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3325000, episode_reward=1326.46 +/- 54.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3325000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3326000, episode_reward=1332.78 +/- 30.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3326000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.673   |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.000106 |\n",
      "|    ent_coef_loss   | 42.4     |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3327000, episode_reward=1323.42 +/- 51.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3327000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3328000, episode_reward=1353.85 +/- 38.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3328000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 49.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3328     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5175     |\n",
      "|    total_timesteps | 3328000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3329000, episode_reward=1356.13 +/- 14.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3329000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.679   |\n",
      "|    critic_loss     | 0.00139  |\n",
      "|    ent_coef        | 0.000109 |\n",
      "|    ent_coef_loss   | 58.8     |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3330000, episode_reward=1354.76 +/- 22.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3330000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3331000, episode_reward=-97.43 +/- 2.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3331000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.668   |\n",
      "|    critic_loss     | 0.00114  |\n",
      "|    ent_coef        | 0.000113 |\n",
      "|    ent_coef_loss   | 52.1     |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3332000, episode_reward=-96.47 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -96.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3332000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 77.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3332     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5182     |\n",
      "|    total_timesteps | 3332000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3333000, episode_reward=-152.89 +/- 1.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3333000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.373   |\n",
      "|    critic_loss     | 6.32e-05 |\n",
      "|    ent_coef        | 0.000117 |\n",
      "|    ent_coef_loss   | -5.15    |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3334000, episode_reward=-152.04 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3334000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3335000, episode_reward=-244.26 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3335000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.389   |\n",
      "|    critic_loss     | 6.87e-05 |\n",
      "|    ent_coef        | 0.000119 |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.00167  |\n",
      "|    n_updates       | 16280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3336000, episode_reward=-243.54 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -244     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3336000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 72.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3336     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5188     |\n",
      "|    total_timesteps | 3336000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3337000, episode_reward=-232.80 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3337000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.392   |\n",
      "|    critic_loss     | 1.98e-05 |\n",
      "|    ent_coef        | 0.000119 |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3338000, episode_reward=-232.98 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3338000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3339000, episode_reward=-284.59 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -285     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3339000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.394   |\n",
      "|    critic_loss     | 9.1e-06  |\n",
      "|    ent_coef        | 0.00012  |\n",
      "|    ent_coef_loss   | 0.717    |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3340000, episode_reward=-284.35 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -284     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3340000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 64.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3340     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5194     |\n",
      "|    total_timesteps | 3340000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3341000, episode_reward=-266.61 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3341000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.391   |\n",
      "|    critic_loss     | 6.25e-06 |\n",
      "|    ent_coef        | 0.00012  |\n",
      "|    ent_coef_loss   | -0.254   |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3342000, episode_reward=-266.81 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -267     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3342000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3343000, episode_reward=-147.01 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3343000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.388   |\n",
      "|    critic_loss     | 8.14e-06 |\n",
      "|    ent_coef        | 0.00012  |\n",
      "|    ent_coef_loss   | -19.9    |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3344000, episode_reward=-147.23 +/- 0.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3344000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 58.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3344     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5201     |\n",
      "|    total_timesteps | 3344000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3345000, episode_reward=-140.24 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -140     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3345000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.38    |\n",
      "|    critic_loss     | 3.97e-05 |\n",
      "|    ent_coef        | 0.000119 |\n",
      "|    ent_coef_loss   | -49.7    |\n",
      "|    learning_rate   | 0.00166  |\n",
      "|    n_updates       | 16330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3346000, episode_reward=-141.09 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3346000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3347000, episode_reward=-259.92 +/- 0.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -260     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3347000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.461   |\n",
      "|    critic_loss     | 0.000787 |\n",
      "|    ent_coef        | 0.000116 |\n",
      "|    ent_coef_loss   | -17.4    |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3348000, episode_reward=-258.86 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -259     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3348000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 65.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3348     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5207     |\n",
      "|    total_timesteps | 3348000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3349000, episode_reward=-501.18 +/- 1.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -501     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3349000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.387   |\n",
      "|    critic_loss     | 8.87e-05 |\n",
      "|    ent_coef        | 0.000114 |\n",
      "|    ent_coef_loss   | 29.6     |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3350000, episode_reward=-503.10 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -503     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3350000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3351000, episode_reward=-569.65 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -570     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3351000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.378   |\n",
      "|    critic_loss     | 9.31e-05 |\n",
      "|    ent_coef        | 0.000115 |\n",
      "|    ent_coef_loss   | 121      |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3352000, episode_reward=-569.61 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -570     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3352000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 48.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3352     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5213     |\n",
      "|    total_timesteps | 3352000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3353000, episode_reward=-532.64 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -533     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3353000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.38    |\n",
      "|    critic_loss     | 1.5e-05  |\n",
      "|    ent_coef        | 0.000122 |\n",
      "|    ent_coef_loss   | 80.6     |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3354000, episode_reward=-532.52 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -533     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3354000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3355000, episode_reward=-515.50 +/- 0.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -515     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3355000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.408   |\n",
      "|    critic_loss     | 0.000307 |\n",
      "|    ent_coef        | 0.000128 |\n",
      "|    ent_coef_loss   | 48.3     |\n",
      "|    learning_rate   | 0.00165  |\n",
      "|    n_updates       | 16380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3356000, episode_reward=-516.01 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -516     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3356000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3356     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5219     |\n",
      "|    total_timesteps | 3356000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3357000, episode_reward=-539.94 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -540     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3357000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.48    |\n",
      "|    critic_loss     | 0.000659 |\n",
      "|    ent_coef        | 0.000133 |\n",
      "|    ent_coef_loss   | 115      |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3358000, episode_reward=-540.53 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -541     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3358000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3359000, episode_reward=-583.58 +/- 0.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -584     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3359000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.562   |\n",
      "|    critic_loss     | 0.000128 |\n",
      "|    ent_coef        | 0.000141 |\n",
      "|    ent_coef_loss   | 188      |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=-582.93 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -583     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3360000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.7      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3360     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5226     |\n",
      "|    total_timesteps | 3360000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3361000, episode_reward=-576.41 +/- 1.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -576     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3361000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.552   |\n",
      "|    critic_loss     | 6.83e-05 |\n",
      "|    ent_coef        | 0.000151 |\n",
      "|    ent_coef_loss   | 126      |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3362000, episode_reward=-576.88 +/- 0.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -577     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3362000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3363000, episode_reward=-563.32 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -563     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3363000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.555   |\n",
      "|    critic_loss     | 2.92e-05 |\n",
      "|    ent_coef        | 0.000161 |\n",
      "|    ent_coef_loss   | 103      |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3364000, episode_reward=-564.41 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -564     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3364000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -14.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3364     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5232     |\n",
      "|    total_timesteps | 3364000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3365000, episode_reward=-567.69 +/- 0.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -568     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3365000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.558   |\n",
      "|    critic_loss     | 3.04e-05 |\n",
      "|    ent_coef        | 0.000169 |\n",
      "|    ent_coef_loss   | 79.7     |\n",
      "|    learning_rate   | 0.00164  |\n",
      "|    n_updates       | 16430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3366000, episode_reward=-568.36 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -568     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3366000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3367000, episode_reward=-533.11 +/- 1.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -533     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3367000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.556   |\n",
      "|    critic_loss     | 5.87e-05 |\n",
      "|    ent_coef        | 0.000176 |\n",
      "|    ent_coef_loss   | 74.9     |\n",
      "|    learning_rate   | 0.00163  |\n",
      "|    n_updates       | 16440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3368000, episode_reward=-532.51 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -533     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3368000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -35.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3368     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5238     |\n",
      "|    total_timesteps | 3368000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3369000, episode_reward=-480.92 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -481     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3369000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.556   |\n",
      "|    critic_loss     | 7.36e-05 |\n",
      "|    ent_coef        | 0.000182 |\n",
      "|    ent_coef_loss   | 55.5     |\n",
      "|    learning_rate   | 0.00163  |\n",
      "|    n_updates       | 16450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3370000, episode_reward=-480.81 +/- 0.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -481     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3370000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3371000, episode_reward=-480.30 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -480     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3371000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3372000, episode_reward=-522.95 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -523     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3372000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.542   |\n",
      "|    critic_loss     | 8.65e-05 |\n",
      "|    ent_coef        | 0.000187 |\n",
      "|    ent_coef_loss   | 60       |\n",
      "|    learning_rate   | 0.00163  |\n",
      "|    n_updates       | 16460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -53.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3372     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5245     |\n",
      "|    total_timesteps | 3372000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3373000, episode_reward=-521.84 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -522     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3373000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3374000, episode_reward=-557.74 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -558     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3374000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.545   |\n",
      "|    critic_loss     | 2.62e-05 |\n",
      "|    ent_coef        | 0.000192 |\n",
      "|    ent_coef_loss   | 104      |\n",
      "|    learning_rate   | 0.00163  |\n",
      "|    n_updates       | 16470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3375000, episode_reward=-557.45 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -557     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3375000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3376000, episode_reward=-533.60 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -534     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3376000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.556   |\n",
      "|    critic_loss     | 3.23e-05 |\n",
      "|    ent_coef        | 0.000199 |\n",
      "|    ent_coef_loss   | 65.9     |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -74.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3376     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5251     |\n",
      "|    total_timesteps | 3376000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3377000, episode_reward=-533.31 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -533     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3377000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3378000, episode_reward=-513.07 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -513     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3378000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.558   |\n",
      "|    critic_loss     | 3.43e-05 |\n",
      "|    ent_coef        | 0.000205 |\n",
      "|    ent_coef_loss   | 55       |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3379000, episode_reward=-513.40 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -513     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3379000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3380000, episode_reward=-220.04 +/- 1.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -220     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.558   |\n",
      "|    critic_loss     | 6e-05    |\n",
      "|    ent_coef        | 0.00021  |\n",
      "|    ent_coef_loss   | 37.9     |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -89.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3380     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5257     |\n",
      "|    total_timesteps | 3380000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3381000, episode_reward=-219.85 +/- 1.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -220     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3381000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3382000, episode_reward=-422.60 +/- 4.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -423     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3382000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.565   |\n",
      "|    critic_loss     | 0.000386 |\n",
      "|    ent_coef        | 0.000214 |\n",
      "|    ent_coef_loss   | 48       |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3383000, episode_reward=-423.73 +/- 4.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -424     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3383000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3384000, episode_reward=40.65 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 40.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3384000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.568   |\n",
      "|    critic_loss     | 0.000751 |\n",
      "|    ent_coef        | 0.000218 |\n",
      "|    ent_coef_loss   | 71.3     |\n",
      "|    learning_rate   | 0.00162  |\n",
      "|    n_updates       | 16520    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -94.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3384     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5264     |\n",
      "|    total_timesteps | 3384000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3385000, episode_reward=42.01 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 42       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3385000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3386000, episode_reward=-71.16 +/- 3.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -71.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3386000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.59    |\n",
      "|    critic_loss     | 0.000979 |\n",
      "|    ent_coef        | 0.000224 |\n",
      "|    ent_coef_loss   | 60.8     |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3387000, episode_reward=-68.06 +/- 5.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -68.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3387000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3388000, episode_reward=-76.28 +/- 12.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -76.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3388000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.573   |\n",
      "|    critic_loss     | 0.0013   |\n",
      "|    ent_coef        | 0.00023  |\n",
      "|    ent_coef_loss   | 62.7     |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16540    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -87.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3388     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5270     |\n",
      "|    total_timesteps | 3388000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3389000, episode_reward=-76.24 +/- 3.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -76.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3389000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3390000, episode_reward=591.27 +/- 10.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 591      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.575   |\n",
      "|    critic_loss     | 0.00129  |\n",
      "|    ent_coef        | 0.000235 |\n",
      "|    ent_coef_loss   | 54.5     |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3391000, episode_reward=589.35 +/- 2.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 589      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3391000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3392000, episode_reward=584.89 +/- 9.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 585      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3392000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.562   |\n",
      "|    critic_loss     | 0.00149  |\n",
      "|    ent_coef        | 0.000241 |\n",
      "|    ent_coef_loss   | 53.4     |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16560    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -73.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3392     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5276     |\n",
      "|    total_timesteps | 3392000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3393000, episode_reward=575.81 +/- 8.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 576      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3393000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3394000, episode_reward=147.61 +/- 39.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 148      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3394000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.582   |\n",
      "|    critic_loss     | 0.000965 |\n",
      "|    ent_coef        | 0.000246 |\n",
      "|    ent_coef_loss   | 47.1     |\n",
      "|    learning_rate   | 0.00161  |\n",
      "|    n_updates       | 16570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3395000, episode_reward=113.81 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 114      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3395000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3396000, episode_reward=-349.35 +/- 2.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -349     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3396000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.579   |\n",
      "|    critic_loss     | 0.000652 |\n",
      "|    ent_coef        | 0.000251 |\n",
      "|    ent_coef_loss   | 60.3     |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16580    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -63.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3396     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5282     |\n",
      "|    total_timesteps | 3396000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3397000, episode_reward=-331.47 +/- 23.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -331     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3397000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3398000, episode_reward=-174.88 +/- 12.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -175     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3398000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.564   |\n",
      "|    critic_loss     | 0.000703 |\n",
      "|    ent_coef        | 0.000257 |\n",
      "|    ent_coef_loss   | 50.8     |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3399000, episode_reward=-168.61 +/- 10.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -169     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3399000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3400000, episode_reward=16.99 +/- 3.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 17       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.567   |\n",
      "|    critic_loss     | 0.000692 |\n",
      "|    ent_coef        | 0.000262 |\n",
      "|    ent_coef_loss   | 40.3     |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16600    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -70.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3400     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5289     |\n",
      "|    total_timesteps | 3400000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3401000, episode_reward=17.08 +/- 4.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 17.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3401000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3402000, episode_reward=-71.92 +/- 3.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -71.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3402000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.575   |\n",
      "|    critic_loss     | 0.00065  |\n",
      "|    ent_coef        | 0.000267 |\n",
      "|    ent_coef_loss   | 47       |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3403000, episode_reward=-68.28 +/- 5.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -68.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3403000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3404000, episode_reward=-37.97 +/- 2.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -38      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3404000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.572   |\n",
      "|    critic_loss     | 0.000587 |\n",
      "|    ent_coef        | 0.000271 |\n",
      "|    ent_coef_loss   | 49.5     |\n",
      "|    learning_rate   | 0.0016   |\n",
      "|    n_updates       | 16620    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -67.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3404     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5295     |\n",
      "|    total_timesteps | 3404000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3405000, episode_reward=-40.75 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3405000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3406000, episode_reward=23.29 +/- 1.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 23.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3406000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.573   |\n",
      "|    critic_loss     | 0.000494 |\n",
      "|    ent_coef        | 0.000277 |\n",
      "|    ent_coef_loss   | 42.3     |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3407000, episode_reward=24.67 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 24.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3407000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3408000, episode_reward=-129.20 +/- 3.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -129     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3408000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.573   |\n",
      "|    critic_loss     | 0.000398 |\n",
      "|    ent_coef        | 0.000281 |\n",
      "|    ent_coef_loss   | 39.5     |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -64.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3408     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5302     |\n",
      "|    total_timesteps | 3408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3409000, episode_reward=-126.87 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3409000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3410000, episode_reward=-57.67 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -57.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.571   |\n",
      "|    critic_loss     | 0.000382 |\n",
      "|    ent_coef        | 0.000286 |\n",
      "|    ent_coef_loss   | 37.6     |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3411000, episode_reward=-57.40 +/- 2.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -57.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3411000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3412000, episode_reward=-40.91 +/- 2.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3412000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.569   |\n",
      "|    critic_loss     | 0.000356 |\n",
      "|    ent_coef        | 0.000291 |\n",
      "|    ent_coef_loss   | 36.6     |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -66.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3412     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5308     |\n",
      "|    total_timesteps | 3412000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3413000, episode_reward=-39.59 +/- 2.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -39.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3413000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3414000, episode_reward=-40.57 +/- 2.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -40.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3414000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3415000, episode_reward=-52.45 +/- 4.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3415000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.571   |\n",
      "|    critic_loss     | 0.000327 |\n",
      "|    ent_coef        | 0.000295 |\n",
      "|    ent_coef_loss   | 33.3     |\n",
      "|    learning_rate   | 0.00159  |\n",
      "|    n_updates       | 16670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3416000, episode_reward=-48.76 +/- 4.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -48.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3416000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -65.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3416     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5314     |\n",
      "|    total_timesteps | 3416000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3417000, episode_reward=2.51 +/- 1.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.51     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3417000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.57    |\n",
      "|    critic_loss     | 0.000319 |\n",
      "|    ent_coef        | 0.000299 |\n",
      "|    ent_coef_loss   | 33.7     |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3418000, episode_reward=5.55 +/- 5.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.55     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3418000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3419000, episode_reward=-31.42 +/- 4.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -31.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3419000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.568   |\n",
      "|    critic_loss     | 0.000263 |\n",
      "|    ent_coef        | 0.000303 |\n",
      "|    ent_coef_loss   | 30       |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3420000, episode_reward=-37.26 +/- 2.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -37.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3420000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -67.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3420     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5320     |\n",
      "|    total_timesteps | 3420000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3421000, episode_reward=-255.19 +/- 4.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3421000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.564   |\n",
      "|    critic_loss     | 0.000194 |\n",
      "|    ent_coef        | 0.000307 |\n",
      "|    ent_coef_loss   | 27.1     |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3422000, episode_reward=-255.23 +/- 6.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3422000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3423000, episode_reward=-426.70 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -427     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3423000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.559   |\n",
      "|    critic_loss     | 0.00017  |\n",
      "|    ent_coef        | 0.00031  |\n",
      "|    ent_coef_loss   | 25.3     |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3424000, episode_reward=-424.44 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -424     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3424000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -105     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3424     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5327     |\n",
      "|    total_timesteps | 3424000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3425000, episode_reward=-400.19 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -400     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3425000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.552   |\n",
      "|    critic_loss     | 0.000113 |\n",
      "|    ent_coef        | 0.000314 |\n",
      "|    ent_coef_loss   | 26.9     |\n",
      "|    learning_rate   | 0.00158  |\n",
      "|    n_updates       | 16720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3426000, episode_reward=-401.09 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -401     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3426000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3427000, episode_reward=-400.11 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -400     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3427000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.552   |\n",
      "|    critic_loss     | 8.31e-05 |\n",
      "|    ent_coef        | 0.000317 |\n",
      "|    ent_coef_loss   | 20       |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3428000, episode_reward=-399.63 +/- 0.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -400     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3428000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -170     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3428     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5333     |\n",
      "|    total_timesteps | 3428000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3429000, episode_reward=-228.85 +/- 13.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -229     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3429000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.553   |\n",
      "|    critic_loss     | 0.000107 |\n",
      "|    ent_coef        | 0.00032  |\n",
      "|    ent_coef_loss   | 16.4     |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3430000, episode_reward=-234.08 +/- 7.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3430000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3431000, episode_reward=-299.76 +/- 2.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -300     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3431000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.553   |\n",
      "|    critic_loss     | 0.000122 |\n",
      "|    ent_coef        | 0.000322 |\n",
      "|    ent_coef_loss   | 13.6     |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3432000, episode_reward=-296.48 +/- 7.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -296     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3432000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -203     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3432     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5340     |\n",
      "|    total_timesteps | 3432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3433000, episode_reward=-123.32 +/- 5.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3433000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.556   |\n",
      "|    critic_loss     | 0.000141 |\n",
      "|    ent_coef        | 0.000324 |\n",
      "|    ent_coef_loss   | 13.2     |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3434000, episode_reward=-118.15 +/- 4.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3434000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3435000, episode_reward=-309.77 +/- 7.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -310     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3435000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.554   |\n",
      "|    critic_loss     | 9.09e-05 |\n",
      "|    ent_coef        | 0.000326 |\n",
      "|    ent_coef_loss   | 19.3     |\n",
      "|    learning_rate   | 0.00157  |\n",
      "|    n_updates       | 16770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3436000, episode_reward=-301.84 +/- 4.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -302     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3436000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -204     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3436     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5346     |\n",
      "|    total_timesteps | 3436000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3437000, episode_reward=-372.97 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3437000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.551   |\n",
      "|    critic_loss     | 5.56e-05 |\n",
      "|    ent_coef        | 0.000328 |\n",
      "|    ent_coef_loss   | 12.8     |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3438000, episode_reward=-373.37 +/- 1.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -373     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3438000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3439000, episode_reward=-196.83 +/- 3.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3439000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.551   |\n",
      "|    critic_loss     | 5.62e-05 |\n",
      "|    ent_coef        | 0.00033  |\n",
      "|    ent_coef_loss   | 12.2     |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3440000, episode_reward=-196.53 +/- 3.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3440000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -202     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3440     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5352     |\n",
      "|    total_timesteps | 3440000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3441000, episode_reward=-116.67 +/- 4.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3441000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.551   |\n",
      "|    critic_loss     | 6.67e-05 |\n",
      "|    ent_coef        | 0.000332 |\n",
      "|    ent_coef_loss   | 16       |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3442000, episode_reward=-115.90 +/- 2.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3442000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3443000, episode_reward=-126.16 +/- 2.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3443000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.552   |\n",
      "|    critic_loss     | 6.68e-05 |\n",
      "|    ent_coef        | 0.000334 |\n",
      "|    ent_coef_loss   | 8.24     |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3444000, episode_reward=-126.81 +/- 2.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -127     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3444000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -202     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3444     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5359     |\n",
      "|    total_timesteps | 3444000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3445000, episode_reward=-294.00 +/- 3.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -294     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3445000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.549   |\n",
      "|    critic_loss     | 4.62e-05 |\n",
      "|    ent_coef        | 0.000335 |\n",
      "|    ent_coef_loss   | 13.8     |\n",
      "|    learning_rate   | 0.00156  |\n",
      "|    n_updates       | 16820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3446000, episode_reward=-315.41 +/- 44.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3446000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3447000, episode_reward=-237.63 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3447000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.547   |\n",
      "|    critic_loss     | 3.86e-05 |\n",
      "|    ent_coef        | 0.000337 |\n",
      "|    ent_coef_loss   | 7.39     |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3448000, episode_reward=-237.94 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -238     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3448000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3448     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5365     |\n",
      "|    total_timesteps | 3448000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3449000, episode_reward=-370.00 +/- 2.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3449000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.548   |\n",
      "|    critic_loss     | 3.78e-05 |\n",
      "|    ent_coef        | 0.000338 |\n",
      "|    ent_coef_loss   | 8.35     |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3450000, episode_reward=-370.69 +/- 2.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3450000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3451000, episode_reward=-381.67 +/- 14.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -382     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3451000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.542   |\n",
      "|    critic_loss     | 3.62e-05 |\n",
      "|    ent_coef        | 0.00034  |\n",
      "|    ent_coef_loss   | 12       |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3452000, episode_reward=-404.87 +/- 4.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -405     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3452000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3452     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5372     |\n",
      "|    total_timesteps | 3452000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3453000, episode_reward=-370.50 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3453000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.544   |\n",
      "|    critic_loss     | 3.41e-05 |\n",
      "|    ent_coef        | 0.000341 |\n",
      "|    ent_coef_loss   | 0.403    |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3454000, episode_reward=-371.81 +/- 1.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -372     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3454000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3455000, episode_reward=-367.82 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -368     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3455000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.543   |\n",
      "|    critic_loss     | 3.37e-05 |\n",
      "|    ent_coef        | 0.000342 |\n",
      "|    ent_coef_loss   | 2.56     |\n",
      "|    learning_rate   | 0.00155  |\n",
      "|    n_updates       | 16870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3456000, episode_reward=-369.51 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3456000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3456     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5378     |\n",
      "|    total_timesteps | 3456000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3457000, episode_reward=-367.82 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -368     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3457000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3458000, episode_reward=-370.65 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -371     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3458000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.542   |\n",
      "|    critic_loss     | 2.88e-05 |\n",
      "|    ent_coef        | 0.000342 |\n",
      "|    ent_coef_loss   | 0.853    |\n",
      "|    learning_rate   | 0.00154  |\n",
      "|    n_updates       | 16880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3459000, episode_reward=-370.29 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3459000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3460000, episode_reward=-372.41 +/- 1.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -372     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.542   |\n",
      "|    critic_loss     | 2.71e-05 |\n",
      "|    ent_coef        | 0.000342 |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.00154  |\n",
      "|    n_updates       | 16890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3460     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5385     |\n",
      "|    total_timesteps | 3460000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3461000, episode_reward=-372.02 +/- 1.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -372     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3461000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3462000, episode_reward=-374.60 +/- 1.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -375     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3462000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.54    |\n",
      "|    critic_loss     | 2.73e-05 |\n",
      "|    ent_coef        | 0.000342 |\n",
      "|    ent_coef_loss   | -2.44    |\n",
      "|    learning_rate   | 0.00154  |\n",
      "|    n_updates       | 16900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3463000, episode_reward=-374.67 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -375     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3463000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3464000, episode_reward=-370.23 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -370     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3464000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.541   |\n",
      "|    critic_loss     | 2.39e-05 |\n",
      "|    ent_coef        | 0.000342 |\n",
      "|    ent_coef_loss   | -1.38    |\n",
      "|    learning_rate   | 0.00154  |\n",
      "|    n_updates       | 16910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -178     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3464     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5391     |\n",
      "|    total_timesteps | 3464000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3465000, episode_reward=-369.11 +/- 0.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -369     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3465000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3466000, episode_reward=-353.74 +/- 3.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -354     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3466000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.54    |\n",
      "|    critic_loss     | 2.23e-05 |\n",
      "|    ent_coef        | 0.000342 |\n",
      "|    ent_coef_loss   | -6.66    |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3467000, episode_reward=-352.66 +/- 3.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3467000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3468000, episode_reward=-344.18 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -344     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3468000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.54    |\n",
      "|    critic_loss     | 2e-05    |\n",
      "|    ent_coef        | 0.000341 |\n",
      "|    ent_coef_loss   | -6.22    |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16930    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -168     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3468     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5398     |\n",
      "|    total_timesteps | 3468000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3469000, episode_reward=-344.64 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -345     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3469000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3470000, episode_reward=-347.03 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -347     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 1.9e-05  |\n",
      "|    ent_coef        | 0.00034  |\n",
      "|    ent_coef_loss   | -7.09    |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3471000, episode_reward=-348.03 +/- 1.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -348     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3471000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3472000, episode_reward=-350.30 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3472000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 1.71e-05 |\n",
      "|    ent_coef        | 0.000339 |\n",
      "|    ent_coef_loss   | -9.49    |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16950    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -162     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3472     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5404     |\n",
      "|    total_timesteps | 3472000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3473000, episode_reward=-350.13 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -350     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3473000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3474000, episode_reward=-354.68 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -355     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3474000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 1.71e-05 |\n",
      "|    ent_coef        | 0.000338 |\n",
      "|    ent_coef_loss   | -9.56    |\n",
      "|    learning_rate   | 0.00153  |\n",
      "|    n_updates       | 16960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3475000, episode_reward=-355.00 +/- 1.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -355     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3475000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3476000, episode_reward=-352.19 +/- 0.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -352     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3476000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 1.77e-05 |\n",
      "|    ent_coef        | 0.000337 |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 16970    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3476     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5410     |\n",
      "|    total_timesteps | 3476000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3477000, episode_reward=-354.26 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -354     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3477000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3478000, episode_reward=-335.63 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3478000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 1.67e-05 |\n",
      "|    ent_coef        | 0.000335 |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 16980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3479000, episode_reward=-336.35 +/- 1.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -336     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3479000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480000, episode_reward=-340.39 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -340     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 1.95e-05 |\n",
      "|    ent_coef        | 0.000333 |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 16990    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -150     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3480     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5417     |\n",
      "|    total_timesteps | 3480000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3481000, episode_reward=-341.86 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -342     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3481000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3482000, episode_reward=-343.41 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -343     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3482000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 1.71e-05 |\n",
      "|    ent_coef        | 0.000331 |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 17000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3483000, episode_reward=-342.72 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -343     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3483000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3484000, episode_reward=-297.67 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -298     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3484000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 2e-05    |\n",
      "|    ent_coef        | 0.000329 |\n",
      "|    ent_coef_loss   | -14.9    |\n",
      "|    learning_rate   | 0.00152  |\n",
      "|    n_updates       | 17010    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -156     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3484     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5423     |\n",
      "|    total_timesteps | 3484000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3485000, episode_reward=-296.95 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -297     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3485000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3486000, episode_reward=-315.13 +/- 8.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3486000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 2.19e-05 |\n",
      "|    ent_coef        | 0.000327 |\n",
      "|    ent_coef_loss   | -17.7    |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3487000, episode_reward=-315.10 +/- 8.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3487000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3488000, episode_reward=-295.60 +/- 12.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -296     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3488000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 2.28e-05 |\n",
      "|    ent_coef        | 0.000325 |\n",
      "|    ent_coef_loss   | -17.4    |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17030    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -173     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3488     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5430     |\n",
      "|    total_timesteps | 3488000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3489000, episode_reward=-296.15 +/- 12.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -296     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3489000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3490000, episode_reward=-307.01 +/- 3.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -307     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 2.37e-05 |\n",
      "|    ent_coef        | 0.000322 |\n",
      "|    ent_coef_loss   | -15.9    |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3491000, episode_reward=-308.49 +/- 3.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -308     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3491000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3492000, episode_reward=-306.47 +/- 3.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -306     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3492000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 2.34e-05 |\n",
      "|    ent_coef        | 0.00032  |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3492     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5436     |\n",
      "|    total_timesteps | 3492000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3493000, episode_reward=-305.82 +/- 3.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -306     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3493000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3494000, episode_reward=-333.70 +/- 6.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -334     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3494000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 2.65e-05 |\n",
      "|    ent_coef        | 0.000318 |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.00151  |\n",
      "|    n_updates       | 17060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3495000, episode_reward=-330.16 +/- 8.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3495000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3496000, episode_reward=-277.41 +/- 2.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -277     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3496000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 2.44e-05 |\n",
      "|    ent_coef        | 0.000316 |\n",
      "|    ent_coef_loss   | -17      |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -217     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3496     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5443     |\n",
      "|    total_timesteps | 3496000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3497000, episode_reward=-277.81 +/- 1.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -278     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3497000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3498000, episode_reward=-317.11 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -317     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3498000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 3.06e-05 |\n",
      "|    ent_coef        | 0.000313 |\n",
      "|    ent_coef_loss   | -15.8    |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3499000, episode_reward=-316.33 +/- 2.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -316     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3499000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500000, episode_reward=-315.10 +/- 4.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -315     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -218     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3500     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5449     |\n",
      "|    total_timesteps | 3500000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3501000, episode_reward=-283.96 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -284     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3501000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 3e-05    |\n",
      "|    ent_coef        | 0.000311 |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3502000, episode_reward=-285.18 +/- 1.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -285     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3502000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3503000, episode_reward=-311.61 +/- 4.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -312     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3503000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 3.09e-05 |\n",
      "|    ent_coef        | 0.000309 |\n",
      "|    ent_coef_loss   | -9.84    |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3504000, episode_reward=-317.50 +/- 2.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -318     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3504000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -229     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3504     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5455     |\n",
      "|    total_timesteps | 3504000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3505000, episode_reward=-225.79 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -226     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3505000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 2.99e-05 |\n",
      "|    ent_coef        | 0.000308 |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.0015   |\n",
      "|    n_updates       | 17110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3506000, episode_reward=-225.43 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -225     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3506000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3507000, episode_reward=-235.47 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -235     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3507000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 2.54e-05 |\n",
      "|    ent_coef        | 0.000306 |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3508000, episode_reward=-233.63 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -234     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3508000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -239     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3508     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5462     |\n",
      "|    total_timesteps | 3508000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3509000, episode_reward=-349.24 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -349     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3509000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 2.74e-05 |\n",
      "|    ent_coef        | 0.000305 |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3510000, episode_reward=-348.72 +/- 2.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -349     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3510000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3511000, episode_reward=-119.90 +/- 1.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3511000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.534   |\n",
      "|    critic_loss     | 2.45e-05 |\n",
      "|    ent_coef        | 0.000303 |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3512000, episode_reward=-121.08 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3512000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -244     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3512     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5468     |\n",
      "|    total_timesteps | 3512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3513000, episode_reward=-233.30 +/- 17.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3513000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.535   |\n",
      "|    critic_loss     | 2.7e-05  |\n",
      "|    ent_coef        | 0.000301 |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3514000, episode_reward=-255.02 +/- 24.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3514000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3515000, episode_reward=-152.69 +/- 2.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -153     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3515000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 3.05e-05 |\n",
      "|    ent_coef        | 0.0003   |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.00149  |\n",
      "|    n_updates       | 17160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3516000, episode_reward=-155.74 +/- 3.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -156     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3516000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -250     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3516     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5475     |\n",
      "|    total_timesteps | 3516000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3517000, episode_reward=-335.38 +/- 6.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -335     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3517000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 2.78e-05 |\n",
      "|    ent_coef        | 0.000298 |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3518000, episode_reward=-320.71 +/- 19.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -321     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3518000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3519000, episode_reward=-298.94 +/- 5.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -299     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3519000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.534   |\n",
      "|    critic_loss     | 2.64e-05 |\n",
      "|    ent_coef        | 0.000296 |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=-302.12 +/- 6.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -302     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3520000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -255     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3520     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5481     |\n",
      "|    total_timesteps | 3520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3521000, episode_reward=-273.19 +/- 13.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -273     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3521000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.534   |\n",
      "|    critic_loss     | 2.73e-05 |\n",
      "|    ent_coef        | 0.000295 |\n",
      "|    ent_coef_loss   | -16.5    |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3522000, episode_reward=-264.37 +/- 16.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -264     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3522000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3523000, episode_reward=60.21 +/- 3.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 60.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3523000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 3.75e-05 |\n",
      "|    ent_coef        | 0.000293 |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3524000, episode_reward=59.28 +/- 0.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 59.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3524000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3524     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5487     |\n",
      "|    total_timesteps | 3524000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3525000, episode_reward=-128.15 +/- 1.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -128     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3525000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 3.58e-05 |\n",
      "|    ent_coef        | 0.000291 |\n",
      "|    ent_coef_loss   | -9.43    |\n",
      "|    learning_rate   | 0.00148  |\n",
      "|    n_updates       | 17210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3526000, episode_reward=-125.65 +/- 3.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3526000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3527000, episode_reward=-305.54 +/- 15.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -306     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3527000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 2.93e-05 |\n",
      "|    ent_coef        | 0.00029  |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3528000, episode_reward=-305.38 +/- 11.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -305     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3528000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -239     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3528     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5494     |\n",
      "|    total_timesteps | 3528000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3529000, episode_reward=-255.76 +/- 37.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -256     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3529000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.535   |\n",
      "|    critic_loss     | 3.13e-05 |\n",
      "|    ent_coef        | 0.000288 |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3530000, episode_reward=-254.26 +/- 22.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -254     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3530000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3531000, episode_reward=-279.00 +/- 5.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -279     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3531000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.543   |\n",
      "|    critic_loss     | 4.85e-05 |\n",
      "|    ent_coef        | 0.000287 |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3532000, episode_reward=-276.77 +/- 10.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -277     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3532000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3532     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5500     |\n",
      "|    total_timesteps | 3532000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3533000, episode_reward=-270.73 +/- 8.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -271     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3533000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.54    |\n",
      "|    critic_loss     | 3.87e-05 |\n",
      "|    ent_coef        | 0.000285 |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3534000, episode_reward=-273.32 +/- 5.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -273     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3534000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3535000, episode_reward=-258.23 +/- 1.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3535000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.54    |\n",
      "|    critic_loss     | 3.46e-05 |\n",
      "|    ent_coef        | 0.000283 |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.00147  |\n",
      "|    n_updates       | 17260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3536000, episode_reward=-252.97 +/- 6.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -253     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3536000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3536     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5506     |\n",
      "|    total_timesteps | 3536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3537000, episode_reward=-257.56 +/- 3.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3537000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 4.64e-05 |\n",
      "|    ent_coef        | 0.000281 |\n",
      "|    ent_coef_loss   | -18.9    |\n",
      "|    learning_rate   | 0.00146  |\n",
      "|    n_updates       | 17270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3538000, episode_reward=-254.52 +/- 8.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -255     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3538000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3539000, episode_reward=-265.88 +/- 13.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -266     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3539000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 3.76e-05 |\n",
      "|    ent_coef        | 0.000279 |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.00146  |\n",
      "|    n_updates       | 17280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3540000, episode_reward=-271.28 +/- 13.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -271     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3540000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -218     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3540     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5513     |\n",
      "|    total_timesteps | 3540000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3541000, episode_reward=-4.37 +/- 12.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -4.37    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3541000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.534   |\n",
      "|    critic_loss     | 3.43e-05 |\n",
      "|    ent_coef        | 0.000277 |\n",
      "|    ent_coef_loss   | -23.8    |\n",
      "|    learning_rate   | 0.00146  |\n",
      "|    n_updates       | 17290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3542000, episode_reward=-0.45 +/- 15.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -0.451   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3542000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3543000, episode_reward=-11.97 +/- 8.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3543000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3544000, episode_reward=-192.39 +/- 3.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3544000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.546   |\n",
      "|    critic_loss     | 6.74e-05 |\n",
      "|    ent_coef        | 0.000275 |\n",
      "|    ent_coef_loss   | -16.5    |\n",
      "|    learning_rate   | 0.00146  |\n",
      "|    n_updates       | 17300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3544     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5519     |\n",
      "|    total_timesteps | 3544000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3545000, episode_reward=-194.28 +/- 4.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3545000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3546000, episode_reward=-283.29 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -283     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3546000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.531   |\n",
      "|    critic_loss     | 5.2e-05  |\n",
      "|    ent_coef        | 0.000272 |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3547000, episode_reward=-281.81 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -282     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3547000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3548000, episode_reward=-157.15 +/- 168.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -157     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3548000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.527   |\n",
      "|    critic_loss     | 2.99e-05 |\n",
      "|    ent_coef        | 0.000271 |\n",
      "|    ent_coef_loss   | -23.5    |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -207     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3548     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5525     |\n",
      "|    total_timesteps | 3548000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3549000, episode_reward=12.09 +/- 206.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 12.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3549000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3550000, episode_reward=-236.60 +/- 4.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -237     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.525   |\n",
      "|    critic_loss     | 5.08e-05 |\n",
      "|    ent_coef        | 0.000269 |\n",
      "|    ent_coef_loss   | -25.9    |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3551000, episode_reward=-235.43 +/- 5.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -235     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3551000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3552000, episode_reward=-114.14 +/- 4.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -114     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3552000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.526   |\n",
      "|    critic_loss     | 8.07e-05 |\n",
      "|    ent_coef        | 0.000266 |\n",
      "|    ent_coef_loss   | -9.17    |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17340    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -200     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3552     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5531     |\n",
      "|    total_timesteps | 3552000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3553000, episode_reward=-103.95 +/- 17.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3553000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3554000, episode_reward=-336.68 +/- 4.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -337     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3554000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.533   |\n",
      "|    critic_loss     | 6.37e-05 |\n",
      "|    ent_coef        | 0.000265 |\n",
      "|    ent_coef_loss   | 10.4     |\n",
      "|    learning_rate   | 0.00145  |\n",
      "|    n_updates       | 17350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3555000, episode_reward=-339.33 +/- 6.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -339     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3555000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3556000, episode_reward=-287.63 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -288     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3556000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.519   |\n",
      "|    critic_loss     | 4.24e-05 |\n",
      "|    ent_coef        | 0.000265 |\n",
      "|    ent_coef_loss   | -15.6    |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17360    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3556     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5538     |\n",
      "|    total_timesteps | 3556000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3557000, episode_reward=-287.56 +/- 1.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -288     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3557000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3558000, episode_reward=-339.38 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -339     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3558000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.513   |\n",
      "|    critic_loss     | 4.72e-05 |\n",
      "|    ent_coef        | 0.000263 |\n",
      "|    ent_coef_loss   | -20      |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3559000, episode_reward=-338.03 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -338     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3559000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3560000, episode_reward=-427.32 +/- 0.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -427     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.506   |\n",
      "|    critic_loss     | 2.68e-05 |\n",
      "|    ent_coef        | 0.000261 |\n",
      "|    ent_coef_loss   | -7.09    |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17380    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -200     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3560     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5544     |\n",
      "|    total_timesteps | 3560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3561000, episode_reward=-426.68 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -427     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3561000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3562000, episode_reward=-441.97 +/- 0.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -442     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3562000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.512   |\n",
      "|    critic_loss     | 1.81e-05 |\n",
      "|    ent_coef        | 0.00026  |\n",
      "|    ent_coef_loss   | 7.48     |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3563000, episode_reward=-442.10 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -442     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3563000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3564000, episode_reward=-453.94 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -454     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3564000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.514   |\n",
      "|    critic_loss     | 1.18e-05 |\n",
      "|    ent_coef        | 0.000261 |\n",
      "|    ent_coef_loss   | 5.87     |\n",
      "|    learning_rate   | 0.00144  |\n",
      "|    n_updates       | 17400    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -203     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3564     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5550     |\n",
      "|    total_timesteps | 3564000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3565000, episode_reward=-455.28 +/- 0.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -455     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3565000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3566000, episode_reward=-442.77 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -443     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3566000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.513   |\n",
      "|    critic_loss     | 6.69e-06 |\n",
      "|    ent_coef        | 0.000261 |\n",
      "|    ent_coef_loss   | 2.22     |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3567000, episode_reward=-444.08 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -444     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3567000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3568000, episode_reward=-437.98 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -438     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3568000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.509   |\n",
      "|    critic_loss     | 8.54e-06 |\n",
      "|    ent_coef        | 0.000261 |\n",
      "|    ent_coef_loss   | -3.29    |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17420    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -207     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3568     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5557     |\n",
      "|    total_timesteps | 3568000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3569000, episode_reward=-438.10 +/- 0.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -438     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3569000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570000, episode_reward=-436.75 +/- 1.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -437     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.509   |\n",
      "|    critic_loss     | 8.56e-06 |\n",
      "|    ent_coef        | 0.000261 |\n",
      "|    ent_coef_loss   | -3.41    |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3571000, episode_reward=-436.84 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -437     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3571000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3572000, episode_reward=-427.98 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -428     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3572000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.509   |\n",
      "|    critic_loss     | 7.01e-06 |\n",
      "|    ent_coef        | 0.000261 |\n",
      "|    ent_coef_loss   | -6.33    |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17440    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3572     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5563     |\n",
      "|    total_timesteps | 3572000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3573000, episode_reward=-428.00 +/- 0.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -428     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3573000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3574000, episode_reward=-412.13 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -412     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3574000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.508   |\n",
      "|    critic_loss     | 8.86e-06 |\n",
      "|    ent_coef        | 0.00026  |\n",
      "|    ent_coef_loss   | -7.57    |\n",
      "|    learning_rate   | 0.00143  |\n",
      "|    n_updates       | 17450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3575000, episode_reward=-411.03 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -411     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3575000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3576000, episode_reward=-388.26 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -388     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3576000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.507   |\n",
      "|    critic_loss     | 8.86e-06 |\n",
      "|    ent_coef        | 0.000259 |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3576     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5569     |\n",
      "|    total_timesteps | 3576000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3577000, episode_reward=-387.61 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -388     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3577000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3578000, episode_reward=-376.94 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -377     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3578000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.508   |\n",
      "|    critic_loss     | 9.41e-06 |\n",
      "|    ent_coef        | 0.000258 |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3579000, episode_reward=-377.59 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -378     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3579000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3580000, episode_reward=-381.33 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -381     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.509   |\n",
      "|    critic_loss     | 8.28e-06 |\n",
      "|    ent_coef        | 0.000257 |\n",
      "|    ent_coef_loss   | -8.79    |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3580     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5576     |\n",
      "|    total_timesteps | 3580000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3581000, episode_reward=-382.54 +/- 0.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -383     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3581000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3582000, episode_reward=-354.34 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -354     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3582000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.506   |\n",
      "|    critic_loss     | 8.59e-06 |\n",
      "|    ent_coef        | 0.000256 |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3583000, episode_reward=-353.07 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -353     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3583000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3584000, episode_reward=-353.79 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -354     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3584000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3584     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5582     |\n",
      "|    total_timesteps | 3584000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3585000, episode_reward=-359.49 +/- 1.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -359     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3585000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.505   |\n",
      "|    critic_loss     | 8.82e-06 |\n",
      "|    ent_coef        | 0.000254 |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.00142  |\n",
      "|    n_updates       | 17500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3586000, episode_reward=-357.45 +/- 0.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -357     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3586000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3587000, episode_reward=-349.04 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -349     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3587000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.503   |\n",
      "|    critic_loss     | 9.91e-06 |\n",
      "|    ent_coef        | 0.000253 |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3588000, episode_reward=-349.20 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -349     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3588000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -213     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3588     |\n",
      "|    fps             | 642      |\n",
      "|    time_elapsed    | 5588     |\n",
      "|    total_timesteps | 3588000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3589000, episode_reward=-329.71 +/- 1.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -330     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3589000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.503   |\n",
      "|    critic_loss     | 1.06e-05 |\n",
      "|    ent_coef        | 0.000252 |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3590000, episode_reward=-329.19 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -329     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3590000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3591000, episode_reward=-227.80 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3591000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.503   |\n",
      "|    critic_loss     | 1.44e-05 |\n",
      "|    ent_coef        | 0.00025  |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3592000, episode_reward=-228.14 +/- 1.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -228     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3592000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3592     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5595     |\n",
      "|    total_timesteps | 3592000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3593000, episode_reward=-218.70 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3593000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.504   |\n",
      "|    critic_loss     | 1.77e-05 |\n",
      "|    ent_coef        | 0.000249 |\n",
      "|    ent_coef_loss   | -17.4    |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3594000, episode_reward=-218.61 +/- 1.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -219     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3594000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3595000, episode_reward=-220.35 +/- 18.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -220     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3595000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.508   |\n",
      "|    critic_loss     | 2.98e-05 |\n",
      "|    ent_coef        | 0.000247 |\n",
      "|    ent_coef_loss   | -18.4    |\n",
      "|    learning_rate   | 0.00141  |\n",
      "|    n_updates       | 17550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3596000, episode_reward=-242.09 +/- 15.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -242     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3596000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -207     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3596     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5601     |\n",
      "|    total_timesteps | 3596000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3597000, episode_reward=-190.94 +/- 10.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -191     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3597000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.507   |\n",
      "|    critic_loss     | 3.52e-05 |\n",
      "|    ent_coef        | 0.000245 |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3598000, episode_reward=-195.04 +/- 5.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -195     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3598000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3599000, episode_reward=-203.25 +/- 17.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3599000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.507   |\n",
      "|    critic_loss     | 3.43e-05 |\n",
      "|    ent_coef        | 0.000243 |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3600000, episode_reward=-211.96 +/- 16.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -212     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3600000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -204     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3600     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5607     |\n",
      "|    total_timesteps | 3600000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3601000, episode_reward=7.38 +/- 15.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 7.38     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3601000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.508   |\n",
      "|    critic_loss     | 4.67e-05 |\n",
      "|    ent_coef        | 0.000241 |\n",
      "|    ent_coef_loss   | -15.8    |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3602000, episode_reward=5.66 +/- 12.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.66     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3602000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3603000, episode_reward=-80.03 +/- 9.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3603000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.514   |\n",
      "|    critic_loss     | 4.59e-05 |\n",
      "|    ent_coef        | 0.000239 |\n",
      "|    ent_coef_loss   | -14.7    |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3604000, episode_reward=-83.98 +/- 10.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -84      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3604000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3604     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5613     |\n",
      "|    total_timesteps | 3604000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3605000, episode_reward=-106.47 +/- 6.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3605000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.512   |\n",
      "|    critic_loss     | 3.66e-05 |\n",
      "|    ent_coef        | 0.000237 |\n",
      "|    ent_coef_loss   | -20.7    |\n",
      "|    learning_rate   | 0.0014   |\n",
      "|    n_updates       | 17600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3606000, episode_reward=-103.89 +/- 9.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3606000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3607000, episode_reward=308.44 +/- 5.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 308      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3607000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.51    |\n",
      "|    critic_loss     | 3.47e-05 |\n",
      "|    ent_coef        | 0.000235 |\n",
      "|    ent_coef_loss   | -25.7    |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3608000, episode_reward=305.00 +/- 2.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 305      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3608000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -191     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3608     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5620     |\n",
      "|    total_timesteps | 3608000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3609000, episode_reward=436.77 +/- 5.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 437      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3609000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.518   |\n",
      "|    critic_loss     | 4.07e-05 |\n",
      "|    ent_coef        | 0.000233 |\n",
      "|    ent_coef_loss   | -21.7    |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3610000, episode_reward=431.23 +/- 7.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3610000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3611000, episode_reward=421.02 +/- 2.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 421      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3611000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.524   |\n",
      "|    critic_loss     | 5.09e-05 |\n",
      "|    ent_coef        | 0.00023  |\n",
      "|    ent_coef_loss   | -18.9    |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3612000, episode_reward=419.34 +/- 2.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 419      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3612000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -173     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3612     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5626     |\n",
      "|    total_timesteps | 3612000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3613000, episode_reward=397.95 +/- 7.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 398      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3613000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.532   |\n",
      "|    critic_loss     | 5.68e-05 |\n",
      "|    ent_coef        | 0.000228 |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3614000, episode_reward=398.85 +/- 10.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3614000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3615000, episode_reward=313.56 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 314      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3615000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.532   |\n",
      "|    critic_loss     | 4.92e-05 |\n",
      "|    ent_coef        | 0.000226 |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.00139  |\n",
      "|    n_updates       | 17650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3616000, episode_reward=314.91 +/- 2.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 315      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3616000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -155     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3616     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5632     |\n",
      "|    total_timesteps | 3616000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3617000, episode_reward=426.08 +/- 5.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 426      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3617000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.525   |\n",
      "|    critic_loss     | 4.07e-05 |\n",
      "|    ent_coef        | 0.000224 |\n",
      "|    ent_coef_loss   | -15.4    |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3618000, episode_reward=423.38 +/- 6.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 423      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3618000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3619000, episode_reward=545.12 +/- 10.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 545      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3619000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.541   |\n",
      "|    critic_loss     | 7.49e-05 |\n",
      "|    ent_coef        | 0.000223 |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3620000, episode_reward=548.05 +/- 11.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 548      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3620000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -130     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3620     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5639     |\n",
      "|    total_timesteps | 3620000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3621000, episode_reward=348.43 +/- 5.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 348      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3621000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.538   |\n",
      "|    critic_loss     | 7.84e-05 |\n",
      "|    ent_coef        | 0.000221 |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3622000, episode_reward=352.19 +/- 3.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 352      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3622000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3623000, episode_reward=517.61 +/- 5.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 518      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3623000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.536   |\n",
      "|    critic_loss     | 5.54e-05 |\n",
      "|    ent_coef        | 0.000219 |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3624000, episode_reward=517.81 +/- 5.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 518      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3624000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -111     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3624     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5645     |\n",
      "|    total_timesteps | 3624000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3625000, episode_reward=539.96 +/- 2.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 540      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3625000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.53    |\n",
      "|    critic_loss     | 5.98e-05 |\n",
      "|    ent_coef        | 0.000218 |\n",
      "|    ent_coef_loss   | -15.7    |\n",
      "|    learning_rate   | 0.00138  |\n",
      "|    n_updates       | 17700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3626000, episode_reward=540.46 +/- 3.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 540      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3626000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3627000, episode_reward=535.70 +/- 6.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 536      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3627000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3628000, episode_reward=655.76 +/- 9.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 656      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3628000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.532   |\n",
      "|    critic_loss     | 7.8e-05  |\n",
      "|    ent_coef        | 0.000216 |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.00137  |\n",
      "|    n_updates       | 17710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -88.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3628     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5651     |\n",
      "|    total_timesteps | 3628000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3629000, episode_reward=651.40 +/- 7.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 651      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3629000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3630000, episode_reward=446.60 +/- 4.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 447      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.543   |\n",
      "|    critic_loss     | 5.99e-05 |\n",
      "|    ent_coef        | 0.000215 |\n",
      "|    ent_coef_loss   | -2.75    |\n",
      "|    learning_rate   | 0.00137  |\n",
      "|    n_updates       | 17720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3631000, episode_reward=442.34 +/- 15.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 442      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3631000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3632000, episode_reward=396.51 +/- 12.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 397      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3632000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.537   |\n",
      "|    critic_loss     | 5.46e-05 |\n",
      "|    ent_coef        | 0.000214 |\n",
      "|    ent_coef_loss   | -2.21    |\n",
      "|    learning_rate   | 0.00137  |\n",
      "|    n_updates       | 17730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -69.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3632     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5658     |\n",
      "|    total_timesteps | 3632000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3633000, episode_reward=394.12 +/- 13.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 394      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3633000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3634000, episode_reward=596.65 +/- 13.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 597      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3634000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.54    |\n",
      "|    critic_loss     | 6.95e-05 |\n",
      "|    ent_coef        | 0.000214 |\n",
      "|    ent_coef_loss   | -14      |\n",
      "|    learning_rate   | 0.00137  |\n",
      "|    n_updates       | 17740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3635000, episode_reward=599.87 +/- 7.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 600      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3635000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3636000, episode_reward=656.36 +/- 9.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 656      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3636000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.545   |\n",
      "|    critic_loss     | 8.34e-05 |\n",
      "|    ent_coef        | 0.000213 |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17750    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -46.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3636     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5664     |\n",
      "|    total_timesteps | 3636000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3637000, episode_reward=649.49 +/- 13.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 649      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3637000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3638000, episode_reward=642.46 +/- 11.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 642      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3638000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.546   |\n",
      "|    critic_loss     | 7.06e-05 |\n",
      "|    ent_coef        | 0.000211 |\n",
      "|    ent_coef_loss   | -8.75    |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3639000, episode_reward=647.42 +/- 11.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 647      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3639000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3640000, episode_reward=600.64 +/- 13.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 601      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.545   |\n",
      "|    critic_loss     | 6.62e-05 |\n",
      "|    ent_coef        | 0.00021  |\n",
      "|    ent_coef_loss   | -6.99    |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17770    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -20.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3640     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5670     |\n",
      "|    total_timesteps | 3640000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3641000, episode_reward=615.45 +/- 8.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 615      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3641000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3642000, episode_reward=630.29 +/- 12.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 630      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3642000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.545   |\n",
      "|    critic_loss     | 6.59e-05 |\n",
      "|    ent_coef        | 0.000209 |\n",
      "|    ent_coef_loss   | -8.38    |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3643000, episode_reward=618.22 +/- 6.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 618      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3643000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3644000, episode_reward=746.81 +/- 14.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 747      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3644000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.548   |\n",
      "|    critic_loss     | 7.99e-05 |\n",
      "|    ent_coef        | 0.000208 |\n",
      "|    ent_coef_loss   | -6.59    |\n",
      "|    learning_rate   | 0.00136  |\n",
      "|    n_updates       | 17790    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.51     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3644     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5676     |\n",
      "|    total_timesteps | 3644000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3645000, episode_reward=733.47 +/- 14.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 733      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3645000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3646000, episode_reward=928.54 +/- 8.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 929      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3646000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.553   |\n",
      "|    critic_loss     | 8.28e-05 |\n",
      "|    ent_coef        | 0.000207 |\n",
      "|    ent_coef_loss   | -5.92    |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3647000, episode_reward=916.95 +/- 5.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 917      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3647000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3648000, episode_reward=1013.78 +/- 28.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3648000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.557   |\n",
      "|    critic_loss     | 9.73e-05 |\n",
      "|    ent_coef        | 0.000207 |\n",
      "|    ent_coef_loss   | -6.87    |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17810    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 40.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3648     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5683     |\n",
      "|    total_timesteps | 3648000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3649000, episode_reward=1033.99 +/- 18.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3649000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3650000, episode_reward=768.48 +/- 26.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 768      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.565   |\n",
      "|    critic_loss     | 0.000121 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | -3.01    |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3651000, episode_reward=766.73 +/- 6.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 767      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3651000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3652000, episode_reward=844.74 +/- 15.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 845      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3652000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.554   |\n",
      "|    critic_loss     | 8.23e-05 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17830    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 73.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 3652     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5689     |\n",
      "|    total_timesteps | 3652000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3653000, episode_reward=837.86 +/- 18.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 838      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3653000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3654000, episode_reward=758.19 +/- 20.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 758      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3654000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.566   |\n",
      "|    critic_loss     | 9.17e-05 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | 2.84     |\n",
      "|    learning_rate   | 0.00135  |\n",
      "|    n_updates       | 17840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3655000, episode_reward=753.44 +/- 22.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 753      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3655000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3656000, episode_reward=709.41 +/- 15.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 709      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3656000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.559   |\n",
      "|    critic_loss     | 8.16e-05 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | 0.865    |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17850    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 114      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3656     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5695     |\n",
      "|    total_timesteps | 3656000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3657000, episode_reward=681.87 +/- 18.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 682      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3657000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3658000, episode_reward=829.46 +/- 17.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 829      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3658000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.555   |\n",
      "|    critic_loss     | 7.32e-05 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | 0.000747 |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3659000, episode_reward=830.13 +/- 14.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 830      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3659000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660000, episode_reward=1177.79 +/- 26.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.563   |\n",
      "|    critic_loss     | 7.44e-05 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | 1.74     |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17870    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3660     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5701     |\n",
      "|    total_timesteps | 3660000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3661000, episode_reward=1201.74 +/- 24.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3661000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3662000, episode_reward=1217.67 +/- 18.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3662000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.58    |\n",
      "|    critic_loss     | 0.000122 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | 2.72     |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3663000, episode_reward=1210.53 +/- 19.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3663000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3664000, episode_reward=1151.49 +/- 36.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3664000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.585   |\n",
      "|    critic_loss     | 0.000137 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | 2.24     |\n",
      "|    learning_rate   | 0.00134  |\n",
      "|    n_updates       | 17890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 214      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3664     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5708     |\n",
      "|    total_timesteps | 3664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3665000, episode_reward=1157.52 +/- 20.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3665000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3666000, episode_reward=1323.26 +/- 9.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3666000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.577   |\n",
      "|    critic_loss     | 0.000126 |\n",
      "|    ent_coef        | 0.000206 |\n",
      "|    ent_coef_loss   | 0.123    |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3667000, episode_reward=1347.60 +/- 17.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3667000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3668000, episode_reward=1337.11 +/- 20.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3668000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.594   |\n",
      "|    critic_loss     | 0.000144 |\n",
      "|    ent_coef        | 0.000207 |\n",
      "|    ent_coef_loss   | 4.31     |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 274      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3668     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5714     |\n",
      "|    total_timesteps | 3668000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3669000, episode_reward=1348.82 +/- 19.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3669000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3670000, episode_reward=1346.61 +/- 15.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3670000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3671000, episode_reward=1308.55 +/- 16.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3671000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.597   |\n",
      "|    critic_loss     | 0.000135 |\n",
      "|    ent_coef        | 0.000207 |\n",
      "|    ent_coef_loss   | 7.12     |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3672000, episode_reward=1307.31 +/- 25.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3672000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 339      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3672     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5720     |\n",
      "|    total_timesteps | 3672000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3673000, episode_reward=1399.91 +/- 15.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3673000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.602   |\n",
      "|    critic_loss     | 0.000141 |\n",
      "|    ent_coef        | 0.000207 |\n",
      "|    ent_coef_loss   | 6.46     |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3674000, episode_reward=1415.01 +/- 17.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3674000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3675000, episode_reward=1190.40 +/- 20.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3675000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.605   |\n",
      "|    critic_loss     | 0.000124 |\n",
      "|    ent_coef        | 0.000208 |\n",
      "|    ent_coef_loss   | 11.2     |\n",
      "|    learning_rate   | 0.00133  |\n",
      "|    n_updates       | 17940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3676000, episode_reward=1195.29 +/- 11.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3676000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 404      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3676     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5726     |\n",
      "|    total_timesteps | 3676000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3677000, episode_reward=1204.21 +/- 13.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3677000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.596   |\n",
      "|    critic_loss     | 0.000125 |\n",
      "|    ent_coef        | 0.000209 |\n",
      "|    ent_coef_loss   | 7.6      |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3678000, episode_reward=1205.57 +/- 12.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3678000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3679000, episode_reward=1231.70 +/- 16.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3679000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.592   |\n",
      "|    critic_loss     | 0.00013  |\n",
      "|    ent_coef        | 0.00021  |\n",
      "|    ent_coef_loss   | 5.5      |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=1243.78 +/- 27.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3680000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 464      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3680     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5732     |\n",
      "|    total_timesteps | 3680000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3681000, episode_reward=1420.90 +/- 11.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3681000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.598   |\n",
      "|    critic_loss     | 0.000141 |\n",
      "|    ent_coef        | 0.000211 |\n",
      "|    ent_coef_loss   | 5.13     |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17970    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3682000, episode_reward=1403.99 +/- 24.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3682000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3683000, episode_reward=1415.09 +/- 25.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3683000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.609   |\n",
      "|    critic_loss     | 0.000156 |\n",
      "|    ent_coef        | 0.000212 |\n",
      "|    ent_coef_loss   | 8.49     |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3684000, episode_reward=1427.76 +/- 11.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3684000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 530      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3684     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5739     |\n",
      "|    total_timesteps | 3684000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3685000, episode_reward=1485.51 +/- 21.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3685000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.609   |\n",
      "|    critic_loss     | 0.000154 |\n",
      "|    ent_coef        | 0.000212 |\n",
      "|    ent_coef_loss   | 8.03     |\n",
      "|    learning_rate   | 0.00132  |\n",
      "|    n_updates       | 17990    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3686000, episode_reward=1495.83 +/- 9.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3686000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3687000, episode_reward=1416.33 +/- 8.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3687000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.619   |\n",
      "|    critic_loss     | 0.000166 |\n",
      "|    ent_coef        | 0.000213 |\n",
      "|    ent_coef_loss   | 10.2     |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3688000, episode_reward=1405.14 +/- 21.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3688000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 597      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3688     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5746     |\n",
      "|    total_timesteps | 3688000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3689000, episode_reward=1457.50 +/- 21.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3689000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.606   |\n",
      "|    critic_loss     | 0.000151 |\n",
      "|    ent_coef        | 0.000214 |\n",
      "|    ent_coef_loss   | 2.25     |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690000, episode_reward=1463.14 +/- 27.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3690000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3691000, episode_reward=1374.71 +/- 34.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3691000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.615   |\n",
      "|    critic_loss     | 0.000164 |\n",
      "|    ent_coef        | 0.000215 |\n",
      "|    ent_coef_loss   | 5.42     |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3692000, episode_reward=1386.75 +/- 27.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3692000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 662      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3692     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5752     |\n",
      "|    total_timesteps | 3692000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3693000, episode_reward=1448.01 +/- 30.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3693000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.61    |\n",
      "|    critic_loss     | 0.000141 |\n",
      "|    ent_coef        | 0.000215 |\n",
      "|    ent_coef_loss   | 2.35     |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3694000, episode_reward=1436.68 +/- 8.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3694000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3695000, episode_reward=1370.42 +/- 8.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3695000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.617   |\n",
      "|    critic_loss     | 0.000154 |\n",
      "|    ent_coef        | 0.000216 |\n",
      "|    ent_coef_loss   | 8.89     |\n",
      "|    learning_rate   | 0.00131  |\n",
      "|    n_updates       | 18040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3696000, episode_reward=1357.32 +/- 15.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3696000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 723      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3696     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5759     |\n",
      "|    total_timesteps | 3696000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3697000, episode_reward=1455.02 +/- 24.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3697000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.608   |\n",
      "|    critic_loss     | 0.000149 |\n",
      "|    ent_coef        | 0.000217 |\n",
      "|    ent_coef_loss   | 5.81     |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3698000, episode_reward=1443.54 +/- 26.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3698000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3699000, episode_reward=1543.83 +/- 32.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3699000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.622   |\n",
      "|    critic_loss     | 0.000171 |\n",
      "|    ent_coef        | 0.000218 |\n",
      "|    ent_coef_loss   | 14.2     |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18060    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3700000, episode_reward=1547.01 +/- 31.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3700000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 786      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3700     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5765     |\n",
      "|    total_timesteps | 3700000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3701000, episode_reward=1561.14 +/- 25.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3701000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.622   |\n",
      "|    critic_loss     | 0.000176 |\n",
      "|    ent_coef        | 0.000219 |\n",
      "|    ent_coef_loss   | 9.15     |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18070    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3702000, episode_reward=1557.36 +/- 19.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3702000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3703000, episode_reward=1386.66 +/- 12.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3703000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.628   |\n",
      "|    critic_loss     | 0.000176 |\n",
      "|    ent_coef        | 0.00022  |\n",
      "|    ent_coef_loss   | 15.9     |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3704000, episode_reward=1372.46 +/- 29.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3704000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 847      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3704     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5773     |\n",
      "|    total_timesteps | 3704000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3705000, episode_reward=1520.81 +/- 11.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3705000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.622   |\n",
      "|    critic_loss     | 0.000155 |\n",
      "|    ent_coef        | 0.000222 |\n",
      "|    ent_coef_loss   | 8.85     |\n",
      "|    learning_rate   | 0.0013   |\n",
      "|    n_updates       | 18090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3706000, episode_reward=1530.64 +/- 22.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3706000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3707000, episode_reward=1608.68 +/- 23.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3707000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.628   |\n",
      "|    critic_loss     | 0.000187 |\n",
      "|    ent_coef        | 0.000223 |\n",
      "|    ent_coef_loss   | 12.7     |\n",
      "|    learning_rate   | 0.00129  |\n",
      "|    n_updates       | 18100    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3708000, episode_reward=1601.58 +/- 28.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3708000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 907      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3708     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5780     |\n",
      "|    total_timesteps | 3708000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3709000, episode_reward=1618.17 +/- 10.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3709000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.638   |\n",
      "|    critic_loss     | 0.000179 |\n",
      "|    ent_coef        | 0.000225 |\n",
      "|    ent_coef_loss   | 16.3     |\n",
      "|    learning_rate   | 0.00129  |\n",
      "|    n_updates       | 18110    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3710000, episode_reward=1618.23 +/- 10.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3710000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3711000, episode_reward=1660.36 +/- 38.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3711000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.635   |\n",
      "|    critic_loss     | 0.000174 |\n",
      "|    ent_coef        | 0.000227 |\n",
      "|    ent_coef_loss   | 9.34     |\n",
      "|    learning_rate   | 0.00129  |\n",
      "|    n_updates       | 18120    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3712000, episode_reward=1649.55 +/- 35.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3712000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 961      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3712     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5786     |\n",
      "|    total_timesteps | 3712000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3713000, episode_reward=1667.87 +/- 32.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3713000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3714000, episode_reward=1576.40 +/- 14.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3714000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.64    |\n",
      "|    critic_loss     | 0.000214 |\n",
      "|    ent_coef        | 0.000228 |\n",
      "|    ent_coef_loss   | 14.3     |\n",
      "|    learning_rate   | 0.00129  |\n",
      "|    n_updates       | 18130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3715000, episode_reward=1595.79 +/- 32.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3715000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3716000, episode_reward=1620.73 +/- 10.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3716000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.633   |\n",
      "|    critic_loss     | 0.000178 |\n",
      "|    ent_coef        | 0.00023  |\n",
      "|    ent_coef_loss   | 9.03     |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3716     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5793     |\n",
      "|    total_timesteps | 3716000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3717000, episode_reward=1634.75 +/- 28.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3717000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3718000, episode_reward=1805.71 +/- 36.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3718000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.632   |\n",
      "|    critic_loss     | 0.000204 |\n",
      "|    ent_coef        | 0.000231 |\n",
      "|    ent_coef_loss   | 9.67     |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18150    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3719000, episode_reward=1796.90 +/- 19.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3719000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3720000, episode_reward=1759.76 +/- 17.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.646   |\n",
      "|    critic_loss     | 0.000217 |\n",
      "|    ent_coef        | 0.000232 |\n",
      "|    ent_coef_loss   | 13.4     |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3720     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5799     |\n",
      "|    total_timesteps | 3720000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3721000, episode_reward=1743.50 +/- 26.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3721000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3722000, episode_reward=1788.64 +/- 16.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3722000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.647   |\n",
      "|    critic_loss     | 0.000206 |\n",
      "|    ent_coef        | 0.000234 |\n",
      "|    ent_coef_loss   | 8.4      |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3723000, episode_reward=1788.36 +/- 36.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3723000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3724000, episode_reward=1773.95 +/- 21.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3724000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.649   |\n",
      "|    critic_loss     | 0.000239 |\n",
      "|    ent_coef        | 0.000235 |\n",
      "|    ent_coef_loss   | 6.09     |\n",
      "|    learning_rate   | 0.00128  |\n",
      "|    n_updates       | 18180    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3724     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5805     |\n",
      "|    total_timesteps | 3724000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3725000, episode_reward=1768.25 +/- 31.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3725000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3726000, episode_reward=1736.33 +/- 36.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3726000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.648   |\n",
      "|    critic_loss     | 0.000209 |\n",
      "|    ent_coef        | 0.000236 |\n",
      "|    ent_coef_loss   | 11.6     |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3727000, episode_reward=1755.33 +/- 35.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3727000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3728000, episode_reward=1839.40 +/- 33.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3728000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.645   |\n",
      "|    critic_loss     | 0.000185 |\n",
      "|    ent_coef        | 0.000238 |\n",
      "|    ent_coef_loss   | 6.55     |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18200    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3728     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5812     |\n",
      "|    total_timesteps | 3728000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3729000, episode_reward=1804.81 +/- 26.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3729000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3730000, episode_reward=1723.87 +/- 9.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.655   |\n",
      "|    critic_loss     | 0.000227 |\n",
      "|    ent_coef        | 0.000239 |\n",
      "|    ent_coef_loss   | 14.5     |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3731000, episode_reward=1698.72 +/- 40.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3731000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3732000, episode_reward=1762.73 +/- 21.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3732000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.65    |\n",
      "|    critic_loss     | 0.000204 |\n",
      "|    ent_coef        | 0.000241 |\n",
      "|    ent_coef_loss   | 9.72     |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18220    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3732     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5818     |\n",
      "|    total_timesteps | 3732000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3733000, episode_reward=1754.54 +/- 20.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3733000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3734000, episode_reward=1799.58 +/- 13.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3734000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.657   |\n",
      "|    critic_loss     | 0.000235 |\n",
      "|    ent_coef        | 0.000242 |\n",
      "|    ent_coef_loss   | 11.9     |\n",
      "|    learning_rate   | 0.00127  |\n",
      "|    n_updates       | 18230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3735000, episode_reward=1794.74 +/- 28.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3735000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3736000, episode_reward=1833.79 +/- 42.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3736000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.652   |\n",
      "|    critic_loss     | 0.00024  |\n",
      "|    ent_coef        | 0.000244 |\n",
      "|    ent_coef_loss   | 14.1     |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18240    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3736     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5824     |\n",
      "|    total_timesteps | 3736000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3737000, episode_reward=1827.38 +/- 31.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3737000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3738000, episode_reward=1764.90 +/- 38.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3738000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.665   |\n",
      "|    critic_loss     | 0.000237 |\n",
      "|    ent_coef        | 0.000246 |\n",
      "|    ent_coef_loss   | 13.4     |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3739000, episode_reward=1750.85 +/- 15.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3739000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3740000, episode_reward=1968.94 +/- 45.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.657   |\n",
      "|    critic_loss     | 0.000216 |\n",
      "|    ent_coef        | 0.000247 |\n",
      "|    ent_coef_loss   | 5.33     |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18260    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3740     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5830     |\n",
      "|    total_timesteps | 3740000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3741000, episode_reward=1983.02 +/- 7.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3741000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3742000, episode_reward=1957.33 +/- 15.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3742000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.674   |\n",
      "|    critic_loss     | 0.00026  |\n",
      "|    ent_coef        | 0.000249 |\n",
      "|    ent_coef_loss   | 14.5     |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3743000, episode_reward=1992.03 +/- 33.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3743000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3744000, episode_reward=1941.72 +/- 19.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3744000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.667   |\n",
      "|    critic_loss     | 0.000291 |\n",
      "|    ent_coef        | 0.00025  |\n",
      "|    ent_coef_loss   | 17.5     |\n",
      "|    learning_rate   | 0.00126  |\n",
      "|    n_updates       | 18280    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3744     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5837     |\n",
      "|    total_timesteps | 3744000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3745000, episode_reward=1919.36 +/- 30.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3745000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3746000, episode_reward=1755.82 +/- 34.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3746000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.673   |\n",
      "|    critic_loss     | 0.000234 |\n",
      "|    ent_coef        | 0.000253 |\n",
      "|    ent_coef_loss   | 18       |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3747000, episode_reward=1743.97 +/- 40.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3747000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3748000, episode_reward=1980.36 +/- 18.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3748000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.655   |\n",
      "|    critic_loss     | 0.000254 |\n",
      "|    ent_coef        | 0.000255 |\n",
      "|    ent_coef_loss   | 8.05     |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3748     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5843     |\n",
      "|    total_timesteps | 3748000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3749000, episode_reward=1973.13 +/- 15.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3749000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750000, episode_reward=1827.52 +/- 19.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.676   |\n",
      "|    critic_loss     | 0.000264 |\n",
      "|    ent_coef        | 0.000257 |\n",
      "|    ent_coef_loss   | 20.2     |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3751000, episode_reward=1834.12 +/- 20.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3751000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3752000, episode_reward=1889.86 +/- 51.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3752000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.664   |\n",
      "|    critic_loss     | 0.00025  |\n",
      "|    ent_coef        | 0.00026  |\n",
      "|    ent_coef_loss   | 7.89     |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3752     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5849     |\n",
      "|    total_timesteps | 3752000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3753000, episode_reward=1871.48 +/- 31.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3753000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3754000, episode_reward=1786.49 +/- 12.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3754000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.669   |\n",
      "|    critic_loss     | 0.000264 |\n",
      "|    ent_coef        | 0.000261 |\n",
      "|    ent_coef_loss   | 9.64     |\n",
      "|    learning_rate   | 0.00125  |\n",
      "|    n_updates       | 18330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3755000, episode_reward=1798.21 +/- 42.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3755000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3756000, episode_reward=1792.40 +/- 22.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3756000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3756     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5856     |\n",
      "|    total_timesteps | 3756000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3757000, episode_reward=1963.22 +/- 56.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3757000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.666   |\n",
      "|    critic_loss     | 0.000289 |\n",
      "|    ent_coef        | 0.000263 |\n",
      "|    ent_coef_loss   | 10.2     |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3758000, episode_reward=1941.16 +/- 38.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3758000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3759000, episode_reward=1869.71 +/- 15.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3759000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.675   |\n",
      "|    critic_loss     | 0.000281 |\n",
      "|    ent_coef        | 0.000265 |\n",
      "|    ent_coef_loss   | 12       |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3760000, episode_reward=1893.88 +/- 12.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3760000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3760     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5862     |\n",
      "|    total_timesteps | 3760000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3761000, episode_reward=1858.39 +/- 21.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3761000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.668   |\n",
      "|    critic_loss     | 0.000306 |\n",
      "|    ent_coef        | 0.000266 |\n",
      "|    ent_coef_loss   | 12.5     |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3762000, episode_reward=1840.24 +/- 28.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3762000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3763000, episode_reward=1993.97 +/- 64.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3763000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.664   |\n",
      "|    critic_loss     | 0.000275 |\n",
      "|    ent_coef        | 0.000268 |\n",
      "|    ent_coef_loss   | 12.1     |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18370    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3764000, episode_reward=1932.44 +/- 28.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3764000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3764     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5868     |\n",
      "|    total_timesteps | 3764000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3765000, episode_reward=2040.24 +/- 9.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3765000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.678   |\n",
      "|    critic_loss     | 0.00029  |\n",
      "|    ent_coef        | 0.00027  |\n",
      "|    ent_coef_loss   | 9.26     |\n",
      "|    learning_rate   | 0.00124  |\n",
      "|    n_updates       | 18380    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3766000, episode_reward=2058.55 +/- 44.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3766000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3767000, episode_reward=1990.59 +/- 23.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3767000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.687   |\n",
      "|    critic_loss     | 0.000247 |\n",
      "|    ent_coef        | 0.000272 |\n",
      "|    ent_coef_loss   | 12.6     |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3768000, episode_reward=1974.59 +/- 30.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3768000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3768     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5875     |\n",
      "|    total_timesteps | 3768000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3769000, episode_reward=1990.49 +/- 25.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3769000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.678   |\n",
      "|    critic_loss     | 0.000319 |\n",
      "|    ent_coef        | 0.000274 |\n",
      "|    ent_coef_loss   | 9.4      |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3770000, episode_reward=1984.79 +/- 31.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3770000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3771000, episode_reward=1966.08 +/- 50.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3771000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.684   |\n",
      "|    critic_loss     | 0.000266 |\n",
      "|    ent_coef        | 0.000276 |\n",
      "|    ent_coef_loss   | 13.9     |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3772000, episode_reward=1981.57 +/- 37.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3772000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3772     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5881     |\n",
      "|    total_timesteps | 3772000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3773000, episode_reward=1937.84 +/- 37.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3773000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.684   |\n",
      "|    critic_loss     | 0.000268 |\n",
      "|    ent_coef        | 0.000278 |\n",
      "|    ent_coef_loss   | 10       |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3774000, episode_reward=1931.58 +/- 40.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3774000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3775000, episode_reward=1928.51 +/- 25.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3775000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.674   |\n",
      "|    critic_loss     | 0.000309 |\n",
      "|    ent_coef        | 0.000279 |\n",
      "|    ent_coef_loss   | 8.45     |\n",
      "|    learning_rate   | 0.00123  |\n",
      "|    n_updates       | 18430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3776000, episode_reward=1915.15 +/- 36.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3776000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3776     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5887     |\n",
      "|    total_timesteps | 3776000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3777000, episode_reward=1910.34 +/- 17.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3777000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.676   |\n",
      "|    critic_loss     | 0.000334 |\n",
      "|    ent_coef        | 0.000281 |\n",
      "|    ent_coef_loss   | 10.9     |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3778000, episode_reward=1922.37 +/- 29.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3778000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3779000, episode_reward=1959.34 +/- 44.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3779000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.677   |\n",
      "|    critic_loss     | 0.000305 |\n",
      "|    ent_coef        | 0.000283 |\n",
      "|    ent_coef_loss   | 8.93     |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3780000, episode_reward=1914.20 +/- 21.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3780000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3780     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5894     |\n",
      "|    total_timesteps | 3780000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3781000, episode_reward=1913.92 +/- 17.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3781000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.671   |\n",
      "|    critic_loss     | 0.000329 |\n",
      "|    ent_coef        | 0.000285 |\n",
      "|    ent_coef_loss   | 11.6     |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3782000, episode_reward=1878.93 +/- 20.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3782000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3783000, episode_reward=1971.38 +/- 26.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3783000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.677   |\n",
      "|    critic_loss     | 0.000309 |\n",
      "|    ent_coef        | 0.000286 |\n",
      "|    ent_coef_loss   | 12.4     |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3784000, episode_reward=1984.05 +/- 28.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3784000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3784     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5900     |\n",
      "|    total_timesteps | 3784000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3785000, episode_reward=1908.61 +/- 49.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3785000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.681   |\n",
      "|    critic_loss     | 0.000286 |\n",
      "|    ent_coef        | 0.000289 |\n",
      "|    ent_coef_loss   | 7.73     |\n",
      "|    learning_rate   | 0.00122  |\n",
      "|    n_updates       | 18480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3786000, episode_reward=1926.58 +/- 24.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3786000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3787000, episode_reward=1803.61 +/- 31.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3787000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.678   |\n",
      "|    critic_loss     | 0.000275 |\n",
      "|    ent_coef        | 0.00029  |\n",
      "|    ent_coef_loss   | 3.81     |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3788000, episode_reward=1826.69 +/- 22.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3788000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3788     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5906     |\n",
      "|    total_timesteps | 3788000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3789000, episode_reward=1808.21 +/- 37.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3789000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.672   |\n",
      "|    critic_loss     | 0.000293 |\n",
      "|    ent_coef        | 0.000291 |\n",
      "|    ent_coef_loss   | 4.25     |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3790000, episode_reward=1829.89 +/- 65.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3790000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3791000, episode_reward=2067.55 +/- 56.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3791000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.675   |\n",
      "|    critic_loss     | 0.000287 |\n",
      "|    ent_coef        | 0.000292 |\n",
      "|    ent_coef_loss   | 11.8     |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18510    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3792000, episode_reward=2072.72 +/- 38.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3792000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3792     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5912     |\n",
      "|    total_timesteps | 3792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3793000, episode_reward=1937.64 +/- 69.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3793000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.685   |\n",
      "|    critic_loss     | 0.00028  |\n",
      "|    ent_coef        | 0.000294 |\n",
      "|    ent_coef_loss   | 12.4     |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3794000, episode_reward=2030.83 +/- 26.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3794000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3795000, episode_reward=2019.40 +/- 19.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3795000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.691   |\n",
      "|    critic_loss     | 0.000321 |\n",
      "|    ent_coef        | 0.000296 |\n",
      "|    ent_coef_loss   | 13.2     |\n",
      "|    learning_rate   | 0.00121  |\n",
      "|    n_updates       | 18530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3796000, episode_reward=2008.10 +/- 30.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3796000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3796     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5919     |\n",
      "|    total_timesteps | 3796000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3797000, episode_reward=2095.38 +/- 19.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3797000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.69    |\n",
      "|    critic_loss     | 0.000282 |\n",
      "|    ent_coef        | 0.000298 |\n",
      "|    ent_coef_loss   | 12.3     |\n",
      "|    learning_rate   | 0.0012   |\n",
      "|    n_updates       | 18540    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3798000, episode_reward=2141.87 +/- 74.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3798000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3799000, episode_reward=2173.17 +/- 39.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3799000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3800000, episode_reward=2092.23 +/- 62.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.689   |\n",
      "|    critic_loss     | 0.000332 |\n",
      "|    ent_coef        | 0.000301 |\n",
      "|    ent_coef_loss   | 11.2     |\n",
      "|    learning_rate   | 0.0012   |\n",
      "|    n_updates       | 18550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3800     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5925     |\n",
      "|    total_timesteps | 3800000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3801000, episode_reward=2138.55 +/- 108.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3801000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3802000, episode_reward=2132.42 +/- 57.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3802000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.697   |\n",
      "|    critic_loss     | 0.000319 |\n",
      "|    ent_coef        | 0.000303 |\n",
      "|    ent_coef_loss   | 10.7     |\n",
      "|    learning_rate   | 0.0012   |\n",
      "|    n_updates       | 18560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3803000, episode_reward=2190.37 +/- 57.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3803000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3804000, episode_reward=1993.04 +/- 33.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3804000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.694   |\n",
      "|    critic_loss     | 0.000307 |\n",
      "|    ent_coef        | 0.000305 |\n",
      "|    ent_coef_loss   | 11       |\n",
      "|    learning_rate   | 0.0012   |\n",
      "|    n_updates       | 18570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3804     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5932     |\n",
      "|    total_timesteps | 3804000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3805000, episode_reward=2036.97 +/- 45.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3805000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3806000, episode_reward=2091.93 +/- 16.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3806000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.689   |\n",
      "|    critic_loss     | 0.000317 |\n",
      "|    ent_coef        | 0.000307 |\n",
      "|    ent_coef_loss   | 9.23     |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3807000, episode_reward=2069.13 +/- 33.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3807000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3808000, episode_reward=2265.01 +/- 53.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3808000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.7     |\n",
      "|    critic_loss     | 0.00032  |\n",
      "|    ent_coef        | 0.000309 |\n",
      "|    ent_coef_loss   | 12.9     |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18590    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3808     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5938     |\n",
      "|    total_timesteps | 3808000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3809000, episode_reward=2266.79 +/- 30.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3809000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3810000, episode_reward=2296.06 +/- 36.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.713   |\n",
      "|    critic_loss     | 0.000326 |\n",
      "|    ent_coef        | 0.000311 |\n",
      "|    ent_coef_loss   | 13.1     |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18600    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3811000, episode_reward=2281.92 +/- 29.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3811000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3812000, episode_reward=2186.60 +/- 21.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3812000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.709   |\n",
      "|    critic_loss     | 0.000331 |\n",
      "|    ent_coef        | 0.000314 |\n",
      "|    ent_coef_loss   | 11       |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18610    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3812     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5945     |\n",
      "|    total_timesteps | 3812000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3813000, episode_reward=2149.59 +/- 61.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3813000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3814000, episode_reward=2118.39 +/- 107.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3814000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.694   |\n",
      "|    critic_loss     | 0.00035  |\n",
      "|    ent_coef        | 0.000316 |\n",
      "|    ent_coef_loss   | 11.6     |\n",
      "|    learning_rate   | 0.00119  |\n",
      "|    n_updates       | 18620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3815000, episode_reward=2096.09 +/- 73.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3815000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3816000, episode_reward=2137.21 +/- 53.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3816000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.703   |\n",
      "|    critic_loss     | 0.000303 |\n",
      "|    ent_coef        | 0.000318 |\n",
      "|    ent_coef_loss   | 12.2     |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18630    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3816     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5952     |\n",
      "|    total_timesteps | 3816000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3817000, episode_reward=2101.68 +/- 29.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3817000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3818000, episode_reward=2185.31 +/- 34.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3818000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.704   |\n",
      "|    critic_loss     | 0.000343 |\n",
      "|    ent_coef        | 0.000321 |\n",
      "|    ent_coef_loss   | 11.2     |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3819000, episode_reward=2181.33 +/- 73.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3819000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3820000, episode_reward=2279.92 +/- 66.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.71    |\n",
      "|    critic_loss     | 0.000335 |\n",
      "|    ent_coef        | 0.000323 |\n",
      "|    ent_coef_loss   | 14.1     |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18650    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3820     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5958     |\n",
      "|    total_timesteps | 3820000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3821000, episode_reward=2243.43 +/- 28.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3821000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3822000, episode_reward=2322.24 +/- 26.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3822000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.71    |\n",
      "|    critic_loss     | 0.000317 |\n",
      "|    ent_coef        | 0.000326 |\n",
      "|    ent_coef_loss   | 10.6     |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18660    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3823000, episode_reward=2322.95 +/- 38.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3823000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3824000, episode_reward=2330.94 +/- 24.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3824000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.715   |\n",
      "|    critic_loss     | 0.000333 |\n",
      "|    ent_coef        | 0.000329 |\n",
      "|    ent_coef_loss   | 14.8     |\n",
      "|    learning_rate   | 0.00118  |\n",
      "|    n_updates       | 18670    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3824     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5965     |\n",
      "|    total_timesteps | 3824000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3825000, episode_reward=2332.46 +/- 36.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3825000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3826000, episode_reward=2253.35 +/- 26.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3826000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.725   |\n",
      "|    critic_loss     | 0.000352 |\n",
      "|    ent_coef        | 0.000331 |\n",
      "|    ent_coef_loss   | 15.7     |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3827000, episode_reward=2212.82 +/- 69.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3827000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3828000, episode_reward=2145.09 +/- 20.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3828000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.703   |\n",
      "|    critic_loss     | 0.000361 |\n",
      "|    ent_coef        | 0.000335 |\n",
      "|    ent_coef_loss   | 11.1     |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18690    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3828     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5971     |\n",
      "|    total_timesteps | 3828000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3829000, episode_reward=2115.10 +/- 85.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3829000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3830000, episode_reward=2155.84 +/- 42.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.704   |\n",
      "|    critic_loss     | 0.000315 |\n",
      "|    ent_coef        | 0.000337 |\n",
      "|    ent_coef_loss   | 5.74     |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3831000, episode_reward=2161.20 +/- 26.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3831000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3832000, episode_reward=2119.54 +/- 49.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3832000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.702   |\n",
      "|    critic_loss     | 0.000324 |\n",
      "|    ent_coef        | 0.000339 |\n",
      "|    ent_coef_loss   | 6.56     |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3832     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5977     |\n",
      "|    total_timesteps | 3832000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3833000, episode_reward=2172.51 +/- 42.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3833000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3834000, episode_reward=2233.87 +/- 30.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3834000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.701   |\n",
      "|    critic_loss     | 0.00034  |\n",
      "|    ent_coef        | 0.000341 |\n",
      "|    ent_coef_loss   | 8.7      |\n",
      "|    learning_rate   | 0.00117  |\n",
      "|    n_updates       | 18720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3835000, episode_reward=2219.39 +/- 66.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3835000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3836000, episode_reward=2132.85 +/- 71.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3836000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.7     |\n",
      "|    critic_loss     | 0.000297 |\n",
      "|    ent_coef        | 0.000342 |\n",
      "|    ent_coef_loss   | 7.15     |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3836     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5984     |\n",
      "|    total_timesteps | 3836000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3837000, episode_reward=2125.02 +/- 18.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3837000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3838000, episode_reward=1966.73 +/- 70.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3838000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.708   |\n",
      "|    critic_loss     | 0.000345 |\n",
      "|    ent_coef        | 0.000344 |\n",
      "|    ent_coef_loss   | 10.6     |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3839000, episode_reward=1999.34 +/- 32.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3839000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=2023.86 +/- 90.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3840000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3840     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5990     |\n",
      "|    total_timesteps | 3840000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3841000, episode_reward=2100.03 +/- 34.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3841000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.695   |\n",
      "|    critic_loss     | 0.000323 |\n",
      "|    ent_coef        | 0.000346 |\n",
      "|    ent_coef_loss   | 7.86     |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3842000, episode_reward=2086.77 +/- 57.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3842000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3843000, episode_reward=2181.58 +/- 98.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3843000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.698   |\n",
      "|    critic_loss     | 0.000348 |\n",
      "|    ent_coef        | 0.000348 |\n",
      "|    ent_coef_loss   | 5.56     |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3844000, episode_reward=2125.34 +/- 69.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3844000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3844     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 5996     |\n",
      "|    total_timesteps | 3844000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3845000, episode_reward=2018.98 +/- 23.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3845000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.705   |\n",
      "|    critic_loss     | 0.000385 |\n",
      "|    ent_coef        | 0.00035  |\n",
      "|    ent_coef_loss   | 6.94     |\n",
      "|    learning_rate   | 0.00116  |\n",
      "|    n_updates       | 18770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3846000, episode_reward=2011.28 +/- 59.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3846000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3847000, episode_reward=2069.11 +/- 57.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3847000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.702   |\n",
      "|    critic_loss     | 0.00037  |\n",
      "|    ent_coef        | 0.000351 |\n",
      "|    ent_coef_loss   | 7.18     |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3848000, episode_reward=2053.88 +/- 45.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3848000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3848     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 6002     |\n",
      "|    total_timesteps | 3848000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3849000, episode_reward=2194.21 +/- 20.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3849000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.698   |\n",
      "|    critic_loss     | 0.000299 |\n",
      "|    ent_coef        | 0.000353 |\n",
      "|    ent_coef_loss   | 3.82     |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850000, episode_reward=2193.84 +/- 34.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3850000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3851000, episode_reward=2151.09 +/- 29.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3851000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.708   |\n",
      "|    critic_loss     | 0.00033  |\n",
      "|    ent_coef        | 0.000354 |\n",
      "|    ent_coef_loss   | 8.81     |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3852000, episode_reward=2146.15 +/- 31.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3852000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3852     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 6008     |\n",
      "|    total_timesteps | 3852000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3853000, episode_reward=2176.06 +/- 32.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3853000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.71    |\n",
      "|    critic_loss     | 0.000293 |\n",
      "|    ent_coef        | 0.000356 |\n",
      "|    ent_coef_loss   | 8.54     |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3854000, episode_reward=2157.65 +/- 39.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3854000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3855000, episode_reward=2215.60 +/- 44.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3855000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.707   |\n",
      "|    critic_loss     | 0.000327 |\n",
      "|    ent_coef        | 0.000358 |\n",
      "|    ent_coef_loss   | 4.98     |\n",
      "|    learning_rate   | 0.00115  |\n",
      "|    n_updates       | 18820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3856000, episode_reward=2239.08 +/- 28.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3856000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3856     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 6015     |\n",
      "|    total_timesteps | 3856000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3857000, episode_reward=2249.70 +/- 20.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3857000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.72    |\n",
      "|    critic_loss     | 0.000306 |\n",
      "|    ent_coef        | 0.000359 |\n",
      "|    ent_coef_loss   | 9.09     |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3858000, episode_reward=2214.36 +/- 41.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3858000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3859000, episode_reward=2279.86 +/- 66.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3859000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.731   |\n",
      "|    critic_loss     | 0.000291 |\n",
      "|    ent_coef        | 0.000361 |\n",
      "|    ent_coef_loss   | 13.1     |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3860000, episode_reward=2274.18 +/- 39.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3860000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3860     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 6021     |\n",
      "|    total_timesteps | 3860000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3861000, episode_reward=2160.98 +/- 24.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3861000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.718   |\n",
      "|    critic_loss     | 0.000329 |\n",
      "|    ent_coef        | 0.000364 |\n",
      "|    ent_coef_loss   | 5.53     |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3862000, episode_reward=2194.15 +/- 44.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3862000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3863000, episode_reward=2234.06 +/- 69.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3863000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.707   |\n",
      "|    critic_loss     | 0.000295 |\n",
      "|    ent_coef        | 0.000366 |\n",
      "|    ent_coef_loss   | 2.63     |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3864000, episode_reward=2201.19 +/- 45.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3864000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3864     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 6027     |\n",
      "|    total_timesteps | 3864000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3865000, episode_reward=2189.16 +/- 24.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3865000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.713   |\n",
      "|    critic_loss     | 0.000328 |\n",
      "|    ent_coef        | 0.000367 |\n",
      "|    ent_coef_loss   | 2.9      |\n",
      "|    learning_rate   | 0.00114  |\n",
      "|    n_updates       | 18870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3866000, episode_reward=2219.73 +/- 53.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3866000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3867000, episode_reward=2271.35 +/- 7.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3867000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.724   |\n",
      "|    critic_loss     | 0.00036  |\n",
      "|    ent_coef        | 0.000368 |\n",
      "|    ent_coef_loss   | 6.79     |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3868000, episode_reward=2254.60 +/- 63.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3868000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3868     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 6033     |\n",
      "|    total_timesteps | 3868000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3869000, episode_reward=2219.15 +/- 53.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3869000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.727   |\n",
      "|    critic_loss     | 0.000337 |\n",
      "|    ent_coef        | 0.000369 |\n",
      "|    ent_coef_loss   | 8.39     |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870000, episode_reward=2228.20 +/- 48.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3870000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3871000, episode_reward=2333.90 +/- 49.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3871000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.726   |\n",
      "|    critic_loss     | 0.000337 |\n",
      "|    ent_coef        | 0.000371 |\n",
      "|    ent_coef_loss   | 6.72     |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18900    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3872000, episode_reward=2382.41 +/- 52.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3872000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3872     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 6040     |\n",
      "|    total_timesteps | 3872000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3873000, episode_reward=2408.46 +/- 26.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3873000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.733   |\n",
      "|    critic_loss     | 0.000349 |\n",
      "|    ent_coef        | 0.000373 |\n",
      "|    ent_coef_loss   | 8.97     |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18910    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3874000, episode_reward=2403.05 +/- 23.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3874000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3875000, episode_reward=2380.88 +/- 43.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3875000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.739   |\n",
      "|    critic_loss     | 0.00033  |\n",
      "|    ent_coef        | 0.000375 |\n",
      "|    ent_coef_loss   | 8.29     |\n",
      "|    learning_rate   | 0.00113  |\n",
      "|    n_updates       | 18920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3876000, episode_reward=2362.29 +/- 51.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3876000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3876     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 6046     |\n",
      "|    total_timesteps | 3876000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3877000, episode_reward=2263.44 +/- 47.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3877000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.737   |\n",
      "|    critic_loss     | 0.000342 |\n",
      "|    ent_coef        | 0.000377 |\n",
      "|    ent_coef_loss   | 9.45     |\n",
      "|    learning_rate   | 0.00112  |\n",
      "|    n_updates       | 18930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3878000, episode_reward=2261.25 +/- 34.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3878000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3879000, episode_reward=2475.29 +/- 66.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3879000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.73    |\n",
      "|    critic_loss     | 0.000358 |\n",
      "|    ent_coef        | 0.00038  |\n",
      "|    ent_coef_loss   | 11       |\n",
      "|    learning_rate   | 0.00112  |\n",
      "|    n_updates       | 18940    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3880000, episode_reward=2466.95 +/- 54.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3880000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3880     |\n",
      "|    fps             | 641      |\n",
      "|    time_elapsed    | 6053     |\n",
      "|    total_timesteps | 3880000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3881000, episode_reward=2524.13 +/- 53.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3881000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.745   |\n",
      "|    critic_loss     | 0.000347 |\n",
      "|    ent_coef        | 0.000382 |\n",
      "|    ent_coef_loss   | 12.8     |\n",
      "|    learning_rate   | 0.00112  |\n",
      "|    n_updates       | 18950    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3882000, episode_reward=2493.43 +/- 32.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3882000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3883000, episode_reward=2487.71 +/- 42.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3883000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3884000, episode_reward=2325.79 +/- 66.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3884000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.747   |\n",
      "|    critic_loss     | 0.000363 |\n",
      "|    ent_coef        | 0.000386 |\n",
      "|    ent_coef_loss   | 11.5     |\n",
      "|    learning_rate   | 0.00112  |\n",
      "|    n_updates       | 18960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3884     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6059     |\n",
      "|    total_timesteps | 3884000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3885000, episode_reward=2299.03 +/- 81.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3885000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3886000, episode_reward=2432.20 +/- 48.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3886000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.729   |\n",
      "|    critic_loss     | 0.000322 |\n",
      "|    ent_coef        | 0.000388 |\n",
      "|    ent_coef_loss   | 5.32     |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 18970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3887000, episode_reward=2380.47 +/- 54.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3887000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3888000, episode_reward=2527.44 +/- 58.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3888000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.73    |\n",
      "|    critic_loss     | 0.000318 |\n",
      "|    ent_coef        | 0.000391 |\n",
      "|    ent_coef_loss   | 6.86     |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 18980    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3888     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6065     |\n",
      "|    total_timesteps | 3888000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3889000, episode_reward=2469.52 +/- 47.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3889000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890000, episode_reward=2476.16 +/- 48.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.733   |\n",
      "|    critic_loss     | 0.000308 |\n",
      "|    ent_coef        | 0.000393 |\n",
      "|    ent_coef_loss   | 8.9      |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 18990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3891000, episode_reward=2402.12 +/- 75.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3891000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3892000, episode_reward=2404.78 +/- 52.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3892000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.752   |\n",
      "|    critic_loss     | 0.000354 |\n",
      "|    ent_coef        | 0.000395 |\n",
      "|    ent_coef_loss   | 11.6     |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 19000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3892     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6071     |\n",
      "|    total_timesteps | 3892000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3893000, episode_reward=2401.45 +/- 23.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3893000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3894000, episode_reward=2469.54 +/- 50.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3894000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.74    |\n",
      "|    critic_loss     | 0.000345 |\n",
      "|    ent_coef        | 0.000398 |\n",
      "|    ent_coef_loss   | 9.26     |\n",
      "|    learning_rate   | 0.00111  |\n",
      "|    n_updates       | 19010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3895000, episode_reward=2476.93 +/- 24.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3895000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3896000, episode_reward=2320.90 +/- 27.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3896000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.744   |\n",
      "|    critic_loss     | 0.000329 |\n",
      "|    ent_coef        | 0.000401 |\n",
      "|    ent_coef_loss   | 8.71     |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19020    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3896     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6078     |\n",
      "|    total_timesteps | 3896000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3897000, episode_reward=2340.08 +/- 55.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3897000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3898000, episode_reward=2459.10 +/- 39.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3898000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.734   |\n",
      "|    critic_loss     | 0.00033  |\n",
      "|    ent_coef        | 0.000403 |\n",
      "|    ent_coef_loss   | 7.76     |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3899000, episode_reward=2430.70 +/- 69.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3899000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900000, episode_reward=2510.38 +/- 19.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.749   |\n",
      "|    critic_loss     | 0.000385 |\n",
      "|    ent_coef        | 0.000405 |\n",
      "|    ent_coef_loss   | 7.83     |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19040    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3900     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6084     |\n",
      "|    total_timesteps | 3900000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3901000, episode_reward=2470.03 +/- 23.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3901000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3902000, episode_reward=2323.23 +/- 49.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3902000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.742   |\n",
      "|    critic_loss     | 0.000344 |\n",
      "|    ent_coef        | 0.000408 |\n",
      "|    ent_coef_loss   | 9.59     |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3903000, episode_reward=2334.57 +/- 40.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3903000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3904000, episode_reward=2552.99 +/- 32.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3904000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.736   |\n",
      "|    critic_loss     | 0.00034  |\n",
      "|    ent_coef        | 0.00041  |\n",
      "|    ent_coef_loss   | 5.83     |\n",
      "|    learning_rate   | 0.0011   |\n",
      "|    n_updates       | 19060    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3904     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6090     |\n",
      "|    total_timesteps | 3904000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3905000, episode_reward=2561.58 +/- 56.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3905000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3906000, episode_reward=2483.73 +/- 88.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3906000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.754   |\n",
      "|    critic_loss     | 0.000351 |\n",
      "|    ent_coef        | 0.000413 |\n",
      "|    ent_coef_loss   | 13.1     |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3907000, episode_reward=2536.42 +/- 60.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3907000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3908000, episode_reward=2524.19 +/- 36.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3908000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.752   |\n",
      "|    critic_loss     | 0.000356 |\n",
      "|    ent_coef        | 0.000416 |\n",
      "|    ent_coef_loss   | 7.95     |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19080    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3908     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6097     |\n",
      "|    total_timesteps | 3908000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3909000, episode_reward=2554.47 +/- 53.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3909000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3910000, episode_reward=2476.48 +/- 45.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.739   |\n",
      "|    critic_loss     | 0.000362 |\n",
      "|    ent_coef        | 0.000418 |\n",
      "|    ent_coef_loss   | 4.78     |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3911000, episode_reward=2479.91 +/- 48.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3911000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3912000, episode_reward=2697.60 +/- 30.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3912000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.75    |\n",
      "|    critic_loss     | 0.000326 |\n",
      "|    ent_coef        | 0.00042  |\n",
      "|    ent_coef_loss   | 4.99     |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19100    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3912     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6103     |\n",
      "|    total_timesteps | 3912000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3913000, episode_reward=2667.16 +/- 16.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3913000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3914000, episode_reward=2758.65 +/- 28.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3914000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.761   |\n",
      "|    critic_loss     | 0.000335 |\n",
      "|    ent_coef        | 0.000422 |\n",
      "|    ent_coef_loss   | 4.71     |\n",
      "|    learning_rate   | 0.00109  |\n",
      "|    n_updates       | 19110    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3915000, episode_reward=2761.38 +/- 67.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3915000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3916000, episode_reward=2667.79 +/- 40.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3916000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.766   |\n",
      "|    critic_loss     | 0.000331 |\n",
      "|    ent_coef        | 0.000423 |\n",
      "|    ent_coef_loss   | 6.15     |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19120    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3916     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6110     |\n",
      "|    total_timesteps | 3916000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3917000, episode_reward=2689.35 +/- 35.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3917000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3918000, episode_reward=2671.45 +/- 14.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3918000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.774   |\n",
      "|    critic_loss     | 0.000322 |\n",
      "|    ent_coef        | 0.000425 |\n",
      "|    ent_coef_loss   | 7.01     |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3919000, episode_reward=2646.20 +/- 56.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3919000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3920000, episode_reward=2759.93 +/- 21.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.768   |\n",
      "|    critic_loss     | 0.00034  |\n",
      "|    ent_coef        | 0.000427 |\n",
      "|    ent_coef_loss   | 5.53     |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3920     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6116     |\n",
      "|    total_timesteps | 3920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3921000, episode_reward=2743.70 +/- 22.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3921000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3922000, episode_reward=2701.35 +/- 37.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3922000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.773   |\n",
      "|    critic_loss     | 0.000334 |\n",
      "|    ent_coef        | 0.000429 |\n",
      "|    ent_coef_loss   | 7.5      |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3923000, episode_reward=2717.22 +/- 52.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3923000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3924000, episode_reward=2672.98 +/- 60.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3924000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.765   |\n",
      "|    critic_loss     | 0.000336 |\n",
      "|    ent_coef        | 0.000431 |\n",
      "|    ent_coef_loss   | 4.69     |\n",
      "|    learning_rate   | 0.00108  |\n",
      "|    n_updates       | 19160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3924     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6123     |\n",
      "|    total_timesteps | 3924000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3925000, episode_reward=2673.43 +/- 27.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3925000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3926000, episode_reward=2727.67 +/- 30.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3926000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3927000, episode_reward=2693.76 +/- 42.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3927000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.761   |\n",
      "|    critic_loss     | 0.000338 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 3.49     |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3928000, episode_reward=2761.96 +/- 71.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3928000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3928     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6129     |\n",
      "|    total_timesteps | 3928000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3929000, episode_reward=2742.35 +/- 14.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3929000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.762   |\n",
      "|    critic_loss     | 0.000367 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | 2.57     |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3930000, episode_reward=2724.38 +/- 27.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3930000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3931000, episode_reward=2790.58 +/- 54.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3931000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.766   |\n",
      "|    critic_loss     | 0.000388 |\n",
      "|    ent_coef        | 0.000435 |\n",
      "|    ent_coef_loss   | 2.69     |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19190    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3932000, episode_reward=2790.15 +/- 50.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3932000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3932     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6135     |\n",
      "|    total_timesteps | 3932000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3933000, episode_reward=2773.57 +/- 46.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3933000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.773   |\n",
      "|    critic_loss     | 0.000316 |\n",
      "|    ent_coef        | 0.000436 |\n",
      "|    ent_coef_loss   | 4.19     |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3934000, episode_reward=2774.01 +/- 46.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3934000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3935000, episode_reward=2761.40 +/- 63.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3935000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.766   |\n",
      "|    critic_loss     | 0.000408 |\n",
      "|    ent_coef        | 0.000437 |\n",
      "|    ent_coef_loss   | 7.92     |\n",
      "|    learning_rate   | 0.00107  |\n",
      "|    n_updates       | 19210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3936000, episode_reward=2768.21 +/- 38.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3936000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3936     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6142     |\n",
      "|    total_timesteps | 3936000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3937000, episode_reward=2797.90 +/- 53.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3937000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.000346 |\n",
      "|    ent_coef        | 0.000439 |\n",
      "|    ent_coef_loss   | 8.89     |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19220    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3938000, episode_reward=2794.79 +/- 44.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3938000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3939000, episode_reward=2746.00 +/- 36.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3939000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.778   |\n",
      "|    critic_loss     | 0.000339 |\n",
      "|    ent_coef        | 0.000442 |\n",
      "|    ent_coef_loss   | 5.98     |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3940000, episode_reward=2769.26 +/- 39.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3940000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3940     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6149     |\n",
      "|    total_timesteps | 3940000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3941000, episode_reward=2743.42 +/- 33.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3941000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.000354 |\n",
      "|    ent_coef        | 0.000444 |\n",
      "|    ent_coef_loss   | 5.41     |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3942000, episode_reward=2799.25 +/- 33.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3942000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3943000, episode_reward=2791.84 +/- 45.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3943000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.774   |\n",
      "|    critic_loss     | 0.000377 |\n",
      "|    ent_coef        | 0.000446 |\n",
      "|    ent_coef_loss   | 4.89     |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3944000, episode_reward=2750.14 +/- 15.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3944000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3944     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6155     |\n",
      "|    total_timesteps | 3944000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3945000, episode_reward=2751.37 +/- 10.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3945000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.000324 |\n",
      "|    ent_coef        | 0.000448 |\n",
      "|    ent_coef_loss   | 5.51     |\n",
      "|    learning_rate   | 0.00106  |\n",
      "|    n_updates       | 19260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3946000, episode_reward=2732.81 +/- 28.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3946000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3947000, episode_reward=2747.88 +/- 26.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3947000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.778   |\n",
      "|    critic_loss     | 0.000345 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | 7.47     |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3948000, episode_reward=2726.06 +/- 17.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3948000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3948     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6161     |\n",
      "|    total_timesteps | 3948000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3949000, episode_reward=2679.31 +/- 29.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3949000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.771   |\n",
      "|    critic_loss     | 0.000335 |\n",
      "|    ent_coef        | 0.000452 |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3950000, episode_reward=2655.73 +/- 49.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3950000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3951000, episode_reward=2614.33 +/- 16.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3951000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.764   |\n",
      "|    critic_loss     | 0.000328 |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3952000, episode_reward=2658.99 +/- 23.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3952000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3952     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6167     |\n",
      "|    total_timesteps | 3952000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3953000, episode_reward=2761.88 +/- 25.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3953000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.752   |\n",
      "|    critic_loss     | 0.000393 |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | 1.81     |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3954000, episode_reward=2716.83 +/- 46.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3954000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3955000, episode_reward=2500.31 +/- 38.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3955000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.77    |\n",
      "|    critic_loss     | 0.000352 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | 3.37     |\n",
      "|    learning_rate   | 0.00105  |\n",
      "|    n_updates       | 19310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3956000, episode_reward=2501.80 +/- 47.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3956000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3956     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6173     |\n",
      "|    total_timesteps | 3956000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3957000, episode_reward=2508.97 +/- 78.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3957000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.736   |\n",
      "|    critic_loss     | 0.000381 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | -1.68    |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3958000, episode_reward=2502.16 +/- 16.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3958000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3959000, episode_reward=2456.31 +/- 53.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3959000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.746   |\n",
      "|    critic_loss     | 0.000353 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | -0.241   |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960000, episode_reward=2447.80 +/- 59.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3960000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3960     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6180     |\n",
      "|    total_timesteps | 3960000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3961000, episode_reward=2632.06 +/- 40.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3961000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.753   |\n",
      "|    critic_loss     | 0.000386 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | 2.01     |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3962000, episode_reward=2619.03 +/- 53.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3962000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3963000, episode_reward=2451.20 +/- 39.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3963000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.756   |\n",
      "|    critic_loss     | 0.000375 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | 2.1      |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3964000, episode_reward=2479.57 +/- 27.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3964000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3964     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6186     |\n",
      "|    total_timesteps | 3964000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3965000, episode_reward=2627.66 +/- 55.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3965000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.746   |\n",
      "|    critic_loss     | 0.000387 |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | 2.22     |\n",
      "|    learning_rate   | 0.00104  |\n",
      "|    n_updates       | 19360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3966000, episode_reward=2630.82 +/- 40.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3966000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3967000, episode_reward=2683.30 +/- 51.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3967000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.763   |\n",
      "|    critic_loss     | 0.000424 |\n",
      "|    ent_coef        | 0.000458 |\n",
      "|    ent_coef_loss   | 0.921    |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    n_updates       | 19370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3968000, episode_reward=2697.68 +/- 27.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3968000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3968     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6192     |\n",
      "|    total_timesteps | 3968000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3969000, episode_reward=2697.06 +/- 51.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3969000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3970000, episode_reward=2777.91 +/- 37.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.769   |\n",
      "|    critic_loss     | 0.000421 |\n",
      "|    ent_coef        | 0.000458 |\n",
      "|    ent_coef_loss   | 2.42     |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    n_updates       | 19380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3971000, episode_reward=2751.89 +/- 54.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3971000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3972000, episode_reward=2708.40 +/- 35.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3972000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.765   |\n",
      "|    critic_loss     | 0.000397 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | 0.555    |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    n_updates       | 19390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3972     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6198     |\n",
      "|    total_timesteps | 3972000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3973000, episode_reward=2730.15 +/- 25.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3973000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3974000, episode_reward=2760.82 +/- 60.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3974000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.773   |\n",
      "|    critic_loss     | 0.000354 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | 2.16     |\n",
      "|    learning_rate   | 0.00103  |\n",
      "|    n_updates       | 19400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3975000, episode_reward=2796.17 +/- 50.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3975000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3976000, episode_reward=2873.03 +/- 38.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3976000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.000397 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | -0.102   |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19410    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3976     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6204     |\n",
      "|    total_timesteps | 3976000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3977000, episode_reward=2866.69 +/- 42.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3977000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3978000, episode_reward=2826.27 +/- 28.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3978000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.000344 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | 3.15     |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3979000, episode_reward=2869.13 +/- 23.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3979000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3980000, episode_reward=2786.36 +/- 24.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.79    |\n",
      "|    critic_loss     | 0.0004   |\n",
      "|    ent_coef        | 0.000461 |\n",
      "|    ent_coef_loss   | 3.98     |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19430    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3980     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6211     |\n",
      "|    total_timesteps | 3980000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3981000, episode_reward=2692.77 +/- 57.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3981000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3982000, episode_reward=2843.65 +/- 52.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3982000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.792   |\n",
      "|    critic_loss     | 0.00032  |\n",
      "|    ent_coef        | 0.000462 |\n",
      "|    ent_coef_loss   | 4.33     |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3983000, episode_reward=2259.60 +/- 1223.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3983000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3984000, episode_reward=2719.08 +/- 35.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3984000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.786   |\n",
      "|    critic_loss     | 0.000351 |\n",
      "|    ent_coef        | 0.000464 |\n",
      "|    ent_coef_loss   | 1.95     |\n",
      "|    learning_rate   | 0.00102  |\n",
      "|    n_updates       | 19450    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3984     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6217     |\n",
      "|    total_timesteps | 3984000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3985000, episode_reward=2720.96 +/- 50.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3985000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3986000, episode_reward=2850.36 +/- 61.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3986000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.779   |\n",
      "|    critic_loss     | 0.00032  |\n",
      "|    ent_coef        | 0.000465 |\n",
      "|    ent_coef_loss   | 4.33     |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3987000, episode_reward=2852.11 +/- 32.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3987000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3988000, episode_reward=2221.06 +/- 1221.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3988000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.783   |\n",
      "|    critic_loss     | 0.000352 |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | 2.8      |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19470    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3988     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6223     |\n",
      "|    total_timesteps | 3988000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3989000, episode_reward=2175.97 +/- 1230.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3989000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990000, episode_reward=2759.68 +/- 50.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.778   |\n",
      "|    critic_loss     | 0.000323 |\n",
      "|    ent_coef        | 0.000467 |\n",
      "|    ent_coef_loss   | 1.44     |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3991000, episode_reward=2725.70 +/- 57.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3991000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3992000, episode_reward=2597.25 +/- 31.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3992000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.688   |\n",
      "|    critic_loss     | 0.0029   |\n",
      "|    ent_coef        | 0.000468 |\n",
      "|    ent_coef_loss   | 5.98     |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19490    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3992     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6229     |\n",
      "|    total_timesteps | 3992000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3993000, episode_reward=2563.80 +/- 52.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3993000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3994000, episode_reward=1357.40 +/- 18.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3994000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.689   |\n",
      "|    critic_loss     | 0.000994 |\n",
      "|    ent_coef        | 0.00047  |\n",
      "|    ent_coef_loss   | 15.5     |\n",
      "|    learning_rate   | 0.00101  |\n",
      "|    n_updates       | 19500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3995000, episode_reward=1376.55 +/- 23.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3995000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3996000, episode_reward=2001.92 +/- 45.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3996000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.635   |\n",
      "|    critic_loss     | 0.000536 |\n",
      "|    ent_coef        | 0.000474 |\n",
      "|    ent_coef_loss   | 19.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19510    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3996     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6236     |\n",
      "|    total_timesteps | 3996000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3997000, episode_reward=1976.93 +/- 80.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3997000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3998000, episode_reward=1925.46 +/- 66.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3998000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.664   |\n",
      "|    critic_loss     | 0.000613 |\n",
      "|    ent_coef        | 0.000481 |\n",
      "|    ent_coef_loss   | 12.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3999000, episode_reward=2004.07 +/- 68.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3999000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000000, episode_reward=2137.33 +/- 63.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.656   |\n",
      "|    critic_loss     | 0.000554 |\n",
      "|    ent_coef        | 0.000486 |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19530    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4000     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6242     |\n",
      "|    total_timesteps | 4000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4001000, episode_reward=2071.63 +/- 34.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4001000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4002000, episode_reward=2273.72 +/- 40.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4002000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.689   |\n",
      "|    critic_loss     | 0.000433 |\n",
      "|    ent_coef        | 0.000488 |\n",
      "|    ent_coef_loss   | 5.04     |\n",
      "|    learning_rate   | 0.000998 |\n",
      "|    n_updates       | 19540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4003000, episode_reward=2274.42 +/- 61.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4003000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4004000, episode_reward=2156.30 +/- 34.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4004000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.72    |\n",
      "|    critic_loss     | 0.000409 |\n",
      "|    ent_coef        | 0.00049  |\n",
      "|    ent_coef_loss   | 7.55     |\n",
      "|    learning_rate   | 0.000996 |\n",
      "|    n_updates       | 19550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4004     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6248     |\n",
      "|    total_timesteps | 4004000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4005000, episode_reward=2165.04 +/- 36.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4005000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4006000, episode_reward=2150.15 +/- 69.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4006000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.715   |\n",
      "|    critic_loss     | 0.000393 |\n",
      "|    ent_coef        | 0.000492 |\n",
      "|    ent_coef_loss   | 2.95     |\n",
      "|    learning_rate   | 0.000994 |\n",
      "|    n_updates       | 19560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4007000, episode_reward=2222.08 +/- 83.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4007000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4008000, episode_reward=2357.89 +/- 23.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4008000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.697   |\n",
      "|    critic_loss     | 0.000387 |\n",
      "|    ent_coef        | 0.000493 |\n",
      "|    ent_coef_loss   | -2.02    |\n",
      "|    learning_rate   | 0.000992 |\n",
      "|    n_updates       | 19570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4008     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6254     |\n",
      "|    total_timesteps | 4008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4009000, episode_reward=2386.63 +/- 45.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4009000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4010000, episode_reward=2569.64 +/- 37.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.722   |\n",
      "|    critic_loss     | 0.000344 |\n",
      "|    ent_coef        | 0.000494 |\n",
      "|    ent_coef_loss   | 4.55     |\n",
      "|    learning_rate   | 0.00099  |\n",
      "|    n_updates       | 19580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4011000, episode_reward=2594.98 +/- 44.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4011000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4012000, episode_reward=2583.70 +/- 55.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4012000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4012     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6260     |\n",
      "|    total_timesteps | 4012000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4013000, episode_reward=2607.18 +/- 18.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4013000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.744   |\n",
      "|    critic_loss     | 0.000363 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | 2.03     |\n",
      "|    learning_rate   | 0.000988 |\n",
      "|    n_updates       | 19590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4014000, episode_reward=2617.69 +/- 27.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4014000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4015000, episode_reward=2788.43 +/- 33.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4015000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.728   |\n",
      "|    critic_loss     | 0.000367 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | -5.68    |\n",
      "|    learning_rate   | 0.000986 |\n",
      "|    n_updates       | 19600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4016000, episode_reward=2781.59 +/- 48.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4016000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4016     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6266     |\n",
      "|    total_timesteps | 4016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4017000, episode_reward=2837.46 +/- 29.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4017000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.756   |\n",
      "|    critic_loss     | 0.000419 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | 3.15     |\n",
      "|    learning_rate   | 0.000984 |\n",
      "|    n_updates       | 19610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4018000, episode_reward=2833.08 +/- 29.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4018000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4019000, episode_reward=2794.10 +/- 52.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4019000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.773   |\n",
      "|    critic_loss     | 0.000426 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | 5.23     |\n",
      "|    learning_rate   | 0.000982 |\n",
      "|    n_updates       | 19620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4020000, episode_reward=2791.28 +/- 27.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4020000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4020     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6272     |\n",
      "|    total_timesteps | 4020000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4021000, episode_reward=2783.20 +/- 30.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4021000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.768   |\n",
      "|    critic_loss     | 0.000412 |\n",
      "|    ent_coef        | 0.000496 |\n",
      "|    ent_coef_loss   | 0.204    |\n",
      "|    learning_rate   | 0.00098  |\n",
      "|    n_updates       | 19630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4022000, episode_reward=2777.81 +/- 27.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4022000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4023000, episode_reward=2739.47 +/- 32.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4023000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.767   |\n",
      "|    critic_loss     | 0.000443 |\n",
      "|    ent_coef        | 0.000497 |\n",
      "|    ent_coef_loss   | 2.61     |\n",
      "|    learning_rate   | 0.000978 |\n",
      "|    n_updates       | 19640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4024000, episode_reward=2783.99 +/- 28.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4024000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4024     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6279     |\n",
      "|    total_timesteps | 4024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4025000, episode_reward=2827.48 +/- 31.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4025000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.765   |\n",
      "|    critic_loss     | 0.000391 |\n",
      "|    ent_coef        | 0.000498 |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.000976 |\n",
      "|    n_updates       | 19650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4026000, episode_reward=2827.30 +/- 7.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4026000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4027000, episode_reward=2844.76 +/- 12.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4027000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.772   |\n",
      "|    critic_loss     | 0.000436 |\n",
      "|    ent_coef        | 0.000499 |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.000974 |\n",
      "|    n_updates       | 19660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4028000, episode_reward=2823.35 +/- 20.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4028000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4028     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6285     |\n",
      "|    total_timesteps | 4028000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4029000, episode_reward=2884.28 +/- 41.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4029000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.771   |\n",
      "|    critic_loss     | 0.000364 |\n",
      "|    ent_coef        | 0.000499 |\n",
      "|    ent_coef_loss   | 0.762    |\n",
      "|    learning_rate   | 0.000972 |\n",
      "|    n_updates       | 19670    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4030000, episode_reward=2849.86 +/- 44.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4030000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4031000, episode_reward=2863.99 +/- 44.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4031000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.774   |\n",
      "|    critic_loss     | 0.000407 |\n",
      "|    ent_coef        | 0.0005   |\n",
      "|    ent_coef_loss   | 0.509    |\n",
      "|    learning_rate   | 0.00097  |\n",
      "|    n_updates       | 19680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4032000, episode_reward=2837.95 +/- 70.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4032000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4032     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6291     |\n",
      "|    total_timesteps | 4032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4033000, episode_reward=2934.49 +/- 54.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4033000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.000447 |\n",
      "|    ent_coef        | 0.0005   |\n",
      "|    ent_coef_loss   | 2.3      |\n",
      "|    learning_rate   | 0.000967 |\n",
      "|    n_updates       | 19690    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4034000, episode_reward=2930.92 +/- 52.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4034000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4035000, episode_reward=2848.03 +/- 20.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4035000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.000404 |\n",
      "|    ent_coef        | 0.000501 |\n",
      "|    ent_coef_loss   | 4.02     |\n",
      "|    learning_rate   | 0.000965 |\n",
      "|    n_updates       | 19700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4036000, episode_reward=2813.63 +/- 26.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4036000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4036     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6297     |\n",
      "|    total_timesteps | 4036000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4037000, episode_reward=2907.62 +/- 30.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4037000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.785   |\n",
      "|    critic_loss     | 0.000392 |\n",
      "|    ent_coef        | 0.000502 |\n",
      "|    ent_coef_loss   | 4.21     |\n",
      "|    learning_rate   | 0.000963 |\n",
      "|    n_updates       | 19710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4038000, episode_reward=2932.27 +/- 36.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4038000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4039000, episode_reward=2872.48 +/- 34.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4039000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.787   |\n",
      "|    critic_loss     | 0.000377 |\n",
      "|    ent_coef        | 0.000504 |\n",
      "|    ent_coef_loss   | 4.23     |\n",
      "|    learning_rate   | 0.000961 |\n",
      "|    n_updates       | 19720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4040000, episode_reward=2906.79 +/- 18.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4040000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4040     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6303     |\n",
      "|    total_timesteps | 4040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4041000, episode_reward=2960.11 +/- 16.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4041000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.000434 |\n",
      "|    ent_coef        | 0.000505 |\n",
      "|    ent_coef_loss   | 2.92     |\n",
      "|    learning_rate   | 0.000959 |\n",
      "|    n_updates       | 19730    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4042000, episode_reward=2941.20 +/- 71.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4042000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4043000, episode_reward=2954.37 +/- 29.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4043000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.632   |\n",
      "|    critic_loss     | 0.000513 |\n",
      "|    ent_coef        | 0.000508 |\n",
      "|    ent_coef_loss   | 14.2     |\n",
      "|    learning_rate   | 0.000957 |\n",
      "|    n_updates       | 19740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4044000, episode_reward=2969.18 +/- 38.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4044000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4044     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6310     |\n",
      "|    total_timesteps | 4044000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4045000, episode_reward=2940.31 +/- 27.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4045000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.723   |\n",
      "|    critic_loss     | 0.000496 |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | -3.82    |\n",
      "|    learning_rate   | 0.000955 |\n",
      "|    n_updates       | 19750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4046000, episode_reward=2953.25 +/- 36.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4046000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4047000, episode_reward=2901.58 +/- 23.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4047000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.792   |\n",
      "|    critic_loss     | 0.000469 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | 3.55     |\n",
      "|    learning_rate   | 0.000953 |\n",
      "|    n_updates       | 19760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4048000, episode_reward=2865.49 +/- 39.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4048000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4048     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6316     |\n",
      "|    total_timesteps | 4048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4049000, episode_reward=2922.86 +/- 26.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4049000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.778   |\n",
      "|    critic_loss     | 0.000475 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | -2.38    |\n",
      "|    learning_rate   | 0.000951 |\n",
      "|    n_updates       | 19770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050000, episode_reward=2899.57 +/- 41.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4050000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4051000, episode_reward=2876.79 +/- 29.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4051000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.000452 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | -2.74    |\n",
      "|    learning_rate   | 0.000949 |\n",
      "|    n_updates       | 19780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4052000, episode_reward=2832.41 +/- 35.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4052000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4052     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6323     |\n",
      "|    total_timesteps | 4052000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4053000, episode_reward=2804.13 +/- 24.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4053000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.000417 |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | -0.534   |\n",
      "|    learning_rate   | 0.000947 |\n",
      "|    n_updates       | 19790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4054000, episode_reward=2821.99 +/- 32.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4054000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4055000, episode_reward=2838.58 +/- 30.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4055000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4056000, episode_reward=2860.46 +/- 15.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4056000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.771   |\n",
      "|    critic_loss     | 0.000432 |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | -2.05    |\n",
      "|    learning_rate   | 0.000945 |\n",
      "|    n_updates       | 19800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4056     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6329     |\n",
      "|    total_timesteps | 4056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4057000, episode_reward=2877.23 +/- 25.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4057000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4058000, episode_reward=2931.05 +/- 42.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4058000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.783   |\n",
      "|    critic_loss     | 0.000431 |\n",
      "|    ent_coef        | 0.00051  |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.000943 |\n",
      "|    n_updates       | 19810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4059000, episode_reward=2934.83 +/- 13.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4059000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4060000, episode_reward=2887.91 +/- 5.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.000401 |\n",
      "|    ent_coef        | 0.00051  |\n",
      "|    ent_coef_loss   | -0.56    |\n",
      "|    learning_rate   | 0.000941 |\n",
      "|    n_updates       | 19820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4060     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6336     |\n",
      "|    total_timesteps | 4060000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4061000, episode_reward=2859.12 +/- 31.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4061000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4062000, episode_reward=2964.65 +/- 13.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4062000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.798   |\n",
      "|    critic_loss     | 0.000427 |\n",
      "|    ent_coef        | 0.00051  |\n",
      "|    ent_coef_loss   | 2.88     |\n",
      "|    learning_rate   | 0.000939 |\n",
      "|    n_updates       | 19830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4063000, episode_reward=2955.99 +/- 18.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4063000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4064000, episode_reward=2893.23 +/- 39.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4064000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000421 |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | 2.83     |\n",
      "|    learning_rate   | 0.000937 |\n",
      "|    n_updates       | 19840    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4064     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6342     |\n",
      "|    total_timesteps | 4064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4065000, episode_reward=2830.98 +/- 9.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4065000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4066000, episode_reward=2806.45 +/- 28.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4066000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.000407 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | 0.0898   |\n",
      "|    learning_rate   | 0.000935 |\n",
      "|    n_updates       | 19850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4067000, episode_reward=2796.68 +/- 29.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4067000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4068000, episode_reward=2880.27 +/- 41.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4068000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.785   |\n",
      "|    critic_loss     | 0.000438 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | 1.07     |\n",
      "|    learning_rate   | 0.000933 |\n",
      "|    n_updates       | 19860    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4068     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6348     |\n",
      "|    total_timesteps | 4068000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4069000, episode_reward=2867.83 +/- 27.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4069000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4070000, episode_reward=2684.73 +/- 21.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.000447 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | 0.128    |\n",
      "|    learning_rate   | 0.000931 |\n",
      "|    n_updates       | 19870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4071000, episode_reward=2683.68 +/- 49.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4071000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4072000, episode_reward=2837.42 +/- 34.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4072000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.000362 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.000929 |\n",
      "|    n_updates       | 19880    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4072     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6354     |\n",
      "|    total_timesteps | 4072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4073000, episode_reward=2800.52 +/- 14.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4073000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4074000, episode_reward=2719.94 +/- 44.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4074000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.786   |\n",
      "|    critic_loss     | 0.000404 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | 0.392    |\n",
      "|    learning_rate   | 0.000927 |\n",
      "|    n_updates       | 19890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4075000, episode_reward=2711.87 +/- 36.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4075000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4076000, episode_reward=2697.21 +/- 12.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4076000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.000397 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | -0.512   |\n",
      "|    learning_rate   | 0.000924 |\n",
      "|    n_updates       | 19900    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4076     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6360     |\n",
      "|    total_timesteps | 4076000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4077000, episode_reward=2673.19 +/- 43.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4077000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4078000, episode_reward=2751.34 +/- 34.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4078000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.783   |\n",
      "|    critic_loss     | 0.000418 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | -1.82    |\n",
      "|    learning_rate   | 0.000922 |\n",
      "|    n_updates       | 19910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4079000, episode_reward=2711.35 +/- 50.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4079000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4080000, episode_reward=2741.57 +/- 18.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.794   |\n",
      "|    critic_loss     | 0.000429 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.00092  |\n",
      "|    n_updates       | 19920    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4080     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6366     |\n",
      "|    total_timesteps | 4080000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4081000, episode_reward=2716.56 +/- 12.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4081000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4082000, episode_reward=2746.49 +/- 11.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4082000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.00043  |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | -0.494   |\n",
      "|    learning_rate   | 0.000918 |\n",
      "|    n_updates       | 19930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4083000, episode_reward=2722.23 +/- 31.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4083000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4084000, episode_reward=2719.60 +/- 26.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4084000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.775   |\n",
      "|    critic_loss     | 0.000425 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.000916 |\n",
      "|    n_updates       | 19940    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4084     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6373     |\n",
      "|    total_timesteps | 4084000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4085000, episode_reward=2710.32 +/- 26.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4085000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4086000, episode_reward=2667.04 +/- 24.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4086000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.000389 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.000914 |\n",
      "|    n_updates       | 19950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4087000, episode_reward=2694.63 +/- 12.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4087000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4088000, episode_reward=2740.66 +/- 13.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4088000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.00037  |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | 0.521    |\n",
      "|    learning_rate   | 0.000912 |\n",
      "|    n_updates       | 19960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4088     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6379     |\n",
      "|    total_timesteps | 4088000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4089000, episode_reward=2704.74 +/- 52.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4089000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4090000, episode_reward=2679.86 +/- 40.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.000409 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | 0.075    |\n",
      "|    learning_rate   | 0.00091  |\n",
      "|    n_updates       | 19970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4091000, episode_reward=2685.82 +/- 38.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4091000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4092000, episode_reward=2508.87 +/- 37.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4092000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.772   |\n",
      "|    critic_loss     | 0.00042  |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | 0.515    |\n",
      "|    learning_rate   | 0.000908 |\n",
      "|    n_updates       | 19980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4092     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6385     |\n",
      "|    total_timesteps | 4092000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4093000, episode_reward=2537.12 +/- 39.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4093000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4094000, episode_reward=2620.01 +/- 53.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4094000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.000381 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.000906 |\n",
      "|    n_updates       | 19990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4095000, episode_reward=2582.89 +/- 19.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4095000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4096000, episode_reward=2563.82 +/- 45.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4096000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4096     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6391     |\n",
      "|    total_timesteps | 4096000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4097000, episode_reward=2570.49 +/- 33.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4097000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.000393 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | -0.374   |\n",
      "|    learning_rate   | 0.000904 |\n",
      "|    n_updates       | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4098000, episode_reward=2568.34 +/- 48.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4098000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4099000, episode_reward=2641.51 +/- 55.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4099000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.764   |\n",
      "|    critic_loss     | 0.000376 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | -2.19    |\n",
      "|    learning_rate   | 0.000902 |\n",
      "|    n_updates       | 20010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4100000, episode_reward=2621.67 +/- 51.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4100000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4100     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6397     |\n",
      "|    total_timesteps | 4100000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4101000, episode_reward=2688.75 +/- 28.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4101000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.769   |\n",
      "|    critic_loss     | 0.000367 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | -0.296   |\n",
      "|    learning_rate   | 0.0009   |\n",
      "|    n_updates       | 20020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4102000, episode_reward=2674.08 +/- 24.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4102000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4103000, episode_reward=2717.27 +/- 34.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4103000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.774   |\n",
      "|    critic_loss     | 0.000377 |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | -0.226   |\n",
      "|    learning_rate   | 0.000898 |\n",
      "|    n_updates       | 20030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4104000, episode_reward=2695.52 +/- 21.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4104000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4104     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6403     |\n",
      "|    total_timesteps | 4104000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4105000, episode_reward=2564.52 +/- 45.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4105000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.793   |\n",
      "|    critic_loss     | 0.000405 |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | 2.43     |\n",
      "|    learning_rate   | 0.000896 |\n",
      "|    n_updates       | 20040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4106000, episode_reward=2561.78 +/- 15.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4106000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4107000, episode_reward=2695.66 +/- 45.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4107000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.76    |\n",
      "|    critic_loss     | 0.000335 |\n",
      "|    ent_coef        | 0.000512 |\n",
      "|    ent_coef_loss   | -3.23    |\n",
      "|    learning_rate   | 0.000894 |\n",
      "|    n_updates       | 20050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4108000, episode_reward=2686.33 +/- 44.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4108000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4108     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6409     |\n",
      "|    total_timesteps | 4108000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4109000, episode_reward=2626.42 +/- 14.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4109000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.761   |\n",
      "|    critic_loss     | 0.000351 |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | -4.31    |\n",
      "|    learning_rate   | 0.000892 |\n",
      "|    n_updates       | 20060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110000, episode_reward=2626.90 +/- 18.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4110000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4111000, episode_reward=2646.41 +/- 36.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4111000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.77    |\n",
      "|    critic_loss     | 0.00033  |\n",
      "|    ent_coef        | 0.000509 |\n",
      "|    ent_coef_loss   | -2.19    |\n",
      "|    learning_rate   | 0.00089  |\n",
      "|    n_updates       | 20070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4112000, episode_reward=2612.43 +/- 38.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4112000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4112     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6416     |\n",
      "|    total_timesteps | 4112000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4113000, episode_reward=2694.66 +/- 19.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4113000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.000369 |\n",
      "|    ent_coef        | 0.000508 |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.000888 |\n",
      "|    n_updates       | 20080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4114000, episode_reward=2657.15 +/- 51.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4114000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4115000, episode_reward=2780.58 +/- 46.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4115000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.777   |\n",
      "|    critic_loss     | 0.000411 |\n",
      "|    ent_coef        | 0.000508 |\n",
      "|    ent_coef_loss   | -0.281   |\n",
      "|    learning_rate   | 0.000886 |\n",
      "|    n_updates       | 20090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4116000, episode_reward=2782.96 +/- 39.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4116000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4116     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6422     |\n",
      "|    total_timesteps | 4116000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4117000, episode_reward=2760.04 +/- 14.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4117000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.774   |\n",
      "|    critic_loss     | 0.000413 |\n",
      "|    ent_coef        | 0.000507 |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.000884 |\n",
      "|    n_updates       | 20100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4118000, episode_reward=2790.43 +/- 42.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4118000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4119000, episode_reward=2923.06 +/- 21.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4119000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.00041  |\n",
      "|    ent_coef        | 0.000507 |\n",
      "|    ent_coef_loss   | -0.596   |\n",
      "|    learning_rate   | 0.000881 |\n",
      "|    n_updates       | 20110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4120000, episode_reward=2935.25 +/- 14.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4120000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4120     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6428     |\n",
      "|    total_timesteps | 4120000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4121000, episode_reward=2882.65 +/- 22.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4121000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.804   |\n",
      "|    critic_loss     | 0.000419 |\n",
      "|    ent_coef        | 0.000506 |\n",
      "|    ent_coef_loss   | 2.58     |\n",
      "|    learning_rate   | 0.000879 |\n",
      "|    n_updates       | 20120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4122000, episode_reward=2868.43 +/- 61.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4122000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4123000, episode_reward=2886.28 +/- 20.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4123000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000407 |\n",
      "|    ent_coef        | 0.000507 |\n",
      "|    ent_coef_loss   | 3.05     |\n",
      "|    learning_rate   | 0.000877 |\n",
      "|    n_updates       | 20130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4124000, episode_reward=2893.20 +/- 34.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4124000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4124     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6434     |\n",
      "|    total_timesteps | 4124000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4125000, episode_reward=2887.68 +/- 35.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4125000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.567   |\n",
      "|    critic_loss     | 0.000347 |\n",
      "|    ent_coef        | 0.000507 |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.000875 |\n",
      "|    n_updates       | 20140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4126000, episode_reward=2933.18 +/- 9.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4126000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4127000, episode_reward=2534.67 +/- 34.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4127000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000426 |\n",
      "|    ent_coef        | 0.000504 |\n",
      "|    ent_coef_loss   | 0.135    |\n",
      "|    learning_rate   | 0.000873 |\n",
      "|    n_updates       | 20150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4128000, episode_reward=2543.61 +/- 52.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4128000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4128     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6440     |\n",
      "|    total_timesteps | 4128000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4129000, episode_reward=2991.68 +/- 26.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4129000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.774   |\n",
      "|    critic_loss     | 0.000382 |\n",
      "|    ent_coef        | 0.000503 |\n",
      "|    ent_coef_loss   | -1.55    |\n",
      "|    learning_rate   | 0.000871 |\n",
      "|    n_updates       | 20160    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4130000, episode_reward=2977.23 +/- 22.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4130000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4131000, episode_reward=2938.47 +/- 19.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4131000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.000444 |\n",
      "|    ent_coef        | 0.000502 |\n",
      "|    ent_coef_loss   | 4.82     |\n",
      "|    learning_rate   | 0.000869 |\n",
      "|    n_updates       | 20170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4132000, episode_reward=2941.65 +/- 25.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4132000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4132     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6447     |\n",
      "|    total_timesteps | 4132000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4133000, episode_reward=3070.43 +/- 37.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4133000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.8     |\n",
      "|    critic_loss     | 0.00039  |\n",
      "|    ent_coef        | 0.000503 |\n",
      "|    ent_coef_loss   | 4.01     |\n",
      "|    learning_rate   | 0.000867 |\n",
      "|    n_updates       | 20180    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4134000, episode_reward=3080.63 +/- 34.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4134000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4135000, episode_reward=2996.12 +/- 41.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4135000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.807   |\n",
      "|    critic_loss     | 0.000441 |\n",
      "|    ent_coef        | 0.000505 |\n",
      "|    ent_coef_loss   | 4.36     |\n",
      "|    learning_rate   | 0.000865 |\n",
      "|    n_updates       | 20190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4136000, episode_reward=2995.01 +/- 53.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4136000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4136     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6453     |\n",
      "|    total_timesteps | 4136000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4137000, episode_reward=2988.08 +/- 26.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4137000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.804   |\n",
      "|    critic_loss     | 0.000483 |\n",
      "|    ent_coef        | 0.000507 |\n",
      "|    ent_coef_loss   | 4.54     |\n",
      "|    learning_rate   | 0.000863 |\n",
      "|    n_updates       | 20200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4138000, episode_reward=3012.00 +/- 26.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4138000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4139000, episode_reward=2989.82 +/- 31.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4139000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4140000, episode_reward=3002.06 +/- 46.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.808   |\n",
      "|    critic_loss     | 0.000493 |\n",
      "|    ent_coef        | 0.000509 |\n",
      "|    ent_coef_loss   | 6.55     |\n",
      "|    learning_rate   | 0.000861 |\n",
      "|    n_updates       | 20210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4140     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6460     |\n",
      "|    total_timesteps | 4140000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4141000, episode_reward=3018.20 +/- 31.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4141000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4142000, episode_reward=2991.56 +/- 6.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4142000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000429 |\n",
      "|    ent_coef        | 0.000511 |\n",
      "|    ent_coef_loss   | 1.59     |\n",
      "|    learning_rate   | 0.000859 |\n",
      "|    n_updates       | 20220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4143000, episode_reward=2987.61 +/- 18.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4143000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4144000, episode_reward=3070.87 +/- 19.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4144000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.000493 |\n",
      "|    ent_coef        | 0.000513 |\n",
      "|    ent_coef_loss   | 3.34     |\n",
      "|    learning_rate   | 0.000857 |\n",
      "|    n_updates       | 20230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4144     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6466     |\n",
      "|    total_timesteps | 4144000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4145000, episode_reward=3074.46 +/- 30.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4145000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4146000, episode_reward=3032.35 +/- 26.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4146000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.804   |\n",
      "|    critic_loss     | 0.000486 |\n",
      "|    ent_coef        | 0.000514 |\n",
      "|    ent_coef_loss   | 2.98     |\n",
      "|    learning_rate   | 0.000855 |\n",
      "|    n_updates       | 20240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4147000, episode_reward=3002.10 +/- 29.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4147000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4148000, episode_reward=3019.00 +/- 22.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4148000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.809   |\n",
      "|    critic_loss     | 0.000463 |\n",
      "|    ent_coef        | 0.000516 |\n",
      "|    ent_coef_loss   | 3.58     |\n",
      "|    learning_rate   | 0.000853 |\n",
      "|    n_updates       | 20250    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4148     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6472     |\n",
      "|    total_timesteps | 4148000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4149000, episode_reward=2995.42 +/- 30.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4149000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4150000, episode_reward=3016.32 +/- 34.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.8     |\n",
      "|    critic_loss     | 0.000459 |\n",
      "|    ent_coef        | 0.000517 |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.000851 |\n",
      "|    n_updates       | 20260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4151000, episode_reward=3009.43 +/- 34.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4151000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4152000, episode_reward=2906.00 +/- 13.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4152000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.793   |\n",
      "|    critic_loss     | 0.000507 |\n",
      "|    ent_coef        | 0.000518 |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.000849 |\n",
      "|    n_updates       | 20270    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4152     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6478     |\n",
      "|    total_timesteps | 4152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4153000, episode_reward=2898.12 +/- 10.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4153000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4154000, episode_reward=3075.46 +/- 19.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4154000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.000466 |\n",
      "|    ent_coef        | 0.000519 |\n",
      "|    ent_coef_loss   | 3.21     |\n",
      "|    learning_rate   | 0.000847 |\n",
      "|    n_updates       | 20280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4155000, episode_reward=3071.47 +/- 30.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4155000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4156000, episode_reward=3052.78 +/- 21.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4156000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.804   |\n",
      "|    critic_loss     | 0.000509 |\n",
      "|    ent_coef        | 0.00052  |\n",
      "|    ent_coef_loss   | 5.22     |\n",
      "|    learning_rate   | 0.000845 |\n",
      "|    n_updates       | 20290    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4156     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6484     |\n",
      "|    total_timesteps | 4156000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4157000, episode_reward=3044.65 +/- 14.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4157000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4158000, episode_reward=3029.74 +/- 20.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4158000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.792   |\n",
      "|    critic_loss     | 0.00057  |\n",
      "|    ent_coef        | 0.000522 |\n",
      "|    ent_coef_loss   | 4.31     |\n",
      "|    learning_rate   | 0.000843 |\n",
      "|    n_updates       | 20300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4159000, episode_reward=2989.78 +/- 31.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4159000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4160000, episode_reward=3028.54 +/- 24.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.797   |\n",
      "|    critic_loss     | 0.000506 |\n",
      "|    ent_coef        | 0.000524 |\n",
      "|    ent_coef_loss   | 3.46     |\n",
      "|    learning_rate   | 0.000841 |\n",
      "|    n_updates       | 20310    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4160     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6491     |\n",
      "|    total_timesteps | 4160000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4161000, episode_reward=3049.69 +/- 28.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4161000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4162000, episode_reward=3026.14 +/- 65.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4162000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.000513 |\n",
      "|    ent_coef        | 0.000526 |\n",
      "|    ent_coef_loss   | 3.95     |\n",
      "|    learning_rate   | 0.000838 |\n",
      "|    n_updates       | 20320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4163000, episode_reward=3085.70 +/- 26.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4163000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4164000, episode_reward=3050.01 +/- 23.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4164000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.671   |\n",
      "|    critic_loss     | 0.000534 |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | -5.72    |\n",
      "|    learning_rate   | 0.000836 |\n",
      "|    n_updates       | 20330    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4164     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6497     |\n",
      "|    total_timesteps | 4164000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4165000, episode_reward=3066.17 +/- 39.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4165000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4166000, episode_reward=2985.66 +/- 14.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4166000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.699   |\n",
      "|    critic_loss     | 0.000481 |\n",
      "|    ent_coef        | 0.000526 |\n",
      "|    ent_coef_loss   | -6.39    |\n",
      "|    learning_rate   | 0.000834 |\n",
      "|    n_updates       | 20340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4167000, episode_reward=2984.31 +/- 21.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4167000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4168000, episode_reward=3017.97 +/- 44.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4168000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.000493 |\n",
      "|    ent_coef        | 0.000525 |\n",
      "|    ent_coef_loss   | 4.02     |\n",
      "|    learning_rate   | 0.000832 |\n",
      "|    n_updates       | 20350    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4168     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6503     |\n",
      "|    total_timesteps | 4168000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4169000, episode_reward=3022.05 +/- 34.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4169000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4170000, episode_reward=3026.10 +/- 28.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.794   |\n",
      "|    critic_loss     | 0.000493 |\n",
      "|    ent_coef        | 0.000525 |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.00083  |\n",
      "|    n_updates       | 20360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4171000, episode_reward=2988.47 +/- 21.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4171000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4172000, episode_reward=3008.57 +/- 27.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4172000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.79    |\n",
      "|    critic_loss     | 0.000417 |\n",
      "|    ent_coef        | 0.000525 |\n",
      "|    ent_coef_loss   | -0.484   |\n",
      "|    learning_rate   | 0.000828 |\n",
      "|    n_updates       | 20370    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4172     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6509     |\n",
      "|    total_timesteps | 4172000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4173000, episode_reward=3016.83 +/- 11.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4173000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4174000, episode_reward=3039.76 +/- 17.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4174000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.795   |\n",
      "|    critic_loss     | 0.000508 |\n",
      "|    ent_coef        | 0.000525 |\n",
      "|    ent_coef_loss   | 2.56     |\n",
      "|    learning_rate   | 0.000826 |\n",
      "|    n_updates       | 20380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4175000, episode_reward=3038.12 +/- 18.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4175000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4176000, episode_reward=3032.28 +/- 38.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4176000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.79    |\n",
      "|    critic_loss     | 0.000514 |\n",
      "|    ent_coef        | 0.000526 |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.000824 |\n",
      "|    n_updates       | 20390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4176     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6515     |\n",
      "|    total_timesteps | 4176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4177000, episode_reward=3051.22 +/- 23.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4177000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4178000, episode_reward=3087.96 +/- 24.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4178000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.794   |\n",
      "|    critic_loss     | 0.000499 |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.000822 |\n",
      "|    n_updates       | 20400    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4179000, episode_reward=3074.62 +/- 32.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4179000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4180000, episode_reward=3087.59 +/- 24.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.797   |\n",
      "|    critic_loss     | 0.000518 |\n",
      "|    ent_coef        | 0.000528 |\n",
      "|    ent_coef_loss   | 2.55     |\n",
      "|    learning_rate   | 0.00082  |\n",
      "|    n_updates       | 20410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4180     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6522     |\n",
      "|    total_timesteps | 4180000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4181000, episode_reward=3091.97 +/- 23.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4181000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4182000, episode_reward=3096.62 +/- 33.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4182000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4183000, episode_reward=3053.73 +/- 18.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4183000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.797   |\n",
      "|    critic_loss     | 0.000471 |\n",
      "|    ent_coef        | 0.000529 |\n",
      "|    ent_coef_loss   | 1.68     |\n",
      "|    learning_rate   | 0.000818 |\n",
      "|    n_updates       | 20420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4184000, episode_reward=3031.43 +/- 43.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4184000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4184     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6528     |\n",
      "|    total_timesteps | 4184000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4185000, episode_reward=3029.98 +/- 26.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4185000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.794   |\n",
      "|    critic_loss     | 0.00046  |\n",
      "|    ent_coef        | 0.000529 |\n",
      "|    ent_coef_loss   | -0.31    |\n",
      "|    learning_rate   | 0.000816 |\n",
      "|    n_updates       | 20430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4186000, episode_reward=3061.04 +/- 15.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4186000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4187000, episode_reward=3053.39 +/- 25.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4187000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.796   |\n",
      "|    critic_loss     | 0.000479 |\n",
      "|    ent_coef        | 0.00053  |\n",
      "|    ent_coef_loss   | -0.707   |\n",
      "|    learning_rate   | 0.000814 |\n",
      "|    n_updates       | 20440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4188000, episode_reward=3039.87 +/- 26.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4188000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4188     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6536     |\n",
      "|    total_timesteps | 4188000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4189000, episode_reward=3003.85 +/- 24.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4189000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.806   |\n",
      "|    critic_loss     | 0.000467 |\n",
      "|    ent_coef        | 0.00053  |\n",
      "|    ent_coef_loss   | 3.73     |\n",
      "|    learning_rate   | 0.000812 |\n",
      "|    n_updates       | 20450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4190000, episode_reward=3026.95 +/- 18.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4190000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4191000, episode_reward=3075.92 +/- 11.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4191000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.000427 |\n",
      "|    ent_coef        | 0.000531 |\n",
      "|    ent_coef_loss   | 1.45     |\n",
      "|    learning_rate   | 0.00081  |\n",
      "|    n_updates       | 20460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4192000, episode_reward=3047.53 +/- 17.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4192000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4192     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6542     |\n",
      "|    total_timesteps | 4192000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4193000, episode_reward=3067.92 +/- 12.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4193000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.804   |\n",
      "|    critic_loss     | 0.000503 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | 2.28     |\n",
      "|    learning_rate   | 0.000808 |\n",
      "|    n_updates       | 20470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4194000, episode_reward=3069.35 +/- 38.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4194000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4195000, episode_reward=3003.58 +/- 27.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4195000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000415 |\n",
      "|    ent_coef        | 0.000533 |\n",
      "|    ent_coef_loss   | 2.37     |\n",
      "|    learning_rate   | 0.000806 |\n",
      "|    n_updates       | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4196000, episode_reward=3003.33 +/- 26.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4196000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4196     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6549     |\n",
      "|    total_timesteps | 4196000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4197000, episode_reward=3111.04 +/- 17.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4197000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.81    |\n",
      "|    critic_loss     | 0.000439 |\n",
      "|    ent_coef        | 0.000534 |\n",
      "|    ent_coef_loss   | 2.81     |\n",
      "|    learning_rate   | 0.000804 |\n",
      "|    n_updates       | 20490    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4198000, episode_reward=3099.02 +/- 31.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4198000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4199000, episode_reward=2968.14 +/- 26.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4199000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.817   |\n",
      "|    critic_loss     | 0.000462 |\n",
      "|    ent_coef        | 0.000535 |\n",
      "|    ent_coef_loss   | 2.07     |\n",
      "|    learning_rate   | 0.000802 |\n",
      "|    n_updates       | 20500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200000, episode_reward=2978.18 +/- 20.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4200000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4200     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6555     |\n",
      "|    total_timesteps | 4200000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4201000, episode_reward=2904.53 +/- 11.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4201000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.802   |\n",
      "|    critic_loss     | 0.00043  |\n",
      "|    ent_coef        | 0.000536 |\n",
      "|    ent_coef_loss   | 0.368    |\n",
      "|    learning_rate   | 0.0008   |\n",
      "|    n_updates       | 20510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4202000, episode_reward=2891.44 +/- 11.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4202000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4203000, episode_reward=2875.25 +/- 19.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4203000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.799   |\n",
      "|    critic_loss     | 0.000454 |\n",
      "|    ent_coef        | 0.000537 |\n",
      "|    ent_coef_loss   | 2.99     |\n",
      "|    learning_rate   | 0.000798 |\n",
      "|    n_updates       | 20520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4204000, episode_reward=2840.26 +/- 20.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4204000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4204     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6561     |\n",
      "|    total_timesteps | 4204000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4205000, episode_reward=2773.20 +/- 24.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4205000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.8     |\n",
      "|    critic_loss     | 0.00041  |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | -0.375   |\n",
      "|    learning_rate   | 0.000795 |\n",
      "|    n_updates       | 20530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4206000, episode_reward=2771.11 +/- 21.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4206000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4207000, episode_reward=2830.19 +/- 27.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4207000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.792   |\n",
      "|    critic_loss     | 0.000431 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.000793 |\n",
      "|    n_updates       | 20540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4208000, episode_reward=2850.99 +/- 20.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4208000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4208     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6568     |\n",
      "|    total_timesteps | 4208000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4209000, episode_reward=2770.53 +/- 24.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4209000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.798   |\n",
      "|    critic_loss     | 0.000421 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | 0.963    |\n",
      "|    learning_rate   | 0.000791 |\n",
      "|    n_updates       | 20550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4210000, episode_reward=2804.78 +/- 18.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4210000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4211000, episode_reward=2855.75 +/- 12.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4211000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.785   |\n",
      "|    critic_loss     | 0.000413 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.000789 |\n",
      "|    n_updates       | 20560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4212000, episode_reward=2869.30 +/- 18.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4212000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4212     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6574     |\n",
      "|    total_timesteps | 4212000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4213000, episode_reward=2742.20 +/- 14.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4213000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.793   |\n",
      "|    critic_loss     | 0.000441 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | 0.662    |\n",
      "|    learning_rate   | 0.000787 |\n",
      "|    n_updates       | 20570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4214000, episode_reward=2757.52 +/- 13.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4214000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4215000, episode_reward=2703.44 +/- 19.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4215000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.785   |\n",
      "|    critic_loss     | 0.000398 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | 0.105    |\n",
      "|    learning_rate   | 0.000785 |\n",
      "|    n_updates       | 20580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4216000, episode_reward=2711.42 +/- 28.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4216000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4216     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6580     |\n",
      "|    total_timesteps | 4216000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4217000, episode_reward=2595.05 +/- 31.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4217000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.000401 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | -0.0849  |\n",
      "|    learning_rate   | 0.000783 |\n",
      "|    n_updates       | 20590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4218000, episode_reward=2592.19 +/- 17.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4218000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4219000, episode_reward=2780.25 +/- 6.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4219000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.773   |\n",
      "|    critic_loss     | 0.000384 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | -4.05    |\n",
      "|    learning_rate   | 0.000781 |\n",
      "|    n_updates       | 20600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4220000, episode_reward=2767.34 +/- 52.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4220000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4220     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6586     |\n",
      "|    total_timesteps | 4220000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4221000, episode_reward=2180.70 +/- 1145.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4221000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.783   |\n",
      "|    critic_loss     | 0.00044  |\n",
      "|    ent_coef        | 0.000537 |\n",
      "|    ent_coef_loss   | -0.872   |\n",
      "|    learning_rate   | 0.000779 |\n",
      "|    n_updates       | 20610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4222000, episode_reward=2740.55 +/- 30.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4222000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4223000, episode_reward=2822.52 +/- 7.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4223000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.000412 |\n",
      "|    ent_coef        | 0.000536 |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.000777 |\n",
      "|    n_updates       | 20620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4224000, episode_reward=2809.89 +/- 19.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4224000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4224     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6592     |\n",
      "|    total_timesteps | 4224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4225000, episode_reward=2809.99 +/- 11.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4225000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4226000, episode_reward=2647.13 +/- 11.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4226000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.773   |\n",
      "|    critic_loss     | 0.000426 |\n",
      "|    ent_coef        | 0.000535 |\n",
      "|    ent_coef_loss   | -2.37    |\n",
      "|    learning_rate   | 0.000775 |\n",
      "|    n_updates       | 20630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4227000, episode_reward=2663.58 +/- 37.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4227000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4228000, episode_reward=2781.05 +/- 33.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4228000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.767   |\n",
      "|    critic_loss     | 0.00038  |\n",
      "|    ent_coef        | 0.000534 |\n",
      "|    ent_coef_loss   | -4.98    |\n",
      "|    learning_rate   | 0.000773 |\n",
      "|    n_updates       | 20640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4228     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6598     |\n",
      "|    total_timesteps | 4228000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4229000, episode_reward=2776.87 +/- 25.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4229000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4230000, episode_reward=2829.65 +/- 33.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.775   |\n",
      "|    critic_loss     | 0.000419 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | 0.0463   |\n",
      "|    learning_rate   | 0.000771 |\n",
      "|    n_updates       | 20650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4231000, episode_reward=2845.93 +/- 16.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4231000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4232000, episode_reward=2955.74 +/- 26.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4232000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.000427 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | -0.72    |\n",
      "|    learning_rate   | 0.000769 |\n",
      "|    n_updates       | 20660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4232     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6605     |\n",
      "|    total_timesteps | 4232000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4233000, episode_reward=2984.09 +/- 16.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4233000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4234000, episode_reward=3046.65 +/- 30.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4234000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.797   |\n",
      "|    critic_loss     | 0.000445 |\n",
      "|    ent_coef        | 0.000531 |\n",
      "|    ent_coef_loss   | 3.4      |\n",
      "|    learning_rate   | 0.000767 |\n",
      "|    n_updates       | 20670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4235000, episode_reward=3047.46 +/- 3.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4235000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4236000, episode_reward=3035.64 +/- 34.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4236000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.805   |\n",
      "|    critic_loss     | 0.000459 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | 2.38     |\n",
      "|    learning_rate   | 0.000765 |\n",
      "|    n_updates       | 20680    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4236     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6611     |\n",
      "|    total_timesteps | 4236000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4237000, episode_reward=3048.66 +/- 40.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4237000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4238000, episode_reward=3067.30 +/- 29.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4238000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000501 |\n",
      "|    ent_coef        | 0.000533 |\n",
      "|    ent_coef_loss   | 3.15     |\n",
      "|    learning_rate   | 0.000763 |\n",
      "|    n_updates       | 20690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4239000, episode_reward=3076.38 +/- 35.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4239000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4240000, episode_reward=3005.69 +/- 39.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.000469 |\n",
      "|    ent_coef        | 0.000535 |\n",
      "|    ent_coef_loss   | 2.2      |\n",
      "|    learning_rate   | 0.000761 |\n",
      "|    n_updates       | 20700    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4240     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6617     |\n",
      "|    total_timesteps | 4240000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4241000, episode_reward=3020.40 +/- 20.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4241000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4242000, episode_reward=3063.25 +/- 16.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4242000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000457 |\n",
      "|    ent_coef        | 0.000536 |\n",
      "|    ent_coef_loss   | 1.66     |\n",
      "|    learning_rate   | 0.000759 |\n",
      "|    n_updates       | 20710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4243000, episode_reward=3069.63 +/- 40.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4243000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4244000, episode_reward=2982.57 +/- 55.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4244000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.799   |\n",
      "|    critic_loss     | 0.0005   |\n",
      "|    ent_coef        | 0.000537 |\n",
      "|    ent_coef_loss   | 0.217    |\n",
      "|    learning_rate   | 0.000757 |\n",
      "|    n_updates       | 20720    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4244     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6623     |\n",
      "|    total_timesteps | 4244000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4245000, episode_reward=2975.77 +/- 41.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4245000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4246000, episode_reward=2974.17 +/- 19.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4246000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000449 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | 1.84     |\n",
      "|    learning_rate   | 0.000754 |\n",
      "|    n_updates       | 20730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4247000, episode_reward=2955.10 +/- 17.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4247000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4248000, episode_reward=2883.16 +/- 12.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4248000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.795   |\n",
      "|    critic_loss     | 0.000477 |\n",
      "|    ent_coef        | 0.000538 |\n",
      "|    ent_coef_loss   | 2.24     |\n",
      "|    learning_rate   | 0.000752 |\n",
      "|    n_updates       | 20740    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4248     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6629     |\n",
      "|    total_timesteps | 4248000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4249000, episode_reward=2894.83 +/- 29.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4249000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4250000, episode_reward=2857.53 +/- 21.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.793   |\n",
      "|    critic_loss     | 0.000529 |\n",
      "|    ent_coef        | 0.000539 |\n",
      "|    ent_coef_loss   | -0.0645  |\n",
      "|    learning_rate   | 0.00075  |\n",
      "|    n_updates       | 20750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4251000, episode_reward=2928.86 +/- 27.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4251000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4252000, episode_reward=2816.30 +/- 19.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4252000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.786   |\n",
      "|    critic_loss     | 0.000478 |\n",
      "|    ent_coef        | 0.00054  |\n",
      "|    ent_coef_loss   | 0.886    |\n",
      "|    learning_rate   | 0.000748 |\n",
      "|    n_updates       | 20760    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4252     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6636     |\n",
      "|    total_timesteps | 4252000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4253000, episode_reward=2799.36 +/- 15.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4253000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4254000, episode_reward=2839.18 +/- 29.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4254000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.000449 |\n",
      "|    ent_coef        | 0.00054  |\n",
      "|    ent_coef_loss   | -0.501   |\n",
      "|    learning_rate   | 0.000746 |\n",
      "|    n_updates       | 20770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4255000, episode_reward=2811.12 +/- 17.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4255000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4256000, episode_reward=2915.85 +/- 43.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4256000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.783   |\n",
      "|    critic_loss     | 0.000454 |\n",
      "|    ent_coef        | 0.00054  |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.000744 |\n",
      "|    n_updates       | 20780    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4256     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6642     |\n",
      "|    total_timesteps | 4256000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4257000, episode_reward=2891.54 +/- 22.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4257000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4258000, episode_reward=3085.45 +/- 43.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4258000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.000444 |\n",
      "|    ent_coef        | 0.000539 |\n",
      "|    ent_coef_loss   | 0.878    |\n",
      "|    learning_rate   | 0.000742 |\n",
      "|    n_updates       | 20790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4259000, episode_reward=3130.70 +/- 30.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4259000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4260000, episode_reward=3062.46 +/- 32.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.773   |\n",
      "|    critic_loss     | 0.000517 |\n",
      "|    ent_coef        | 0.00054  |\n",
      "|    ent_coef_loss   | 0.991    |\n",
      "|    learning_rate   | 0.00074  |\n",
      "|    n_updates       | 20800    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4260     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6648     |\n",
      "|    total_timesteps | 4260000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4261000, episode_reward=3073.77 +/- 21.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4261000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4262000, episode_reward=3015.88 +/- 51.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4262000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.000513 |\n",
      "|    ent_coef        | 0.00054  |\n",
      "|    ent_coef_loss   | 3.03     |\n",
      "|    learning_rate   | 0.000738 |\n",
      "|    n_updates       | 20810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4263000, episode_reward=3043.15 +/- 13.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4263000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4264000, episode_reward=3063.98 +/- 33.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4264000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.807   |\n",
      "|    critic_loss     | 0.000487 |\n",
      "|    ent_coef        | 0.000541 |\n",
      "|    ent_coef_loss   | 2.4      |\n",
      "|    learning_rate   | 0.000736 |\n",
      "|    n_updates       | 20820    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4264     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6654     |\n",
      "|    total_timesteps | 4264000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4265000, episode_reward=3079.85 +/- 39.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4265000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4266000, episode_reward=3070.02 +/- 23.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4266000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.787   |\n",
      "|    critic_loss     | 0.000569 |\n",
      "|    ent_coef        | 0.000542 |\n",
      "|    ent_coef_loss   | 2.39     |\n",
      "|    learning_rate   | 0.000734 |\n",
      "|    n_updates       | 20830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4267000, episode_reward=3070.05 +/- 37.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4267000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4268000, episode_reward=3072.01 +/- 15.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4268000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4268     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6660     |\n",
      "|    total_timesteps | 4268000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4269000, episode_reward=3189.98 +/- 18.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4269000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.000506 |\n",
      "|    ent_coef        | 0.000544 |\n",
      "|    ent_coef_loss   | 6.88     |\n",
      "|    learning_rate   | 0.000732 |\n",
      "|    n_updates       | 20840    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4270000, episode_reward=3186.58 +/- 33.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4270000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4271000, episode_reward=3208.42 +/- 28.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4271000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.815   |\n",
      "|    critic_loss     | 0.000484 |\n",
      "|    ent_coef        | 0.000547 |\n",
      "|    ent_coef_loss   | 3.98     |\n",
      "|    learning_rate   | 0.00073  |\n",
      "|    n_updates       | 20850    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4272000, episode_reward=3234.71 +/- 7.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4272000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4272     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6667     |\n",
      "|    total_timesteps | 4272000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4273000, episode_reward=3189.45 +/- 7.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4273000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.802   |\n",
      "|    critic_loss     | 0.000558 |\n",
      "|    ent_coef        | 0.000549 |\n",
      "|    ent_coef_loss   | 2.55     |\n",
      "|    learning_rate   | 0.000728 |\n",
      "|    n_updates       | 20860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4274000, episode_reward=3209.50 +/- 41.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4274000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4275000, episode_reward=3138.13 +/- 52.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4275000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.818   |\n",
      "|    critic_loss     | 0.000532 |\n",
      "|    ent_coef        | 0.000551 |\n",
      "|    ent_coef_loss   | 7.23     |\n",
      "|    learning_rate   | 0.000726 |\n",
      "|    n_updates       | 20870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4276000, episode_reward=3154.17 +/- 32.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4276000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4276     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6673     |\n",
      "|    total_timesteps | 4276000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4277000, episode_reward=3037.68 +/- 16.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4277000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.816   |\n",
      "|    critic_loss     | 0.000523 |\n",
      "|    ent_coef        | 0.000555 |\n",
      "|    ent_coef_loss   | 3.89     |\n",
      "|    learning_rate   | 0.000724 |\n",
      "|    n_updates       | 20880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4278000, episode_reward=3029.15 +/- 15.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4278000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4279000, episode_reward=3095.55 +/- 54.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4279000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.737   |\n",
      "|    critic_loss     | 0.000476 |\n",
      "|    ent_coef        | 0.000556 |\n",
      "|    ent_coef_loss   | -6.97    |\n",
      "|    learning_rate   | 0.000722 |\n",
      "|    n_updates       | 20890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4280000, episode_reward=3125.44 +/- 38.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4280000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4280     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6680     |\n",
      "|    total_timesteps | 4280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4281000, episode_reward=3086.45 +/- 23.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4281000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.629   |\n",
      "|    critic_loss     | 0.00035  |\n",
      "|    ent_coef        | 0.000554 |\n",
      "|    ent_coef_loss   | -19.7    |\n",
      "|    learning_rate   | 0.00072  |\n",
      "|    n_updates       | 20900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4282000, episode_reward=3081.04 +/- 28.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4282000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4283000, episode_reward=3165.65 +/- 30.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4283000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.797   |\n",
      "|    critic_loss     | 0.000494 |\n",
      "|    ent_coef        | 0.000548 |\n",
      "|    ent_coef_loss   | 2.49     |\n",
      "|    learning_rate   | 0.000718 |\n",
      "|    n_updates       | 20910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4284000, episode_reward=3186.72 +/- 49.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4284000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4284     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6686     |\n",
      "|    total_timesteps | 4284000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4285000, episode_reward=3221.70 +/- 15.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4285000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.803   |\n",
      "|    critic_loss     | 0.000537 |\n",
      "|    ent_coef        | 0.000546 |\n",
      "|    ent_coef_loss   | 4.43     |\n",
      "|    learning_rate   | 0.000716 |\n",
      "|    n_updates       | 20920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4286000, episode_reward=3243.36 +/- 37.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4286000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4287000, episode_reward=3175.16 +/- 28.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4287000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.815   |\n",
      "|    critic_loss     | 0.00057  |\n",
      "|    ent_coef        | 0.000547 |\n",
      "|    ent_coef_loss   | 6.84     |\n",
      "|    learning_rate   | 0.000714 |\n",
      "|    n_updates       | 20930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4288000, episode_reward=3190.81 +/- 44.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4288000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4288     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6692     |\n",
      "|    total_timesteps | 4288000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4289000, episode_reward=3228.75 +/- 32.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4289000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.802   |\n",
      "|    critic_loss     | 0.00053  |\n",
      "|    ent_coef        | 0.00055  |\n",
      "|    ent_coef_loss   | 2.66     |\n",
      "|    learning_rate   | 0.000711 |\n",
      "|    n_updates       | 20940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4290000, episode_reward=3227.17 +/- 40.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4290000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4291000, episode_reward=3104.41 +/- 44.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4291000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.802   |\n",
      "|    critic_loss     | 0.000587 |\n",
      "|    ent_coef        | 0.000552 |\n",
      "|    ent_coef_loss   | 1.62     |\n",
      "|    learning_rate   | 0.000709 |\n",
      "|    n_updates       | 20950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4292000, episode_reward=3122.08 +/- 42.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4292000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4292     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6698     |\n",
      "|    total_timesteps | 4292000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4293000, episode_reward=3239.21 +/- 64.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4293000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.8     |\n",
      "|    critic_loss     | 0.000499 |\n",
      "|    ent_coef        | 0.000553 |\n",
      "|    ent_coef_loss   | 3.04     |\n",
      "|    learning_rate   | 0.000707 |\n",
      "|    n_updates       | 20960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4294000, episode_reward=3228.25 +/- 27.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4294000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4295000, episode_reward=3194.91 +/- 45.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4295000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.806   |\n",
      "|    critic_loss     | 0.000509 |\n",
      "|    ent_coef        | 0.000554 |\n",
      "|    ent_coef_loss   | 2.14     |\n",
      "|    learning_rate   | 0.000705 |\n",
      "|    n_updates       | 20970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4296000, episode_reward=3205.50 +/- 26.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4296000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4296     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6705     |\n",
      "|    total_timesteps | 4296000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4297000, episode_reward=3191.74 +/- 23.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4297000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.804   |\n",
      "|    critic_loss     | 0.000577 |\n",
      "|    ent_coef        | 0.000556 |\n",
      "|    ent_coef_loss   | 2.12     |\n",
      "|    learning_rate   | 0.000703 |\n",
      "|    n_updates       | 20980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4298000, episode_reward=3225.19 +/- 35.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4298000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4299000, episode_reward=3181.43 +/- 16.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4299000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.811   |\n",
      "|    critic_loss     | 0.000559 |\n",
      "|    ent_coef        | 0.000557 |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.000701 |\n",
      "|    n_updates       | 20990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4300000, episode_reward=3161.35 +/- 38.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4300000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4300     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6711     |\n",
      "|    total_timesteps | 4300000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4301000, episode_reward=3260.87 +/- 21.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4301000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.802   |\n",
      "|    critic_loss     | 0.000543 |\n",
      "|    ent_coef        | 0.000558 |\n",
      "|    ent_coef_loss   | 3.56     |\n",
      "|    learning_rate   | 0.000699 |\n",
      "|    n_updates       | 21000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4302000, episode_reward=3234.35 +/- 26.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4302000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4303000, episode_reward=3185.51 +/- 18.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4303000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.799   |\n",
      "|    critic_loss     | 0.000542 |\n",
      "|    ent_coef        | 0.00056  |\n",
      "|    ent_coef_loss   | 3.09     |\n",
      "|    learning_rate   | 0.000697 |\n",
      "|    n_updates       | 21010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4304000, episode_reward=3188.04 +/- 30.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4304000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4304     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6717     |\n",
      "|    total_timesteps | 4304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4305000, episode_reward=3236.93 +/- 58.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4305000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.808   |\n",
      "|    critic_loss     | 0.000566 |\n",
      "|    ent_coef        | 0.000562 |\n",
      "|    ent_coef_loss   | 4.18     |\n",
      "|    learning_rate   | 0.000695 |\n",
      "|    n_updates       | 21020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4306000, episode_reward=3250.21 +/- 35.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4306000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4307000, episode_reward=3256.26 +/- 44.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4307000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000607 |\n",
      "|    ent_coef        | 0.000564 |\n",
      "|    ent_coef_loss   | 2.84     |\n",
      "|    learning_rate   | 0.000693 |\n",
      "|    n_updates       | 21030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4308000, episode_reward=3269.30 +/- 51.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4308000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4308     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6724     |\n",
      "|    total_timesteps | 4308000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4309000, episode_reward=3202.09 +/- 30.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4309000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.797   |\n",
      "|    critic_loss     | 0.000531 |\n",
      "|    ent_coef        | 0.000566 |\n",
      "|    ent_coef_loss   | 1.59     |\n",
      "|    learning_rate   | 0.000691 |\n",
      "|    n_updates       | 21040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4310000, episode_reward=3205.56 +/- 71.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4310000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4311000, episode_reward=3228.98 +/- 40.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4311000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4312000, episode_reward=3247.40 +/- 34.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4312000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.789   |\n",
      "|    critic_loss     | 0.000568 |\n",
      "|    ent_coef        | 0.000567 |\n",
      "|    ent_coef_loss   | 1.59     |\n",
      "|    learning_rate   | 0.000689 |\n",
      "|    n_updates       | 21050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4312     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6730     |\n",
      "|    total_timesteps | 4312000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4313000, episode_reward=3198.37 +/- 29.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4313000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4314000, episode_reward=3212.75 +/- 32.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4314000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.787   |\n",
      "|    critic_loss     | 0.000583 |\n",
      "|    ent_coef        | 0.000568 |\n",
      "|    ent_coef_loss   | 0.912    |\n",
      "|    learning_rate   | 0.000687 |\n",
      "|    n_updates       | 21060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4315000, episode_reward=3199.13 +/- 41.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4315000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4316000, episode_reward=3200.49 +/- 27.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4316000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.787   |\n",
      "|    critic_loss     | 0.000581 |\n",
      "|    ent_coef        | 0.000568 |\n",
      "|    ent_coef_loss   | 0.364    |\n",
      "|    learning_rate   | 0.000685 |\n",
      "|    n_updates       | 21070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4316     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6736     |\n",
      "|    total_timesteps | 4316000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4317000, episode_reward=3179.41 +/- 41.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4317000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4318000, episode_reward=3164.74 +/- 55.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4318000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.778   |\n",
      "|    critic_loss     | 0.000578 |\n",
      "|    ent_coef        | 0.000569 |\n",
      "|    ent_coef_loss   | 1.72     |\n",
      "|    learning_rate   | 0.000683 |\n",
      "|    n_updates       | 21080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4319000, episode_reward=3195.99 +/- 17.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4319000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320000, episode_reward=3139.71 +/- 59.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.786   |\n",
      "|    critic_loss     | 0.00059  |\n",
      "|    ent_coef        | 0.00057  |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.000681 |\n",
      "|    n_updates       | 21090    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4320     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6742     |\n",
      "|    total_timesteps | 4320000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4321000, episode_reward=3199.44 +/- 17.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4321000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4322000, episode_reward=3130.97 +/- 16.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4322000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.785   |\n",
      "|    critic_loss     | 0.00054  |\n",
      "|    ent_coef        | 0.00057  |\n",
      "|    ent_coef_loss   | -0.306   |\n",
      "|    learning_rate   | 0.000679 |\n",
      "|    n_updates       | 21100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4323000, episode_reward=3136.05 +/- 33.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4323000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4324000, episode_reward=3144.48 +/- 24.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4324000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.000547 |\n",
      "|    ent_coef        | 0.00057  |\n",
      "|    ent_coef_loss   | -0.856   |\n",
      "|    learning_rate   | 0.000677 |\n",
      "|    n_updates       | 21110    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4324     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6749     |\n",
      "|    total_timesteps | 4324000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4325000, episode_reward=3155.28 +/- 27.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4325000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4326000, episode_reward=3216.66 +/- 36.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4326000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.779   |\n",
      "|    critic_loss     | 0.000531 |\n",
      "|    ent_coef        | 0.00057  |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.000675 |\n",
      "|    n_updates       | 21120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4327000, episode_reward=3171.27 +/- 26.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4327000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4328000, episode_reward=3169.50 +/- 25.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4328000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.779   |\n",
      "|    critic_loss     | 0.000542 |\n",
      "|    ent_coef        | 0.000569 |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.000673 |\n",
      "|    n_updates       | 21130    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4328     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6755     |\n",
      "|    total_timesteps | 4328000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4329000, episode_reward=3225.43 +/- 22.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4329000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4330000, episode_reward=3193.05 +/- 52.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.801   |\n",
      "|    critic_loss     | 0.000519 |\n",
      "|    ent_coef        | 0.00057  |\n",
      "|    ent_coef_loss   | 3.17     |\n",
      "|    learning_rate   | 0.000671 |\n",
      "|    n_updates       | 21140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4331000, episode_reward=3227.06 +/- 44.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4331000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4332000, episode_reward=3248.22 +/- 19.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4332000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.797   |\n",
      "|    critic_loss     | 0.000536 |\n",
      "|    ent_coef        | 0.000571 |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.000668 |\n",
      "|    n_updates       | 21150    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4332     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6761     |\n",
      "|    total_timesteps | 4332000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4333000, episode_reward=3203.31 +/- 42.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4333000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4334000, episode_reward=3245.00 +/- 36.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4334000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.79    |\n",
      "|    critic_loss     | 0.000563 |\n",
      "|    ent_coef        | 0.000572 |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.000666 |\n",
      "|    n_updates       | 21160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4335000, episode_reward=3236.32 +/- 34.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4335000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4336000, episode_reward=3238.38 +/- 43.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4336000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.787   |\n",
      "|    critic_loss     | 0.000611 |\n",
      "|    ent_coef        | 0.000573 |\n",
      "|    ent_coef_loss   | 2.46     |\n",
      "|    learning_rate   | 0.000664 |\n",
      "|    n_updates       | 21170    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4336     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6767     |\n",
      "|    total_timesteps | 4336000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4337000, episode_reward=3204.35 +/- 27.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4337000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4338000, episode_reward=3294.64 +/- 14.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4338000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.772   |\n",
      "|    critic_loss     | 0.000699 |\n",
      "|    ent_coef        | 0.000574 |\n",
      "|    ent_coef_loss   | -0.172   |\n",
      "|    learning_rate   | 0.000662 |\n",
      "|    n_updates       | 21180    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4339000, episode_reward=3274.60 +/- 47.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4339000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4340000, episode_reward=3145.47 +/- 45.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.000654 |\n",
      "|    ent_coef        | 0.000574 |\n",
      "|    ent_coef_loss   | 0.99     |\n",
      "|    learning_rate   | 0.00066  |\n",
      "|    n_updates       | 21190    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4340     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6774     |\n",
      "|    total_timesteps | 4340000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4341000, episode_reward=3190.88 +/- 17.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4341000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4342000, episode_reward=3185.84 +/- 40.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4342000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.000565 |\n",
      "|    ent_coef        | 0.000575 |\n",
      "|    ent_coef_loss   | -0.362   |\n",
      "|    learning_rate   | 0.000658 |\n",
      "|    n_updates       | 21200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4343000, episode_reward=3225.04 +/- 42.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4343000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4344000, episode_reward=3181.96 +/- 20.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4344000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.779   |\n",
      "|    critic_loss     | 0.000556 |\n",
      "|    ent_coef        | 0.000575 |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.000656 |\n",
      "|    n_updates       | 21210    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4344     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6780     |\n",
      "|    total_timesteps | 4344000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4345000, episode_reward=3188.05 +/- 27.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4345000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4346000, episode_reward=3192.62 +/- 23.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4346000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.777   |\n",
      "|    critic_loss     | 0.000547 |\n",
      "|    ent_coef        | 0.000574 |\n",
      "|    ent_coef_loss   | -0.533   |\n",
      "|    learning_rate   | 0.000654 |\n",
      "|    n_updates       | 21220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4347000, episode_reward=3199.13 +/- 32.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4347000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4348000, episode_reward=3174.08 +/- 61.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4348000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.787   |\n",
      "|    critic_loss     | 0.00054  |\n",
      "|    ent_coef        | 0.000574 |\n",
      "|    ent_coef_loss   | 2.76     |\n",
      "|    learning_rate   | 0.000652 |\n",
      "|    n_updates       | 21230    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4348     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6786     |\n",
      "|    total_timesteps | 4348000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4349000, episode_reward=3172.72 +/- 48.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4349000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4350000, episode_reward=3184.50 +/- 13.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.78    |\n",
      "|    critic_loss     | 0.00053  |\n",
      "|    ent_coef        | 0.000575 |\n",
      "|    ent_coef_loss   | 1.72     |\n",
      "|    learning_rate   | 0.00065  |\n",
      "|    n_updates       | 21240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4351000, episode_reward=3191.39 +/- 40.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4351000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4352000, episode_reward=3151.41 +/- 34.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4352000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4352     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6792     |\n",
      "|    total_timesteps | 4352000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4353000, episode_reward=3200.45 +/- 37.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4353000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.000519 |\n",
      "|    ent_coef        | 0.000576 |\n",
      "|    ent_coef_loss   | -0.869   |\n",
      "|    learning_rate   | 0.000648 |\n",
      "|    n_updates       | 21250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4354000, episode_reward=3226.54 +/- 37.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4354000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4355000, episode_reward=3268.99 +/- 31.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4355000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.777   |\n",
      "|    critic_loss     | 0.000575 |\n",
      "|    ent_coef        | 0.000576 |\n",
      "|    ent_coef_loss   | 2.17     |\n",
      "|    learning_rate   | 0.000646 |\n",
      "|    n_updates       | 21260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4356000, episode_reward=3254.01 +/- 57.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4356000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4356     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6799     |\n",
      "|    total_timesteps | 4356000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4357000, episode_reward=3232.27 +/- 58.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4357000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.793   |\n",
      "|    critic_loss     | 0.000582 |\n",
      "|    ent_coef        | 0.000577 |\n",
      "|    ent_coef_loss   | 4.71     |\n",
      "|    learning_rate   | 0.000644 |\n",
      "|    n_updates       | 21270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4358000, episode_reward=3239.93 +/- 39.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4358000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4359000, episode_reward=3259.97 +/- 31.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4359000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.792   |\n",
      "|    critic_loss     | 0.000597 |\n",
      "|    ent_coef        | 0.000579 |\n",
      "|    ent_coef_loss   | 3.6      |\n",
      "|    learning_rate   | 0.000642 |\n",
      "|    n_updates       | 21280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4360000, episode_reward=3250.12 +/- 17.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4360000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4360     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6805     |\n",
      "|    total_timesteps | 4360000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4361000, episode_reward=3223.86 +/- 46.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4361000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.792   |\n",
      "|    critic_loss     | 0.000622 |\n",
      "|    ent_coef        | 0.000581 |\n",
      "|    ent_coef_loss   | 4.78     |\n",
      "|    learning_rate   | 0.00064  |\n",
      "|    n_updates       | 21290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4362000, episode_reward=3248.21 +/- 39.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4362000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4363000, episode_reward=3252.89 +/- 20.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4363000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.788   |\n",
      "|    critic_loss     | 0.000593 |\n",
      "|    ent_coef        | 0.000584 |\n",
      "|    ent_coef_loss   | 3.64     |\n",
      "|    learning_rate   | 0.000638 |\n",
      "|    n_updates       | 21300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4364000, episode_reward=3258.70 +/- 30.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4364000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4364     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6811     |\n",
      "|    total_timesteps | 4364000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4365000, episode_reward=3235.78 +/- 23.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4365000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.000619 |\n",
      "|    ent_coef        | 0.000586 |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.000636 |\n",
      "|    n_updates       | 21310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4366000, episode_reward=3246.92 +/- 42.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4366000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4367000, episode_reward=3249.44 +/- 35.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4367000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.792   |\n",
      "|    critic_loss     | 0.000587 |\n",
      "|    ent_coef        | 0.000588 |\n",
      "|    ent_coef_loss   | 2.78     |\n",
      "|    learning_rate   | 0.000634 |\n",
      "|    n_updates       | 21320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4368000, episode_reward=3245.06 +/- 37.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4368000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4368     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6817     |\n",
      "|    total_timesteps | 4368000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4369000, episode_reward=3221.17 +/- 19.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4369000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.771   |\n",
      "|    critic_loss     | 0.000597 |\n",
      "|    ent_coef        | 0.000589 |\n",
      "|    ent_coef_loss   | -0.417   |\n",
      "|    learning_rate   | 0.000632 |\n",
      "|    n_updates       | 21330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4370000, episode_reward=3260.56 +/- 32.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4370000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4371000, episode_reward=3251.31 +/- 30.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4371000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.000536 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | -0.76    |\n",
      "|    learning_rate   | 0.00063  |\n",
      "|    n_updates       | 21340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4372000, episode_reward=3221.24 +/- 14.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4372000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4372     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6823     |\n",
      "|    total_timesteps | 4372000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4373000, episode_reward=3259.78 +/- 38.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4373000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.793   |\n",
      "|    critic_loss     | 0.000585 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | 1.47     |\n",
      "|    learning_rate   | 0.000628 |\n",
      "|    n_updates       | 21350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4374000, episode_reward=3272.27 +/- 16.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4374000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4375000, episode_reward=3160.71 +/- 31.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4375000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.786   |\n",
      "|    critic_loss     | 0.000622 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | 0.0626   |\n",
      "|    learning_rate   | 0.000625 |\n",
      "|    n_updates       | 21360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4376000, episode_reward=3140.01 +/- 46.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4376000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4376     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6830     |\n",
      "|    total_timesteps | 4376000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4377000, episode_reward=3259.77 +/- 17.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4377000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.788   |\n",
      "|    critic_loss     | 0.00056  |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | 0.214    |\n",
      "|    learning_rate   | 0.000623 |\n",
      "|    n_updates       | 21370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4378000, episode_reward=3261.42 +/- 36.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4378000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4379000, episode_reward=3053.94 +/- 25.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4379000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.789   |\n",
      "|    critic_loss     | 0.000577 |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | 3.27     |\n",
      "|    learning_rate   | 0.000621 |\n",
      "|    n_updates       | 21380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380000, episode_reward=3034.43 +/- 21.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4380000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4380     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6836     |\n",
      "|    total_timesteps | 4380000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4381000, episode_reward=3076.14 +/- 35.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4381000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.771   |\n",
      "|    critic_loss     | 0.000516 |\n",
      "|    ent_coef        | 0.000592 |\n",
      "|    ent_coef_loss   | -2.84    |\n",
      "|    learning_rate   | 0.000619 |\n",
      "|    n_updates       | 21390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4382000, episode_reward=3087.09 +/- 14.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4382000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4383000, episode_reward=3077.37 +/- 29.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4383000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.775   |\n",
      "|    critic_loss     | 0.000565 |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | -3.5     |\n",
      "|    learning_rate   | 0.000617 |\n",
      "|    n_updates       | 21400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4384000, episode_reward=3064.63 +/- 49.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4384000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4384     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6842     |\n",
      "|    total_timesteps | 4384000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4385000, episode_reward=2898.21 +/- 46.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4385000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.769   |\n",
      "|    critic_loss     | 0.000526 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | -2       |\n",
      "|    learning_rate   | 0.000615 |\n",
      "|    n_updates       | 21410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4386000, episode_reward=2856.74 +/- 27.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4386000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4387000, episode_reward=3138.47 +/- 21.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4387000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.765   |\n",
      "|    critic_loss     | 0.000498 |\n",
      "|    ent_coef        | 0.000588 |\n",
      "|    ent_coef_loss   | -2.19    |\n",
      "|    learning_rate   | 0.000613 |\n",
      "|    n_updates       | 21420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4388000, episode_reward=3163.32 +/- 25.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4388000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4388     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6848     |\n",
      "|    total_timesteps | 4388000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4389000, episode_reward=3097.54 +/- 18.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4389000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.000532 |\n",
      "|    ent_coef        | 0.000587 |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.000611 |\n",
      "|    n_updates       | 21430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4390000, episode_reward=3110.98 +/- 32.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4390000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4391000, episode_reward=3168.41 +/- 15.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4391000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.773   |\n",
      "|    critic_loss     | 0.000568 |\n",
      "|    ent_coef        | 0.000586 |\n",
      "|    ent_coef_loss   | 0.379    |\n",
      "|    learning_rate   | 0.000609 |\n",
      "|    n_updates       | 21440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4392000, episode_reward=3131.50 +/- 31.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4392000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4392     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6854     |\n",
      "|    total_timesteps | 4392000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4393000, episode_reward=3120.65 +/- 33.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4393000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.00054  |\n",
      "|    ent_coef        | 0.000586 |\n",
      "|    ent_coef_loss   | 2.48     |\n",
      "|    learning_rate   | 0.000607 |\n",
      "|    n_updates       | 21450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4394000, episode_reward=3120.12 +/- 35.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4394000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4395000, episode_reward=3109.33 +/- 27.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4395000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4396000, episode_reward=3201.32 +/- 41.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4396000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.785   |\n",
      "|    critic_loss     | 0.000536 |\n",
      "|    ent_coef        | 0.000587 |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.000605 |\n",
      "|    n_updates       | 21460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4396     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6860     |\n",
      "|    total_timesteps | 4396000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4397000, episode_reward=3199.61 +/- 66.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4397000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4398000, episode_reward=3131.36 +/- 24.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4398000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.000588 |\n",
      "|    ent_coef        | 0.000588 |\n",
      "|    ent_coef_loss   | 0.79     |\n",
      "|    learning_rate   | 0.000603 |\n",
      "|    n_updates       | 21470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4399000, episode_reward=3164.73 +/- 37.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4399000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4400000, episode_reward=3311.41 +/- 27.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.777   |\n",
      "|    critic_loss     | 0.000592 |\n",
      "|    ent_coef        | 0.000588 |\n",
      "|    ent_coef_loss   | 0.248    |\n",
      "|    learning_rate   | 0.000601 |\n",
      "|    n_updates       | 21480    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4400     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6867     |\n",
      "|    total_timesteps | 4400000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4401000, episode_reward=3347.44 +/- 39.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4401000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4402000, episode_reward=3223.05 +/- 52.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4402000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.796   |\n",
      "|    critic_loss     | 0.000621 |\n",
      "|    ent_coef        | 0.000589 |\n",
      "|    ent_coef_loss   | 3.24     |\n",
      "|    learning_rate   | 0.000599 |\n",
      "|    n_updates       | 21490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4403000, episode_reward=3215.90 +/- 30.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4403000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4404000, episode_reward=3254.96 +/- 43.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4404000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.782   |\n",
      "|    critic_loss     | 0.000543 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | -0.622   |\n",
      "|    learning_rate   | 0.000597 |\n",
      "|    n_updates       | 21500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4404     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6873     |\n",
      "|    total_timesteps | 4404000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4405000, episode_reward=3300.93 +/- 17.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4405000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4406000, episode_reward=3249.56 +/- 44.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4406000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.000588 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | 0.341    |\n",
      "|    learning_rate   | 0.000595 |\n",
      "|    n_updates       | 21510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4407000, episode_reward=3245.08 +/- 21.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4407000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4408000, episode_reward=3288.40 +/- 43.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4408000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.791   |\n",
      "|    critic_loss     | 0.000595 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | 0.702    |\n",
      "|    learning_rate   | 0.000593 |\n",
      "|    n_updates       | 21520    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4408     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6880     |\n",
      "|    total_timesteps | 4408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4409000, episode_reward=3235.47 +/- 32.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4409000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410000, episode_reward=3308.70 +/- 29.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.785   |\n",
      "|    critic_loss     | 0.000579 |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.000591 |\n",
      "|    n_updates       | 21530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4411000, episode_reward=3269.33 +/- 35.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4411000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4412000, episode_reward=3249.55 +/- 49.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4412000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.784   |\n",
      "|    critic_loss     | 0.000579 |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | 1.62     |\n",
      "|    learning_rate   | 0.000589 |\n",
      "|    n_updates       | 21540    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4412     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6886     |\n",
      "|    total_timesteps | 4412000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4413000, episode_reward=3283.70 +/- 47.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4413000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4414000, episode_reward=3180.34 +/- 37.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4414000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.787   |\n",
      "|    critic_loss     | 0.000579 |\n",
      "|    ent_coef        | 0.000592 |\n",
      "|    ent_coef_loss   | 0.0764   |\n",
      "|    learning_rate   | 0.000587 |\n",
      "|    n_updates       | 21550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4415000, episode_reward=3217.21 +/- 24.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4415000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4416000, episode_reward=3338.41 +/- 47.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4416000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.774   |\n",
      "|    critic_loss     | 0.000595 |\n",
      "|    ent_coef        | 0.000592 |\n",
      "|    ent_coef_loss   | -2.1     |\n",
      "|    learning_rate   | 0.000585 |\n",
      "|    n_updates       | 21560    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4416     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6892     |\n",
      "|    total_timesteps | 4416000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4417000, episode_reward=3359.94 +/- 23.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4417000  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4418000, episode_reward=3306.92 +/- 47.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4418000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.787   |\n",
      "|    critic_loss     | 0.000647 |\n",
      "|    ent_coef        | 0.000592 |\n",
      "|    ent_coef_loss   | 2.28     |\n",
      "|    learning_rate   | 0.000582 |\n",
      "|    n_updates       | 21570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4419000, episode_reward=3305.53 +/- 27.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4419000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4420000, episode_reward=3216.26 +/- 33.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.783   |\n",
      "|    critic_loss     | 0.000592 |\n",
      "|    ent_coef        | 0.000592 |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.00058  |\n",
      "|    n_updates       | 21580    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4420     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6898     |\n",
      "|    total_timesteps | 4420000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4421000, episode_reward=3210.77 +/- 13.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4421000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4422000, episode_reward=3254.92 +/- 30.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4422000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.779   |\n",
      "|    critic_loss     | 0.000548 |\n",
      "|    ent_coef        | 0.000592 |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.000578 |\n",
      "|    n_updates       | 21590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4423000, episode_reward=3229.06 +/- 31.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4423000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4424000, episode_reward=3267.94 +/- 50.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4424000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.774   |\n",
      "|    critic_loss     | 0.000583 |\n",
      "|    ent_coef        | 0.000591 |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.000576 |\n",
      "|    n_updates       | 21600    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4424     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6905     |\n",
      "|    total_timesteps | 4424000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4425000, episode_reward=3254.61 +/- 31.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4425000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4426000, episode_reward=3273.18 +/- 30.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4426000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.785   |\n",
      "|    critic_loss     | 0.000544 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | -0.485   |\n",
      "|    learning_rate   | 0.000574 |\n",
      "|    n_updates       | 21610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4427000, episode_reward=3258.04 +/- 44.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4427000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4428000, episode_reward=3284.81 +/- 50.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4428000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.78    |\n",
      "|    critic_loss     | 0.000567 |\n",
      "|    ent_coef        | 0.00059  |\n",
      "|    ent_coef_loss   | 0.103    |\n",
      "|    learning_rate   | 0.000572 |\n",
      "|    n_updates       | 21620    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4428     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6911     |\n",
      "|    total_timesteps | 4428000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4429000, episode_reward=3244.71 +/- 62.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4429000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4430000, episode_reward=3222.51 +/- 44.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.781   |\n",
      "|    critic_loss     | 0.000619 |\n",
      "|    ent_coef        | 0.000589 |\n",
      "|    ent_coef_loss   | -0.232   |\n",
      "|    learning_rate   | 0.00057  |\n",
      "|    n_updates       | 21630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4431000, episode_reward=3223.62 +/- 12.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4431000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4432000, episode_reward=3315.63 +/- 31.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4432000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.783   |\n",
      "|    critic_loss     | 0.000543 |\n",
      "|    ent_coef        | 0.000589 |\n",
      "|    ent_coef_loss   | 0.356    |\n",
      "|    learning_rate   | 0.000568 |\n",
      "|    n_updates       | 21640    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4432     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6917     |\n",
      "|    total_timesteps | 4432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4433000, episode_reward=3342.85 +/- 25.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4433000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4434000, episode_reward=3144.14 +/- 47.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4434000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.795   |\n",
      "|    critic_loss     | 0.000612 |\n",
      "|    ent_coef        | 0.000589 |\n",
      "|    ent_coef_loss   | 0.475    |\n",
      "|    learning_rate   | 0.000566 |\n",
      "|    n_updates       | 21650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4435000, episode_reward=3168.65 +/- 34.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4435000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4436000, episode_reward=3179.43 +/- 35.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4436000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.776   |\n",
      "|    critic_loss     | 0.000551 |\n",
      "|    ent_coef        | 0.000589 |\n",
      "|    ent_coef_loss   | -0.664   |\n",
      "|    learning_rate   | 0.000564 |\n",
      "|    n_updates       | 21660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4436     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6923     |\n",
      "|    total_timesteps | 4436000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4437000, episode_reward=3143.82 +/- 43.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4437000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4438000, episode_reward=3151.53 +/- 28.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4438000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4439000, episode_reward=3103.31 +/- 56.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4439000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.77    |\n",
      "|    critic_loss     | 0.000579 |\n",
      "|    ent_coef        | 0.000589 |\n",
      "|    ent_coef_loss   | -0.959   |\n",
      "|    learning_rate   | 0.000562 |\n",
      "|    n_updates       | 21670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4440000, episode_reward=3102.31 +/- 39.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4440000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4440     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6929     |\n",
      "|    total_timesteps | 4440000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4441000, episode_reward=3062.46 +/- 19.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4441000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.767   |\n",
      "|    critic_loss     | 0.000595 |\n",
      "|    ent_coef        | 0.000589 |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.00056  |\n",
      "|    n_updates       | 21680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4442000, episode_reward=3057.12 +/- 41.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4442000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4443000, episode_reward=3051.28 +/- 19.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4443000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.762   |\n",
      "|    critic_loss     | 0.000574 |\n",
      "|    ent_coef        | 0.000588 |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.000558 |\n",
      "|    n_updates       | 21690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4444000, episode_reward=2995.16 +/- 29.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4444000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4444     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6936     |\n",
      "|    total_timesteps | 4444000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4445000, episode_reward=3211.48 +/- 44.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4445000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.756   |\n",
      "|    critic_loss     | 0.000589 |\n",
      "|    ent_coef        | 0.000587 |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.000556 |\n",
      "|    n_updates       | 21700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4446000, episode_reward=3176.71 +/- 44.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4446000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4447000, episode_reward=3003.33 +/- 46.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4447000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.774   |\n",
      "|    critic_loss     | 0.000593 |\n",
      "|    ent_coef        | 0.000586 |\n",
      "|    ent_coef_loss   | -0.222   |\n",
      "|    learning_rate   | 0.000554 |\n",
      "|    n_updates       | 21710    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4448000, episode_reward=3008.93 +/- 31.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4448000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4448     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6942     |\n",
      "|    total_timesteps | 4448000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4449000, episode_reward=3210.41 +/- 46.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4449000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.764   |\n",
      "|    critic_loss     | 0.000601 |\n",
      "|    ent_coef        | 0.000586 |\n",
      "|    ent_coef_loss   | -1.55    |\n",
      "|    learning_rate   | 0.000552 |\n",
      "|    n_updates       | 21720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4450000, episode_reward=3166.40 +/- 34.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4450000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4451000, episode_reward=3005.42 +/- 33.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4451000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.769   |\n",
      "|    critic_loss     | 0.000624 |\n",
      "|    ent_coef        | 0.000585 |\n",
      "|    ent_coef_loss   | -1.52    |\n",
      "|    learning_rate   | 0.00055  |\n",
      "|    n_updates       | 21730    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4452000, episode_reward=3040.77 +/- 11.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4452000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4452     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6948     |\n",
      "|    total_timesteps | 4452000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4453000, episode_reward=3167.35 +/- 19.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4453000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.754   |\n",
      "|    critic_loss     | 0.000588 |\n",
      "|    ent_coef        | 0.000584 |\n",
      "|    ent_coef_loss   | -2.3     |\n",
      "|    learning_rate   | 0.000548 |\n",
      "|    n_updates       | 21740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4454000, episode_reward=3146.23 +/- 78.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4454000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4455000, episode_reward=3271.47 +/- 33.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4455000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.766   |\n",
      "|    critic_loss     | 0.000586 |\n",
      "|    ent_coef        | 0.000583 |\n",
      "|    ent_coef_loss   | -0.253   |\n",
      "|    learning_rate   | 0.000546 |\n",
      "|    n_updates       | 21750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4456000, episode_reward=3283.10 +/- 39.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4456000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4456     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6954     |\n",
      "|    total_timesteps | 4456000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4457000, episode_reward=3316.52 +/- 39.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4457000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.766   |\n",
      "|    critic_loss     | 0.000609 |\n",
      "|    ent_coef        | 0.000582 |\n",
      "|    ent_coef_loss   | -2.36    |\n",
      "|    learning_rate   | 0.000544 |\n",
      "|    n_updates       | 21760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4458000, episode_reward=3299.61 +/- 33.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4458000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4459000, episode_reward=3244.16 +/- 51.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4459000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.765   |\n",
      "|    critic_loss     | 0.000585 |\n",
      "|    ent_coef        | 0.000581 |\n",
      "|    ent_coef_loss   | -1.77    |\n",
      "|    learning_rate   | 0.000542 |\n",
      "|    n_updates       | 21770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4460000, episode_reward=3259.69 +/- 38.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4460000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4460     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6960     |\n",
      "|    total_timesteps | 4460000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4461000, episode_reward=3288.45 +/- 37.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4461000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.767   |\n",
      "|    critic_loss     | 0.000646 |\n",
      "|    ent_coef        | 0.00058  |\n",
      "|    ent_coef_loss   | -0.747   |\n",
      "|    learning_rate   | 0.000539 |\n",
      "|    n_updates       | 21780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4462000, episode_reward=3286.06 +/- 58.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4462000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4463000, episode_reward=3120.54 +/- 33.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4463000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.763   |\n",
      "|    critic_loss     | 0.000617 |\n",
      "|    ent_coef        | 0.000579 |\n",
      "|    ent_coef_loss   | -2.1     |\n",
      "|    learning_rate   | 0.000537 |\n",
      "|    n_updates       | 21790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4464000, episode_reward=3126.34 +/- 39.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4464000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4464     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6966     |\n",
      "|    total_timesteps | 4464000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4465000, episode_reward=3337.73 +/- 12.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4465000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.761   |\n",
      "|    critic_loss     | 0.000622 |\n",
      "|    ent_coef        | 0.000578 |\n",
      "|    ent_coef_loss   | -0.968   |\n",
      "|    learning_rate   | 0.000535 |\n",
      "|    n_updates       | 21800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4466000, episode_reward=3326.86 +/- 27.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4466000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4467000, episode_reward=3215.94 +/- 38.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4467000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.779   |\n",
      "|    critic_loss     | 0.000672 |\n",
      "|    ent_coef        | 0.000578 |\n",
      "|    ent_coef_loss   | 2.12     |\n",
      "|    learning_rate   | 0.000533 |\n",
      "|    n_updates       | 21810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4468000, episode_reward=3226.42 +/- 26.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4468000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4468     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6973     |\n",
      "|    total_timesteps | 4468000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4469000, episode_reward=3302.43 +/- 20.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4469000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.766   |\n",
      "|    critic_loss     | 0.000566 |\n",
      "|    ent_coef        | 0.000578 |\n",
      "|    ent_coef_loss   | -2.55    |\n",
      "|    learning_rate   | 0.000531 |\n",
      "|    n_updates       | 21820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470000, episode_reward=3281.10 +/- 26.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4470000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4471000, episode_reward=3144.81 +/- 17.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4471000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.771   |\n",
      "|    critic_loss     | 0.000633 |\n",
      "|    ent_coef        | 0.000577 |\n",
      "|    ent_coef_loss   | -0.684   |\n",
      "|    learning_rate   | 0.000529 |\n",
      "|    n_updates       | 21830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4472000, episode_reward=3178.42 +/- 22.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4472000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4472     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6979     |\n",
      "|    total_timesteps | 4472000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4473000, episode_reward=3174.05 +/- 65.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4473000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.746   |\n",
      "|    critic_loss     | 0.000676 |\n",
      "|    ent_coef        | 0.000576 |\n",
      "|    ent_coef_loss   | -4.72    |\n",
      "|    learning_rate   | 0.000527 |\n",
      "|    n_updates       | 21840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4474000, episode_reward=3183.53 +/- 23.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4474000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4475000, episode_reward=3247.83 +/- 32.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4475000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.77    |\n",
      "|    critic_loss     | 0.000604 |\n",
      "|    ent_coef        | 0.000574 |\n",
      "|    ent_coef_loss   | -0.922   |\n",
      "|    learning_rate   | 0.000525 |\n",
      "|    n_updates       | 21850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4476000, episode_reward=3257.61 +/- 21.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4476000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4476     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6985     |\n",
      "|    total_timesteps | 4476000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4477000, episode_reward=3086.68 +/- 8.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4477000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.775   |\n",
      "|    critic_loss     | 0.000622 |\n",
      "|    ent_coef        | 0.000573 |\n",
      "|    ent_coef_loss   | -0.436   |\n",
      "|    learning_rate   | 0.000523 |\n",
      "|    n_updates       | 21860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4478000, episode_reward=3090.12 +/- 21.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4478000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4479000, episode_reward=3073.25 +/- 28.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4479000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.757   |\n",
      "|    critic_loss     | 0.00061  |\n",
      "|    ent_coef        | 0.000573 |\n",
      "|    ent_coef_loss   | -3.53    |\n",
      "|    learning_rate   | 0.000521 |\n",
      "|    n_updates       | 21870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4480000, episode_reward=3113.85 +/- 36.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4480000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4480     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6991     |\n",
      "|    total_timesteps | 4480000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4481000, episode_reward=3060.69 +/- 27.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4481000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4482000, episode_reward=2928.49 +/- 43.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4482000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.759   |\n",
      "|    critic_loss     | 0.000613 |\n",
      "|    ent_coef        | 0.000571 |\n",
      "|    ent_coef_loss   | -3.5     |\n",
      "|    learning_rate   | 0.000519 |\n",
      "|    n_updates       | 21880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4483000, episode_reward=2924.61 +/- 35.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4483000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4484000, episode_reward=2982.49 +/- 24.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4484000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.749   |\n",
      "|    critic_loss     | 0.00059  |\n",
      "|    ent_coef        | 0.000569 |\n",
      "|    ent_coef_loss   | -3.92    |\n",
      "|    learning_rate   | 0.000517 |\n",
      "|    n_updates       | 21890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4484     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 6997     |\n",
      "|    total_timesteps | 4484000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4485000, episode_reward=3029.23 +/- 21.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4485000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4486000, episode_reward=3000.08 +/- 24.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4486000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.735   |\n",
      "|    critic_loss     | 0.000611 |\n",
      "|    ent_coef        | 0.000567 |\n",
      "|    ent_coef_loss   | -5.69    |\n",
      "|    learning_rate   | 0.000515 |\n",
      "|    n_updates       | 21900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4487000, episode_reward=2998.35 +/- 26.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4487000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4488000, episode_reward=3034.79 +/- 38.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4488000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.743   |\n",
      "|    critic_loss     | 0.000605 |\n",
      "|    ent_coef        | 0.000564 |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 0.000513 |\n",
      "|    n_updates       | 21910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4488     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7004     |\n",
      "|    total_timesteps | 4488000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4489000, episode_reward=3018.63 +/- 31.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4489000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4490000, episode_reward=2930.72 +/- 42.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.753   |\n",
      "|    critic_loss     | 0.000641 |\n",
      "|    ent_coef        | 0.000562 |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.000511 |\n",
      "|    n_updates       | 21920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4491000, episode_reward=2933.14 +/- 35.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4491000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4492000, episode_reward=3124.09 +/- 34.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4492000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.745   |\n",
      "|    critic_loss     | 0.000555 |\n",
      "|    ent_coef        | 0.000561 |\n",
      "|    ent_coef_loss   | -2.44    |\n",
      "|    learning_rate   | 0.000509 |\n",
      "|    n_updates       | 21930    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4492     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7010     |\n",
      "|    total_timesteps | 4492000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4493000, episode_reward=3103.88 +/- 24.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4493000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4494000, episode_reward=2897.97 +/- 32.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4494000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.749   |\n",
      "|    critic_loss     | 0.000652 |\n",
      "|    ent_coef        | 0.000559 |\n",
      "|    ent_coef_loss   | -0.996   |\n",
      "|    learning_rate   | 0.000507 |\n",
      "|    n_updates       | 21940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4495000, episode_reward=2888.14 +/- 25.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4495000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4496000, episode_reward=3020.13 +/- 51.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4496000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.736   |\n",
      "|    critic_loss     | 0.000568 |\n",
      "|    ent_coef        | 0.000558 |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.000505 |\n",
      "|    n_updates       | 21950    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4496     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7016     |\n",
      "|    total_timesteps | 4496000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4497000, episode_reward=3029.62 +/- 20.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4497000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4498000, episode_reward=2790.36 +/- 39.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4498000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.755   |\n",
      "|    critic_loss     | 0.000596 |\n",
      "|    ent_coef        | 0.000558 |\n",
      "|    ent_coef_loss   | 0.241    |\n",
      "|    learning_rate   | 0.000503 |\n",
      "|    n_updates       | 21960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4499000, episode_reward=2812.02 +/- 12.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4499000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500000, episode_reward=2989.96 +/- 17.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.729   |\n",
      "|    critic_loss     | 0.000555 |\n",
      "|    ent_coef        | 0.000557 |\n",
      "|    ent_coef_loss   | -4.26    |\n",
      "|    learning_rate   | 0.000501 |\n",
      "|    n_updates       | 21970    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4500     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7023     |\n",
      "|    total_timesteps | 4500000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4501000, episode_reward=2957.07 +/- 28.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4501000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4502000, episode_reward=2978.45 +/- 31.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4502000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.746   |\n",
      "|    critic_loss     | 0.000565 |\n",
      "|    ent_coef        | 0.000556 |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.000498 |\n",
      "|    n_updates       | 21980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4503000, episode_reward=2954.73 +/- 23.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4503000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4504000, episode_reward=2863.79 +/- 44.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4504000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.752   |\n",
      "|    critic_loss     | 0.000587 |\n",
      "|    ent_coef        | 0.000554 |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.000496 |\n",
      "|    n_updates       | 21990    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4504     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7030     |\n",
      "|    total_timesteps | 4504000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4505000, episode_reward=2843.53 +/- 40.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4505000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4506000, episode_reward=3088.29 +/- 38.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4506000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.736   |\n",
      "|    critic_loss     | 0.000548 |\n",
      "|    ent_coef        | 0.000553 |\n",
      "|    ent_coef_loss   | -3.39    |\n",
      "|    learning_rate   | 0.000494 |\n",
      "|    n_updates       | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4507000, episode_reward=3056.61 +/- 44.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4507000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4508000, episode_reward=2788.24 +/- 31.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4508000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.749   |\n",
      "|    critic_loss     | 0.000553 |\n",
      "|    ent_coef        | 0.000552 |\n",
      "|    ent_coef_loss   | -1.96    |\n",
      "|    learning_rate   | 0.000492 |\n",
      "|    n_updates       | 22010    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4508     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7036     |\n",
      "|    total_timesteps | 4508000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4509000, episode_reward=2782.70 +/- 41.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4509000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4510000, episode_reward=2892.21 +/- 24.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.713   |\n",
      "|    critic_loss     | 0.000534 |\n",
      "|    ent_coef        | 0.00055  |\n",
      "|    ent_coef_loss   | -4.14    |\n",
      "|    learning_rate   | 0.00049  |\n",
      "|    n_updates       | 22020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4511000, episode_reward=2929.12 +/- 22.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4511000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4512000, episode_reward=2922.33 +/- 24.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4512000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.73    |\n",
      "|    critic_loss     | 0.000563 |\n",
      "|    ent_coef        | 0.000549 |\n",
      "|    ent_coef_loss   | -2.05    |\n",
      "|    learning_rate   | 0.000488 |\n",
      "|    n_updates       | 22030    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4512     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7042     |\n",
      "|    total_timesteps | 4512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4513000, episode_reward=2923.72 +/- 8.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4513000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4514000, episode_reward=2899.47 +/- 43.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4514000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.735   |\n",
      "|    critic_loss     | 0.000581 |\n",
      "|    ent_coef        | 0.000547 |\n",
      "|    ent_coef_loss   | -2.14    |\n",
      "|    learning_rate   | 0.000486 |\n",
      "|    n_updates       | 22040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4515000, episode_reward=2925.91 +/- 22.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4515000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4516000, episode_reward=2892.34 +/- 35.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4516000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.739   |\n",
      "|    critic_loss     | 0.000561 |\n",
      "|    ent_coef        | 0.000546 |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.000484 |\n",
      "|    n_updates       | 22050    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4516     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7049     |\n",
      "|    total_timesteps | 4516000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4517000, episode_reward=2925.24 +/- 26.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4517000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4518000, episode_reward=2782.42 +/- 46.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4518000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.737   |\n",
      "|    critic_loss     | 0.000591 |\n",
      "|    ent_coef        | 0.000545 |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.000482 |\n",
      "|    n_updates       | 22060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4519000, episode_reward=2771.62 +/- 18.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4519000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4520000, episode_reward=2941.63 +/- 16.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.717   |\n",
      "|    critic_loss     | 0.000516 |\n",
      "|    ent_coef        | 0.000544 |\n",
      "|    ent_coef_loss   | -2.75    |\n",
      "|    learning_rate   | 0.00048  |\n",
      "|    n_updates       | 22070    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4520     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7055     |\n",
      "|    total_timesteps | 4520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4521000, episode_reward=2967.05 +/- 17.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4521000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4522000, episode_reward=2694.96 +/- 30.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4522000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.734   |\n",
      "|    critic_loss     | 0.000545 |\n",
      "|    ent_coef        | 0.000543 |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 0.000478 |\n",
      "|    n_updates       | 22080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4523000, episode_reward=2707.86 +/- 48.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4523000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4524000, episode_reward=2706.02 +/- 36.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4524000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4524     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7061     |\n",
      "|    total_timesteps | 4524000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4525000, episode_reward=2876.27 +/- 44.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4525000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.707   |\n",
      "|    critic_loss     | 0.000527 |\n",
      "|    ent_coef        | 0.000541 |\n",
      "|    ent_coef_loss   | -5.47    |\n",
      "|    learning_rate   | 0.000476 |\n",
      "|    n_updates       | 22090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4526000, episode_reward=2875.95 +/- 41.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4526000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4527000, episode_reward=2900.51 +/- 20.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4527000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.724   |\n",
      "|    critic_loss     | 0.000591 |\n",
      "|    ent_coef        | 0.000539 |\n",
      "|    ent_coef_loss   | -2.1     |\n",
      "|    learning_rate   | 0.000474 |\n",
      "|    n_updates       | 22100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4528000, episode_reward=2831.41 +/- 32.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4528000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4528     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7067     |\n",
      "|    total_timesteps | 4528000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4529000, episode_reward=2968.95 +/- 43.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4529000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.727   |\n",
      "|    critic_loss     | 0.000572 |\n",
      "|    ent_coef        | 0.000537 |\n",
      "|    ent_coef_loss   | 0.117    |\n",
      "|    learning_rate   | 0.000472 |\n",
      "|    n_updates       | 22110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4530000, episode_reward=2950.85 +/- 21.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4530000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4531000, episode_reward=2906.86 +/- 14.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4531000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.734   |\n",
      "|    critic_loss     | 0.000564 |\n",
      "|    ent_coef        | 0.000537 |\n",
      "|    ent_coef_loss   | -0.283   |\n",
      "|    learning_rate   | 0.00047  |\n",
      "|    n_updates       | 22120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4532000, episode_reward=2879.21 +/- 54.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4532000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4532     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7074     |\n",
      "|    total_timesteps | 4532000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4533000, episode_reward=2878.22 +/- 24.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4533000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.729   |\n",
      "|    critic_loss     | 0.000585 |\n",
      "|    ent_coef        | 0.000537 |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.000468 |\n",
      "|    n_updates       | 22130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4534000, episode_reward=2876.44 +/- 35.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4534000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4535000, episode_reward=3134.67 +/- 43.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4535000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.73    |\n",
      "|    critic_loss     | 0.000553 |\n",
      "|    ent_coef        | 0.000536 |\n",
      "|    ent_coef_loss   | -0.428   |\n",
      "|    learning_rate   | 0.000466 |\n",
      "|    n_updates       | 22140    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4536000, episode_reward=3142.53 +/- 28.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4536000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4536     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7080     |\n",
      "|    total_timesteps | 4536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4537000, episode_reward=3068.51 +/- 22.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4537000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.758   |\n",
      "|    critic_loss     | 0.00059  |\n",
      "|    ent_coef        | 0.000536 |\n",
      "|    ent_coef_loss   | 3.02     |\n",
      "|    learning_rate   | 0.000464 |\n",
      "|    n_updates       | 22150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4538000, episode_reward=3067.68 +/- 48.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4538000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4539000, episode_reward=3026.98 +/- 48.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4539000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.742   |\n",
      "|    critic_loss     | 0.000563 |\n",
      "|    ent_coef        | 0.000537 |\n",
      "|    ent_coef_loss   | -0.78    |\n",
      "|    learning_rate   | 0.000462 |\n",
      "|    n_updates       | 22160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4540000, episode_reward=3007.24 +/- 13.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4540000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4540     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7086     |\n",
      "|    total_timesteps | 4540000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4541000, episode_reward=2989.92 +/- 20.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4541000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.741   |\n",
      "|    critic_loss     | 0.000578 |\n",
      "|    ent_coef        | 0.000537 |\n",
      "|    ent_coef_loss   | 0.763    |\n",
      "|    learning_rate   | 0.00046  |\n",
      "|    n_updates       | 22170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4542000, episode_reward=3024.01 +/- 24.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4542000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4543000, episode_reward=2994.64 +/- 38.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4543000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.742   |\n",
      "|    critic_loss     | 0.000579 |\n",
      "|    ent_coef        | 0.000537 |\n",
      "|    ent_coef_loss   | -0.314   |\n",
      "|    learning_rate   | 0.000458 |\n",
      "|    n_updates       | 22180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4544000, episode_reward=2976.30 +/- 46.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4544000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4544     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7094     |\n",
      "|    total_timesteps | 4544000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4545000, episode_reward=3098.01 +/- 19.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4545000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.626   |\n",
      "|    critic_loss     | 0.000578 |\n",
      "|    ent_coef        | 0.000536 |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.000455 |\n",
      "|    n_updates       | 22190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4546000, episode_reward=3086.87 +/- 48.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4546000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4547000, episode_reward=2899.12 +/- 25.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4547000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.628   |\n",
      "|    critic_loss     | 0.000475 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | -14.5    |\n",
      "|    learning_rate   | 0.000453 |\n",
      "|    n_updates       | 22200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4548000, episode_reward=2913.48 +/- 65.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4548000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4548     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7102     |\n",
      "|    total_timesteps | 4548000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4549000, episode_reward=2808.36 +/- 30.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4549000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.722   |\n",
      "|    critic_loss     | 0.00058  |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.000451 |\n",
      "|    n_updates       | 22210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4550000, episode_reward=2753.46 +/- 21.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4550000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4551000, episode_reward=2892.31 +/- 30.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4551000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.707   |\n",
      "|    critic_loss     | 0.00052  |\n",
      "|    ent_coef        | 0.000524 |\n",
      "|    ent_coef_loss   | -3.05    |\n",
      "|    learning_rate   | 0.000449 |\n",
      "|    n_updates       | 22220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4552000, episode_reward=2837.28 +/- 30.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4552000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4552     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7109     |\n",
      "|    total_timesteps | 4552000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4553000, episode_reward=3015.25 +/- 38.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4553000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.731   |\n",
      "|    critic_loss     | 0.000553 |\n",
      "|    ent_coef        | 0.000523 |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.000447 |\n",
      "|    n_updates       | 22230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4554000, episode_reward=2997.87 +/- 51.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4554000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4555000, episode_reward=2976.63 +/- 39.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4555000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.738   |\n",
      "|    critic_loss     | 0.000592 |\n",
      "|    ent_coef        | 0.000522 |\n",
      "|    ent_coef_loss   | 2.51     |\n",
      "|    learning_rate   | 0.000445 |\n",
      "|    n_updates       | 22240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4556000, episode_reward=2983.68 +/- 33.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4556000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4556     |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 7117     |\n",
      "|    total_timesteps | 4556000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4557000, episode_reward=2906.83 +/- 41.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4557000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.737   |\n",
      "|    critic_loss     | 0.000555 |\n",
      "|    ent_coef        | 0.000523 |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.000443 |\n",
      "|    n_updates       | 22250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4558000, episode_reward=2926.76 +/- 41.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4558000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4559000, episode_reward=2228.88 +/- 1317.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4559000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.724   |\n",
      "|    critic_loss     | 0.000609 |\n",
      "|    ent_coef        | 0.000524 |\n",
      "|    ent_coef_loss   | 0.773    |\n",
      "|    learning_rate   | 0.000441 |\n",
      "|    n_updates       | 22260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4560000, episode_reward=2901.97 +/- 24.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4560000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4560     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7125     |\n",
      "|    total_timesteps | 4560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4561000, episode_reward=2997.52 +/- 45.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4561000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.725   |\n",
      "|    critic_loss     | 0.000619 |\n",
      "|    ent_coef        | 0.000524 |\n",
      "|    ent_coef_loss   | 0.701    |\n",
      "|    learning_rate   | 0.000439 |\n",
      "|    n_updates       | 22270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4562000, episode_reward=3021.59 +/- 12.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4562000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4563000, episode_reward=2886.11 +/- 41.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4563000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.732   |\n",
      "|    critic_loss     | 0.000581 |\n",
      "|    ent_coef        | 0.000525 |\n",
      "|    ent_coef_loss   | 1.35     |\n",
      "|    learning_rate   | 0.000437 |\n",
      "|    n_updates       | 22280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4564000, episode_reward=2901.39 +/- 18.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4564000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4564     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7132     |\n",
      "|    total_timesteps | 4564000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4565000, episode_reward=3125.40 +/- 54.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4565000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.713   |\n",
      "|    critic_loss     | 0.000534 |\n",
      "|    ent_coef        | 0.000525 |\n",
      "|    ent_coef_loss   | 0.526    |\n",
      "|    learning_rate   | 0.000435 |\n",
      "|    n_updates       | 22290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4566000, episode_reward=3110.90 +/- 51.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4566000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4567000, episode_reward=2445.24 +/- 1391.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4567000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4568000, episode_reward=3076.25 +/- 53.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4568000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.741   |\n",
      "|    critic_loss     | 0.000599 |\n",
      "|    ent_coef        | 0.000526 |\n",
      "|    ent_coef_loss   | 3.53     |\n",
      "|    learning_rate   | 0.000433 |\n",
      "|    n_updates       | 22300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4568     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7138     |\n",
      "|    total_timesteps | 4568000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4569000, episode_reward=2393.11 +/- 1393.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4569000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4570000, episode_reward=3054.35 +/- 57.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.718   |\n",
      "|    critic_loss     | 0.000613 |\n",
      "|    ent_coef        | 0.000527 |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.000431 |\n",
      "|    n_updates       | 22310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4571000, episode_reward=3074.58 +/- 37.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4571000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4572000, episode_reward=3082.02 +/- 19.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4572000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.733   |\n",
      "|    critic_loss     | 0.000608 |\n",
      "|    ent_coef        | 0.000528 |\n",
      "|    ent_coef_loss   | 2.96     |\n",
      "|    learning_rate   | 0.000429 |\n",
      "|    n_updates       | 22320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4572     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7145     |\n",
      "|    total_timesteps | 4572000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4573000, episode_reward=3114.35 +/- 29.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4573000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4574000, episode_reward=2413.23 +/- 1411.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4574000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.743   |\n",
      "|    critic_loss     | 0.000568 |\n",
      "|    ent_coef        | 0.000529 |\n",
      "|    ent_coef_loss   | 3.15     |\n",
      "|    learning_rate   | 0.000427 |\n",
      "|    n_updates       | 22330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4575000, episode_reward=3118.45 +/- 19.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4575000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4576000, episode_reward=2465.12 +/- 1345.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4576000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.738   |\n",
      "|    critic_loss     | 0.000571 |\n",
      "|    ent_coef        | 0.000531 |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.000425 |\n",
      "|    n_updates       | 22340    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4576     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7151     |\n",
      "|    total_timesteps | 4576000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4577000, episode_reward=3087.76 +/- 34.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4577000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4578000, episode_reward=3078.51 +/- 32.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4578000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.73    |\n",
      "|    critic_loss     | 0.000665 |\n",
      "|    ent_coef        | 0.000531 |\n",
      "|    ent_coef_loss   | -0.122   |\n",
      "|    learning_rate   | 0.000423 |\n",
      "|    n_updates       | 22350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4579000, episode_reward=3109.54 +/- 28.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4579000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4580000, episode_reward=3065.43 +/- 25.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.734   |\n",
      "|    critic_loss     | 0.000584 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | 2.16     |\n",
      "|    learning_rate   | 0.000421 |\n",
      "|    n_updates       | 22360    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4580     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7157     |\n",
      "|    total_timesteps | 4580000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4581000, episode_reward=3068.12 +/- 43.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4581000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4582000, episode_reward=3172.74 +/- 23.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4582000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.732   |\n",
      "|    critic_loss     | 0.000605 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | 0.0202   |\n",
      "|    learning_rate   | 0.000419 |\n",
      "|    n_updates       | 22370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4583000, episode_reward=2400.86 +/- 1406.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4583000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4584000, episode_reward=3108.39 +/- 40.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4584000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.618   |\n",
      "|    critic_loss     | 0.000916 |\n",
      "|    ent_coef        | 0.000533 |\n",
      "|    ent_coef_loss   | 0.458    |\n",
      "|    learning_rate   | 0.000417 |\n",
      "|    n_updates       | 22380    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4584     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7164     |\n",
      "|    total_timesteps | 4584000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4585000, episode_reward=2404.90 +/- 1420.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4585000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4586000, episode_reward=2901.43 +/- 28.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4586000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.471   |\n",
      "|    critic_loss     | 0.000706 |\n",
      "|    ent_coef        | 0.000532 |\n",
      "|    ent_coef_loss   | -9.88    |\n",
      "|    learning_rate   | 0.000415 |\n",
      "|    n_updates       | 22390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4587000, episode_reward=2923.13 +/- 12.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4587000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4588000, episode_reward=2129.97 +/- 1157.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4588000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.598   |\n",
      "|    critic_loss     | 0.000702 |\n",
      "|    ent_coef        | 0.000529 |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.000412 |\n",
      "|    n_updates       | 22400    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4588     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7171     |\n",
      "|    total_timesteps | 4588000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4589000, episode_reward=2068.12 +/- 1257.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4589000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4590000, episode_reward=2856.49 +/- 19.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.699   |\n",
      "|    critic_loss     | 0.000642 |\n",
      "|    ent_coef        | 0.000525 |\n",
      "|    ent_coef_loss   | -5.04    |\n",
      "|    learning_rate   | 0.00041  |\n",
      "|    n_updates       | 22410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4591000, episode_reward=2892.48 +/- 18.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4591000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4592000, episode_reward=874.70 +/- 1621.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 875      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4592000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.557   |\n",
      "|    critic_loss     | 0.000536 |\n",
      "|    ent_coef        | 0.000521 |\n",
      "|    ent_coef_loss   | -19      |\n",
      "|    learning_rate   | 0.000408 |\n",
      "|    n_updates       | 22420    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4592     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7178     |\n",
      "|    total_timesteps | 4592000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4593000, episode_reward=935.87 +/- 1581.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 936      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4593000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4594000, episode_reward=2052.47 +/- 1099.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4594000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.625   |\n",
      "|    critic_loss     | 0.000509 |\n",
      "|    ent_coef        | 0.000515 |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.000406 |\n",
      "|    n_updates       | 22430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4595000, episode_reward=2548.01 +/- 63.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4595000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4596000, episode_reward=1880.18 +/- 1167.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4596000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.662   |\n",
      "|    critic_loss     | 0.000493 |\n",
      "|    ent_coef        | 0.000509 |\n",
      "|    ent_coef_loss   | -9.14    |\n",
      "|    learning_rate   | 0.000404 |\n",
      "|    n_updates       | 22440    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4596     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7186     |\n",
      "|    total_timesteps | 4596000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4597000, episode_reward=2434.12 +/- 18.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4597000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4598000, episode_reward=2163.21 +/- 1155.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4598000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.66    |\n",
      "|    critic_loss     | 0.000463 |\n",
      "|    ent_coef        | 0.000505 |\n",
      "|    ent_coef_loss   | -6.98    |\n",
      "|    learning_rate   | 0.000402 |\n",
      "|    n_updates       | 22450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4599000, episode_reward=2232.89 +/- 1187.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4599000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4600000, episode_reward=2785.09 +/- 27.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.489   |\n",
      "|    critic_loss     | 0.000308 |\n",
      "|    ent_coef        | 0.000501 |\n",
      "|    ent_coef_loss   | -29.4    |\n",
      "|    learning_rate   | 0.0004   |\n",
      "|    n_updates       | 22460    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4600     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7193     |\n",
      "|    total_timesteps | 4600000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4601000, episode_reward=1555.72 +/- 1534.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4601000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4602000, episode_reward=2534.33 +/- 22.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4602000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.687   |\n",
      "|    critic_loss     | 0.000589 |\n",
      "|    ent_coef        | 0.000495 |\n",
      "|    ent_coef_loss   | -3.48    |\n",
      "|    learning_rate   | 0.000398 |\n",
      "|    n_updates       | 22470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4603000, episode_reward=2537.80 +/- 46.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4603000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4604000, episode_reward=2153.19 +/- 1137.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4604000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.421   |\n",
      "|    critic_loss     | 0.000953 |\n",
      "|    ent_coef        | 0.000492 |\n",
      "|    ent_coef_loss   | -0.244   |\n",
      "|    learning_rate   | 0.000396 |\n",
      "|    n_updates       | 22480    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4604     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7199     |\n",
      "|    total_timesteps | 4604000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4605000, episode_reward=2756.89 +/- 56.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4605000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4606000, episode_reward=2979.60 +/- 45.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4606000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.684   |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000491 |\n",
      "|    ent_coef_loss   | -4.72    |\n",
      "|    learning_rate   | 0.000394 |\n",
      "|    n_updates       | 22490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4607000, episode_reward=2864.26 +/- 193.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4607000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4608000, episode_reward=2973.66 +/- 37.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4608000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4608     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7205     |\n",
      "|    total_timesteps | 4608000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4609000, episode_reward=2806.29 +/- 35.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4609000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.489   |\n",
      "|    critic_loss     | 0.000518 |\n",
      "|    ent_coef        | 0.000488 |\n",
      "|    ent_coef_loss   | -26.2    |\n",
      "|    learning_rate   | 0.000392 |\n",
      "|    n_updates       | 22500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4610000, episode_reward=2811.25 +/- 40.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4610000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4611000, episode_reward=2746.77 +/- 21.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4611000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.45    |\n",
      "|    critic_loss     | 0.00068  |\n",
      "|    ent_coef        | 0.000483 |\n",
      "|    ent_coef_loss   | -19.1    |\n",
      "|    learning_rate   | 0.00039  |\n",
      "|    n_updates       | 22510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4612000, episode_reward=2760.22 +/- 19.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4612000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4612     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7211     |\n",
      "|    total_timesteps | 4612000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4613000, episode_reward=2672.84 +/- 35.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4613000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.675   |\n",
      "|    critic_loss     | 0.000793 |\n",
      "|    ent_coef        | 0.000479 |\n",
      "|    ent_coef_loss   | -7.23    |\n",
      "|    learning_rate   | 0.000388 |\n",
      "|    n_updates       | 22520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4614000, episode_reward=2699.38 +/- 12.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4614000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4615000, episode_reward=2579.53 +/- 32.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4615000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.644   |\n",
      "|    critic_loss     | 0.000572 |\n",
      "|    ent_coef        | 0.000475 |\n",
      "|    ent_coef_loss   | -8.16    |\n",
      "|    learning_rate   | 0.000386 |\n",
      "|    n_updates       | 22530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4616000, episode_reward=2558.70 +/- 37.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4616000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4616     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7218     |\n",
      "|    total_timesteps | 4616000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4617000, episode_reward=2713.66 +/- 18.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4617000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.479   |\n",
      "|    critic_loss     | 0.000407 |\n",
      "|    ent_coef        | 0.000472 |\n",
      "|    ent_coef_loss   | -30.1    |\n",
      "|    learning_rate   | 0.000384 |\n",
      "|    n_updates       | 22540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4618000, episode_reward=2216.97 +/- 1078.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4618000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4619000, episode_reward=2760.98 +/- 28.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4619000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.66    |\n",
      "|    critic_loss     | 0.000561 |\n",
      "|    ent_coef        | 0.000468 |\n",
      "|    ent_coef_loss   | -5.97    |\n",
      "|    learning_rate   | 0.000382 |\n",
      "|    n_updates       | 22550    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620000, episode_reward=2771.20 +/- 24.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4620000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4620     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7225     |\n",
      "|    total_timesteps | 4620000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4621000, episode_reward=2754.30 +/- 49.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4621000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.66    |\n",
      "|    critic_loss     | 0.000486 |\n",
      "|    ent_coef        | 0.000465 |\n",
      "|    ent_coef_loss   | -3.98    |\n",
      "|    learning_rate   | 0.00038  |\n",
      "|    n_updates       | 22560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4622000, episode_reward=2750.87 +/- 34.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4622000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4623000, episode_reward=2873.07 +/- 41.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4623000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.673   |\n",
      "|    critic_loss     | 0.000463 |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | -4.77    |\n",
      "|    learning_rate   | 0.000378 |\n",
      "|    n_updates       | 22570    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4624000, episode_reward=2838.97 +/- 15.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4624000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4624     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7231     |\n",
      "|    total_timesteps | 4624000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4625000, episode_reward=2894.77 +/- 16.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4625000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.682   |\n",
      "|    critic_loss     | 0.000454 |\n",
      "|    ent_coef        | 0.000462 |\n",
      "|    ent_coef_loss   | -1.46    |\n",
      "|    learning_rate   | 0.000376 |\n",
      "|    n_updates       | 22580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4626000, episode_reward=2907.37 +/- 22.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4626000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4627000, episode_reward=2846.89 +/- 37.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4627000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.681   |\n",
      "|    critic_loss     | 0.000469 |\n",
      "|    ent_coef        | 0.000461 |\n",
      "|    ent_coef_loss   | -1.99    |\n",
      "|    learning_rate   | 0.000374 |\n",
      "|    n_updates       | 22590    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4628000, episode_reward=2249.02 +/- 1178.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4628000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4628     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7237     |\n",
      "|    total_timesteps | 4628000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4629000, episode_reward=2841.06 +/- 28.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4629000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.685   |\n",
      "|    critic_loss     | 0.000464 |\n",
      "|    ent_coef        | 0.000461 |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.000372 |\n",
      "|    n_updates       | 22600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4630000, episode_reward=2826.54 +/- 23.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4630000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4631000, episode_reward=3040.05 +/- 21.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4631000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.681   |\n",
      "|    critic_loss     | 0.000466 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | -0.521   |\n",
      "|    learning_rate   | 0.000369 |\n",
      "|    n_updates       | 22610    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4632000, episode_reward=3074.00 +/- 48.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4632000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4632     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7244     |\n",
      "|    total_timesteps | 4632000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4633000, episode_reward=3022.03 +/- 37.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4633000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.706   |\n",
      "|    critic_loss     | 0.000494 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | 2.93     |\n",
      "|    learning_rate   | 0.000367 |\n",
      "|    n_updates       | 22620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4634000, episode_reward=3029.83 +/- 36.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4634000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4635000, episode_reward=2925.81 +/- 33.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4635000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.697   |\n",
      "|    critic_loss     | 0.000501 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | 2.01     |\n",
      "|    learning_rate   | 0.000365 |\n",
      "|    n_updates       | 22630    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4636000, episode_reward=2874.88 +/- 48.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4636000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4636     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7250     |\n",
      "|    total_timesteps | 4636000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4637000, episode_reward=2908.82 +/- 43.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4637000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.688   |\n",
      "|    critic_loss     | 0.000483 |\n",
      "|    ent_coef        | 0.000461 |\n",
      "|    ent_coef_loss   | -0.362   |\n",
      "|    learning_rate   | 0.000363 |\n",
      "|    n_updates       | 22640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4638000, episode_reward=2903.39 +/- 59.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4638000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4639000, episode_reward=2944.03 +/- 47.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4639000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.514   |\n",
      "|    critic_loss     | 0.000992 |\n",
      "|    ent_coef        | 0.000461 |\n",
      "|    ent_coef_loss   | -3.56    |\n",
      "|    learning_rate   | 0.000361 |\n",
      "|    n_updates       | 22650    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4640000, episode_reward=2933.84 +/- 34.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4640000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4640     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7256     |\n",
      "|    total_timesteps | 4640000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4641000, episode_reward=3003.99 +/- 48.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4641000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.625   |\n",
      "|    critic_loss     | 0.000906 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | -1.6     |\n",
      "|    learning_rate   | 0.000359 |\n",
      "|    n_updates       | 22660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4642000, episode_reward=2376.23 +/- 1239.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4642000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4643000, episode_reward=2430.18 +/- 1275.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4643000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.501   |\n",
      "|    critic_loss     | 0.000427 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | -23.5    |\n",
      "|    learning_rate   | 0.000357 |\n",
      "|    n_updates       | 22670    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4644000, episode_reward=3075.95 +/- 10.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4644000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4644     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7263     |\n",
      "|    total_timesteps | 4644000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4645000, episode_reward=2400.29 +/- 1269.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4645000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.697   |\n",
      "|    critic_loss     | 0.000628 |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.000355 |\n",
      "|    n_updates       | 22680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4646000, episode_reward=3023.88 +/- 52.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4646000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4647000, episode_reward=2983.47 +/- 23.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4647000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.702   |\n",
      "|    critic_loss     | 0.000555 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | 2.65     |\n",
      "|    learning_rate   | 0.000353 |\n",
      "|    n_updates       | 22690    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4648000, episode_reward=2961.51 +/- 18.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4648000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4648     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7269     |\n",
      "|    total_timesteps | 4648000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4649000, episode_reward=2972.90 +/- 13.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4649000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.699   |\n",
      "|    critic_loss     | 0.000531 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | 1.83     |\n",
      "|    learning_rate   | 0.000351 |\n",
      "|    n_updates       | 22700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4650000, episode_reward=3009.83 +/- 36.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4650000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4651000, episode_reward=2363.55 +/- 1244.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4651000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4652000, episode_reward=3018.31 +/- 39.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4652000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.696   |\n",
      "|    critic_loss     | 0.000521 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | 1.5      |\n",
      "|    learning_rate   | 0.000349 |\n",
      "|    n_updates       | 22710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4652     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7275     |\n",
      "|    total_timesteps | 4652000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4653000, episode_reward=3017.63 +/- 41.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4653000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4654000, episode_reward=2375.51 +/- 1261.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4654000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.7     |\n",
      "|    critic_loss     | 0.000476 |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.000347 |\n",
      "|    n_updates       | 22720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4655000, episode_reward=2385.41 +/- 1266.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4655000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4656000, episode_reward=2935.76 +/- 51.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4656000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.703   |\n",
      "|    critic_loss     | 0.000481 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | 3.17     |\n",
      "|    learning_rate   | 0.000345 |\n",
      "|    n_updates       | 22730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4656     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7281     |\n",
      "|    total_timesteps | 4656000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4657000, episode_reward=2312.64 +/- 1215.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4657000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4658000, episode_reward=2275.82 +/- 1205.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4658000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.685   |\n",
      "|    critic_loss     | 0.000491 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.000343 |\n",
      "|    n_updates       | 22740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4659000, episode_reward=2859.22 +/- 34.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4659000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4660000, episode_reward=2930.34 +/- 29.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.699   |\n",
      "|    critic_loss     | 0.000466 |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | 0.996    |\n",
      "|    learning_rate   | 0.000341 |\n",
      "|    n_updates       | 22750    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4660     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7288     |\n",
      "|    total_timesteps | 4660000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4661000, episode_reward=2877.25 +/- 23.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4661000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4662000, episode_reward=2928.39 +/- 18.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4662000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.688   |\n",
      "|    critic_loss     | 0.000508 |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | 1.78     |\n",
      "|    learning_rate   | 0.000339 |\n",
      "|    n_updates       | 22760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4663000, episode_reward=2923.67 +/- 46.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4663000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4664000, episode_reward=2955.84 +/- 25.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4664000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.691   |\n",
      "|    critic_loss     | 0.00049  |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | 3.67     |\n",
      "|    learning_rate   | 0.000337 |\n",
      "|    n_updates       | 22770    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4664     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7294     |\n",
      "|    total_timesteps | 4664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4665000, episode_reward=1684.47 +/- 1533.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4665000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4666000, episode_reward=3079.12 +/- 23.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4666000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.699   |\n",
      "|    critic_loss     | 0.000526 |\n",
      "|    ent_coef        | 0.000458 |\n",
      "|    ent_coef_loss   | 4.21     |\n",
      "|    learning_rate   | 0.000335 |\n",
      "|    n_updates       | 22780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4667000, episode_reward=3091.17 +/- 29.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4667000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4668000, episode_reward=2933.43 +/- 22.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4668000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.712   |\n",
      "|    critic_loss     | 0.000479 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | 4.15     |\n",
      "|    learning_rate   | 0.000333 |\n",
      "|    n_updates       | 22790    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4668     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7300     |\n",
      "|    total_timesteps | 4668000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4669000, episode_reward=2925.67 +/- 42.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4669000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4670000, episode_reward=2854.73 +/- 91.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.69    |\n",
      "|    critic_loss     | 0.000482 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | 2.83     |\n",
      "|    learning_rate   | 0.000331 |\n",
      "|    n_updates       | 22800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4671000, episode_reward=2942.36 +/- 32.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4671000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4672000, episode_reward=2762.77 +/- 50.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4672000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.693   |\n",
      "|    critic_loss     | 0.000526 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | 2.8      |\n",
      "|    learning_rate   | 0.000329 |\n",
      "|    n_updates       | 22810    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4672     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7307     |\n",
      "|    total_timesteps | 4672000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4673000, episode_reward=2211.72 +/- 1166.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4673000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4674000, episode_reward=2875.01 +/- 16.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4674000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.69    |\n",
      "|    critic_loss     | 0.000501 |\n",
      "|    ent_coef        | 0.000461 |\n",
      "|    ent_coef_loss   | 3.27     |\n",
      "|    learning_rate   | 0.000326 |\n",
      "|    n_updates       | 22820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4675000, episode_reward=2851.37 +/- 23.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4675000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4676000, episode_reward=2875.56 +/- 29.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4676000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.691   |\n",
      "|    critic_loss     | 0.000485 |\n",
      "|    ent_coef        | 0.000462 |\n",
      "|    ent_coef_loss   | 3.08     |\n",
      "|    learning_rate   | 0.000324 |\n",
      "|    n_updates       | 22830    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4676     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7313     |\n",
      "|    total_timesteps | 4676000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4677000, episode_reward=2901.23 +/- 39.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4677000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4678000, episode_reward=2880.71 +/- 31.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4678000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.688   |\n",
      "|    critic_loss     | 0.000456 |\n",
      "|    ent_coef        | 0.000462 |\n",
      "|    ent_coef_loss   | 2.12     |\n",
      "|    learning_rate   | 0.000322 |\n",
      "|    n_updates       | 22840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4679000, episode_reward=2835.73 +/- 36.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4679000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680000, episode_reward=2895.63 +/- 18.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.682   |\n",
      "|    critic_loss     | 0.000464 |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | 2.04     |\n",
      "|    learning_rate   | 0.00032  |\n",
      "|    n_updates       | 22850    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4680     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7319     |\n",
      "|    total_timesteps | 4680000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4681000, episode_reward=2871.49 +/- 46.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4681000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4682000, episode_reward=2974.77 +/- 24.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4682000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.693   |\n",
      "|    critic_loss     | 0.00046  |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | 2.46     |\n",
      "|    learning_rate   | 0.000318 |\n",
      "|    n_updates       | 22860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4683000, episode_reward=2958.36 +/- 23.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4683000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4684000, episode_reward=2991.64 +/- 36.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4684000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.701   |\n",
      "|    critic_loss     | 0.000527 |\n",
      "|    ent_coef        | 0.000464 |\n",
      "|    ent_coef_loss   | 4.16     |\n",
      "|    learning_rate   | 0.000316 |\n",
      "|    n_updates       | 22870    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4684     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7325     |\n",
      "|    total_timesteps | 4684000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4685000, episode_reward=2991.74 +/- 30.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4685000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4686000, episode_reward=2980.87 +/- 24.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4686000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.691   |\n",
      "|    critic_loss     | 0.000492 |\n",
      "|    ent_coef        | 0.000464 |\n",
      "|    ent_coef_loss   | 2.22     |\n",
      "|    learning_rate   | 0.000314 |\n",
      "|    n_updates       | 22880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4687000, episode_reward=2967.32 +/- 35.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4687000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4688000, episode_reward=1621.40 +/- 1441.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4688000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.691   |\n",
      "|    critic_loss     | 0.000537 |\n",
      "|    ent_coef        | 0.000465 |\n",
      "|    ent_coef_loss   | 2.49     |\n",
      "|    learning_rate   | 0.000312 |\n",
      "|    n_updates       | 22890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4688     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7332     |\n",
      "|    total_timesteps | 4688000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4689000, episode_reward=2805.26 +/- 32.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4689000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4690000, episode_reward=2267.14 +/- 1207.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.676   |\n",
      "|    critic_loss     | 0.000474 |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | 0.134    |\n",
      "|    learning_rate   | 0.00031  |\n",
      "|    n_updates       | 22900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4691000, episode_reward=2838.31 +/- 44.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4691000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4692000, episode_reward=3036.88 +/- 30.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4692000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.686   |\n",
      "|    critic_loss     | 0.000482 |\n",
      "|    ent_coef        | 0.000466 |\n",
      "|    ent_coef_loss   | 2.41     |\n",
      "|    learning_rate   | 0.000308 |\n",
      "|    n_updates       | 22910    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4692     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7338     |\n",
      "|    total_timesteps | 4692000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4693000, episode_reward=3042.57 +/- 26.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4693000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4694000, episode_reward=1764.88 +/- 1562.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4694000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4695000, episode_reward=3045.63 +/- 11.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4695000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.492   |\n",
      "|    critic_loss     | 0.000496 |\n",
      "|    ent_coef        | 0.000465 |\n",
      "|    ent_coef_loss   | -21.9    |\n",
      "|    learning_rate   | 0.000306 |\n",
      "|    n_updates       | 22920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4696000, episode_reward=2392.25 +/- 1282.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4696000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4696     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7344     |\n",
      "|    total_timesteps | 4696000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4697000, episode_reward=2851.43 +/- 49.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4697000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.699   |\n",
      "|    critic_loss     | 0.000642 |\n",
      "|    ent_coef        | 0.000464 |\n",
      "|    ent_coef_loss   | 3.75     |\n",
      "|    learning_rate   | 0.000304 |\n",
      "|    n_updates       | 22930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4698000, episode_reward=2232.08 +/- 1187.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4698000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4699000, episode_reward=2834.09 +/- 13.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4699000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.676   |\n",
      "|    critic_loss     | 0.000485 |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | 1.42     |\n",
      "|    learning_rate   | 0.000302 |\n",
      "|    n_updates       | 22940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4700000, episode_reward=2824.88 +/- 36.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4700000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4700     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7351     |\n",
      "|    total_timesteps | 4700000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4701000, episode_reward=2695.02 +/- 29.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4701000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.693   |\n",
      "|    critic_loss     | 0.00051  |\n",
      "|    ent_coef        | 0.000463 |\n",
      "|    ent_coef_loss   | 0.575    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4702000, episode_reward=2703.61 +/- 43.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4702000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4703000, episode_reward=2816.06 +/- 43.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4703000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.483   |\n",
      "|    critic_loss     | 0.000347 |\n",
      "|    ent_coef        | 0.000462 |\n",
      "|    ent_coef_loss   | -28.2    |\n",
      "|    learning_rate   | 0.000298 |\n",
      "|    n_updates       | 22960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4704000, episode_reward=2833.08 +/- 20.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4704000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4704     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7357     |\n",
      "|    total_timesteps | 4704000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4705000, episode_reward=2916.07 +/- 41.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4705000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.677   |\n",
      "|    critic_loss     | 0.000575 |\n",
      "|    ent_coef        | 0.00046  |\n",
      "|    ent_coef_loss   | 0.603    |\n",
      "|    learning_rate   | 0.000296 |\n",
      "|    n_updates       | 22970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4706000, episode_reward=2305.00 +/- 1235.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4706000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4707000, episode_reward=2900.62 +/- 45.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4707000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.677   |\n",
      "|    critic_loss     | 0.000553 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.000294 |\n",
      "|    n_updates       | 22980    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4708000, episode_reward=1660.65 +/- 1502.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4708000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4708     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7363     |\n",
      "|    total_timesteps | 4708000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4709000, episode_reward=2919.58 +/- 37.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4709000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.679   |\n",
      "|    critic_loss     | 0.000458 |\n",
      "|    ent_coef        | 0.000458 |\n",
      "|    ent_coef_loss   | 0.818    |\n",
      "|    learning_rate   | 0.000292 |\n",
      "|    n_updates       | 22990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710000, episode_reward=2334.00 +/- 1246.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4710000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4711000, episode_reward=2800.23 +/- 23.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4711000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.683   |\n",
      "|    critic_loss     | 0.000488 |\n",
      "|    ent_coef        | 0.000458 |\n",
      "|    ent_coef_loss   | 2.15     |\n",
      "|    learning_rate   | 0.00029  |\n",
      "|    n_updates       | 23000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4712000, episode_reward=2861.14 +/- 18.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4712000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4712     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7369     |\n",
      "|    total_timesteps | 4712000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4713000, episode_reward=2864.02 +/- 38.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4713000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.676   |\n",
      "|    critic_loss     | 0.000516 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.000288 |\n",
      "|    n_updates       | 23010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4714000, episode_reward=2257.44 +/- 1219.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4714000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4715000, episode_reward=2955.40 +/- 38.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4715000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.585   |\n",
      "|    critic_loss     | 0.000432 |\n",
      "|    ent_coef        | 0.000459 |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.000286 |\n",
      "|    n_updates       | 23020    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4716000, episode_reward=2968.63 +/- 23.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4716000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4716     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7376     |\n",
      "|    total_timesteps | 4716000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4717000, episode_reward=2978.94 +/- 20.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4717000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.59    |\n",
      "|    critic_loss     | 0.000454 |\n",
      "|    ent_coef        | 0.000457 |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.000283 |\n",
      "|    n_updates       | 23030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4718000, episode_reward=3001.07 +/- 21.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4718000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4719000, episode_reward=1655.96 +/- 1509.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4719000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.68    |\n",
      "|    critic_loss     | 0.000514 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | 3.51     |\n",
      "|    learning_rate   | 0.000281 |\n",
      "|    n_updates       | 23040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4720000, episode_reward=2942.61 +/- 17.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4720000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4720     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7382     |\n",
      "|    total_timesteps | 4720000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4721000, episode_reward=2891.66 +/- 24.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4721000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.684   |\n",
      "|    critic_loss     | 0.000487 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | 2.98     |\n",
      "|    learning_rate   | 0.000279 |\n",
      "|    n_updates       | 23050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4722000, episode_reward=2286.61 +/- 1240.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4722000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4723000, episode_reward=2822.50 +/- 14.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4723000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.686   |\n",
      "|    critic_loss     | 0.000422 |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | 2.43     |\n",
      "|    learning_rate   | 0.000277 |\n",
      "|    n_updates       | 23060    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4724000, episode_reward=2260.67 +/- 1216.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4724000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4724     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7389     |\n",
      "|    total_timesteps | 4724000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4725000, episode_reward=2232.32 +/- 1206.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4725000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.468   |\n",
      "|    critic_loss     | 0.00044  |\n",
      "|    ent_coef        | 0.000456 |\n",
      "|    ent_coef_loss   | -27.9    |\n",
      "|    learning_rate   | 0.000275 |\n",
      "|    n_updates       | 23070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4726000, episode_reward=2876.07 +/- 28.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4726000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4727000, episode_reward=2993.12 +/- 33.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4727000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.682   |\n",
      "|    critic_loss     | 0.000549 |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | 2.11     |\n",
      "|    learning_rate   | 0.000273 |\n",
      "|    n_updates       | 23080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4728000, episode_reward=2336.45 +/- 1257.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4728000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4728     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7395     |\n",
      "|    total_timesteps | 4728000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4729000, episode_reward=2948.58 +/- 18.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4729000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.679   |\n",
      "|    critic_loss     | 0.000496 |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | 2.38     |\n",
      "|    learning_rate   | 0.000271 |\n",
      "|    n_updates       | 23090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4730000, episode_reward=2927.27 +/- 34.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4730000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4731000, episode_reward=2286.61 +/- 1227.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4731000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.672   |\n",
      "|    critic_loss     | 0.000483 |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | 0.748    |\n",
      "|    learning_rate   | 0.000269 |\n",
      "|    n_updates       | 23100    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4732000, episode_reward=2898.89 +/- 38.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4732000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4732     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7401     |\n",
      "|    total_timesteps | 4732000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4733000, episode_reward=2934.39 +/- 28.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4733000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.675   |\n",
      "|    critic_loss     | 0.00048  |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | 1.42     |\n",
      "|    learning_rate   | 0.000267 |\n",
      "|    n_updates       | 23110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4734000, episode_reward=2935.45 +/- 40.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4734000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4735000, episode_reward=2966.35 +/- 18.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4735000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.687   |\n",
      "|    critic_loss     | 0.000512 |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | 2.3      |\n",
      "|    learning_rate   | 0.000265 |\n",
      "|    n_updates       | 23120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4736000, episode_reward=2926.04 +/- 25.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4736000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4736     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7408     |\n",
      "|    total_timesteps | 4736000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4737000, episode_reward=2935.43 +/- 35.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4737000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4738000, episode_reward=2986.15 +/- 19.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4738000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.662   |\n",
      "|    critic_loss     | 0.000458 |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | -0.655   |\n",
      "|    learning_rate   | 0.000263 |\n",
      "|    n_updates       | 23130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4739000, episode_reward=2994.02 +/- 39.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4739000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4740000, episode_reward=2929.90 +/- 24.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.681   |\n",
      "|    critic_loss     | 0.000495 |\n",
      "|    ent_coef        | 0.000453 |\n",
      "|    ent_coef_loss   | 2.77     |\n",
      "|    learning_rate   | 0.000261 |\n",
      "|    n_updates       | 23140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4740     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7414     |\n",
      "|    total_timesteps | 4740000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4741000, episode_reward=2945.85 +/- 23.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4741000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4742000, episode_reward=3000.02 +/- 19.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4742000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.677   |\n",
      "|    critic_loss     | 0.000473 |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | 2.93     |\n",
      "|    learning_rate   | 0.000259 |\n",
      "|    n_updates       | 23150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4743000, episode_reward=1728.17 +/- 1551.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4743000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4744000, episode_reward=2980.10 +/- 19.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4744000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.682   |\n",
      "|    critic_loss     | 0.000483 |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | 2.78     |\n",
      "|    learning_rate   | 0.000257 |\n",
      "|    n_updates       | 23160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4744     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7420     |\n",
      "|    total_timesteps | 4744000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4745000, episode_reward=2997.23 +/- 10.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4745000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4746000, episode_reward=3023.91 +/- 55.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4746000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.683   |\n",
      "|    critic_loss     | 0.000506 |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | 4.37     |\n",
      "|    learning_rate   | 0.000255 |\n",
      "|    n_updates       | 23170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4747000, episode_reward=2408.00 +/- 1287.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4747000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4748000, episode_reward=2433.36 +/- 1303.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4748000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.636   |\n",
      "|    critic_loss     | 0.00056  |\n",
      "|    ent_coef        | 0.000455 |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.000253 |\n",
      "|    n_updates       | 23180    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4748     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7426     |\n",
      "|    total_timesteps | 4748000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4749000, episode_reward=3111.51 +/- 21.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4749000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4750000, episode_reward=2388.52 +/- 1299.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.342   |\n",
      "|    critic_loss     | 0.000152 |\n",
      "|    ent_coef        | 0.000454 |\n",
      "|    ent_coef_loss   | -48.8    |\n",
      "|    learning_rate   | 0.000251 |\n",
      "|    n_updates       | 23190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4751000, episode_reward=3097.07 +/- 31.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4751000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4752000, episode_reward=3051.60 +/- 23.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4752000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.681   |\n",
      "|    critic_loss     | 0.00061  |\n",
      "|    ent_coef        | 0.000451 |\n",
      "|    ent_coef_loss   | 4.34     |\n",
      "|    learning_rate   | 0.000249 |\n",
      "|    n_updates       | 23200    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4752     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7433     |\n",
      "|    total_timesteps | 4752000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4753000, episode_reward=3056.53 +/- 23.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4753000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4754000, episode_reward=2927.17 +/- 24.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4754000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.68    |\n",
      "|    critic_loss     | 0.000498 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | 4.08     |\n",
      "|    learning_rate   | 0.000247 |\n",
      "|    n_updates       | 23210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4755000, episode_reward=2298.61 +/- 1257.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4755000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4756000, episode_reward=2893.56 +/- 31.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4756000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.675   |\n",
      "|    critic_loss     | 0.000539 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | 2.09     |\n",
      "|    learning_rate   | 0.000245 |\n",
      "|    n_updates       | 23220    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4756     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7439     |\n",
      "|    total_timesteps | 4756000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4757000, episode_reward=2893.78 +/- 31.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4757000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4758000, episode_reward=2297.30 +/- 1274.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4758000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.664   |\n",
      "|    critic_loss     | 0.000539 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | 1.92     |\n",
      "|    learning_rate   | 0.000242 |\n",
      "|    n_updates       | 23230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4759000, episode_reward=2919.06 +/- 22.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4759000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4760000, episode_reward=2988.38 +/- 42.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.669   |\n",
      "|    critic_loss     | 0.000449 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.00024  |\n",
      "|    n_updates       | 23240    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4760     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7445     |\n",
      "|    total_timesteps | 4760000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4761000, episode_reward=2988.66 +/- 24.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4761000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4762000, episode_reward=3008.22 +/- 21.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4762000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.683   |\n",
      "|    critic_loss     | 0.000518 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | 4.59     |\n",
      "|    learning_rate   | 0.000238 |\n",
      "|    n_updates       | 23250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4763000, episode_reward=2324.00 +/- 1284.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4763000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4764000, episode_reward=2305.81 +/- 1276.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4764000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.669   |\n",
      "|    critic_loss     | 0.00047  |\n",
      "|    ent_coef        | 0.000451 |\n",
      "|    ent_coef_loss   | 1.85     |\n",
      "|    learning_rate   | 0.000236 |\n",
      "|    n_updates       | 23260    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4764     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7452     |\n",
      "|    total_timesteps | 4764000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4765000, episode_reward=2279.83 +/- 1252.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4765000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4766000, episode_reward=1038.62 +/- 1549.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4766000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.528   |\n",
      "|    critic_loss     | 0.000527 |\n",
      "|    ent_coef        | 0.000451 |\n",
      "|    ent_coef_loss   | -16      |\n",
      "|    learning_rate   | 0.000234 |\n",
      "|    n_updates       | 23270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4767000, episode_reward=2965.46 +/- 16.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4767000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4768000, episode_reward=2379.23 +/- 1303.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4768000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.605   |\n",
      "|    critic_loss     | 0.000485 |\n",
      "|    ent_coef        | 0.00045  |\n",
      "|    ent_coef_loss   | -6       |\n",
      "|    learning_rate   | 0.000232 |\n",
      "|    n_updates       | 23280    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4768     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7458     |\n",
      "|    total_timesteps | 4768000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4769000, episode_reward=2357.94 +/- 1293.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4769000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4770000, episode_reward=3012.97 +/- 30.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.676   |\n",
      "|    critic_loss     | 0.000552 |\n",
      "|    ent_coef        | 0.000449 |\n",
      "|    ent_coef_loss   | 4.5      |\n",
      "|    learning_rate   | 0.00023  |\n",
      "|    n_updates       | 23290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4771000, episode_reward=2961.38 +/- 45.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4771000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4772000, episode_reward=2985.81 +/- 32.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4772000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.478   |\n",
      "|    critic_loss     | 0.000421 |\n",
      "|    ent_coef        | 0.000449 |\n",
      "|    ent_coef_loss   | -24.9    |\n",
      "|    learning_rate   | 0.000228 |\n",
      "|    n_updates       | 23300    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4772     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7464     |\n",
      "|    total_timesteps | 4772000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4773000, episode_reward=2307.24 +/- 1261.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4773000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4774000, episode_reward=2971.91 +/- 29.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4774000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.46    |\n",
      "|    critic_loss     | 0.000418 |\n",
      "|    ent_coef        | 0.000447 |\n",
      "|    ent_coef_loss   | -24.7    |\n",
      "|    learning_rate   | 0.000226 |\n",
      "|    n_updates       | 23310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4775000, episode_reward=2959.55 +/- 29.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4775000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4776000, episode_reward=2377.74 +/- 1260.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4776000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.667   |\n",
      "|    critic_loss     | 0.000583 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | 2.83     |\n",
      "|    learning_rate   | 0.000224 |\n",
      "|    n_updates       | 23320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4776     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7471     |\n",
      "|    total_timesteps | 4776000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4777000, episode_reward=3023.47 +/- 54.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4777000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4778000, episode_reward=2448.87 +/- 1311.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4778000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.673   |\n",
      "|    critic_loss     | 0.000543 |\n",
      "|    ent_coef        | 0.000444 |\n",
      "|    ent_coef_loss   | 3.24     |\n",
      "|    learning_rate   | 0.000222 |\n",
      "|    n_updates       | 23330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4779000, episode_reward=2439.67 +/- 1298.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4779000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4780000, episode_reward=2437.18 +/- 1323.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4780000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4780     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7477     |\n",
      "|    total_timesteps | 4780000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4781000, episode_reward=2978.33 +/- 16.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4781000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.672   |\n",
      "|    critic_loss     | 0.000495 |\n",
      "|    ent_coef        | 0.000444 |\n",
      "|    ent_coef_loss   | 3.39     |\n",
      "|    learning_rate   | 0.00022  |\n",
      "|    n_updates       | 23340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4782000, episode_reward=2364.18 +/- 1272.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4782000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4783000, episode_reward=2322.34 +/- 1247.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4783000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.673   |\n",
      "|    critic_loss     | 0.000538 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | 3.53     |\n",
      "|    learning_rate   | 0.000218 |\n",
      "|    n_updates       | 23350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4784000, episode_reward=2954.50 +/- 31.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4784000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4784     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7483     |\n",
      "|    total_timesteps | 4784000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4785000, episode_reward=2336.79 +/- 1271.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4785000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.665   |\n",
      "|    critic_loss     | 0.000556 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | 2.99     |\n",
      "|    learning_rate   | 0.000216 |\n",
      "|    n_updates       | 23360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4786000, episode_reward=2926.02 +/- 35.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4786000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4787000, episode_reward=2316.33 +/- 1239.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4787000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.662   |\n",
      "|    critic_loss     | 0.000489 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | 2.49     |\n",
      "|    learning_rate   | 0.000214 |\n",
      "|    n_updates       | 23370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4788000, episode_reward=2942.34 +/- 11.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4788000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4788     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7490     |\n",
      "|    total_timesteps | 4788000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4789000, episode_reward=3041.78 +/- 19.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4789000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.666   |\n",
      "|    critic_loss     | 0.000513 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | 2.93     |\n",
      "|    learning_rate   | 0.000212 |\n",
      "|    n_updates       | 23380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4790000, episode_reward=2990.64 +/- 23.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4790000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4791000, episode_reward=3019.32 +/- 25.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4791000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.459   |\n",
      "|    critic_loss     | 0.000567 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | -22.2    |\n",
      "|    learning_rate   | 0.00021  |\n",
      "|    n_updates       | 23390    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4792000, episode_reward=3011.17 +/- 45.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4792000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4792     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7496     |\n",
      "|    total_timesteps | 4792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4793000, episode_reward=3048.26 +/- 23.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4793000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.666   |\n",
      "|    critic_loss     | 0.000791 |\n",
      "|    ent_coef        | 0.000445 |\n",
      "|    ent_coef_loss   | 4.39     |\n",
      "|    learning_rate   | 0.000208 |\n",
      "|    n_updates       | 23400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4794000, episode_reward=3059.03 +/- 28.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4794000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4795000, episode_reward=2963.56 +/- 48.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4795000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.588   |\n",
      "|    critic_loss     | 0.000943 |\n",
      "|    ent_coef        | 0.000444 |\n",
      "|    ent_coef_loss   | -3.76    |\n",
      "|    learning_rate   | 0.000206 |\n",
      "|    n_updates       | 23410    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4796000, episode_reward=2980.94 +/- 46.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4796000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4796     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7502     |\n",
      "|    total_timesteps | 4796000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4797000, episode_reward=2931.56 +/- 24.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4797000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.527   |\n",
      "|    critic_loss     | 0.000523 |\n",
      "|    ent_coef        | 0.000444 |\n",
      "|    ent_coef_loss   | -17.7    |\n",
      "|    learning_rate   | 0.000204 |\n",
      "|    n_updates       | 23420    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4798000, episode_reward=2968.11 +/- 33.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4798000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4799000, episode_reward=2892.89 +/- 20.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4799000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.66    |\n",
      "|    critic_loss     | 0.000544 |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | 1.68     |\n",
      "|    learning_rate   | 0.000202 |\n",
      "|    n_updates       | 23430    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800000, episode_reward=2909.93 +/- 14.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4800000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4800     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7509     |\n",
      "|    total_timesteps | 4800000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4801000, episode_reward=2206.02 +/- 1232.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4801000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.649   |\n",
      "|    critic_loss     | 0.000499 |\n",
      "|    ent_coef        | 0.000442 |\n",
      "|    ent_coef_loss   | 2.42     |\n",
      "|    learning_rate   | 0.000199 |\n",
      "|    n_updates       | 23440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4802000, episode_reward=2816.71 +/- 31.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4802000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4803000, episode_reward=2840.32 +/- 46.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4803000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.645   |\n",
      "|    critic_loss     | 0.000485 |\n",
      "|    ent_coef        | 0.000442 |\n",
      "|    ent_coef_loss   | 0.621    |\n",
      "|    learning_rate   | 0.000197 |\n",
      "|    n_updates       | 23450    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4804000, episode_reward=2793.92 +/- 32.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4804000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4804     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7515     |\n",
      "|    total_timesteps | 4804000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4805000, episode_reward=2747.29 +/- 33.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4805000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.652   |\n",
      "|    critic_loss     | 0.000508 |\n",
      "|    ent_coef        | 0.000442 |\n",
      "|    ent_coef_loss   | 1.91     |\n",
      "|    learning_rate   | 0.000195 |\n",
      "|    n_updates       | 23460    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4806000, episode_reward=2183.33 +/- 1175.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4806000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4807000, episode_reward=2868.50 +/- 25.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4807000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.645   |\n",
      "|    critic_loss     | 0.000442 |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | 0.293    |\n",
      "|    learning_rate   | 0.000193 |\n",
      "|    n_updates       | 23470    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4808000, episode_reward=2893.68 +/- 64.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4808000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4808     |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 7522     |\n",
      "|    total_timesteps | 4808000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4809000, episode_reward=2914.49 +/- 26.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4809000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.648   |\n",
      "|    critic_loss     | 0.000496 |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | 2.23     |\n",
      "|    learning_rate   | 0.000191 |\n",
      "|    n_updates       | 23480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4810000, episode_reward=2898.67 +/- 25.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4810000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4811000, episode_reward=2165.05 +/- 1214.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4811000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.642   |\n",
      "|    critic_loss     | 0.000474 |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | 0.639    |\n",
      "|    learning_rate   | 0.000189 |\n",
      "|    n_updates       | 23490    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4812000, episode_reward=2804.47 +/- 21.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4812000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4812     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7530     |\n",
      "|    total_timesteps | 4812000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4813000, episode_reward=2234.16 +/- 1245.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4813000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.647   |\n",
      "|    critic_loss     | 0.000453 |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | 0.81     |\n",
      "|    learning_rate   | 0.000187 |\n",
      "|    n_updates       | 23500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4814000, episode_reward=2203.71 +/- 1232.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4814000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4815000, episode_reward=2909.72 +/- 10.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4815000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.648   |\n",
      "|    critic_loss     | 0.00047  |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | 0.688    |\n",
      "|    learning_rate   | 0.000185 |\n",
      "|    n_updates       | 23510    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4816000, episode_reward=2934.39 +/- 22.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4816000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4816     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7540     |\n",
      "|    total_timesteps | 4816000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4817000, episode_reward=2279.34 +/- 1267.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4817000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.653   |\n",
      "|    critic_loss     | 0.000444 |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.000183 |\n",
      "|    n_updates       | 23520    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4818000, episode_reward=2910.99 +/- 37.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4818000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4819000, episode_reward=2891.39 +/- 16.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4819000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.429   |\n",
      "|    critic_loss     | 0.00119  |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | -8.07    |\n",
      "|    learning_rate   | 0.000181 |\n",
      "|    n_updates       | 23530    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4820000, episode_reward=2914.13 +/- 26.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4820000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4820     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7547     |\n",
      "|    total_timesteps | 4820000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4821000, episode_reward=2968.60 +/- 29.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4821000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.617   |\n",
      "|    critic_loss     | 0.00116  |\n",
      "|    ent_coef        | 0.000443 |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.000179 |\n",
      "|    n_updates       | 23540    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4822000, episode_reward=2976.45 +/- 17.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4822000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4823000, episode_reward=3013.82 +/- 50.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4823000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4824000, episode_reward=3004.01 +/- 45.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4824000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.224   |\n",
      "|    critic_loss     | 0.000189 |\n",
      "|    ent_coef        | 0.000442 |\n",
      "|    ent_coef_loss   | -16.4    |\n",
      "|    learning_rate   | 0.000177 |\n",
      "|    n_updates       | 23550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4824     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7553     |\n",
      "|    total_timesteps | 4824000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4825000, episode_reward=3028.30 +/- 30.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4825000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4826000, episode_reward=2952.12 +/- 47.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4826000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.655   |\n",
      "|    critic_loss     | 0.000842 |\n",
      "|    ent_coef        | 0.000442 |\n",
      "|    ent_coef_loss   | 2.38     |\n",
      "|    learning_rate   | 0.000175 |\n",
      "|    n_updates       | 23560    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4827000, episode_reward=2330.84 +/- 1349.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4827000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4828000, episode_reward=2912.30 +/- 33.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4828000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.638   |\n",
      "|    critic_loss     | 0.000579 |\n",
      "|    ent_coef        | 0.000441 |\n",
      "|    ent_coef_loss   | -0.104   |\n",
      "|    learning_rate   | 0.000173 |\n",
      "|    n_updates       | 23570    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4828     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7560     |\n",
      "|    total_timesteps | 4828000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4829000, episode_reward=2910.37 +/- 16.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4829000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4830000, episode_reward=2802.51 +/- 16.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.646   |\n",
      "|    critic_loss     | 0.000532 |\n",
      "|    ent_coef        | 0.000441 |\n",
      "|    ent_coef_loss   | 0.124    |\n",
      "|    learning_rate   | 0.000171 |\n",
      "|    n_updates       | 23580    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4831000, episode_reward=2814.59 +/- 13.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4831000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4832000, episode_reward=2217.62 +/- 1199.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4832000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.584   |\n",
      "|    critic_loss     | 0.000985 |\n",
      "|    ent_coef        | 0.000441 |\n",
      "|    ent_coef_loss   | -2.55    |\n",
      "|    learning_rate   | 0.000169 |\n",
      "|    n_updates       | 23590    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4832     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7566     |\n",
      "|    total_timesteps | 4832000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4833000, episode_reward=2801.91 +/- 22.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4833000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4834000, episode_reward=2806.22 +/- 33.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4834000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.399   |\n",
      "|    critic_loss     | 0.00123  |\n",
      "|    ent_coef        | 0.000441 |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.000167 |\n",
      "|    n_updates       | 23600    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4835000, episode_reward=2183.27 +/- 1282.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4835000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4836000, episode_reward=2869.30 +/- 38.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4836000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.454   |\n",
      "|    critic_loss     | 0.00123  |\n",
      "|    ent_coef        | 0.00044  |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.000165 |\n",
      "|    n_updates       | 23610    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4836     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7573     |\n",
      "|    total_timesteps | 4836000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4837000, episode_reward=2839.70 +/- 30.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4837000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4838000, episode_reward=2926.10 +/- 46.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4838000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.543   |\n",
      "|    critic_loss     | 0.00117  |\n",
      "|    ent_coef        | 0.00044  |\n",
      "|    ent_coef_loss   | -4.99    |\n",
      "|    learning_rate   | 0.000163 |\n",
      "|    n_updates       | 23620    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4839000, episode_reward=2894.26 +/- 20.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4839000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4840000, episode_reward=2882.00 +/- 46.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.416   |\n",
      "|    critic_loss     | 0.0012   |\n",
      "|    ent_coef        | 0.000439 |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.000161 |\n",
      "|    n_updates       | 23630    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4840     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7579     |\n",
      "|    total_timesteps | 4840000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4841000, episode_reward=2880.35 +/- 39.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4841000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4842000, episode_reward=2828.59 +/- 29.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4842000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.489   |\n",
      "|    critic_loss     | 0.00119  |\n",
      "|    ent_coef        | 0.000438 |\n",
      "|    ent_coef_loss   | -6.19    |\n",
      "|    learning_rate   | 0.000159 |\n",
      "|    n_updates       | 23640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4843000, episode_reward=2845.23 +/- 34.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4843000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4844000, episode_reward=2197.06 +/- 1266.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4844000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.624   |\n",
      "|    critic_loss     | 0.000665 |\n",
      "|    ent_coef        | 0.000438 |\n",
      "|    ent_coef_loss   | -0.12    |\n",
      "|    learning_rate   | 0.000156 |\n",
      "|    n_updates       | 23650    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4844     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7585     |\n",
      "|    total_timesteps | 4844000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4845000, episode_reward=2192.73 +/- 1263.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4845000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4846000, episode_reward=2173.09 +/- 1255.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4846000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.638   |\n",
      "|    critic_loss     | 0.000527 |\n",
      "|    ent_coef        | 0.000438 |\n",
      "|    ent_coef_loss   | 0.655    |\n",
      "|    learning_rate   | 0.000154 |\n",
      "|    n_updates       | 23660    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4847000, episode_reward=2826.57 +/- 23.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4847000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4848000, episode_reward=2694.08 +/- 38.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4848000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.626   |\n",
      "|    critic_loss     | 0.00044  |\n",
      "|    ent_coef        | 0.000438 |\n",
      "|    ent_coef_loss   | -0.579   |\n",
      "|    learning_rate   | 0.000152 |\n",
      "|    n_updates       | 23670    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4848     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7592     |\n",
      "|    total_timesteps | 4848000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4849000, episode_reward=2689.83 +/- 18.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4849000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4850000, episode_reward=2650.78 +/- 59.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.621   |\n",
      "|    critic_loss     | 0.000398 |\n",
      "|    ent_coef        | 0.000438 |\n",
      "|    ent_coef_loss   | -3.22    |\n",
      "|    learning_rate   | 0.00015  |\n",
      "|    n_updates       | 23680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4851000, episode_reward=2652.27 +/- 28.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4851000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4852000, episode_reward=2600.35 +/- 25.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4852000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.612   |\n",
      "|    critic_loss     | 0.000421 |\n",
      "|    ent_coef        | 0.000437 |\n",
      "|    ent_coef_loss   | -3.75    |\n",
      "|    learning_rate   | 0.000148 |\n",
      "|    n_updates       | 23690    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4852     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7598     |\n",
      "|    total_timesteps | 4852000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4853000, episode_reward=2583.60 +/- 29.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4853000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4854000, episode_reward=2598.81 +/- 29.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4854000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.61    |\n",
      "|    critic_loss     | 0.000426 |\n",
      "|    ent_coef        | 0.000437 |\n",
      "|    ent_coef_loss   | -2.68    |\n",
      "|    learning_rate   | 0.000146 |\n",
      "|    n_updates       | 23700    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4855000, episode_reward=2020.47 +/- 1175.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4855000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4856000, episode_reward=2627.22 +/- 26.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4856000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.444   |\n",
      "|    critic_loss     | 0.00109  |\n",
      "|    ent_coef        | 0.000437 |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.000144 |\n",
      "|    n_updates       | 23710    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4856     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7604     |\n",
      "|    total_timesteps | 4856000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4857000, episode_reward=2660.81 +/- 53.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4857000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4858000, episode_reward=2718.82 +/- 27.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4858000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.554   |\n",
      "|    critic_loss     | 0.000916 |\n",
      "|    ent_coef        | 0.000436 |\n",
      "|    ent_coef_loss   | -4.01    |\n",
      "|    learning_rate   | 0.000142 |\n",
      "|    n_updates       | 23720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4859000, episode_reward=2666.20 +/- 29.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4859000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4860000, episode_reward=2783.91 +/- 37.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.63    |\n",
      "|    critic_loss     | 0.000464 |\n",
      "|    ent_coef        | 0.000436 |\n",
      "|    ent_coef_loss   | 0.119    |\n",
      "|    learning_rate   | 0.00014  |\n",
      "|    n_updates       | 23730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4860     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7610     |\n",
      "|    total_timesteps | 4860000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4861000, episode_reward=2797.39 +/- 44.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4861000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4862000, episode_reward=2735.34 +/- 31.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4862000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.632   |\n",
      "|    critic_loss     | 0.00049  |\n",
      "|    ent_coef        | 0.000436 |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.000138 |\n",
      "|    n_updates       | 23740    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4863000, episode_reward=2728.73 +/- 43.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4863000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4864000, episode_reward=2760.60 +/- 30.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4864000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4864     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7617     |\n",
      "|    total_timesteps | 4864000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4865000, episode_reward=2708.32 +/- 35.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4865000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.444   |\n",
      "|    critic_loss     | 0.000383 |\n",
      "|    ent_coef        | 0.000435 |\n",
      "|    ent_coef_loss   | -29      |\n",
      "|    learning_rate   | 0.000136 |\n",
      "|    n_updates       | 23750    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4866000, episode_reward=2691.48 +/- 42.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4866000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4867000, episode_reward=2678.95 +/- 34.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4867000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.602   |\n",
      "|    critic_loss     | 0.00048  |\n",
      "|    ent_coef        | 0.000435 |\n",
      "|    ent_coef_loss   | -1.67    |\n",
      "|    learning_rate   | 0.000134 |\n",
      "|    n_updates       | 23760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4868000, episode_reward=2666.06 +/- 18.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4868000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4868     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7623     |\n",
      "|    total_timesteps | 4868000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4869000, episode_reward=2688.26 +/- 27.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4869000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.61    |\n",
      "|    critic_loss     | 0.000446 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | -2.18    |\n",
      "|    learning_rate   | 0.000132 |\n",
      "|    n_updates       | 23770    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4870000, episode_reward=2074.80 +/- 1197.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4870000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4871000, episode_reward=2677.91 +/- 45.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4871000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.618   |\n",
      "|    critic_loss     | 0.000406 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | -1.18    |\n",
      "|    learning_rate   | 0.00013  |\n",
      "|    n_updates       | 23780    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4872000, episode_reward=2689.23 +/- 38.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4872000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4872     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7629     |\n",
      "|    total_timesteps | 4872000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4873000, episode_reward=2648.16 +/- 28.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4873000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.611   |\n",
      "|    critic_loss     | 0.000412 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | -0.113   |\n",
      "|    learning_rate   | 0.000128 |\n",
      "|    n_updates       | 23790    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4874000, episode_reward=2609.74 +/- 19.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4874000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4875000, episode_reward=2628.80 +/- 31.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4875000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.604   |\n",
      "|    critic_loss     | 0.000373 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | -2.37    |\n",
      "|    learning_rate   | 0.000126 |\n",
      "|    n_updates       | 23800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4876000, episode_reward=2651.36 +/- 30.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4876000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4876     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7636     |\n",
      "|    total_timesteps | 4876000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4877000, episode_reward=2644.73 +/- 36.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4877000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.61    |\n",
      "|    critic_loss     | 0.000409 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -0.424   |\n",
      "|    learning_rate   | 0.000124 |\n",
      "|    n_updates       | 23810    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4878000, episode_reward=2692.01 +/- 48.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4878000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4879000, episode_reward=2683.00 +/- 53.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4879000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.61    |\n",
      "|    critic_loss     | 0.000391 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.121    |\n",
      "|    learning_rate   | 0.000122 |\n",
      "|    n_updates       | 23820    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4880000, episode_reward=2658.60 +/- 14.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4880000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4880     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7642     |\n",
      "|    total_timesteps | 4880000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4881000, episode_reward=2073.40 +/- 1196.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4881000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.615   |\n",
      "|    critic_loss     | 0.000399 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.228    |\n",
      "|    learning_rate   | 0.00012  |\n",
      "|    n_updates       | 23830    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4882000, episode_reward=2715.86 +/- 49.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4882000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4883000, episode_reward=2705.65 +/- 50.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4883000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.613   |\n",
      "|    critic_loss     | 0.000422 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -0.235   |\n",
      "|    learning_rate   | 0.000118 |\n",
      "|    n_updates       | 23840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4884000, episode_reward=2706.28 +/- 25.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4884000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4884     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7648     |\n",
      "|    total_timesteps | 4884000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4885000, episode_reward=2716.80 +/- 50.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4885000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.613   |\n",
      "|    critic_loss     | 0.000396 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.191    |\n",
      "|    learning_rate   | 0.000116 |\n",
      "|    n_updates       | 23850    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4886000, episode_reward=2707.73 +/- 57.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4886000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4887000, episode_reward=2720.50 +/- 24.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4887000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.614   |\n",
      "|    critic_loss     | 0.000404 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.864    |\n",
      "|    learning_rate   | 0.000113 |\n",
      "|    n_updates       | 23860    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4888000, episode_reward=2725.55 +/- 47.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4888000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4888     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7655     |\n",
      "|    total_timesteps | 4888000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4889000, episode_reward=2680.58 +/- 39.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4889000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.402   |\n",
      "|    critic_loss     | 0.00101  |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -16.3    |\n",
      "|    learning_rate   | 0.000111 |\n",
      "|    n_updates       | 23870    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4890000, episode_reward=2697.27 +/- 39.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4890000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4891000, episode_reward=2776.95 +/- 58.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4891000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.6     |\n",
      "|    critic_loss     | 0.000691 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.64     |\n",
      "|    learning_rate   | 0.000109 |\n",
      "|    n_updates       | 23880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4892000, episode_reward=2775.02 +/- 72.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4892000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4892     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7661     |\n",
      "|    total_timesteps | 4892000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4893000, episode_reward=2799.74 +/- 39.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4893000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.62    |\n",
      "|    critic_loss     | 0.00047  |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.919    |\n",
      "|    learning_rate   | 0.000107 |\n",
      "|    n_updates       | 23890    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4894000, episode_reward=2830.48 +/- 46.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4894000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4895000, episode_reward=2741.61 +/- 51.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4895000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.618   |\n",
      "|    critic_loss     | 0.00044  |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.251    |\n",
      "|    learning_rate   | 0.000105 |\n",
      "|    n_updates       | 23900    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4896000, episode_reward=2788.26 +/- 36.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4896000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4896     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7667     |\n",
      "|    total_timesteps | 4896000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4897000, episode_reward=2769.22 +/- 17.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4897000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.61    |\n",
      "|    critic_loss     | 0.000403 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.219    |\n",
      "|    learning_rate   | 0.000103 |\n",
      "|    n_updates       | 23910    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4898000, episode_reward=2761.13 +/- 37.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4898000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4899000, episode_reward=2761.22 +/- 28.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4899000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.605   |\n",
      "|    critic_loss     | 0.000419 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -0.499   |\n",
      "|    learning_rate   | 0.000101 |\n",
      "|    n_updates       | 23920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4900000, episode_reward=2783.63 +/- 35.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4900000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4900     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7673     |\n",
      "|    total_timesteps | 4900000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4901000, episode_reward=2760.53 +/- 32.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4901000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.602   |\n",
      "|    critic_loss     | 0.000426 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 9.91e-05 |\n",
      "|    n_updates       | 23930    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4902000, episode_reward=2779.71 +/- 34.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4902000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4903000, episode_reward=2767.82 +/- 53.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4903000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.615   |\n",
      "|    critic_loss     | 0.000385 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.723    |\n",
      "|    learning_rate   | 9.71e-05 |\n",
      "|    n_updates       | 23940    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4904000, episode_reward=2769.80 +/- 36.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4904000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4904     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7680     |\n",
      "|    total_timesteps | 4904000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4905000, episode_reward=2753.02 +/- 33.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4905000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.599   |\n",
      "|    critic_loss     | 0.000419 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -0.203   |\n",
      "|    learning_rate   | 9.5e-05  |\n",
      "|    n_updates       | 23950    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4906000, episode_reward=2783.73 +/- 49.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4906000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4907000, episode_reward=2715.85 +/- 22.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4907000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4908000, episode_reward=2744.93 +/- 44.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4908000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.602   |\n",
      "|    critic_loss     | 0.000374 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -0.909   |\n",
      "|    learning_rate   | 9.3e-05  |\n",
      "|    n_updates       | 23960    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4908     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7686     |\n",
      "|    total_timesteps | 4908000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4909000, episode_reward=2722.73 +/- 41.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4909000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4910000, episode_reward=2697.82 +/- 39.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.596   |\n",
      "|    critic_loss     | 0.000398 |\n",
      "|    ent_coef        | 0.000432 |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 9.09e-05 |\n",
      "|    n_updates       | 23970    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4911000, episode_reward=2696.33 +/- 52.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4911000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4912000, episode_reward=2723.71 +/- 33.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4912000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.611   |\n",
      "|    critic_loss     | 0.000405 |\n",
      "|    ent_coef        | 0.000432 |\n",
      "|    ent_coef_loss   | 0.0823   |\n",
      "|    learning_rate   | 8.89e-05 |\n",
      "|    n_updates       | 23980    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4912     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7692     |\n",
      "|    total_timesteps | 4912000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4913000, episode_reward=2760.89 +/- 48.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4913000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4914000, episode_reward=2753.73 +/- 48.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4914000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.603   |\n",
      "|    critic_loss     | 0.000432 |\n",
      "|    ent_coef        | 0.000432 |\n",
      "|    ent_coef_loss   | 0.694    |\n",
      "|    learning_rate   | 8.68e-05 |\n",
      "|    n_updates       | 23990    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4915000, episode_reward=2765.85 +/- 27.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4915000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4916000, episode_reward=2141.49 +/- 1219.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4916000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.607   |\n",
      "|    critic_loss     | 0.000392 |\n",
      "|    ent_coef        | 0.000432 |\n",
      "|    ent_coef_loss   | 1.13     |\n",
      "|    learning_rate   | 8.48e-05 |\n",
      "|    n_updates       | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4916     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7699     |\n",
      "|    total_timesteps | 4916000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4917000, episode_reward=2787.22 +/- 44.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4917000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4918000, episode_reward=2795.87 +/- 32.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4918000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.611   |\n",
      "|    critic_loss     | 0.000406 |\n",
      "|    ent_coef        | 0.000432 |\n",
      "|    ent_coef_loss   | 0.733    |\n",
      "|    learning_rate   | 8.28e-05 |\n",
      "|    n_updates       | 24010    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4919000, episode_reward=2825.86 +/- 41.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4919000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4920000, episode_reward=2737.45 +/- 40.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.599   |\n",
      "|    critic_loss     | 0.000437 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 8.07e-05 |\n",
      "|    n_updates       | 24020    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4920     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7705     |\n",
      "|    total_timesteps | 4920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4921000, episode_reward=2778.01 +/- 10.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4921000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4922000, episode_reward=2778.34 +/- 30.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4922000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.604   |\n",
      "|    critic_loss     | 0.000395 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.667    |\n",
      "|    learning_rate   | 7.87e-05 |\n",
      "|    n_updates       | 24030    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4923000, episode_reward=2808.64 +/- 52.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4923000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4924000, episode_reward=2821.91 +/- 41.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4924000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.607   |\n",
      "|    critic_loss     | 0.000409 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.62     |\n",
      "|    learning_rate   | 7.66e-05 |\n",
      "|    n_updates       | 24040    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4924     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7711     |\n",
      "|    total_timesteps | 4924000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4925000, episode_reward=2781.82 +/- 19.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4925000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4926000, episode_reward=2840.58 +/- 60.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4926000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.607   |\n",
      "|    critic_loss     | 0.000392 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 7.46e-05 |\n",
      "|    n_updates       | 24050    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4927000, episode_reward=2809.63 +/- 47.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4927000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4928000, episode_reward=2783.65 +/- 40.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4928000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.61    |\n",
      "|    critic_loss     | 0.000413 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.44     |\n",
      "|    learning_rate   | 7.25e-05 |\n",
      "|    n_updates       | 24060    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4928     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7718     |\n",
      "|    total_timesteps | 4928000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4929000, episode_reward=2797.04 +/- 23.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4929000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4930000, episode_reward=2832.88 +/- 36.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.608   |\n",
      "|    critic_loss     | 0.000416 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 7.05e-05 |\n",
      "|    n_updates       | 24070    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4931000, episode_reward=2762.87 +/- 52.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4931000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4932000, episode_reward=2853.70 +/- 30.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4932000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.605   |\n",
      "|    critic_loss     | 0.000412 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 6.84e-05 |\n",
      "|    n_updates       | 24080    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4932     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7724     |\n",
      "|    total_timesteps | 4932000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4933000, episode_reward=2838.77 +/- 32.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4933000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4934000, episode_reward=2863.53 +/- 33.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4934000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.6     |\n",
      "|    critic_loss     | 0.000387 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.468    |\n",
      "|    learning_rate   | 6.64e-05 |\n",
      "|    n_updates       | 24090    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4935000, episode_reward=2856.33 +/- 24.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4935000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4936000, episode_reward=2910.79 +/- 35.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4936000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.603   |\n",
      "|    critic_loss     | 0.000426 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.991    |\n",
      "|    learning_rate   | 6.43e-05 |\n",
      "|    n_updates       | 24100    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4936     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7730     |\n",
      "|    total_timesteps | 4936000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4937000, episode_reward=2877.04 +/- 14.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4937000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4938000, episode_reward=2908.01 +/- 44.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4938000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.606   |\n",
      "|    critic_loss     | 0.000409 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.25     |\n",
      "|    learning_rate   | 6.23e-05 |\n",
      "|    n_updates       | 24110    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4939000, episode_reward=2899.17 +/- 30.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4939000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4940000, episode_reward=2860.48 +/- 33.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.611   |\n",
      "|    critic_loss     | 0.000406 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.96     |\n",
      "|    learning_rate   | 6.02e-05 |\n",
      "|    n_updates       | 24120    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4940     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7736     |\n",
      "|    total_timesteps | 4940000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4941000, episode_reward=2869.16 +/- 41.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4941000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4942000, episode_reward=2898.71 +/- 36.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4942000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.612   |\n",
      "|    critic_loss     | 0.000423 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 2.31     |\n",
      "|    learning_rate   | 5.82e-05 |\n",
      "|    n_updates       | 24130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4943000, episode_reward=2875.51 +/- 43.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4943000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4944000, episode_reward=2857.63 +/- 56.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4944000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.606   |\n",
      "|    critic_loss     | 0.000402 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 2.32     |\n",
      "|    learning_rate   | 5.61e-05 |\n",
      "|    n_updates       | 24140    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4944     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7743     |\n",
      "|    total_timesteps | 4944000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4945000, episode_reward=2875.90 +/- 48.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4945000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4946000, episode_reward=2873.43 +/- 46.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4946000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.62    |\n",
      "|    critic_loss     | 0.000438 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 3.44     |\n",
      "|    learning_rate   | 5.41e-05 |\n",
      "|    n_updates       | 24150    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4947000, episode_reward=2868.71 +/- 35.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4947000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4948000, episode_reward=2883.25 +/- 27.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4948000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.611   |\n",
      "|    critic_loss     | 0.000451 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 3.22     |\n",
      "|    learning_rate   | 5.2e-05  |\n",
      "|    n_updates       | 24160    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4948     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7749     |\n",
      "|    total_timesteps | 4948000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4949000, episode_reward=2244.06 +/- 1267.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4949000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4950000, episode_reward=2878.53 +/- 52.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4950000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4951000, episode_reward=2887.00 +/- 28.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4951000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.605   |\n",
      "|    critic_loss     | 0.00045  |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 2.73     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 24170    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4952000, episode_reward=2903.28 +/- 42.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4952000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4952     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7755     |\n",
      "|    total_timesteps | 4952000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4953000, episode_reward=2915.72 +/- 25.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4953000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.603   |\n",
      "|    critic_loss     | 0.00047  |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.96     |\n",
      "|    learning_rate   | 4.79e-05 |\n",
      "|    n_updates       | 24180    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4954000, episode_reward=2906.47 +/- 56.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4954000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4955000, episode_reward=2865.08 +/- 52.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4955000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.601   |\n",
      "|    critic_loss     | 0.000444 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 4.59e-05 |\n",
      "|    n_updates       | 24190    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4956000, episode_reward=2889.97 +/- 55.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4956000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4956     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7762     |\n",
      "|    total_timesteps | 4956000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4957000, episode_reward=2905.30 +/- 29.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4957000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.606   |\n",
      "|    critic_loss     | 0.000446 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | 2.13     |\n",
      "|    learning_rate   | 4.38e-05 |\n",
      "|    n_updates       | 24200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4958000, episode_reward=2853.19 +/- 53.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4958000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4959000, episode_reward=2880.76 +/- 38.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4959000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.607   |\n",
      "|    critic_loss     | 0.00043  |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | 2.22     |\n",
      "|    learning_rate   | 4.18e-05 |\n",
      "|    n_updates       | 24210    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4960000, episode_reward=2894.61 +/- 33.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4960000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4960     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7768     |\n",
      "|    total_timesteps | 4960000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4961000, episode_reward=2861.91 +/- 54.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4961000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.601   |\n",
      "|    critic_loss     | 0.000412 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | 0.729    |\n",
      "|    learning_rate   | 3.97e-05 |\n",
      "|    n_updates       | 24220    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4962000, episode_reward=2893.34 +/- 65.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4962000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4963000, episode_reward=2901.37 +/- 79.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4963000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.435   |\n",
      "|    critic_loss     | 0.000602 |\n",
      "|    ent_coef        | 0.000434 |\n",
      "|    ent_coef_loss   | -24      |\n",
      "|    learning_rate   | 3.77e-05 |\n",
      "|    n_updates       | 24230    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4964000, episode_reward=2871.30 +/- 67.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4964000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4964     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7774     |\n",
      "|    total_timesteps | 4964000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4965000, episode_reward=2960.43 +/- 27.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4965000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.605   |\n",
      "|    critic_loss     | 0.000434 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.544    |\n",
      "|    learning_rate   | 3.56e-05 |\n",
      "|    n_updates       | 24240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4966000, episode_reward=2898.50 +/- 44.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4966000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4967000, episode_reward=2922.73 +/- 43.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4967000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.607   |\n",
      "|    critic_loss     | 0.000439 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 3.36e-05 |\n",
      "|    n_updates       | 24250    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4968000, episode_reward=2923.57 +/- 9.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4968000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4968     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7780     |\n",
      "|    total_timesteps | 4968000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4969000, episode_reward=2973.58 +/- 67.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4969000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.601   |\n",
      "|    critic_loss     | 0.00049  |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 2.54     |\n",
      "|    learning_rate   | 3.16e-05 |\n",
      "|    n_updates       | 24260    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4970000, episode_reward=2943.06 +/- 39.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4970000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4971000, episode_reward=2886.21 +/- 24.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4971000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.605   |\n",
      "|    critic_loss     | 0.000412 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.908    |\n",
      "|    learning_rate   | 2.95e-05 |\n",
      "|    n_updates       | 24270    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4972000, episode_reward=2943.18 +/- 57.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4972000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4972     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7787     |\n",
      "|    total_timesteps | 4972000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4973000, episode_reward=2933.80 +/- 36.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4973000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.606   |\n",
      "|    critic_loss     | 0.000437 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 2.84     |\n",
      "|    learning_rate   | 2.75e-05 |\n",
      "|    n_updates       | 24280    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4974000, episode_reward=2958.24 +/- 16.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4974000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4975000, episode_reward=2966.42 +/- 78.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4975000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.603   |\n",
      "|    critic_loss     | 0.00045  |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 2.74     |\n",
      "|    learning_rate   | 2.54e-05 |\n",
      "|    n_updates       | 24290    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4976000, episode_reward=2920.62 +/- 34.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4976000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4976     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7793     |\n",
      "|    total_timesteps | 4976000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4977000, episode_reward=3043.51 +/- 69.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4977000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.604   |\n",
      "|    critic_loss     | 0.000419 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.69     |\n",
      "|    learning_rate   | 2.34e-05 |\n",
      "|    n_updates       | 24300    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4978000, episode_reward=2950.68 +/- 47.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4978000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4979000, episode_reward=2951.91 +/- 32.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4979000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.605   |\n",
      "|    critic_loss     | 0.000447 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 2.13e-05 |\n",
      "|    n_updates       | 24310    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4980000, episode_reward=2970.53 +/- 85.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4980000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4980     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7799     |\n",
      "|    total_timesteps | 4980000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4981000, episode_reward=2921.95 +/- 53.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4981000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.6     |\n",
      "|    critic_loss     | 0.000414 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.68     |\n",
      "|    learning_rate   | 1.93e-05 |\n",
      "|    n_updates       | 24320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4982000, episode_reward=2895.21 +/- 57.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4982000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4983000, episode_reward=2927.06 +/- 49.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4983000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.403   |\n",
      "|    critic_loss     | 0.000894 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | -19.4    |\n",
      "|    learning_rate   | 1.72e-05 |\n",
      "|    n_updates       | 24330    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4984000, episode_reward=2959.56 +/- 65.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4984000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4984     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7806     |\n",
      "|    total_timesteps | 4984000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4985000, episode_reward=2949.44 +/- 22.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4985000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.606   |\n",
      "|    critic_loss     | 0.000459 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 3.3      |\n",
      "|    learning_rate   | 1.52e-05 |\n",
      "|    n_updates       | 24340    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4986000, episode_reward=2941.57 +/- 32.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4986000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4987000, episode_reward=2901.05 +/- 51.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4987000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.595   |\n",
      "|    critic_loss     | 0.000441 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 1.31e-05 |\n",
      "|    n_updates       | 24350    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4988000, episode_reward=2970.88 +/- 47.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4988000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4988     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7812     |\n",
      "|    total_timesteps | 4988000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4989000, episode_reward=2928.33 +/- 20.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4989000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.592   |\n",
      "|    critic_loss     | 0.000443 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 0.877    |\n",
      "|    learning_rate   | 1.11e-05 |\n",
      "|    n_updates       | 24360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4990000, episode_reward=2972.28 +/- 69.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4990000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4991000, episode_reward=2926.73 +/- 31.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4991000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.601   |\n",
      "|    critic_loss     | 0.000446 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.8      |\n",
      "|    learning_rate   | 9.02e-06 |\n",
      "|    n_updates       | 24370    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4992000, episode_reward=3011.13 +/- 59.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4992000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4992     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7818     |\n",
      "|    total_timesteps | 4992000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4993000, episode_reward=3008.38 +/- 58.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4993000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4994000, episode_reward=2929.01 +/- 52.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4994000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.599   |\n",
      "|    critic_loss     | 0.00041  |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 2.27     |\n",
      "|    learning_rate   | 6.98e-06 |\n",
      "|    n_updates       | 24380    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4995000, episode_reward=2944.43 +/- 62.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4995000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4996000, episode_reward=2898.32 +/- 77.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4996000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.608   |\n",
      "|    critic_loss     | 0.000449 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 3.09     |\n",
      "|    learning_rate   | 4.93e-06 |\n",
      "|    n_updates       | 24390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4996     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7824     |\n",
      "|    total_timesteps | 4996000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4997000, episode_reward=2919.70 +/- 67.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4997000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4998000, episode_reward=2997.05 +/- 80.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4998000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.597   |\n",
      "|    critic_loss     | 0.000409 |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 2.88e-06 |\n",
      "|    n_updates       | 24400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4999000, episode_reward=2978.96 +/- 74.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4999000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000000, episode_reward=2970.86 +/- 72.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.599   |\n",
      "|    critic_loss     | 0.00045  |\n",
      "|    ent_coef        | 0.000433 |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 8.32e-07 |\n",
      "|    n_updates       | 24410    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5000     |\n",
      "|    fps             | 638      |\n",
      "|    time_elapsed    | 7831     |\n",
      "|    total_timesteps | 5000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5001000, episode_reward=2935.33 +/- 18.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5001000  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_id = 'HalfCheetah-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "env = VecNormalize(env)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = VecNormalize(eval_env)\n",
    "\n",
    "mdpo_model = SAC(\"MlpPolicy\",\n",
    "                  env,\n",
    "                  buffer_size=2048,\n",
    "                  train_freq=2048,\n",
    "                  verbose=1,\n",
    "                  learning_rate=linear_schedule(5e-3))\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='logs/{}-mdpo/'.format(env_id),\n",
    "                             log_path='logs/{}-mdpo/'.format(env_id), eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "mdpo_model.learn(total_timesteps=5e6, callback=eval_callback)\n",
    "\n",
    "results = np.array(eval_callback.evaluations_results)\n",
    "mdpo_mean_reward = np.mean(results, axis=1)\n",
    "mdpo_std_reward = np.std(results, axis=1)\n",
    "np.save(\"{}-mdpo-mean-redundent.npy\".format(env_id), mdpo_mean_reward)\n",
    "np.save(\"{}-mdpo-std-redundent.npy\".format(env_id), mdpo_std_reward)\n",
    "\n",
    "plot_costs([mdpo_mean_reward], names=['MDPO'], smoothing_window=50, n=1, fig_name=env_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d2e891c9d48cbc5657a17ab4ab08b2c1d2ec0060cc3e51694592beb2a6aa825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
